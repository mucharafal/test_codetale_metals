[{
  "comments": [{
    "author": {
      "login": "chouqin"
    },
    "body": "Can you use a while loop to do this?\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-21T07:22:53Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],\n+  var cur: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // To keep track of the past checkpointed RDDs.\n+  val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  var rddUpdateCount = 0\n+  if (checkpointDir != None) {\n+    cur.sparkContext.setCheckpointDir(checkpointDir.get)\n+  }\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.\n+   * @param nodeIdUpdaters A map of node index updaters.\n+   *                       The key is the indices of nodes that we want to update.\n+   * @param bins Bin information needed to find child node indices.\n+   */\n+  def updateNodeIndices(\n+      nodeIdUpdaters: Array[Map[Int, NodeIndexUpdater]],\n+      bins: Array[Array[Bin]]): Unit = {\n+    val updatedRDD = data.zip(cur).map {\n+      dataPoint => {\n+        cfor(0)(_ < nodeIdUpdaters.length, _ + 1)("
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "I second that, especially if it eliminates the dependence on spire (since spire is not used elsewhere in Spark).\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-21T19:43:07Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],\n+  var cur: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // To keep track of the past checkpointed RDDs.\n+  val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  var rddUpdateCount = 0\n+  if (checkpointDir != None) {\n+    cur.sparkContext.setCheckpointDir(checkpointDir.get)\n+  }\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.\n+   * @param nodeIdUpdaters A map of node index updaters.\n+   *                       The key is the indices of nodes that we want to update.\n+   * @param bins Bin information needed to find child node indices.\n+   */\n+  def updateNodeIndices(\n+      nodeIdUpdaters: Array[Map[Int, NodeIndexUpdater]],\n+      bins: Array[Array[Bin]]): Unit = {\n+    val updatedRDD = data.zip(cur).map {\n+      dataPoint => {\n+        cfor(0)(_ < nodeIdUpdaters.length, _ + 1)("
  }, {
    "author": {
      "login": "codedeft"
    },
    "body": "Will do. I used it because spire was included somehow (maybe one of the dependent packages use it).\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-22T05:51:35Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],\n+  var cur: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // To keep track of the past checkpointed RDDs.\n+  val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  var rddUpdateCount = 0\n+  if (checkpointDir != None) {\n+    cur.sparkContext.setCheckpointDir(checkpointDir.get)\n+  }\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.\n+   * @param nodeIdUpdaters A map of node index updaters.\n+   *                       The key is the indices of nodes that we want to update.\n+   * @param bins Bin information needed to find child node indices.\n+   */\n+  def updateNodeIndices(\n+      nodeIdUpdaters: Array[Map[Int, NodeIndexUpdater]],\n+      bins: Array[Array[Bin]]): Unit = {\n+    val updatedRDD = data.zip(cur).map {\n+      dataPoint => {\n+        cfor(0)(_ < nodeIdUpdaters.length, _ + 1)("
  }],
  "prId": 2868
}, {
  "comments": [{
    "author": {
      "login": "chouqin"
    },
    "body": "Just a suggestion, we can remove this `data` field from this class because it is only used for  the `updateNodeIndices` function. The name `NodeIdCache` will make more sense in this way.\nWe can call `updateNodeIndices` with a `RDD[BaggedPoint[TreePoint]]` explicitly.\n\nBy the way, can you give a meaningful name to `cur`( such as `nodeIdsForInstances`)?\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-21T07:33:02Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "When you rename `cur` to `SOMENAME`, it would be great to rename `updatedRDD` as well to `updatedSOMENAME`\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-21T20:05:18Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],"
  }, {
    "author": {
      "login": "codedeft"
    },
    "body": "Will do.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-22T05:52:59Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],"
  }],
  "prId": 2868
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Currently, this skips checkpointing if checkpointDir == None.  However, a user could set the SparkContext checkpointDir before calling DecisionTree.  Can the behavior be changed as follows:\n- If a checkpointDir is given here, then it should overwrite any preset checkpointDir in SparkContext.\n- If no checkpointDir is given, then the code should check the SparkContext (via cur.sparkContext.getCheckpointDir) to see if one has already been set.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-21T19:53:51Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where"
  }, {
    "author": {
      "login": "codedeft"
    },
    "body": "Will do.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-22T05:56:29Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where"
  }],
  "prId": 2868
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "It would be cheaper (and maybe easier) to pass an `Array[Map[Int, Split]]` (replacing NodeIndexUpdater with Split).  You already have that info when you construct nodeIdUpdaters in DecisionTree.scala, and you don't have to explicitly store the left/right child IDs.  You can compute child IDs from the parent ID using Node.leftChildIndex (and Node.rightChildIndex).  Hopefully that simplifies the code some.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-21T20:00:34Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],\n+  var cur: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // To keep track of the past checkpointed RDDs.\n+  val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  var rddUpdateCount = 0\n+  if (checkpointDir != None) {\n+    cur.sparkContext.setCheckpointDir(checkpointDir.get)\n+  }\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.",
    "line": 103
  }, {
    "author": {
      "login": "codedeft"
    },
    "body": "Yes, I noticed that you have a rule for determining the node indices through bit shifts. However, I was wondering if this is something that could potentially change in the future, and maybe leave that logic outside.\n\nE.g. this seems to be a primary reason that 30+ level trees can't be trained at the moment and you might want to use a different logic in the future.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-22T05:50:40Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],\n+  var cur: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // To keep track of the past checkpointed RDDs.\n+  val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  var rddUpdateCount = 0\n+  if (checkpointDir != None) {\n+    cur.sparkContext.setCheckpointDir(checkpointDir.get)\n+  }\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.",
    "line": 103
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "True, since Node indices are Integers (not Longs), 30+ level trees are a problem.  We could definitely switch to Long at some point---or even eliminate indices, though that might require extra work in some places.  I tried to keep the node indexing logic grouped within Node.scala at least.\n\nMy opinion is that the current indexing system is reasonable; if we really support 62+ depth trees, then moving to larger integer types seems OK.  It would be a bit more storage but not that much relative to node size.  Also, at that depth, individual trees would likely need to be distributed anyways (making storage less of an issue) (unless the tree were extremely unbalanced).  What do you think?\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-22T22:01:44Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],\n+  var cur: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // To keep track of the past checkpointed RDDs.\n+  val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  var rddUpdateCount = 0\n+  if (checkpointDir != None) {\n+    cur.sparkContext.setCheckpointDir(checkpointDir.get)\n+  }\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.",
    "line": 103
  }, {
    "author": {
      "login": "codedeft"
    },
    "body": "I think that this is fine as long as you get relatively balanced trees, since the tree would be extremely big by the time you reach level 30.\n\nHowever, based on my experience, the problem in practice is that you often get unbalanced trees. E.g., when I train on 60,000 sample mnist without pruning, I often got a tree with close to ~100 level deep, even though the number of nodes was only around 5000 or so.\n\nI will do it the way you suggested by simply calling Node.leftChildId, Node.rightChildId.\n\nFor future, though, I think that a relatively easy way to do this might be by assigning Ids in a first-come basIs (keepLastAssignedId and +1 for every child node that comes in). It will likely lead to a bit more complicated code, but I think that it'll be easier than removing node Ids entirely.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-22T22:41:44Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],\n+  var cur: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // To keep track of the past checkpointed RDDs.\n+  val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  var rddUpdateCount = 0\n+  if (checkpointDir != None) {\n+    cur.sparkContext.setCheckpointDir(checkpointDir.get)\n+  }\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.",
    "line": 103
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "I agree it could get deep quickly, and switching indexing systems seems good for a future PR.  (W.r.t. training without pruning, I suspect it would be better and more efficient to use outright nearest neighbor since completely unpruned trees are basically doing nearest neighbor.)\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-23T19:12:04Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],\n+  var cur: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // To keep track of the past checkpointed RDDs.\n+  val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  var rddUpdateCount = 0\n+  if (checkpointDir != None) {\n+    cur.sparkContext.setCheckpointDir(checkpointDir.get)\n+  }\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.",
    "line": 103
  }, {
    "author": {
      "login": "codedeft"
    },
    "body": "I think that one good benefit of Decision Tree over NearestNeighbor, even in unpruned state, is the compactness of representation. It's still a lot less storage for models of decision trees, compared to having to store entire datasets.\n\nBut yea, it still suffers like a lot of other non-parametric algorithms (e.g. SVM, etc.) whose models grow proportionally to dataset sizes, especially if unpruned.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-23T20:45:22Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],\n+  var cur: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // To keep track of the past checkpointed RDDs.\n+  val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  var rddUpdateCount = 0\n+  if (checkpointDir != None) {\n+    cur.sparkContext.setCheckpointDir(checkpointDir.get)\n+  }\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.",
    "line": 103
  }],
  "prId": 2868
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "To remove a checkpoint, you'll need to remove the checkpoint file manually.  You should be able to do something like:\n\n```\nval fs = new Path(checkpointDir).getFileSystem(sparkContext.hadoopConfiguration)\nfs.delete(checkpointFile)\n```\n\nafter the next checkpoint has finished.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-21T20:26:01Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],\n+  var cur: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // To keep track of the past checkpointed RDDs.\n+  val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  var rddUpdateCount = 0\n+  if (checkpointDir != None) {\n+    cur.sparkContext.setCheckpointDir(checkpointDir.get)\n+  }\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.\n+   * @param nodeIdUpdaters A map of node index updaters.\n+   *                       The key is the indices of nodes that we want to update.\n+   * @param bins Bin information needed to find child node indices.\n+   */\n+  def updateNodeIndices(\n+      nodeIdUpdaters: Array[Map[Int, NodeIndexUpdater]],\n+      bins: Array[Array[Bin]]): Unit = {\n+    val updatedRDD = data.zip(cur).map {\n+      dataPoint => {\n+        cfor(0)(_ < nodeIdUpdaters.length, _ + 1)(\n+          treeId => {\n+            val nodeIdUpdater = nodeIdUpdaters(treeId).getOrElse(dataPoint._2(treeId), null)\n+            if (nodeIdUpdater != null) {\n+              val newNodeIndex = nodeIdUpdater.updateNodeIndex(\n+                binnedFeatures = dataPoint._1.datum.binnedFeatures,\n+                bins = bins)\n+              dataPoint._2(treeId) = newNodeIndex\n+            }\n+          }\n+        )\n+\n+        dataPoint._2\n+      }\n+    }\n+\n+    cur = updatedRDD\n+    rddUpdateCount += 1\n+\n+    // Handle checkpointing if the directory is not None.\n+    if (checkpointDir != None && (rddUpdateCount % checkpointInterval) == 0) {\n+      // Let's see if we can unpersist previous checkpoints.\n+      var canUnpersist = true\n+      while (checkpointQueue.size > 1 && canUnpersist) {\n+        // We can unpersist the oldest checkpoint iff\n+        // the next checkpoint actually exists in the file system.\n+        if (checkpointQueue.get(1).get.getCheckpointFile != None) {\n+          val old = checkpointQueue.dequeue()\n+          old.unpersist()"
  }, {
    "author": {
      "login": "codedeft"
    },
    "body": "I could do this. But then do you really want to manually delete checkpoints here? I was wondering if you guys had any plan on managing this implicitly underneath, since that seems like a more logical place.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-22T05:54:11Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],\n+  var cur: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // To keep track of the past checkpointed RDDs.\n+  val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  var rddUpdateCount = 0\n+  if (checkpointDir != None) {\n+    cur.sparkContext.setCheckpointDir(checkpointDir.get)\n+  }\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.\n+   * @param nodeIdUpdaters A map of node index updaters.\n+   *                       The key is the indices of nodes that we want to update.\n+   * @param bins Bin information needed to find child node indices.\n+   */\n+  def updateNodeIndices(\n+      nodeIdUpdaters: Array[Map[Int, NodeIndexUpdater]],\n+      bins: Array[Array[Bin]]): Unit = {\n+    val updatedRDD = data.zip(cur).map {\n+      dataPoint => {\n+        cfor(0)(_ < nodeIdUpdaters.length, _ + 1)(\n+          treeId => {\n+            val nodeIdUpdater = nodeIdUpdaters(treeId).getOrElse(dataPoint._2(treeId), null)\n+            if (nodeIdUpdater != null) {\n+              val newNodeIndex = nodeIdUpdater.updateNodeIndex(\n+                binnedFeatures = dataPoint._1.datum.binnedFeatures,\n+                bins = bins)\n+              dataPoint._2(treeId) = newNodeIndex\n+            }\n+          }\n+        )\n+\n+        dataPoint._2\n+      }\n+    }\n+\n+    cur = updatedRDD\n+    rddUpdateCount += 1\n+\n+    // Handle checkpointing if the directory is not None.\n+    if (checkpointDir != None && (rddUpdateCount % checkpointInterval) == 0) {\n+      // Let's see if we can unpersist previous checkpoints.\n+      var canUnpersist = true\n+      while (checkpointQueue.size > 1 && canUnpersist) {\n+        // We can unpersist the oldest checkpoint iff\n+        // the next checkpoint actually exists in the file system.\n+        if (checkpointQueue.get(1).get.getCheckpointFile != None) {\n+          val old = checkpointQueue.dequeue()\n+          old.unpersist()"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "@mengxr Do you know of such plans?\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-22T22:02:16Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],\n+  var cur: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // To keep track of the past checkpointed RDDs.\n+  val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  var rddUpdateCount = 0\n+  if (checkpointDir != None) {\n+    cur.sparkContext.setCheckpointDir(checkpointDir.get)\n+  }\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.\n+   * @param nodeIdUpdaters A map of node index updaters.\n+   *                       The key is the indices of nodes that we want to update.\n+   * @param bins Bin information needed to find child node indices.\n+   */\n+  def updateNodeIndices(\n+      nodeIdUpdaters: Array[Map[Int, NodeIndexUpdater]],\n+      bins: Array[Array[Bin]]): Unit = {\n+    val updatedRDD = data.zip(cur).map {\n+      dataPoint => {\n+        cfor(0)(_ < nodeIdUpdaters.length, _ + 1)(\n+          treeId => {\n+            val nodeIdUpdater = nodeIdUpdaters(treeId).getOrElse(dataPoint._2(treeId), null)\n+            if (nodeIdUpdater != null) {\n+              val newNodeIndex = nodeIdUpdater.updateNodeIndex(\n+                binnedFeatures = dataPoint._1.datum.binnedFeatures,\n+                bins = bins)\n+              dataPoint._2(treeId) = newNodeIndex\n+            }\n+          }\n+        )\n+\n+        dataPoint._2\n+      }\n+    }\n+\n+    cur = updatedRDD\n+    rddUpdateCount += 1\n+\n+    // Handle checkpointing if the directory is not None.\n+    if (checkpointDir != None && (rddUpdateCount % checkpointInterval) == 0) {\n+      // Let's see if we can unpersist previous checkpoints.\n+      var canUnpersist = true\n+      while (checkpointQueue.size > 1 && canUnpersist) {\n+        // We can unpersist the oldest checkpoint iff\n+        // the next checkpoint actually exists in the file system.\n+        if (checkpointQueue.get(1).get.getCheckpointFile != None) {\n+          val old = checkpointQueue.dequeue()\n+          old.unpersist()"
  }],
  "prId": 2868
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "\"inital\" ---> \"initial\"\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-21T20:30:35Z",
    "diffHunk": "@@ -0,0 +1,171 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import spire.implicits._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param leftChildIndex Left child index.\n+ * @param rightChildIndex Right child index.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    leftChildIndex: Int,\n+    rightChildIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        leftChildIndex\n+      } else {\n+        rightChildIndex\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param data The RDD of training rows.\n+ * @param cur The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  val data: RDD[BaggedPoint[TreePoint]],\n+  var cur: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // To keep track of the past checkpointed RDDs.\n+  val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  var rddUpdateCount = 0\n+  if (checkpointDir != None) {\n+    cur.sparkContext.setCheckpointDir(checkpointDir.get)\n+  }\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.\n+   * @param nodeIdUpdaters A map of node index updaters.\n+   *                       The key is the indices of nodes that we want to update.\n+   * @param bins Bin information needed to find child node indices.\n+   */\n+  def updateNodeIndices(\n+      nodeIdUpdaters: Array[Map[Int, NodeIndexUpdater]],\n+      bins: Array[Array[Bin]]): Unit = {\n+    val updatedRDD = data.zip(cur).map {\n+      dataPoint => {\n+        cfor(0)(_ < nodeIdUpdaters.length, _ + 1)(\n+          treeId => {\n+            val nodeIdUpdater = nodeIdUpdaters(treeId).getOrElse(dataPoint._2(treeId), null)\n+            if (nodeIdUpdater != null) {\n+              val newNodeIndex = nodeIdUpdater.updateNodeIndex(\n+                binnedFeatures = dataPoint._1.datum.binnedFeatures,\n+                bins = bins)\n+              dataPoint._2(treeId) = newNodeIndex\n+            }\n+          }\n+        )\n+\n+        dataPoint._2\n+      }\n+    }\n+\n+    cur = updatedRDD\n+    rddUpdateCount += 1\n+\n+    // Handle checkpointing if the directory is not None.\n+    if (checkpointDir != None && (rddUpdateCount % checkpointInterval) == 0) {\n+      // Let's see if we can unpersist previous checkpoints.\n+      var canUnpersist = true\n+      while (checkpointQueue.size > 1 && canUnpersist) {\n+        // We can unpersist the oldest checkpoint iff\n+        // the next checkpoint actually exists in the file system.\n+        if (checkpointQueue.get(1).get.getCheckpointFile != None) {\n+          val old = checkpointQueue.dequeue()\n+          old.unpersist()\n+        } else {\n+          canUnpersist = false\n+        }\n+      }\n+\n+      cur.persist(StorageLevel.MEMORY_AND_DISK)\n+      cur.checkpoint()\n+      checkpointQueue.enqueue(cur)\n+    }\n+  }\n+}\n+\n+@DeveloperApi\n+private[tree] object NodeIdCache {\n+  /**\n+   * Initialize the node Id cache with initial node Id values.\n+   * @param data The RDD of training rows.\n+   * @param numTrees The number of trees that we want to create cache for.\n+   * @param checkpointDir The checkpoint directory where the checkpointed files will be stored.\n+   * @param checkpointInterval The checkpointing interval\n+   *                           (how often should the cache be checkpointed.).\n+   * @param initVal The inital values in the cache."
  }],
  "prId": 2868
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Organize imports (this one above the org.apache.spark imports)\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-28T22:26:24Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Node, Split}\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}"
  }, {
    "author": {
      "login": "codedeft"
    },
    "body": "Will do.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-28T22:57:09Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Node, Split}\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}"
  }],
  "prId": 2868
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Can `checkpointQueue` and `rddUpdateCount` be made private?\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-28T22:44:54Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Node, Split}\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    nodeIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  var prevNodeIdsForInstances: RDD[Array[Int]] = null\n+\n+  // To keep track of the past checkpointed RDDs.\n+  val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()"
  }, {
    "author": {
      "login": "codedeft"
    },
    "body": "Will do.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-28T23:05:08Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Node, Split}\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    nodeIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  var prevNodeIdsForInstances: RDD[Array[Int]] = null\n+\n+  // To keep track of the past checkpointed RDDs.\n+  val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()"
  }],
  "prId": 2868
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Can this be a temp value within `updateNodeIndices()`?  It could hold a reference to the current RDD until the next RDD is ready and persisted/checkpointed, and then it could be unpersisted.  Currently, it looks like 2 RDDs will be persisted at any given time (rather than just during `updateNodeIndices()`).\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-28T22:46:26Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Node, Split}\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    nodeIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  var prevNodeIdsForInstances: RDD[Array[Int]] = null"
  }, {
    "author": {
      "login": "codedeft"
    },
    "body": "That's what I tried first, but then I found out that the RDD doesn't actually get persisted when I call 'persist'. It seems that the actual persistence won't happen until some action method gets called outside of this method. Do you know if there's a way to trigger persistence right there and then? Same with checkpointing, because that would make life much easier from the management perspective (I personally don't like these implicit under-the-hood things very much... ;)).\n\nAdditionally, I think that there'll always be some window where you'll have two RDDs persisted since for performance, you want to get the next persisted one from the previously persisted one. So that's one downside of this approach since this will require more memory than direct-tree approach. At least until the trees get really big.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-28T23:04:48Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Node, Split}\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    nodeIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  var prevNodeIdsForInstances: RDD[Array[Int]] = null"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "@mengxr  Can you weigh in on this (if you know of a good fix)?  Thanks!\n\nI'm not sure if it's worth calling a `count()` or `take(1)` to trigger persisting, or if it's better to have 2 RDDs persisted for more time.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-28T23:38:27Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Node, Split}\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    nodeIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  var prevNodeIdsForInstances: RDD[Array[Int]] = null"
  }, {
    "author": {
      "login": "codedeft"
    },
    "body": "take(1) is probably a reasonable thing to do. But a hack nonetheless. Lol\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-29T00:27:58Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Node, Split}\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    nodeIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  var prevNodeIdsForInstances: RDD[Array[Int]] = null"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "The current implementation should be correct. When we compute `nodeIds` (this is where `nodeIds` gets materialized), `prevNodeIds` should be cached. So after the computation, we have both RDDs cached. `count()` doesn't help here. `take(1)` may only cache the first partition.\n\nOne thing to watch is the closure size of those RDDs. If we happen to reference lots of stuff, it will increase the storage requirement on the driver, who needs to remember how to re-compute each RDD.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-29T03:29:15Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Node, Split}\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    nodeIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  var prevNodeIdsForInstances: RDD[Array[Int]] = null"
  }],
  "prId": 2868
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Please add \":: DeveloperApi ::\" to start of doc here.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-28T22:47:55Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Node, Split}\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    nodeIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree."
  }, {
    "author": {
      "login": "codedeft"
    },
    "body": "Will do.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-28T22:57:15Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Node, Split}\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    nodeIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree."
  }],
  "prId": 2868
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Can a new method be added to clean up any remaining checkpoint files at the end of training?\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-28T22:49:51Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Node, Split}\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    nodeIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(",
    "line": 80
  }, {
    "author": {
      "login": "codedeft"
    },
    "body": "Will do.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-28T22:57:28Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Node, Split}\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    nodeIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(",
    "line": 80
  }],
  "prId": 2868
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "An alternative solution to checkpointing is to recompute `nodeIds` from scratch every few iterations. It is reliable as long as the driver stays alive. If we do that, users don't need to set checkpoint dir and we don't need to manage checkpoint files. I hope the overhead is not big.\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-29T03:29:17Z",
    "diffHunk": "@@ -0,0 +1,189 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Node, Split}\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    nodeIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *            (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointDir The checkpoint directory where\n+ *                      the checkpointed files will be stored.\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+@DeveloperApi\n+private[tree] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointDir: Option[String],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  var prevNodeIdsForInstances: RDD[Array[Int]] = null\n+\n+  // To keep track of the past checkpointed RDDs."
  }],
  "prId": 2868
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "A bit unclear; perhaps update to: \"For each TreePoint, an array over trees of the node index in each tree.  (Initially, values should all be 1 for root node.)\"\n",
    "commit": "5f5a1564af1a8a1cbf6d257941ad969169295fe7",
    "createdAt": "2014-10-31T18:51:31Z",
    "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.mllib.tree.configuration.FeatureType._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.model.{Bin, Node, Split}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+@DeveloperApi\n+private[tree] case class NodeIndexUpdater(\n+    split: Split,\n+    nodeIndex: Int) {\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeatures Binned feature values.\n+   * @param bins Bin information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeatures: Array[Int], bins: Array[Array[Bin]]): Int = {\n+    if (split.featureType == Continuous) {\n+      val featureIndex = split.feature\n+      val binIndex = binnedFeatures(featureIndex)\n+      val featureValueUpperBound = bins(featureIndex)(binIndex).highSplit.threshold\n+      if (featureValueUpperBound <= split.threshold) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    } else {\n+      if (split.categories.contains(binnedFeatures(split.feature).toDouble)) {\n+        Node.leftChildIndex(nodeIndex)\n+      } else {\n+        Node.rightChildIndex(nodeIndex)\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A given TreePoint would belong to a particular node per tree.\n+ * This is used to keep track of which node for a particular tree that a TreePoint belongs to.\n+ * A separate RDD of Array[Int] needs to be maintained and updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache"
  }],
  "prId": 2868
}]