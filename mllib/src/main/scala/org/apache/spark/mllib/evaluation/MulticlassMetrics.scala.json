[{
  "comments": [{
    "author": {
      "login": "WeichenXu123"
    },
    "body": "I prefer to define two constructor as:\r\n```\r\nthis(predAndLabelsWithOptWeight: RDD[(Double, Double, Double)]\r\nthis(predAndLabels: RDD[(Double, Double)])\r\n```\r\nso it will do more strict type checking.",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-04-17T09:12:42Z",
    "diffHunk": "@@ -27,10 +27,11 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                                   (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"1.1.0\") (predAndLabelsWithOptWeight: RDD[_]) {"
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "good idea, this also simplifies the calculation of the confusions, fpByClass, tpByClass and labelCountByClass",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-04-18T03:51:06Z",
    "diffHunk": "@@ -27,10 +27,11 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                                   (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"1.1.0\") (predAndLabelsWithOptWeight: RDD[_]) {"
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "hmm the build fails here though with an error indicating that the methods are the same after type erasure, perhaps I should revert this code back:\r\n\r\n[error] /home/jenkins/workspace/SparkPullRequestBuilder@2/mllib/src/main/scala/org/apache/spark/mllib/evaluation/MulticlassMetrics.scala:35: double definition:\r\n[error] constructor MulticlassMetrics: (predLabelsWeight: org.apache.spark.rdd.RDD[(Double, Double, Double)])org.apache.spark.mllib.evaluation.MulticlassMetrics at line 33 and\r\n[error] constructor MulticlassMetrics: (predAndLabels: org.apache.spark.rdd.RDD[(Double, Double)])org.apache.spark.mllib.evaluation.MulticlassMetrics at line 35\r\n[error] have same type after erasure: (predLabelsWeight: org.apache.spark.rdd.RDD)org.apache.spark.mllib.evaluation.MulticlassMetrics\r\n[error]   def this(predAndLabels: RDD[(Double, Double)]) =\r\n\r\n",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-04-18T04:09:16Z",
    "diffHunk": "@@ -27,10 +27,11 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                                   (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"1.1.0\") (predAndLabelsWithOptWeight: RDD[_]) {"
  }, {
    "author": {
      "login": "WeichenXu123"
    },
    "body": "You can add a member var into the class like `val predAndLabelsWithOptWeight: RDD[(Double, Double, Double)`, and ctor assign this member var. so the following calculation code will be easier.",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-04-18T09:37:42Z",
    "diffHunk": "@@ -27,10 +27,11 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                                   (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"1.1.0\") (predAndLabelsWithOptWeight: RDD[_]) {"
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "good idea, done!",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-04-19T04:44:35Z",
    "diffHunk": "@@ -27,10 +27,11 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                                   (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"1.1.0\") (predAndLabelsWithOptWeight: RDD[_]) {"
  }],
  "prId": 17086
}, {
  "comments": [{
    "author": {
      "login": "WeichenXu123"
    },
    "body": "The `.mapValues(weight => weight)` is redundant, it generate the same RDD.",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-04-24T08:22:23Z",
    "diffHunk": "@@ -39,21 +46,28 @@ class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Doubl\n   private[mllib] def this(predictionAndLabels: DataFrame) =\n     this(predictionAndLabels.rdd.map(r => (r.getDouble(0), r.getDouble(1))))\n \n-  private lazy val labelCountByClass: Map[Double, Long] = predictionAndLabels.values.countByValue()\n-  private lazy val labelCount: Long = labelCountByClass.values.sum\n-  private lazy val tpByClass: Map[Double, Int] = predictionAndLabels\n-    .map { case (prediction, label) =>\n-      (label, if (label == prediction) 1 else 0)\n+  private lazy val labelCountByClass: Map[Double, Double] =\n+    predLabelsWeight.map {\n+      case (prediction: Double, label: Double, weight: Double) =>\n+        (label, weight)\n+    }.mapValues(weight => weight).reduceByKey(_ + _).collect().toMap"
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "good catch!  removed",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-04-26T14:58:03Z",
    "diffHunk": "@@ -39,21 +46,28 @@ class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Doubl\n   private[mllib] def this(predictionAndLabels: DataFrame) =\n     this(predictionAndLabels.rdd.map(r => (r.getDouble(0), r.getDouble(1))))\n \n-  private lazy val labelCountByClass: Map[Double, Long] = predictionAndLabels.values.countByValue()\n-  private lazy val labelCount: Long = labelCountByClass.values.sum\n-  private lazy val tpByClass: Map[Double, Int] = predictionAndLabels\n-    .map { case (prediction, label) =>\n-      (label, if (label == prediction) 1 else 0)\n+  private lazy val labelCountByClass: Map[Double, Double] =\n+    predLabelsWeight.map {\n+      case (prediction: Double, label: Double, weight: Double) =>\n+        (label, weight)\n+    }.mapValues(weight => weight).reduceByKey(_ + _).collect().toMap"
  }],
  "prId": 17086
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Although I think we're not updating .mllib much at all now, I think this is a simple and backwards-compatible change so think it's OK. It is the implementation behind the .ml version anyway.",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-05T23:15:15Z",
    "diffHunk": "@@ -27,10 +27,17 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                         (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"2.4.0\") (predAndLabelsWithOptWeight: RDD[_]) {"
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "thanks!  yes, it is backwards compatible.",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-06T03:50:08Z",
    "diffHunk": "@@ -27,10 +27,17 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                         (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"2.4.0\") (predAndLabelsWithOptWeight: RDD[_]) {"
  }],
  "prId": 17086
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Oh, wait a sec, this changed the signature. I think you have to retain both. The `RDD[(Double, Double)]` constructor should stay, one way or the other, and add a new `RDD[(Double, Double, Double)]` constructor, with appropriate Since tags on each.\r\n\r\nBelow there's a `DataFrame` constructor and I'm not sure how to handle that. It should also handle the case where there's a weight col, but I'm not sure how to do that cleanly. There can be a second argument like `hasWeightCol` but that's starting to feel hacky.",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-06T13:37:25Z",
    "diffHunk": "@@ -27,10 +27,17 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                         (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"3.0.0\") (predAndLabelsWithOptWeight: RDD[_]) {"
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "@srowen hmm, this was already suggested, please see this comment: https://github.com/apache/spark/pull/17086#discussion_r182303815\r\nthe build fails with an error due to Java type erasure, so this wouldn't work... you can't have two constructors with the same type erased signature... maybe I am misunderstanding something, and you meant something else?  Are you sure this changes the signature in a way that breaks others, it should still allow RDD with a tuple of 2 Double values.\r\nThe error I get is:\r\n[error] /home/jenkins/workspace/SparkPullRequestBuilder@2/mllib/src/main/scala/org/apache/spark/mllib/evaluation/MulticlassMetrics.scala:35: double definition:\r\n[error] constructor MulticlassMetrics: (predLabelsWeight: org.apache.spark.rdd.RDD[(Double, Double, Double)])org.apache.spark.mllib.evaluation.MulticlassMetrics at line 33 and\r\n[error] constructor MulticlassMetrics: (predAndLabels: org.apache.spark.rdd.RDD[(Double, Double)])org.apache.spark.mllib.evaluation.MulticlassMetrics at line 35\r\n[error] have same type after erasure: (predLabelsWeight: org.apache.spark.rdd.RDD)org.apache.spark.mllib.evaluation.MulticlassMetrics\r\n[error] def this(predAndLabels: RDD[(Double, Double)]) =",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-06T17:14:58Z",
    "diffHunk": "@@ -27,10 +27,17 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                         (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"3.0.0\") (predAndLabelsWithOptWeight: RDD[_]) {"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Darn, OK. Hm, so this doesn't actually cause a source or binary change? OK, that could be fine. I guess MiMa didn't complain. I guess you can now do weird things like pass `RDD[String]` here and it'll fail quickly. I'm a little uneasy about it but it's probably acceptable. Any other opinions?\r\n\r\nI am not sure what to do about the DataFrame issue though. I suspect most people will want to call with a DataFrame now.",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-06T17:20:41Z",
    "diffHunk": "@@ -27,10 +27,17 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                         (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"3.0.0\") (predAndLabelsWithOptWeight: RDD[_]) {"
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "\"I am not sure what to do about the DataFrame issue though\", ah, I think I see your concern.\r\nBut, isn't this dataframe constructor private anyway, so it can't be used by anyone outside mllib:\r\n\r\n  private[mllib] def this(predictionAndLabels: DataFrame) =\r\n    this(predictionAndLabels.rdd.map(r => (r.getDouble(0), r.getDouble(1))))\r\n\r\nI only modified the RDD part because that is what is used by the ML evaluator and it is what users outside spark can access.  This is to add weight column for the evaluators.\r\n\r\nHowever, even if we wanted to add weight column support for the private API, I'm unsure about how to add this.  Should I just check if there are 3 columns or two, and if there are 3 use the third one as the weight column?  I guess I am on the fence about this, I could change it but I don't think it is absolutely necessary, since it's not used anywhere outside spark MLLIB anyway.\r\n\r\nActually, this constructor is a bit weird, it looks like it was added as part of this PR:\r\nhttps://github.com/apache/spark/pull/6011/files\r\nIt is only used here in the python API:\r\nhttps://github.com/apache/spark/pull/6011/files#diff-443f766289f8090078531c3e1a1d6027R186\r\nBut I don't see why we couldn't just get the rdd there and remove the private constructor altogether (?)",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-06T17:55:01Z",
    "diffHunk": "@@ -27,10 +27,17 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                         (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"3.0.0\") (predAndLabelsWithOptWeight: RDD[_]) {"
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "The python API takes an RDD, creates a DF, and then calls this private constructor with the DF, but I would think we could just pass the RDD directly",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-06T17:56:35Z",
    "diffHunk": "@@ -27,10 +27,17 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                         (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"3.0.0\") (predAndLabelsWithOptWeight: RDD[_]) {"
  }],
  "prId": 17086
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: you can write prediction as _ here",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-06T13:37:40Z",
    "diffHunk": "@@ -39,21 +46,28 @@ class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Doubl\n   private[mllib] def this(predictionAndLabels: DataFrame) =\n     this(predictionAndLabels.rdd.map(r => (r.getDouble(0), r.getDouble(1))))\n \n-  private lazy val labelCountByClass: Map[Double, Long] = predictionAndLabels.values.countByValue()\n-  private lazy val labelCount: Long = labelCountByClass.values.sum\n-  private lazy val tpByClass: Map[Double, Int] = predictionAndLabels\n-    .map { case (prediction, label) =>\n-      (label, if (label == prediction) 1 else 0)\n+  private lazy val labelCountByClass: Map[Double, Double] =\n+    predLabelsWeight.map {\n+      case (prediction: Double, label: Double, weight: Double) =>"
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "done!",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-06T17:17:11Z",
    "diffHunk": "@@ -39,21 +46,28 @@ class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Doubl\n   private[mllib] def this(predictionAndLabels: DataFrame) =\n     this(predictionAndLabels.rdd.map(r => (r.getDouble(0), r.getDouble(1))))\n \n-  private lazy val labelCountByClass: Map[Double, Long] = predictionAndLabels.values.countByValue()\n-  private lazy val labelCount: Long = labelCountByClass.values.sum\n-  private lazy val tpByClass: Map[Double, Int] = predictionAndLabels\n-    .map { case (prediction, label) =>\n-      (label, if (label == prediction) 1 else 0)\n+  private lazy val labelCountByClass: Map[Double, Double] =\n+    predLabelsWeight.map {\n+      case (prediction: Double, label: Double, weight: Double) =>"
  }],
  "prId": 17086
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": ".collectAsMap()?",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-06T13:37:55Z",
    "diffHunk": "@@ -39,21 +46,28 @@ class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Doubl\n   private[mllib] def this(predictionAndLabels: DataFrame) =\n     this(predictionAndLabels.rdd.map(r => (r.getDouble(0), r.getDouble(1))))\n \n-  private lazy val labelCountByClass: Map[Double, Long] = predictionAndLabels.values.countByValue()\n-  private lazy val labelCount: Long = labelCountByClass.values.sum\n-  private lazy val tpByClass: Map[Double, Int] = predictionAndLabels\n-    .map { case (prediction, label) =>\n-      (label, if (label == prediction) 1 else 0)\n+  private lazy val labelCountByClass: Map[Double, Double] =\n+    predLabelsWeight.map {\n+      case (prediction: Double, label: Double, weight: Double) =>\n+        (label, weight)\n+    }.reduceByKey(_ + _).collect().toMap"
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "done!",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-06T17:17:41Z",
    "diffHunk": "@@ -39,21 +46,28 @@ class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Doubl\n   private[mllib] def this(predictionAndLabels: DataFrame) =\n     this(predictionAndLabels.rdd.map(r => (r.getDouble(0), r.getDouble(1))))\n \n-  private lazy val labelCountByClass: Map[Double, Long] = predictionAndLabels.values.countByValue()\n-  private lazy val labelCount: Long = labelCountByClass.values.sum\n-  private lazy val tpByClass: Map[Double, Int] = predictionAndLabels\n-    .map { case (prediction, label) =>\n-      (label, if (label == prediction) 1 else 0)\n+  private lazy val labelCountByClass: Map[Double, Double] =\n+    predLabelsWeight.map {\n+      case (prediction: Double, label: Double, weight: Double) =>\n+        (label, weight)\n+    }.reduceByKey(_ + _).collect().toMap"
  }],
  "prId": 17086
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "I'm not sure, but I wonder if it's more efficient to filter on label == prediction, then reduce? The original code didn't do it, but could be worth it.",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-06T13:40:18Z",
    "diffHunk": "@@ -39,21 +46,28 @@ class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Doubl\n   private[mllib] def this(predictionAndLabels: DataFrame) =\n     this(predictionAndLabels.rdd.map(r => (r.getDouble(0), r.getDouble(1))))\n \n-  private lazy val labelCountByClass: Map[Double, Long] = predictionAndLabels.values.countByValue()\n-  private lazy val labelCount: Long = labelCountByClass.values.sum\n-  private lazy val tpByClass: Map[Double, Int] = predictionAndLabels\n-    .map { case (prediction, label) =>\n-      (label, if (label == prediction) 1 else 0)\n+  private lazy val labelCountByClass: Map[Double, Double] =\n+    predLabelsWeight.map {\n+      case (prediction: Double, label: Double, weight: Double) =>\n+        (label, weight)\n+    }.reduceByKey(_ + _).collect().toMap\n+  private lazy val labelCount: Double = labelCountByClass.values.sum\n+  private lazy val tpByClass: Map[Double, Double] = predLabelsWeight\n+    .map {",
    "line": 39
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "done, not sure if more efficient, it seemed to take same time for me, but I didn't test extensively",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-06T20:08:47Z",
    "diffHunk": "@@ -39,21 +46,28 @@ class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Doubl\n   private[mllib] def this(predictionAndLabels: DataFrame) =\n     this(predictionAndLabels.rdd.map(r => (r.getDouble(0), r.getDouble(1))))\n \n-  private lazy val labelCountByClass: Map[Double, Long] = predictionAndLabels.values.countByValue()\n-  private lazy val labelCount: Long = labelCountByClass.values.sum\n-  private lazy val tpByClass: Map[Double, Int] = predictionAndLabels\n-    .map { case (prediction, label) =>\n-      (label, if (label == prediction) 1 else 0)\n+  private lazy val labelCountByClass: Map[Double, Double] =\n+    predLabelsWeight.map {\n+      case (prediction: Double, label: Double, weight: Double) =>\n+        (label, weight)\n+    }.reduceByKey(_ + _).collect().toMap\n+  private lazy val labelCount: Double = labelCountByClass.values.sum\n+  private lazy val tpByClass: Map[Double, Double] = predLabelsWeight\n+    .map {",
    "line": 39
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "it looks like this will actually cause tests to fail, because the key may become missing if we filter everything out first, whereas we would want it to be present otherwise but have a 0 value",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-06T21:12:58Z",
    "diffHunk": "@@ -39,21 +46,28 @@ class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Doubl\n   private[mllib] def this(predictionAndLabels: DataFrame) =\n     this(predictionAndLabels.rdd.map(r => (r.getDouble(0), r.getDouble(1))))\n \n-  private lazy val labelCountByClass: Map[Double, Long] = predictionAndLabels.values.countByValue()\n-  private lazy val labelCount: Long = labelCountByClass.values.sum\n-  private lazy val tpByClass: Map[Double, Int] = predictionAndLabels\n-    .map { case (prediction, label) =>\n-      (label, if (label == prediction) 1 else 0)\n+  private lazy val labelCountByClass: Map[Double, Double] =\n+    predLabelsWeight.map {\n+      case (prediction: Double, label: Double, weight: Double) =>\n+        (label, weight)\n+    }.reduceByKey(_ + _).collect().toMap\n+  private lazy val labelCount: Double = labelCountByClass.values.sum\n+  private lazy val tpByClass: Map[Double, Double] = predLabelsWeight\n+    .map {",
    "line": 39
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "see the test failure here for reference: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/98529/testReport/",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-06T21:17:32Z",
    "diffHunk": "@@ -39,21 +46,28 @@ class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Doubl\n   private[mllib] def this(predictionAndLabels: DataFrame) =\n     this(predictionAndLabels.rdd.map(r => (r.getDouble(0), r.getDouble(1))))\n \n-  private lazy val labelCountByClass: Map[Double, Long] = predictionAndLabels.values.countByValue()\n-  private lazy val labelCount: Long = labelCountByClass.values.sum\n-  private lazy val tpByClass: Map[Double, Int] = predictionAndLabels\n-    .map { case (prediction, label) =>\n-      (label, if (label == prediction) 1 else 0)\n+  private lazy val labelCountByClass: Map[Double, Double] =\n+    predLabelsWeight.map {\n+      case (prediction: Double, label: Double, weight: Double) =>\n+        (label, weight)\n+    }.reduceByKey(_ + _).collect().toMap\n+  private lazy val labelCount: Double = labelCountByClass.values.sum\n+  private lazy val tpByClass: Map[Double, Double] = predLabelsWeight\n+    .map {",
    "line": 39
  }],
  "prId": 17086
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: I think this Since has to go back to 1.1.0. It hasn't changed signature after all.\r\n\r\n@yanboliang can you comment on the DataFrame constructor? although it might be a separate issue, would be good to know if we're missing something.",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-08T00:02:22Z",
    "diffHunk": "@@ -27,10 +27,17 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                         (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"3.0.0\") (predAndLabelsWithOptWeight: RDD[_]) {"
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "done!",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-08T04:10:24Z",
    "diffHunk": "@@ -27,10 +27,17 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                         (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"3.0.0\") (predAndLabelsWithOptWeight: RDD[_]) {"
  }],
  "prId": 17086
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "If you're making one more change, might make an explicit check here on the type of the RDD, now that it can be anything at compile time. Like `case other => throw new IllegalArgumentException(s\"Expected tuples, got $other\")`\r\n\r\nActually, in order to tighten this back down a little, I wonder if the method argument can be `RDD[_ <: Product]` ? That includes Tuple2 and Tuple3, and a lot of other things, but is more specific than 'anything'.",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-08T00:05:47Z",
    "diffHunk": "@@ -27,10 +27,17 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                         (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"3.0.0\") (predAndLabelsWithOptWeight: RDD[_]) {\n+  val predLabelsWeight: RDD[(Double, Double, Double)] = predAndLabelsWithOptWeight.map {\n+    case (prediction: Double, label: Double, weight: Double) =>\n+      (prediction, label, weight)\n+    case (prediction: Double, label: Double) =>\n+      (prediction, label, 1.0)",
    "line": 15
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "wow, great idea!  done!",
    "commit": "50864497d013ba7f8a160d5142b0cfdd41f00f8d",
    "createdAt": "2018-11-08T04:12:38Z",
    "diffHunk": "@@ -27,10 +27,17 @@ import org.apache.spark.sql.DataFrame\n /**\n  * Evaluator for multiclass classification.\n  *\n- * @param predictionAndLabels an RDD of (prediction, label) pairs.\n+ * @param predAndLabelsWithOptWeight an RDD of (prediction, label, weight) or\n+ *                         (prediction, label) pairs.\n  */\n @Since(\"1.1.0\")\n-class MulticlassMetrics @Since(\"1.1.0\") (predictionAndLabels: RDD[(Double, Double)]) {\n+class MulticlassMetrics @Since(\"3.0.0\") (predAndLabelsWithOptWeight: RDD[_]) {\n+  val predLabelsWeight: RDD[(Double, Double, Double)] = predAndLabelsWithOptWeight.map {\n+    case (prediction: Double, label: Double, weight: Double) =>\n+      (prediction, label, weight)\n+    case (prediction: Double, label: Double) =>\n+      (prediction, label, 1.0)",
    "line": 15
  }],
  "prId": 17086
}]