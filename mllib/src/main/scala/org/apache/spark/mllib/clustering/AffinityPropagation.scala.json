[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Please only import `mutable` and use `mutable.Map` and `mutable.Set` in the code. Sometimes, mixing the default `Map` with mutable `Map` causes problems that are hard to debug.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:47:39Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "So we store the entire clustering result to driver. How large could it be? It would be nice if we can store the model distributively. In PIC, the result is stored as `RDD[(Long, Int)]`. Please also consider Java users. Both `Seq` and `Set` are Scala collections. The best way to check this is to provide a Java example.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:47:40Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Document the behavior if the given vertex doesn't belong to any cluster.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:47:42Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`Option` is not Java friendly. Would `-1` work here?\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:47:44Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "ok, would work.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-04-07T16:15:45Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "use this style for consistency:\n\n```\ncluster.foreach { cluster =>\n   ...\n}\n```\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:47:45Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`AffinityPropagation` -> `Affinity propagation`.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:47:47Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\""
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Use DOI link: http://doi.org/10.1126/science.1136800\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:47:49Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]]."
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`(Wikipedia)` -> `Affinity propagation (Wikipedia)`\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:47:51Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Put backticks around `0.5`. Otherwise, ScalaDoc will only show \"... lambda: 0.\" as the preview.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:47:52Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "So they could be negative?\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:47:56Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix "
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Yes. It can be. Will document it.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-04-07T16:15:03Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix "
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`not equal to` -> `different from`\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:47:58Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Could we set it as a parameter? Also document the expect input if `symmetric == true`. Do we expect both `(i, j)` and `(j, i)` in the input?\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:47:59Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Do you mean to set `symmetric` as a parameter of `AffinityPropagation`? Because it is closely related to the characters of the given `similarities`, I list it as a parameter of `run` method. But I don't feel strong about it.\n\nI think if it is better to use other name instead of `symmetric`. This parameter indicates that `(i, j)` equals to `(j, i)`, so in order to avoid redundancy, the given `similarities` only cover triangular matrix. May just use `triangular`?\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-04-07T16:14:46Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Do we expect users use the same AP object but run with different datasets (symmetric/asymmetric)? If not, it should be okay to make symmetric a parameter. This is similar to the `k` in k-means. `k` is associated with the dataset, but we set it in the KMeans object.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-04-07T23:14:59Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Chop down the arguments and use 4-space indentation.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:48:01Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true\n+   * @return a [[AffinityPropagationModel]] that contains the clustering result\n+   */\n+  def run(similarities: RDD[(Long, Long, Double)], symmetric: Boolean = true)\n+    : AffinityPropagationModel = {\n+    val s = constructGraph(similarities, normalization, symmetric)\n+    ap(s)\n+  }\n+\n+  /**\n+   * Runs the AP algorithm.\n+   *\n+   * @param s The (normalized) similarity matrix, which is the matrix S in the AP paper with vertex\n+   *          similarities and the initial availabilities and responsibilities as its edge\n+   *          properties.\n+   */\n+  private def ap(s: Graph[Seq[Double], Seq[Double]]): AffinityPropagationModel = {\n+    val g = apIter(s, maxIterations, lambda)\n+    chooseExemplars(g)\n+  }\n+}\n+\n+private[clustering] object AffinityPropagation extends Logging {\n+  /**\n+   * Construct the similarity matrix (S) and do normalization if needed.\n+   * Returns the (normalized) similarity matrix (S).\n+   */\n+  def constructGraph(similarities: RDD[(Long, Long, Double)], normalize: Boolean,\n+    symmetric: Boolean):"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Put this to the same line as the last argument.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:48:03Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true\n+   * @return a [[AffinityPropagationModel]] that contains the clustering result\n+   */\n+  def run(similarities: RDD[(Long, Long, Double)], symmetric: Boolean = true)\n+    : AffinityPropagationModel = {\n+    val s = constructGraph(similarities, normalization, symmetric)\n+    ap(s)\n+  }\n+\n+  /**\n+   * Runs the AP algorithm.\n+   *\n+   * @param s The (normalized) similarity matrix, which is the matrix S in the AP paper with vertex\n+   *          similarities and the initial availabilities and responsibilities as its edge\n+   *          properties.\n+   */\n+  private def ap(s: Graph[Seq[Double], Seq[Double]]): AffinityPropagationModel = {\n+    val g = apIter(s, maxIterations, lambda)\n+    chooseExemplars(g)\n+  }\n+}\n+\n+private[clustering] object AffinityPropagation extends Logging {\n+  /**\n+   * Construct the similarity matrix (S) and do normalization if needed.\n+   * Returns the (normalized) similarity matrix (S).\n+   */\n+  def constructGraph(similarities: RDD[(Long, Long, Double)], normalize: Boolean,\n+    symmetric: Boolean):\n+    Graph[Seq[Double], Seq[Double]] = {"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Using `Seq(s, 0.0, 0.0)` may have performance issues. If the tuples are used frequently, we can define a case class for it. It also makes the code easier to read. It is definitely not clear to me what the second and the third values are.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:48:04Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true\n+   * @return a [[AffinityPropagationModel]] that contains the clustering result\n+   */\n+  def run(similarities: RDD[(Long, Long, Double)], symmetric: Boolean = true)\n+    : AffinityPropagationModel = {\n+    val s = constructGraph(similarities, normalization, symmetric)\n+    ap(s)\n+  }\n+\n+  /**\n+   * Runs the AP algorithm.\n+   *\n+   * @param s The (normalized) similarity matrix, which is the matrix S in the AP paper with vertex\n+   *          similarities and the initial availabilities and responsibilities as its edge\n+   *          properties.\n+   */\n+  private def ap(s: Graph[Seq[Double], Seq[Double]]): AffinityPropagationModel = {\n+    val g = apIter(s, maxIterations, lambda)\n+    chooseExemplars(g)\n+  }\n+}\n+\n+private[clustering] object AffinityPropagation extends Logging {\n+  /**\n+   * Construct the similarity matrix (S) and do normalization if needed.\n+   * Returns the (normalized) similarity matrix (S).\n+   */\n+  def constructGraph(similarities: RDD[(Long, Long, Double)], normalize: Boolean,\n+    symmetric: Boolean):\n+    Graph[Seq[Double], Seq[Double]] = {\n+    val edges = similarities.flatMap { case (i, j, s) =>\n+      if (symmetric && i != j) {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)), Edge(j, i, Seq(s, 0.0, 0.0)))"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Why do you need to wrap doubles with `Seq`?\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:48:05Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true\n+   * @return a [[AffinityPropagationModel]] that contains the clustering result\n+   */\n+  def run(similarities: RDD[(Long, Long, Double)], symmetric: Boolean = true)\n+    : AffinityPropagationModel = {\n+    val s = constructGraph(similarities, normalization, symmetric)\n+    ap(s)\n+  }\n+\n+  /**\n+   * Runs the AP algorithm.\n+   *\n+   * @param s The (normalized) similarity matrix, which is the matrix S in the AP paper with vertex\n+   *          similarities and the initial availabilities and responsibilities as its edge\n+   *          properties.\n+   */\n+  private def ap(s: Graph[Seq[Double], Seq[Double]]): AffinityPropagationModel = {\n+    val g = apIter(s, maxIterations, lambda)\n+    chooseExemplars(g)\n+  }\n+}\n+\n+private[clustering] object AffinityPropagation extends Logging {\n+  /**\n+   * Construct the similarity matrix (S) and do normalization if needed.\n+   * Returns the (normalized) similarity matrix (S).\n+   */\n+  def constructGraph(similarities: RDD[(Long, Long, Double)], normalize: Boolean,\n+    symmetric: Boolean):\n+    Graph[Seq[Double], Seq[Double]] = {\n+    val edges = similarities.flatMap { case (i, j, s) =>\n+      if (symmetric && i != j) {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)), Edge(j, i, Seq(s, 0.0, 0.0)))\n+      } else {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)))\n+      }\n+    }\n+\n+    if (normalize) {\n+      val gA = Graph.fromEdges(edges, Seq(0.0))"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "style:\n\n```\n.mapTriplets({ e => \n  ...\n}, TripletFields.Src)\n```\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:48:07Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true\n+   * @return a [[AffinityPropagationModel]] that contains the clustering result\n+   */\n+  def run(similarities: RDD[(Long, Long, Double)], symmetric: Boolean = true)\n+    : AffinityPropagationModel = {\n+    val s = constructGraph(similarities, normalization, symmetric)\n+    ap(s)\n+  }\n+\n+  /**\n+   * Runs the AP algorithm.\n+   *\n+   * @param s The (normalized) similarity matrix, which is the matrix S in the AP paper with vertex\n+   *          similarities and the initial availabilities and responsibilities as its edge\n+   *          properties.\n+   */\n+  private def ap(s: Graph[Seq[Double], Seq[Double]]): AffinityPropagationModel = {\n+    val g = apIter(s, maxIterations, lambda)\n+    chooseExemplars(g)\n+  }\n+}\n+\n+private[clustering] object AffinityPropagation extends Logging {\n+  /**\n+   * Construct the similarity matrix (S) and do normalization if needed.\n+   * Returns the (normalized) similarity matrix (S).\n+   */\n+  def constructGraph(similarities: RDD[(Long, Long, Double)], normalize: Boolean,\n+    symmetric: Boolean):\n+    Graph[Seq[Double], Seq[Double]] = {\n+    val edges = similarities.flatMap { case (i, j, s) =>\n+      if (symmetric && i != j) {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)), Edge(j, i, Seq(s, 0.0, 0.0)))\n+      } else {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)))\n+      }\n+    }\n+\n+    if (normalize) {\n+      val gA = Graph.fromEdges(edges, Seq(0.0))\n+      val vD = gA.aggregateMessages[Seq[Double]](\n+        sendMsg = ctx => {\n+          ctx.sendToSrc(Seq(ctx.attr(0)))\n+        },\n+        mergeMsg = (s1, s2) => Seq(s1(0) + s2(0)),\n+        TripletFields.EdgeOnly)\n+      val normalized = GraphImpl.fromExistingRDDs(vD, gA.edges)\n+        .mapTriplets("
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Remove `{ .. }` if you use `if ... else` in one line.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:48:08Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true\n+   * @return a [[AffinityPropagationModel]] that contains the clustering result\n+   */\n+  def run(similarities: RDD[(Long, Long, Double)], symmetric: Boolean = true)\n+    : AffinityPropagationModel = {\n+    val s = constructGraph(similarities, normalization, symmetric)\n+    ap(s)\n+  }\n+\n+  /**\n+   * Runs the AP algorithm.\n+   *\n+   * @param s The (normalized) similarity matrix, which is the matrix S in the AP paper with vertex\n+   *          similarities and the initial availabilities and responsibilities as its edge\n+   *          properties.\n+   */\n+  private def ap(s: Graph[Seq[Double], Seq[Double]]): AffinityPropagationModel = {\n+    val g = apIter(s, maxIterations, lambda)\n+    chooseExemplars(g)\n+  }\n+}\n+\n+private[clustering] object AffinityPropagation extends Logging {\n+  /**\n+   * Construct the similarity matrix (S) and do normalization if needed.\n+   * Returns the (normalized) similarity matrix (S).\n+   */\n+  def constructGraph(similarities: RDD[(Long, Long, Double)], normalize: Boolean,\n+    symmetric: Boolean):\n+    Graph[Seq[Double], Seq[Double]] = {\n+    val edges = similarities.flatMap { case (i, j, s) =>\n+      if (symmetric && i != j) {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)), Edge(j, i, Seq(s, 0.0, 0.0)))\n+      } else {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)))\n+      }\n+    }\n+\n+    if (normalize) {\n+      val gA = Graph.fromEdges(edges, Seq(0.0))\n+      val vD = gA.aggregateMessages[Seq[Double]](\n+        sendMsg = ctx => {\n+          ctx.sendToSrc(Seq(ctx.attr(0)))\n+        },\n+        mergeMsg = (s1, s2) => Seq(s1(0) + s2(0)),\n+        TripletFields.EdgeOnly)\n+      val normalized = GraphImpl.fromExistingRDDs(vD, gA.edges)\n+        .mapTriplets(\n+          e => {\n+            val s = if (e.srcAttr(0) == 0.0) { e.attr(0) } else { e.attr(0) / e.srcAttr(0) }"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "We already have the vertex RDD. Can we re-use it?\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:48:10Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true\n+   * @return a [[AffinityPropagationModel]] that contains the clustering result\n+   */\n+  def run(similarities: RDD[(Long, Long, Double)], symmetric: Boolean = true)\n+    : AffinityPropagationModel = {\n+    val s = constructGraph(similarities, normalization, symmetric)\n+    ap(s)\n+  }\n+\n+  /**\n+   * Runs the AP algorithm.\n+   *\n+   * @param s The (normalized) similarity matrix, which is the matrix S in the AP paper with vertex\n+   *          similarities and the initial availabilities and responsibilities as its edge\n+   *          properties.\n+   */\n+  private def ap(s: Graph[Seq[Double], Seq[Double]]): AffinityPropagationModel = {\n+    val g = apIter(s, maxIterations, lambda)\n+    chooseExemplars(g)\n+  }\n+}\n+\n+private[clustering] object AffinityPropagation extends Logging {\n+  /**\n+   * Construct the similarity matrix (S) and do normalization if needed.\n+   * Returns the (normalized) similarity matrix (S).\n+   */\n+  def constructGraph(similarities: RDD[(Long, Long, Double)], normalize: Boolean,\n+    symmetric: Boolean):\n+    Graph[Seq[Double], Seq[Double]] = {\n+    val edges = similarities.flatMap { case (i, j, s) =>\n+      if (symmetric && i != j) {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)), Edge(j, i, Seq(s, 0.0, 0.0)))\n+      } else {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)))\n+      }\n+    }\n+\n+    if (normalize) {\n+      val gA = Graph.fromEdges(edges, Seq(0.0))\n+      val vD = gA.aggregateMessages[Seq[Double]](\n+        sendMsg = ctx => {\n+          ctx.sendToSrc(Seq(ctx.attr(0)))\n+        },\n+        mergeMsg = (s1, s2) => Seq(s1(0) + s2(0)),\n+        TripletFields.EdgeOnly)\n+      val normalized = GraphImpl.fromExistingRDDs(vD, gA.edges)\n+        .mapTriplets(\n+          e => {\n+            val s = if (e.srcAttr(0) == 0.0) { e.attr(0) } else { e.attr(0) / e.srcAttr(0) }\n+            Seq(s, 0.0, 0.0)\n+          }, TripletFields.Src)\n+      Graph.fromEdges(normalized.edges, Seq(0.0, 0.0))"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "If you mean `vD`, I think its type is different to what we use here (`Seq(0.0, 0.0)`)?\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-04-07T16:45:09Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true\n+   * @return a [[AffinityPropagationModel]] that contains the clustering result\n+   */\n+  def run(similarities: RDD[(Long, Long, Double)], symmetric: Boolean = true)\n+    : AffinityPropagationModel = {\n+    val s = constructGraph(similarities, normalization, symmetric)\n+    ap(s)\n+  }\n+\n+  /**\n+   * Runs the AP algorithm.\n+   *\n+   * @param s The (normalized) similarity matrix, which is the matrix S in the AP paper with vertex\n+   *          similarities and the initial availabilities and responsibilities as its edge\n+   *          properties.\n+   */\n+  private def ap(s: Graph[Seq[Double], Seq[Double]]): AffinityPropagationModel = {\n+    val g = apIter(s, maxIterations, lambda)\n+    chooseExemplars(g)\n+  }\n+}\n+\n+private[clustering] object AffinityPropagation extends Logging {\n+  /**\n+   * Construct the similarity matrix (S) and do normalization if needed.\n+   * Returns the (normalized) similarity matrix (S).\n+   */\n+  def constructGraph(similarities: RDD[(Long, Long, Double)], normalize: Boolean,\n+    symmetric: Boolean):\n+    Graph[Seq[Double], Seq[Double]] = {\n+    val edges = similarities.flatMap { case (i, j, s) =>\n+      if (symmetric && i != j) {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)), Edge(j, i, Seq(s, 0.0, 0.0)))\n+      } else {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)))\n+      }\n+    }\n+\n+    if (normalize) {\n+      val gA = Graph.fromEdges(edges, Seq(0.0))\n+      val vD = gA.aggregateMessages[Seq[Double]](\n+        sendMsg = ctx => {\n+          ctx.sendToSrc(Seq(ctx.attr(0)))\n+        },\n+        mergeMsg = (s1, s2) => Seq(s1(0) + s2(0)),\n+        TripletFields.EdgeOnly)\n+      val normalized = GraphImpl.fromExistingRDDs(vD, gA.edges)\n+        .mapTriplets(\n+          e => {\n+            val s = if (e.srcAttr(0) == 0.0) { e.attr(0) } else { e.attr(0) / e.srcAttr(0) }\n+            Seq(s, 0.0, 0.0)\n+          }, TripletFields.Src)\n+      Graph.fromEdges(normalized.edges, Seq(0.0, 0.0))"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "We can map attributes from `vD`. `fromEdges` needs to re-generate the vertex RDD, which could be expensive.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-04-07T23:15:05Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true\n+   * @return a [[AffinityPropagationModel]] that contains the clustering result\n+   */\n+  def run(similarities: RDD[(Long, Long, Double)], symmetric: Boolean = true)\n+    : AffinityPropagationModel = {\n+    val s = constructGraph(similarities, normalization, symmetric)\n+    ap(s)\n+  }\n+\n+  /**\n+   * Runs the AP algorithm.\n+   *\n+   * @param s The (normalized) similarity matrix, which is the matrix S in the AP paper with vertex\n+   *          similarities and the initial availabilities and responsibilities as its edge\n+   *          properties.\n+   */\n+  private def ap(s: Graph[Seq[Double], Seq[Double]]): AffinityPropagationModel = {\n+    val g = apIter(s, maxIterations, lambda)\n+    chooseExemplars(g)\n+  }\n+}\n+\n+private[clustering] object AffinityPropagation extends Logging {\n+  /**\n+   * Construct the similarity matrix (S) and do normalization if needed.\n+   * Returns the (normalized) similarity matrix (S).\n+   */\n+  def constructGraph(similarities: RDD[(Long, Long, Double)], normalize: Boolean,\n+    symmetric: Boolean):\n+    Graph[Seq[Double], Seq[Double]] = {\n+    val edges = similarities.flatMap { case (i, j, s) =>\n+      if (symmetric && i != j) {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)), Edge(j, i, Seq(s, 0.0, 0.0)))\n+      } else {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)))\n+      }\n+    }\n+\n+    if (normalize) {\n+      val gA = Graph.fromEdges(edges, Seq(0.0))\n+      val vD = gA.aggregateMessages[Seq[Double]](\n+        sendMsg = ctx => {\n+          ctx.sendToSrc(Seq(ctx.attr(0)))\n+        },\n+        mergeMsg = (s1, s2) => Seq(s1(0) + s2(0)),\n+        TripletFields.EdgeOnly)\n+      val normalized = GraphImpl.fromExistingRDDs(vD, gA.edges)\n+        .mapTriplets(\n+          e => {\n+            val s = if (e.srcAttr(0) == 0.0) { e.attr(0) } else { e.attr(0) / e.srcAttr(0) }\n+            Seq(s, 0.0, 0.0)\n+          }, TripletFields.Src)\n+      Graph.fromEdges(normalized.edges, Seq(0.0, 0.0))"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Chop down `lambda`.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:48:11Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true\n+   * @return a [[AffinityPropagationModel]] that contains the clustering result\n+   */\n+  def run(similarities: RDD[(Long, Long, Double)], symmetric: Boolean = true)\n+    : AffinityPropagationModel = {\n+    val s = constructGraph(similarities, normalization, symmetric)\n+    ap(s)\n+  }\n+\n+  /**\n+   * Runs the AP algorithm.\n+   *\n+   * @param s The (normalized) similarity matrix, which is the matrix S in the AP paper with vertex\n+   *          similarities and the initial availabilities and responsibilities as its edge\n+   *          properties.\n+   */\n+  private def ap(s: Graph[Seq[Double], Seq[Double]]): AffinityPropagationModel = {\n+    val g = apIter(s, maxIterations, lambda)\n+    chooseExemplars(g)\n+  }\n+}\n+\n+private[clustering] object AffinityPropagation extends Logging {\n+  /**\n+   * Construct the similarity matrix (S) and do normalization if needed.\n+   * Returns the (normalized) similarity matrix (S).\n+   */\n+  def constructGraph(similarities: RDD[(Long, Long, Double)], normalize: Boolean,\n+    symmetric: Boolean):\n+    Graph[Seq[Double], Seq[Double]] = {\n+    val edges = similarities.flatMap { case (i, j, s) =>\n+      if (symmetric && i != j) {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)), Edge(j, i, Seq(s, 0.0, 0.0)))\n+      } else {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)))\n+      }\n+    }\n+\n+    if (normalize) {\n+      val gA = Graph.fromEdges(edges, Seq(0.0))\n+      val vD = gA.aggregateMessages[Seq[Double]](\n+        sendMsg = ctx => {\n+          ctx.sendToSrc(Seq(ctx.attr(0)))\n+        },\n+        mergeMsg = (s1, s2) => Seq(s1(0) + s2(0)),\n+        TripletFields.EdgeOnly)\n+      val normalized = GraphImpl.fromExistingRDDs(vD, gA.edges)\n+        .mapTriplets(\n+          e => {\n+            val s = if (e.srcAttr(0) == 0.0) { e.attr(0) } else { e.attr(0) / e.srcAttr(0) }\n+            Seq(s, 0.0, 0.0)\n+          }, TripletFields.Src)\n+      Graph.fromEdges(normalized.edges, Seq(0.0, 0.0))\n+    } else {\n+      Graph.fromEdges(edges, Seq(0.0, 0.0))\n+    }\n+  }\n+\n+  /**\n+   * Runs AP's iteration.\n+   * @param g input graph with edges representing the (normalized) similarity matrix (S) and\n+   *          the initial availabilities and responsibilities.\n+   * @param maxIterations maximum number of iterations.\n+   * @return a [[Graph]] representing the final graph.\n+   */\n+  def apIter(\n+      g: Graph[Seq[Double], Seq[Double]],\n+      maxIterations: Int, lambda: Double): Graph[Seq[Double], Seq[Double]] = {"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "same style issus\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:48:13Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true\n+   * @return a [[AffinityPropagationModel]] that contains the clustering result\n+   */\n+  def run(similarities: RDD[(Long, Long, Double)], symmetric: Boolean = true)\n+    : AffinityPropagationModel = {\n+    val s = constructGraph(similarities, normalization, symmetric)\n+    ap(s)\n+  }\n+\n+  /**\n+   * Runs the AP algorithm.\n+   *\n+   * @param s The (normalized) similarity matrix, which is the matrix S in the AP paper with vertex\n+   *          similarities and the initial availabilities and responsibilities as its edge\n+   *          properties.\n+   */\n+  private def ap(s: Graph[Seq[Double], Seq[Double]]): AffinityPropagationModel = {\n+    val g = apIter(s, maxIterations, lambda)\n+    chooseExemplars(g)\n+  }\n+}\n+\n+private[clustering] object AffinityPropagation extends Logging {\n+  /**\n+   * Construct the similarity matrix (S) and do normalization if needed.\n+   * Returns the (normalized) similarity matrix (S).\n+   */\n+  def constructGraph(similarities: RDD[(Long, Long, Double)], normalize: Boolean,\n+    symmetric: Boolean):\n+    Graph[Seq[Double], Seq[Double]] = {\n+    val edges = similarities.flatMap { case (i, j, s) =>\n+      if (symmetric && i != j) {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)), Edge(j, i, Seq(s, 0.0, 0.0)))\n+      } else {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)))\n+      }\n+    }\n+\n+    if (normalize) {\n+      val gA = Graph.fromEdges(edges, Seq(0.0))\n+      val vD = gA.aggregateMessages[Seq[Double]](\n+        sendMsg = ctx => {\n+          ctx.sendToSrc(Seq(ctx.attr(0)))\n+        },\n+        mergeMsg = (s1, s2) => Seq(s1(0) + s2(0)),\n+        TripletFields.EdgeOnly)\n+      val normalized = GraphImpl.fromExistingRDDs(vD, gA.edges)\n+        .mapTriplets(\n+          e => {\n+            val s = if (e.srcAttr(0) == 0.0) { e.attr(0) } else { e.attr(0) / e.srcAttr(0) }\n+            Seq(s, 0.0, 0.0)\n+          }, TripletFields.Src)\n+      Graph.fromEdges(normalized.edges, Seq(0.0, 0.0))\n+    } else {\n+      Graph.fromEdges(edges, Seq(0.0, 0.0))\n+    }\n+  }\n+\n+  /**\n+   * Runs AP's iteration.\n+   * @param g input graph with edges representing the (normalized) similarity matrix (S) and\n+   *          the initial availabilities and responsibilities.\n+   * @param maxIterations maximum number of iterations.\n+   * @return a [[Graph]] representing the final graph.\n+   */\n+  def apIter(\n+      g: Graph[Seq[Double], Seq[Double]],\n+      maxIterations: Int, lambda: Double): Graph[Seq[Double], Seq[Double]] = {\n+    val tol = math.max(1e-5 / g.vertices.count(), 1e-8)\n+    var prevDelta = (Double.MaxValue, Double.MaxValue)\n+    var diffDelta = (Double.MaxValue, Double.MaxValue)\n+    var curG = g\n+    for (iter <- 0 until maxIterations\n+      if math.abs(diffDelta._1) > tol || math.abs(diffDelta._2) > tol) {\n+      val msgPrefix = s\"Iteration $iter\"\n+\n+      // update responsibilities\n+      val vD_r = curG.aggregateMessages[Seq[Double]](\n+        sendMsg = ctx => ctx.sendToSrc(Seq(ctx.attr(0) + ctx.attr(1))),\n+        mergeMsg = _ ++ _,\n+        TripletFields.EdgeOnly)\n+    \n+      val updated_r = GraphImpl.fromExistingRDDs(vD_r, curG.edges)\n+        .mapTriplets("
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "use `filtered :+ (a.attr(0) + e.attr(1)`\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:48:14Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true\n+   * @return a [[AffinityPropagationModel]] that contains the clustering result\n+   */\n+  def run(similarities: RDD[(Long, Long, Double)], symmetric: Boolean = true)\n+    : AffinityPropagationModel = {\n+    val s = constructGraph(similarities, normalization, symmetric)\n+    ap(s)\n+  }\n+\n+  /**\n+   * Runs the AP algorithm.\n+   *\n+   * @param s The (normalized) similarity matrix, which is the matrix S in the AP paper with vertex\n+   *          similarities and the initial availabilities and responsibilities as its edge\n+   *          properties.\n+   */\n+  private def ap(s: Graph[Seq[Double], Seq[Double]]): AffinityPropagationModel = {\n+    val g = apIter(s, maxIterations, lambda)\n+    chooseExemplars(g)\n+  }\n+}\n+\n+private[clustering] object AffinityPropagation extends Logging {\n+  /**\n+   * Construct the similarity matrix (S) and do normalization if needed.\n+   * Returns the (normalized) similarity matrix (S).\n+   */\n+  def constructGraph(similarities: RDD[(Long, Long, Double)], normalize: Boolean,\n+    symmetric: Boolean):\n+    Graph[Seq[Double], Seq[Double]] = {\n+    val edges = similarities.flatMap { case (i, j, s) =>\n+      if (symmetric && i != j) {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)), Edge(j, i, Seq(s, 0.0, 0.0)))\n+      } else {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)))\n+      }\n+    }\n+\n+    if (normalize) {\n+      val gA = Graph.fromEdges(edges, Seq(0.0))\n+      val vD = gA.aggregateMessages[Seq[Double]](\n+        sendMsg = ctx => {\n+          ctx.sendToSrc(Seq(ctx.attr(0)))\n+        },\n+        mergeMsg = (s1, s2) => Seq(s1(0) + s2(0)),\n+        TripletFields.EdgeOnly)\n+      val normalized = GraphImpl.fromExistingRDDs(vD, gA.edges)\n+        .mapTriplets(\n+          e => {\n+            val s = if (e.srcAttr(0) == 0.0) { e.attr(0) } else { e.attr(0) / e.srcAttr(0) }\n+            Seq(s, 0.0, 0.0)\n+          }, TripletFields.Src)\n+      Graph.fromEdges(normalized.edges, Seq(0.0, 0.0))\n+    } else {\n+      Graph.fromEdges(edges, Seq(0.0, 0.0))\n+    }\n+  }\n+\n+  /**\n+   * Runs AP's iteration.\n+   * @param g input graph with edges representing the (normalized) similarity matrix (S) and\n+   *          the initial availabilities and responsibilities.\n+   * @param maxIterations maximum number of iterations.\n+   * @return a [[Graph]] representing the final graph.\n+   */\n+  def apIter(\n+      g: Graph[Seq[Double], Seq[Double]],\n+      maxIterations: Int, lambda: Double): Graph[Seq[Double], Seq[Double]] = {\n+    val tol = math.max(1e-5 / g.vertices.count(), 1e-8)\n+    var prevDelta = (Double.MaxValue, Double.MaxValue)\n+    var diffDelta = (Double.MaxValue, Double.MaxValue)\n+    var curG = g\n+    for (iter <- 0 until maxIterations\n+      if math.abs(diffDelta._1) > tol || math.abs(diffDelta._2) > tol) {\n+      val msgPrefix = s\"Iteration $iter\"\n+\n+      // update responsibilities\n+      val vD_r = curG.aggregateMessages[Seq[Double]](\n+        sendMsg = ctx => ctx.sendToSrc(Seq(ctx.attr(0) + ctx.attr(1))),\n+        mergeMsg = _ ++ _,\n+        TripletFields.EdgeOnly)\n+    \n+      val updated_r = GraphImpl.fromExistingRDDs(vD_r, curG.edges)\n+        .mapTriplets(\n+          (e) => {\n+            val filtered = e.srcAttr.filter(_ != (e.attr(0) + e.attr(1)))\n+            val pool = if (filtered.size < e.srcAttr.size - 1) {\n+              filtered.:+(e.attr(0) + e.attr(1))"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove `{ ... }`\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-02-20T18:48:16Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.Map\n+import scala.collection.mutable.Set\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the vertexIDs of each cluster.\n+ * @param exemplars the vertexIDs of all exemplars.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: Seq[Set[Long]],\n+    val exemplars: Seq[Long]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  def getK(): Int = clusters.size\n+\n+  /**\n+   * Find the cluster the given vertex belongs\n+   */\n+  def findCluster(vertexID: Long): Set[Long] = {\n+    clusters.filter(_.contains(vertexID))(0)\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs\n+   */\n+  def findClusterID(vertexID: Long): Option[Int] = {\n+    var i = 0\n+    clusters.foreach(cluster => {\n+      if (cluster.contains(vertexID)) {\n+        return Some(i)\n+      }\n+      i += i\n+    })\n+    None \n+  } \n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * AffinityPropagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://www.psi.toronto.edu/affinitypropagation/FreyDueckScience07.pdf Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: 0.5,\n+   *    normalization: false}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+ \n+  /**\n+   * Run the AP algorithm.\n+   *\n+   * @param similarities an RDD of (i, j, s,,ij,,) tuples representing the similarity matrix, which\n+   *                     is the matrix S in the AP paper. The similarity s,,ij,, is set to\n+   *                     real-valued similarities. This is not required to be a symmetric matrix \n+   *                     and hence s,,ij,, can be not equal to s,,ji,,. Tuples with i = j are\n+   *                     referred to as \"preferences\" in the AP paper. The data points with larger\n+   *                     values of s,,ii,, are more likely to be chosen as exemplars.\n+   *\n+   * @param symmetric the given similarity matrix is symmetric or not. Default value: true\n+   * @return a [[AffinityPropagationModel]] that contains the clustering result\n+   */\n+  def run(similarities: RDD[(Long, Long, Double)], symmetric: Boolean = true)\n+    : AffinityPropagationModel = {\n+    val s = constructGraph(similarities, normalization, symmetric)\n+    ap(s)\n+  }\n+\n+  /**\n+   * Runs the AP algorithm.\n+   *\n+   * @param s The (normalized) similarity matrix, which is the matrix S in the AP paper with vertex\n+   *          similarities and the initial availabilities and responsibilities as its edge\n+   *          properties.\n+   */\n+  private def ap(s: Graph[Seq[Double], Seq[Double]]): AffinityPropagationModel = {\n+    val g = apIter(s, maxIterations, lambda)\n+    chooseExemplars(g)\n+  }\n+}\n+\n+private[clustering] object AffinityPropagation extends Logging {\n+  /**\n+   * Construct the similarity matrix (S) and do normalization if needed.\n+   * Returns the (normalized) similarity matrix (S).\n+   */\n+  def constructGraph(similarities: RDD[(Long, Long, Double)], normalize: Boolean,\n+    symmetric: Boolean):\n+    Graph[Seq[Double], Seq[Double]] = {\n+    val edges = similarities.flatMap { case (i, j, s) =>\n+      if (symmetric && i != j) {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)), Edge(j, i, Seq(s, 0.0, 0.0)))\n+      } else {\n+        Seq(Edge(i, j, Seq(s, 0.0, 0.0)))\n+      }\n+    }\n+\n+    if (normalize) {\n+      val gA = Graph.fromEdges(edges, Seq(0.0))\n+      val vD = gA.aggregateMessages[Seq[Double]](\n+        sendMsg = ctx => {\n+          ctx.sendToSrc(Seq(ctx.attr(0)))\n+        },\n+        mergeMsg = (s1, s2) => Seq(s1(0) + s2(0)),\n+        TripletFields.EdgeOnly)\n+      val normalized = GraphImpl.fromExistingRDDs(vD, gA.edges)\n+        .mapTriplets(\n+          e => {\n+            val s = if (e.srcAttr(0) == 0.0) { e.attr(0) } else { e.attr(0) / e.srcAttr(0) }\n+            Seq(s, 0.0, 0.0)\n+          }, TripletFields.Src)\n+      Graph.fromEdges(normalized.edges, Seq(0.0, 0.0))\n+    } else {\n+      Graph.fromEdges(edges, Seq(0.0, 0.0))\n+    }\n+  }\n+\n+  /**\n+   * Runs AP's iteration.\n+   * @param g input graph with edges representing the (normalized) similarity matrix (S) and\n+   *          the initial availabilities and responsibilities.\n+   * @param maxIterations maximum number of iterations.\n+   * @return a [[Graph]] representing the final graph.\n+   */\n+  def apIter(\n+      g: Graph[Seq[Double], Seq[Double]],\n+      maxIterations: Int, lambda: Double): Graph[Seq[Double], Seq[Double]] = {\n+    val tol = math.max(1e-5 / g.vertices.count(), 1e-8)\n+    var prevDelta = (Double.MaxValue, Double.MaxValue)\n+    var diffDelta = (Double.MaxValue, Double.MaxValue)\n+    var curG = g\n+    for (iter <- 0 until maxIterations\n+      if math.abs(diffDelta._1) > tol || math.abs(diffDelta._2) > tol) {\n+      val msgPrefix = s\"Iteration $iter\"\n+\n+      // update responsibilities\n+      val vD_r = curG.aggregateMessages[Seq[Double]](\n+        sendMsg = ctx => ctx.sendToSrc(Seq(ctx.attr(0) + ctx.attr(1))),\n+        mergeMsg = _ ++ _,\n+        TripletFields.EdgeOnly)\n+    \n+      val updated_r = GraphImpl.fromExistingRDDs(vD_r, curG.edges)\n+        .mapTriplets(\n+          (e) => {\n+            val filtered = e.srcAttr.filter(_ != (e.attr(0) + e.attr(1)))\n+            val pool = if (filtered.size < e.srcAttr.size - 1) {\n+              filtered.:+(e.attr(0) + e.attr(1))\n+            } else {\n+              filtered\n+            }\n+            val maxValue = if (pool.isEmpty) { 0.0 } else { pool.max }"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Is it reasonable to assume that `k` is small but the number of vertices are large? Then storing members as `Array[Long]` may run out of memory. We can store `id` and `exemplar` and the driver and cluster assignments distributively as `RDD[(Long, Int)]` (vertex id, cluster id). Lookup becomes less expensive in this setup. Or we can store `(id, exemplar)` as an RDD too, which may not be necessary.\n\nBtw, is it sufficient to use `Int` for cluster id? It won't provide much information if AP outputs more than `Int.MaxValue` clusters.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-05T15:08:14Z",
    "diffHunk": "@@ -0,0 +1,475 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster members.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`getK` -> `k`\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-05T15:32:02Z",
    "diffHunk": "@@ -0,0 +1,475 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster members.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the clusters of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: RDD[AffinityPropagationCluster]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  lazy val getK: Long = clusters.count()"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "This could be simplified if we store cluster assignment in `RDD[(Long, Int)]`.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-05T15:33:14Z",
    "diffHunk": "@@ -0,0 +1,475 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster members.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the clusters of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: RDD[AffinityPropagationCluster]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  lazy val getK: Long = clusters.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[Array]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): Array[Long] = {\n+    val cluster = clusters.filter(_.members.contains(vertexID)).collect()\n+    if (cluster.nonEmpty) {\n+      cluster(0).members\n+    } else {\n+      null\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {",
    "line": 91
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove `()`\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-05T15:52:45Z",
    "diffHunk": "@@ -0,0 +1,475 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster members.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the clusters of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: RDD[AffinityPropagationCluster]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  lazy val getK: Long = clusters.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[Array]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): Array[Long] = {\n+    val cluster = clusters.filter(_.members.contains(vertexID)).collect()\n+    if (cluster.nonEmpty) {\n+      cluster(0).members\n+    } else {\n+      null\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val clusterIds = clusters.flatMap { cluster =>\n+      if (cluster.members.contains(vertexID)) {\n+        Seq(cluster.id)\n+      } else {\n+        Seq()\n+      }\n+    }.collect()\n+    if (clusterIds.nonEmpty) {\n+      clusterIds(0)\n+    } else {\n+      -1\n+    }\n+  } \n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Is it needed?\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-05T15:53:03Z",
    "diffHunk": "@@ -0,0 +1,475 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster members.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the clusters of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: RDD[AffinityPropagationCluster]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  lazy val getK: Long = clusters.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[Array]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): Array[Long] = {\n+    val cluster = clusters.filter(_.members.contains(vertexID)).collect()\n+    if (cluster.nonEmpty) {\n+      cluster(0).members\n+    } else {\n+      null\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val clusterIds = clusters.flatMap { cluster =>\n+      if (cluster.members.contains(vertexID)) {\n+        Seq(cluster.id)\n+      } else {\n+        Seq()\n+      }\n+    }.collect()\n+    if (clusterIds.nonEmpty) {\n+      clusterIds(0)\n+    } else {\n+      -1\n+    }\n+  } \n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+\n+  /**\n+   * Set whether the input similarities are symmetric or not.\n+   * When symmetric is set to true, we assume that input similarities only contain triangular\n+   * matrix. That means, only s,,ij,, is included in the similarities. If both s,,ij,, and\n+   * s,,ji,, are given in the similarities, it very possibly causes error.\n+   */\n+  def setSymmetric(symmetric: Boolean): this.type = {\n+    this.symmetric = symmetric\n+    this\n+  }\n+\n+  /**\n+   * Get whether the input similarities are symmetric or not\n+   */\n+  def getSymmetric(): Boolean = {\n+    this.symmetric\n+  }\n+\n+  /**\n+   * Calculate the median value of similarities\n+   */\n+  private def getMedian(similarities: RDD[(Long, Long, Double)]): Double = {\n+    import org.apache.spark.SparkContext._"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "No more. Removed it.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-06T16:49:41Z",
    "diffHunk": "@@ -0,0 +1,475 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster members.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the clusters of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: RDD[AffinityPropagationCluster]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  lazy val getK: Long = clusters.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[Array]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): Array[Long] = {\n+    val cluster = clusters.filter(_.members.contains(vertexID)).collect()\n+    if (cluster.nonEmpty) {\n+      cluster(0).members\n+    } else {\n+      null\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val clusterIds = clusters.flatMap { cluster =>\n+      if (cluster.members.contains(vertexID)) {\n+        Seq(cluster.id)\n+      } else {\n+        Seq()\n+      }\n+    }.collect()\n+    if (clusterIds.nonEmpty) {\n+      clusterIds(0)\n+    } else {\n+      -1\n+    }\n+  } \n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+\n+  /**\n+   * Set whether the input similarities are symmetric or not.\n+   * When symmetric is set to true, we assume that input similarities only contain triangular\n+   * matrix. That means, only s,,ij,, is included in the similarities. If both s,,ij,, and\n+   * s,,ji,, are given in the similarities, it very possibly causes error.\n+   */\n+  def setSymmetric(symmetric: Boolean): this.type = {\n+    this.symmetric = symmetric\n+    this\n+  }\n+\n+  /**\n+   * Get whether the input similarities are symmetric or not\n+   */\n+  def getSymmetric(): Boolean = {\n+    this.symmetric\n+  }\n+\n+  /**\n+   * Calculate the median value of similarities\n+   */\n+  private def getMedian(similarities: RDD[(Long, Long, Double)]): Double = {\n+    import org.apache.spark.SparkContext._"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "Do you want to persistence the sorted RDD, and unpersistence later after you find the median? I will add type of `sorted` to explicitly indicate it's RDD. PS, `val count = sorted.count()` can be used to persistence the sorted RDD. \n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-06T07:21:40Z",
    "diffHunk": "@@ -0,0 +1,475 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster members.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the clusters of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: RDD[AffinityPropagationCluster]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  lazy val getK: Long = clusters.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[Array]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): Array[Long] = {\n+    val cluster = clusters.filter(_.members.contains(vertexID)).collect()\n+    if (cluster.nonEmpty) {\n+      cluster(0).members\n+    } else {\n+      null\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val clusterIds = clusters.flatMap { cluster =>\n+      if (cluster.members.contains(vertexID)) {\n+        Seq(cluster.id)\n+      } else {\n+        Seq()\n+      }\n+    }.collect()\n+    if (clusterIds.nonEmpty) {\n+      clusterIds(0)\n+    } else {\n+      -1\n+    }\n+  } \n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+\n+  /**\n+   * Set whether the input similarities are symmetric or not.\n+   * When symmetric is set to true, we assume that input similarities only contain triangular\n+   * matrix. That means, only s,,ij,, is included in the similarities. If both s,,ij,, and\n+   * s,,ji,, are given in the similarities, it very possibly causes error.\n+   */\n+  def setSymmetric(symmetric: Boolean): this.type = {\n+    this.symmetric = symmetric\n+    this\n+  }\n+\n+  /**\n+   * Get whether the input similarities are symmetric or not\n+   */\n+  def getSymmetric(): Boolean = {\n+    this.symmetric\n+  }\n+\n+  /**\n+   * Calculate the median value of similarities\n+   */\n+  private def getMedian(similarities: RDD[(Long, Long, Double)]): Double = {\n+    import org.apache.spark.SparkContext._\n+\n+    val sorted = similarities.sortBy(_._3).zipWithIndex().map {\n+      case (v, idx) => (idx, v)\n+    }"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Because the sorted RDD is used for one time only to get the median, I think it doesn't need to be persisted?\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-06T16:48:46Z",
    "diffHunk": "@@ -0,0 +1,475 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster members.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the clusters of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: RDD[AffinityPropagationCluster]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  lazy val getK: Long = clusters.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[Array]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): Array[Long] = {\n+    val cluster = clusters.filter(_.members.contains(vertexID)).collect()\n+    if (cluster.nonEmpty) {\n+      cluster(0).members\n+    } else {\n+      null\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val clusterIds = clusters.flatMap { cluster =>\n+      if (cluster.members.contains(vertexID)) {\n+        Seq(cluster.id)\n+      } else {\n+        Seq()\n+      }\n+    }.collect()\n+    if (clusterIds.nonEmpty) {\n+      clusterIds(0)\n+    } else {\n+      -1\n+    }\n+  } \n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+\n+  /**\n+   * Set whether the input similarities are symmetric or not.\n+   * When symmetric is set to true, we assume that input similarities only contain triangular\n+   * matrix. That means, only s,,ij,, is included in the similarities. If both s,,ij,, and\n+   * s,,ji,, are given in the similarities, it very possibly causes error.\n+   */\n+  def setSymmetric(symmetric: Boolean): this.type = {\n+    this.symmetric = symmetric\n+    this\n+  }\n+\n+  /**\n+   * Get whether the input similarities are symmetric or not\n+   */\n+  def getSymmetric(): Boolean = {\n+    this.symmetric\n+  }\n+\n+  /**\n+   * Calculate the median value of similarities\n+   */\n+  private def getMedian(similarities: RDD[(Long, Long, Double)]): Double = {\n+    import org.apache.spark.SparkContext._\n+\n+    val sorted = similarities.sortBy(_._3).zipWithIndex().map {\n+      case (v, idx) => (idx, v)\n+    }"
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "You materialized sorted RDD once when you do `sorted.count()`, and then materialized it again when you do the lookup.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-06T16:59:49Z",
    "diffHunk": "@@ -0,0 +1,475 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster members.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the clusters of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: RDD[AffinityPropagationCluster]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  lazy val getK: Long = clusters.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[Array]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): Array[Long] = {\n+    val cluster = clusters.filter(_.members.contains(vertexID)).collect()\n+    if (cluster.nonEmpty) {\n+      cluster(0).members\n+    } else {\n+      null\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val clusterIds = clusters.flatMap { cluster =>\n+      if (cluster.members.contains(vertexID)) {\n+        Seq(cluster.id)\n+      } else {\n+        Seq()\n+      }\n+    }.collect()\n+    if (clusterIds.nonEmpty) {\n+      clusterIds(0)\n+    } else {\n+      -1\n+    }\n+  } \n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+\n+  /**\n+   * Set whether the input similarities are symmetric or not.\n+   * When symmetric is set to true, we assume that input similarities only contain triangular\n+   * matrix. That means, only s,,ij,, is included in the similarities. If both s,,ij,, and\n+   * s,,ji,, are given in the similarities, it very possibly causes error.\n+   */\n+  def setSymmetric(symmetric: Boolean): this.type = {\n+    this.symmetric = symmetric\n+    this\n+  }\n+\n+  /**\n+   * Get whether the input similarities are symmetric or not\n+   */\n+  def getSymmetric(): Boolean = {\n+    this.symmetric\n+  }\n+\n+  /**\n+   * Calculate the median value of similarities\n+   */\n+  private def getMedian(similarities: RDD[(Long, Long, Double)]): Double = {\n+    import org.apache.spark.SparkContext._\n+\n+    val sorted = similarities.sortBy(_._3).zipWithIndex().map {\n+      case (v, idx) => (idx, v)\n+    }"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Ah, right. Update later.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-06T17:15:05Z",
    "diffHunk": "@@ -0,0 +1,475 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster members.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the clusters of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: RDD[AffinityPropagationCluster]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  lazy val getK: Long = clusters.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[Array]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): Array[Long] = {\n+    val cluster = clusters.filter(_.members.contains(vertexID)).collect()\n+    if (cluster.nonEmpty) {\n+      cluster(0).members\n+    } else {\n+      null\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val clusterIds = clusters.flatMap { cluster =>\n+      if (cluster.members.contains(vertexID)) {\n+        Seq(cluster.id)\n+      } else {\n+        Seq()\n+      }\n+    }.collect()\n+    if (clusterIds.nonEmpty) {\n+      clusterIds(0)\n+    } else {\n+      -1\n+    }\n+  } \n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations(): Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+\n+  /**\n+   * Set whether the input similarities are symmetric or not.\n+   * When symmetric is set to true, we assume that input similarities only contain triangular\n+   * matrix. That means, only s,,ij,, is included in the similarities. If both s,,ij,, and\n+   * s,,ji,, are given in the similarities, it very possibly causes error.\n+   */\n+  def setSymmetric(symmetric: Boolean): this.type = {\n+    this.symmetric = symmetric\n+    this\n+  }\n+\n+  /**\n+   * Get whether the input similarities are symmetric or not\n+   */\n+  def getSymmetric(): Boolean = {\n+    this.symmetric\n+  }\n+\n+  /**\n+   * Calculate the median value of similarities\n+   */\n+  private def getMedian(similarities: RDD[(Long, Long, Double)]): Double = {\n+    import org.apache.spark.SparkContext._\n+\n+    val sorted = similarities.sortBy(_._3).zipWithIndex().map {\n+      case (v, idx) => (idx, v)\n+    }"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": " I have the same concern @mengxr has. Can that particular cluster contains lots of vertexes that run out of memory?  This can return `RDD[Long]`, but with data structure of `(vertex id, cluster id)`, it seems it requires two passes.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-06T07:21:46Z",
    "diffHunk": "@@ -0,0 +1,475 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster members.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param clusters the clusters of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val clusters: RDD[AffinityPropagationCluster]) extends Serializable {\n+\n+  /**\n+   * Set the number of clusters\n+   */\n+  lazy val getK: Long = clusters.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[Array]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): Array[Long] = {"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "persist(StorageLevel.MEMORY_AND_DISK)\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-06T19:08:50Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param member cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationAssignment(val id: Long, val exemplar: Long, val member: Long)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param assignments the cluster assignments of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val assignments: RDD[AffinityPropagationAssignment]) extends Serializable {\n+\n+  /**\n+   * Get the number of clusters\n+   */\n+  lazy val k: Long = assignments.map(_.id).distinct.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[RDD]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): RDD[Long] = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assignments.filter(_.id == assign(0).id).map(_.member)\n+    } else {\n+      assignments.sparkContext.emptyRDD[Long]\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assign(0).id\n+    } else {\n+      -1\n+    }\n+  } \n+\n+  /**\n+   * Turn cluster assignments to cluster representations [[AffinityPropagationCluster]].\n+   * @return a [[RDD]] that contains all clusters generated by Affinity Propagation. Because the\n+   * cluster members in [[AffinityPropagationCluster]] is an [[Array]], it could consume too much\n+   * memory even run out of memory when you call collect() on the returned [[RDD]].\n+   */\n+  def fromAssignToClusters(): RDD[AffinityPropagationCluster] = {\n+    assignments.map { assign => ((assign.id, assign.exemplar), assign.member) }\n+      .aggregateByKey(mutable.Set[Long]())(\n+        seqOp = (s, d) => s ++ mutable.Set(d),\n+        combOp = (s1, s2) => s1 ++ s2\n+      ).map(kv => new AffinityPropagationCluster(kv._1._1, kv._1._2, kv._2.toArray))\n+  }\n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations: Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+\n+  /**\n+   * Set whether the input similarities are symmetric or not.\n+   * When symmetric is set to true, we assume that input similarities only contain triangular\n+   * matrix. That means, only s,,ij,, is included in the similarities. If both s,,ij,, and\n+   * s,,ji,, are given in the similarities, it very possibly causes error.\n+   */\n+  def setSymmetric(symmetric: Boolean): this.type = {\n+    this.symmetric = symmetric\n+    this\n+  }\n+\n+  /**\n+   * Get whether the input similarities are symmetric or not\n+   */\n+  def getSymmetric(): Boolean = {\n+    this.symmetric\n+  }\n+\n+  /**\n+   * Calculate the median value of similarities\n+   */\n+  private def getMedian(similarities: RDD[(Long, Long, Double)]): Double = {\n+    val sorted: RDD[(Long, Double)] = similarities.sortBy(_._3).zipWithIndex().map {\n+      case (v, idx) => (idx, v._3)\n+    }.persist()\n+"
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "PS, I don't know if we can use approximated Median here since sort is very expensive.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-06T19:10:06Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param member cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationAssignment(val id: Long, val exemplar: Long, val member: Long)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param assignments the cluster assignments of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val assignments: RDD[AffinityPropagationAssignment]) extends Serializable {\n+\n+  /**\n+   * Get the number of clusters\n+   */\n+  lazy val k: Long = assignments.map(_.id).distinct.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[RDD]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): RDD[Long] = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assignments.filter(_.id == assign(0).id).map(_.member)\n+    } else {\n+      assignments.sparkContext.emptyRDD[Long]\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assign(0).id\n+    } else {\n+      -1\n+    }\n+  } \n+\n+  /**\n+   * Turn cluster assignments to cluster representations [[AffinityPropagationCluster]].\n+   * @return a [[RDD]] that contains all clusters generated by Affinity Propagation. Because the\n+   * cluster members in [[AffinityPropagationCluster]] is an [[Array]], it could consume too much\n+   * memory even run out of memory when you call collect() on the returned [[RDD]].\n+   */\n+  def fromAssignToClusters(): RDD[AffinityPropagationCluster] = {\n+    assignments.map { assign => ((assign.id, assign.exemplar), assign.member) }\n+      .aggregateByKey(mutable.Set[Long]())(\n+        seqOp = (s, d) => s ++ mutable.Set(d),\n+        combOp = (s1, s2) => s1 ++ s2\n+      ).map(kv => new AffinityPropagationCluster(kv._1._1, kv._1._2, kv._2.toArray))\n+  }\n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations: Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+\n+  /**\n+   * Set whether the input similarities are symmetric or not.\n+   * When symmetric is set to true, we assume that input similarities only contain triangular\n+   * matrix. That means, only s,,ij,, is included in the similarities. If both s,,ij,, and\n+   * s,,ji,, are given in the similarities, it very possibly causes error.\n+   */\n+  def setSymmetric(symmetric: Boolean): this.type = {\n+    this.symmetric = symmetric\n+    this\n+  }\n+\n+  /**\n+   * Get whether the input similarities are symmetric or not\n+   */\n+  def getSymmetric(): Boolean = {\n+    this.symmetric\n+  }\n+\n+  /**\n+   * Calculate the median value of similarities\n+   */\n+  private def getMedian(similarities: RDD[(Long, Long, Double)]): Double = {\n+    val sorted: RDD[(Long, Double)] = similarities.sortBy(_._3).zipWithIndex().map {\n+      case (v, idx) => (idx, v._3)\n+    }.persist()\n+"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Approximated median might be good to reduce computation time. But looks like there is no corresponding algorithm in Spark? I just saw approximated mean but no approximated median. Maybe we can use it.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-07T18:50:36Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param member cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationAssignment(val id: Long, val exemplar: Long, val member: Long)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param assignments the cluster assignments of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val assignments: RDD[AffinityPropagationAssignment]) extends Serializable {\n+\n+  /**\n+   * Get the number of clusters\n+   */\n+  lazy val k: Long = assignments.map(_.id).distinct.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[RDD]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): RDD[Long] = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assignments.filter(_.id == assign(0).id).map(_.member)\n+    } else {\n+      assignments.sparkContext.emptyRDD[Long]\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assign(0).id\n+    } else {\n+      -1\n+    }\n+  } \n+\n+  /**\n+   * Turn cluster assignments to cluster representations [[AffinityPropagationCluster]].\n+   * @return a [[RDD]] that contains all clusters generated by Affinity Propagation. Because the\n+   * cluster members in [[AffinityPropagationCluster]] is an [[Array]], it could consume too much\n+   * memory even run out of memory when you call collect() on the returned [[RDD]].\n+   */\n+  def fromAssignToClusters(): RDD[AffinityPropagationCluster] = {\n+    assignments.map { assign => ((assign.id, assign.exemplar), assign.member) }\n+      .aggregateByKey(mutable.Set[Long]())(\n+        seqOp = (s, d) => s ++ mutable.Set(d),\n+        combOp = (s1, s2) => s1 ++ s2\n+      ).map(kv => new AffinityPropagationCluster(kv._1._1, kv._1._2, kv._2.toArray))\n+  }\n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations: Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+\n+  /**\n+   * Set whether the input similarities are symmetric or not.\n+   * When symmetric is set to true, we assume that input similarities only contain triangular\n+   * matrix. That means, only s,,ij,, is included in the similarities. If both s,,ij,, and\n+   * s,,ji,, are given in the similarities, it very possibly causes error.\n+   */\n+  def setSymmetric(symmetric: Boolean): this.type = {\n+    this.symmetric = symmetric\n+    this\n+  }\n+\n+  /**\n+   * Get whether the input similarities are symmetric or not\n+   */\n+  def getSymmetric(): Boolean = {\n+    this.symmetric\n+  }\n+\n+  /**\n+   * Calculate the median value of similarities\n+   */\n+  private def getMedian(similarities: RDD[(Long, Long, Double)]): Double = {\n+    val sorted: RDD[(Long, Double)] = similarities.sortBy(_._3).zipWithIndex().map {\n+      case (v, idx) => (idx, v._3)\n+    }.persist()\n+"
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "duncanfinney"
    },
    "body": "This is going to do a lookup for a fraction because `(count % 2 !== 0)`. Is that how it's supposed to work? \n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-07T04:18:40Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param member cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationAssignment(val id: Long, val exemplar: Long, val member: Long)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param assignments the cluster assignments of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val assignments: RDD[AffinityPropagationAssignment]) extends Serializable {\n+\n+  /**\n+   * Get the number of clusters\n+   */\n+  lazy val k: Long = assignments.map(_.id).distinct.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[RDD]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): RDD[Long] = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assignments.filter(_.id == assign(0).id).map(_.member)\n+    } else {\n+      assignments.sparkContext.emptyRDD[Long]\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assign(0).id\n+    } else {\n+      -1\n+    }\n+  } \n+\n+  /**\n+   * Turn cluster assignments to cluster representations [[AffinityPropagationCluster]].\n+   * @return a [[RDD]] that contains all clusters generated by Affinity Propagation. Because the\n+   * cluster members in [[AffinityPropagationCluster]] is an [[Array]], it could consume too much\n+   * memory even run out of memory when you call collect() on the returned [[RDD]].\n+   */\n+  def fromAssignToClusters(): RDD[AffinityPropagationCluster] = {\n+    assignments.map { assign => ((assign.id, assign.exemplar), assign.member) }\n+      .aggregateByKey(mutable.Set[Long]())(\n+        seqOp = (s, d) => s ++ mutable.Set(d),\n+        combOp = (s1, s2) => s1 ++ s2\n+      ).map(kv => new AffinityPropagationCluster(kv._1._1, kv._1._2, kv._2.toArray))\n+  }\n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations: Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+\n+  /**\n+   * Set whether the input similarities are symmetric or not.\n+   * When symmetric is set to true, we assume that input similarities only contain triangular\n+   * matrix. That means, only s,,ij,, is included in the similarities. If both s,,ij,, and\n+   * s,,ji,, are given in the similarities, it very possibly causes error.\n+   */\n+  def setSymmetric(symmetric: Boolean): this.type = {\n+    this.symmetric = symmetric\n+    this\n+  }\n+\n+  /**\n+   * Get whether the input similarities are symmetric or not\n+   */\n+  def getSymmetric(): Boolean = {\n+    this.symmetric\n+  }\n+\n+  /**\n+   * Calculate the median value of similarities\n+   */\n+  private def getMedian(similarities: RDD[(Long, Long, Double)]): Double = {\n+    val sorted: RDD[(Long, Double)] = similarities.sortBy(_._3).zipWithIndex().map {\n+      case (v, idx) => (idx, v._3)\n+    }.persist()\n+\n+    val count = sorted.count()\n+\n+    val median: Double =\n+      if (count % 2 == 0) {\n+        val l = count / 2 - 1\n+        val r = l + 1\n+        (sorted.lookup(l).head + sorted.lookup(r).head).toDouble / 2\n+      } else {\n+        sorted.lookup(count / 2).head",
    "line": 246
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "We will get the largest integer less than or equal to the fraction, i.e., the result of `Math.floor()`.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-07T18:56:08Z",
    "diffHunk": "@@ -0,0 +1,496 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param member cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationAssignment(val id: Long, val exemplar: Long, val member: Long)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param assignments the cluster assignments of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val assignments: RDD[AffinityPropagationAssignment]) extends Serializable {\n+\n+  /**\n+   * Get the number of clusters\n+   */\n+  lazy val k: Long = assignments.map(_.id).distinct.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[RDD]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): RDD[Long] = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assignments.filter(_.id == assign(0).id).map(_.member)\n+    } else {\n+      assignments.sparkContext.emptyRDD[Long]\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assign(0).id\n+    } else {\n+      -1\n+    }\n+  } \n+\n+  /**\n+   * Turn cluster assignments to cluster representations [[AffinityPropagationCluster]].\n+   * @return a [[RDD]] that contains all clusters generated by Affinity Propagation. Because the\n+   * cluster members in [[AffinityPropagationCluster]] is an [[Array]], it could consume too much\n+   * memory even run out of memory when you call collect() on the returned [[RDD]].\n+   */\n+  def fromAssignToClusters(): RDD[AffinityPropagationCluster] = {\n+    assignments.map { assign => ((assign.id, assign.exemplar), assign.member) }\n+      .aggregateByKey(mutable.Set[Long]())(\n+        seqOp = (s, d) => s ++ mutable.Set(d),\n+        combOp = (s1, s2) => s1 ++ s2\n+      ).map(kv => new AffinityPropagationCluster(kv._1._1, kv._1._2, kv._2.toArray))\n+  }\n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations: Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+\n+  /**\n+   * Set whether the input similarities are symmetric or not.\n+   * When symmetric is set to true, we assume that input similarities only contain triangular\n+   * matrix. That means, only s,,ij,, is included in the similarities. If both s,,ij,, and\n+   * s,,ji,, are given in the similarities, it very possibly causes error.\n+   */\n+  def setSymmetric(symmetric: Boolean): this.type = {\n+    this.symmetric = symmetric\n+    this\n+  }\n+\n+  /**\n+   * Get whether the input similarities are symmetric or not\n+   */\n+  def getSymmetric(): Boolean = {\n+    this.symmetric\n+  }\n+\n+  /**\n+   * Calculate the median value of similarities\n+   */\n+  private def getMedian(similarities: RDD[(Long, Long, Double)]): Double = {\n+    val sorted: RDD[(Long, Double)] = similarities.sortBy(_._3).zipWithIndex().map {\n+      case (v, idx) => (idx, v._3)\n+    }.persist()\n+\n+    val count = sorted.count()\n+\n+    val median: Double =\n+      if (count % 2 == 0) {\n+        val l = count / 2 - 1\n+        val r = l + 1\n+        (sorted.lookup(l).head + sorted.lookup(r).head).toDouble / 2\n+      } else {\n+        sorted.lookup(count / 2).head",
    "line": 246
  }],
  "prId": 4622
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "I still think this will not work well in real big data setting. Doing a total sort will require a lot of shuffle and will be extremely slow. \n\nI will recommend that we implement the streaming quantiles and median which computes an estimate of the median but has the benefit of not requiring the data to be sorted in `org.apache.spark.util.StatCounter` first. \n\nMahout and pig datafu have this implementation. We may port the logic to Spark. Please open a JIRA for this.\nhttp://datafu.incubator.apache.org/docs/datafu/getting-started.html\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-07T21:17:31Z",
    "diffHunk": "@@ -0,0 +1,497 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param member cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationAssignment(val id: Long, val exemplar: Long, val member: Long)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param assignments the cluster assignments of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val assignments: RDD[AffinityPropagationAssignment]) extends Serializable {\n+\n+  /**\n+   * Get the number of clusters\n+   */\n+  lazy val k: Long = assignments.map(_.id).distinct.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[RDD]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): RDD[Long] = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assignments.filter(_.id == assign(0).id).map(_.member)\n+    } else {\n+      assignments.sparkContext.emptyRDD[Long]\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assign(0).id\n+    } else {\n+      -1\n+    }\n+  } \n+\n+  /**\n+   * Turn cluster assignments to cluster representations [[AffinityPropagationCluster]].\n+   * @return a [[RDD]] that contains all clusters generated by Affinity Propagation. Because the\n+   * cluster members in [[AffinityPropagationCluster]] is an [[Array]], it could consume too much\n+   * memory even run out of memory when you call collect() on the returned [[RDD]].\n+   */\n+  def fromAssignToClusters(): RDD[AffinityPropagationCluster] = {\n+    assignments.map { assign => ((assign.id, assign.exemplar), assign.member) }\n+      .aggregateByKey(mutable.Set[Long]())(\n+        seqOp = (s, d) => s ++ mutable.Set(d),\n+        combOp = (s1, s2) => s1 ++ s2\n+      ).map(kv => new AffinityPropagationCluster(kv._1._1, kv._1._2, kv._2.toArray))\n+  }\n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations: Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+\n+  /**\n+   * Set whether the input similarities are symmetric or not.\n+   * When symmetric is set to true, we assume that input similarities only contain triangular\n+   * matrix. That means, only s,,ij,, is included in the similarities. If both s,,ij,, and\n+   * s,,ji,, are given in the similarities, it very possibly causes error.\n+   */\n+  def setSymmetric(symmetric: Boolean): this.type = {\n+    this.symmetric = symmetric\n+    this\n+  }\n+\n+  /**\n+   * Get whether the input similarities are symmetric or not\n+   */\n+  def getSymmetric(): Boolean = {\n+    this.symmetric\n+  }\n+\n+  /**\n+   * Calculate the median value of similarities\n+   */\n+  private def getMedian(similarities: RDD[(Long, Long, Double)]): Double = {\n+    val sorted: RDD[(Long, Double)] = similarities.sortBy(_._3).zipWithIndex().map {\n+      case (v, idx) => (idx, v._3)\n+    }.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val count = sorted.count()\n+",
    "line": 239
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "@dbtsai Thanks for suggesting the information. I open the JIRA at https://issues.apache.org/jira/browse/SPARK-7486. I will take a look at the implementation of datafu.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-08T15:28:27Z",
    "diffHunk": "@@ -0,0 +1,497 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param member cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationAssignment(val id: Long, val exemplar: Long, val member: Long)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param assignments the cluster assignments of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val assignments: RDD[AffinityPropagationAssignment]) extends Serializable {\n+\n+  /**\n+   * Get the number of clusters\n+   */\n+  lazy val k: Long = assignments.map(_.id).distinct.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[RDD]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): RDD[Long] = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assignments.filter(_.id == assign(0).id).map(_.member)\n+    } else {\n+      assignments.sparkContext.emptyRDD[Long]\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assign(0).id\n+    } else {\n+      -1\n+    }\n+  } \n+\n+  /**\n+   * Turn cluster assignments to cluster representations [[AffinityPropagationCluster]].\n+   * @return a [[RDD]] that contains all clusters generated by Affinity Propagation. Because the\n+   * cluster members in [[AffinityPropagationCluster]] is an [[Array]], it could consume too much\n+   * memory even run out of memory when you call collect() on the returned [[RDD]].\n+   */\n+  def fromAssignToClusters(): RDD[AffinityPropagationCluster] = {\n+    assignments.map { assign => ((assign.id, assign.exemplar), assign.member) }\n+      .aggregateByKey(mutable.Set[Long]())(\n+        seqOp = (s, d) => s ++ mutable.Set(d),\n+        combOp = (s1, s2) => s1 ++ s2\n+      ).map(kv => new AffinityPropagationCluster(kv._1._1, kv._1._2, kv._2.toArray))\n+  }\n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations: Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+\n+  /**\n+   * Set whether the input similarities are symmetric or not.\n+   * When symmetric is set to true, we assume that input similarities only contain triangular\n+   * matrix. That means, only s,,ij,, is included in the similarities. If both s,,ij,, and\n+   * s,,ji,, are given in the similarities, it very possibly causes error.\n+   */\n+  def setSymmetric(symmetric: Boolean): this.type = {\n+    this.symmetric = symmetric\n+    this\n+  }\n+\n+  /**\n+   * Get whether the input similarities are symmetric or not\n+   */\n+  def getSymmetric(): Boolean = {\n+    this.symmetric\n+  }\n+\n+  /**\n+   * Calculate the median value of similarities\n+   */\n+  private def getMedian(similarities: RDD[(Long, Long, Double)]): Double = {\n+    val sorted: RDD[(Long, Double)] = similarities.sortBy(_._3).zipWithIndex().map {\n+      case (v, idx) => (idx, v._3)\n+    }.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val count = sorted.count()\n+",
    "line": 239
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "@dbtsai I submit the support of approximate quantile at #6042. It can be used to find approximate median by this pr.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-11T08:32:41Z",
    "diffHunk": "@@ -0,0 +1,497 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param member cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationAssignment(val id: Long, val exemplar: Long, val member: Long)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param assignments the cluster assignments of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val assignments: RDD[AffinityPropagationAssignment]) extends Serializable {\n+\n+  /**\n+   * Get the number of clusters\n+   */\n+  lazy val k: Long = assignments.map(_.id).distinct.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[RDD]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): RDD[Long] = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assignments.filter(_.id == assign(0).id).map(_.member)\n+    } else {\n+      assignments.sparkContext.emptyRDD[Long]\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assign(0).id\n+    } else {\n+      -1\n+    }\n+  } \n+\n+  /**\n+   * Turn cluster assignments to cluster representations [[AffinityPropagationCluster]].\n+   * @return a [[RDD]] that contains all clusters generated by Affinity Propagation. Because the\n+   * cluster members in [[AffinityPropagationCluster]] is an [[Array]], it could consume too much\n+   * memory even run out of memory when you call collect() on the returned [[RDD]].\n+   */\n+  def fromAssignToClusters(): RDD[AffinityPropagationCluster] = {\n+    assignments.map { assign => ((assign.id, assign.exemplar), assign.member) }\n+      .aggregateByKey(mutable.Set[Long]())(\n+        seqOp = (s, d) => s ++ mutable.Set(d),\n+        combOp = (s1, s2) => s1 ++ s2\n+      ).map(kv => new AffinityPropagationCluster(kv._1._1, kv._1._2, kv._2.toArray))\n+  }\n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations: Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+\n+  /**\n+   * Set whether the input similarities are symmetric or not.\n+   * When symmetric is set to true, we assume that input similarities only contain triangular\n+   * matrix. That means, only s,,ij,, is included in the similarities. If both s,,ij,, and\n+   * s,,ji,, are given in the similarities, it very possibly causes error.\n+   */\n+  def setSymmetric(symmetric: Boolean): this.type = {\n+    this.symmetric = symmetric\n+    this\n+  }\n+\n+  /**\n+   * Get whether the input similarities are symmetric or not\n+   */\n+  def getSymmetric(): Boolean = {\n+    this.symmetric\n+  }\n+\n+  /**\n+   * Calculate the median value of similarities\n+   */\n+  private def getMedian(similarities: RDD[(Long, Long, Double)]): Double = {\n+    val sorted: RDD[(Long, Double)] = similarities.sortBy(_._3).zipWithIndex().map {\n+      case (v, idx) => (idx, v._3)\n+    }.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val count = sorted.count()\n+",
    "line": 239
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "@viirya Sounds great. I'll take a look at #6042 Thanks.\n",
    "commit": "0c7a26f56e85febfe1edac0d85251de9b0bf91e0",
    "createdAt": "2015-05-11T15:56:35Z",
    "diffHunk": "@@ -0,0 +1,497 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.graphx._\n+import org.apache.spark.graphx.impl.GraphImpl\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param member cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationAssignment(val id: Long, val exemplar: Long, val member: Long)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param id cluster id.\n+ * @param exemplar cluster exemplar.\n+ * @param members cluster member.\n+ */\n+@Experimental\n+case class AffinityPropagationCluster(val id: Long, val exemplar: Long, val members: Array[Long])\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Model produced by [[AffinityPropagation]].\n+ *\n+ * @param assignments the cluster assignments of AffinityPropagation clustering results.\n+ */\n+@Experimental\n+class AffinityPropagationModel(\n+    val assignments: RDD[AffinityPropagationAssignment]) extends Serializable {\n+\n+  /**\n+   * Get the number of clusters\n+   */\n+  lazy val k: Long = assignments.map(_.id).distinct.count()\n+ \n+  /**\n+   * Find the cluster the given vertex belongs\n+   * @param vertexID vertex id.\n+   * @return a [[RDD]] that contains vertex ids in the same cluster of given vertexID. If\n+   *         the given vertex doesn't belong to any cluster, return null.\n+   */\n+  def findCluster(vertexID: Long): RDD[Long] = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assignments.filter(_.id == assign(0).id).map(_.member)\n+    } else {\n+      assignments.sparkContext.emptyRDD[Long]\n+    }\n+  } \n+ \n+  /**\n+   * Find the cluster id the given vertex belongs to\n+   * @param vertexID vertex id.\n+   * @return the cluster id that the given vertex belongs to. If the given vertex doesn't belong to\n+   *         any cluster, return -1.\n+   */\n+  def findClusterID(vertexID: Long): Long = {\n+    val assign = assignments.filter(_.member == vertexID).collect()\n+    if (assign.nonEmpty) {\n+      assign(0).id\n+    } else {\n+      -1\n+    }\n+  } \n+\n+  /**\n+   * Turn cluster assignments to cluster representations [[AffinityPropagationCluster]].\n+   * @return a [[RDD]] that contains all clusters generated by Affinity Propagation. Because the\n+   * cluster members in [[AffinityPropagationCluster]] is an [[Array]], it could consume too much\n+   * memory even run out of memory when you call collect() on the returned [[RDD]].\n+   */\n+  def fromAssignToClusters(): RDD[AffinityPropagationCluster] = {\n+    assignments.map { assign => ((assign.id, assign.exemplar), assign.member) }\n+      .aggregateByKey(mutable.Set[Long]())(\n+        seqOp = (s, d) => s ++ mutable.Set(d),\n+        combOp = (s1, s2) => s1 ++ s2\n+      ).map(kv => new AffinityPropagationCluster(kv._1._1, kv._1._2, kv._2.toArray))\n+  }\n+}\n+\n+/**\n+ * The message exchanged on the node graph\n+ */\n+private case class EdgeMessage(\n+    similarity: Double,\n+    availability: Double,\n+    responsibility: Double) extends Equals {\n+  override def canEqual(that: Any): Boolean = {\n+    that match {\n+      case e: EdgeMessage =>\n+        similarity == e.similarity && availability == e.availability &&\n+          responsibility == e.responsibility\n+      case _ =>\n+        false\n+    }\n+  }\n+}\n+\n+/**\n+ * The data stored in each vertex on the graph\n+ */\n+private case class VertexData(availability: Double, responsibility: Double)\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Affinity propagation (AP), a graph clustering algorithm based on the concept of \"message passing\"\n+ * between data points. Unlike clustering algorithms such as k-means or k-medoids, AP does not\n+ * require the number of clusters to be determined or estimated before running it. AP is developed\n+ * by [[http://doi.org/10.1126/science.1136800 Frey and Dueck]].\n+ *\n+ * @param maxIterations Maximum number of iterations of the AP algorithm.\n+ * @param lambda lambda parameter used in the messaging iteration loop\n+ * @param normalization Indication of performing normalization\n+ * @param symmetric Indication of using symmetric similarity input\n+ *\n+ * @see [[http://en.wikipedia.org/wiki/Affinity_propagation Affinity propagation (Wikipedia)]]\n+ */\n+@Experimental\n+class AffinityPropagation private[clustering] (\n+    private var maxIterations: Int,\n+    private var lambda: Double,\n+    private var normalization: Boolean,\n+    private var symmetric: Boolean) extends Serializable {\n+\n+  import org.apache.spark.mllib.clustering.AffinityPropagation._\n+\n+  /** Constructs a AP instance with default parameters: {maxIterations: 100, lambda: `0.5`,\n+   *    normalization: false, symmetric: true}.\n+   */\n+  def this() = this(maxIterations = 100, lambda = 0.5, normalization = false, symmetric = true)\n+\n+  /**\n+   * Set maximum number of iterations of the messaging iteration loop\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+ \n+  /**\n+   * Get maximum number of iterations of the messaging iteration loop\n+   */\n+  def getMaxIterations: Int = {\n+    this.maxIterations\n+  }\n+ \n+  /**\n+   * Set lambda of the messaging iteration loop\n+   */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+ \n+  /**\n+   * Get lambda of the messaging iteration loop\n+   */\n+  def getLambda(): Double = {\n+    this.lambda\n+  }\n+ \n+  /**\n+   * Set whether to do normalization or not\n+   */\n+  def setNormalization(normalization: Boolean): this.type = {\n+    this.normalization = normalization\n+    this\n+  }\n+ \n+  /**\n+   * Get whether to do normalization or not\n+   */\n+  def getNormalization(): Boolean = {\n+    this.normalization\n+  }\n+\n+  /**\n+   * Set whether the input similarities are symmetric or not.\n+   * When symmetric is set to true, we assume that input similarities only contain triangular\n+   * matrix. That means, only s,,ij,, is included in the similarities. If both s,,ij,, and\n+   * s,,ji,, are given in the similarities, it very possibly causes error.\n+   */\n+  def setSymmetric(symmetric: Boolean): this.type = {\n+    this.symmetric = symmetric\n+    this\n+  }\n+\n+  /**\n+   * Get whether the input similarities are symmetric or not\n+   */\n+  def getSymmetric(): Boolean = {\n+    this.symmetric\n+  }\n+\n+  /**\n+   * Calculate the median value of similarities\n+   */\n+  private def getMedian(similarities: RDD[(Long, Long, Double)]): Double = {\n+    val sorted: RDD[(Long, Double)] = similarities.sortBy(_._3).zipWithIndex().map {\n+      case (v, idx) => (idx, v._3)\n+    }.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val count = sorted.count()\n+",
    "line": 239
  }],
  "prId": 4622
}]