[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "organize imports\n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-03-25T17:13:32Z",
    "diffHunk": "@@ -0,0 +1,402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable.Stack\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.util.InfoTheory\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import scala.collection.mutable"
  }],
  "prId": 216
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "A discretizer may be trained from one RDD but applied to another. I would recommend separating `Discretizer` from data by having a `train` method that takes an RDD as input and a `discretize` method to discretize an RDD of the same element type/dimension.\n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-03-25T17:18:22Z",
    "diffHunk": "@@ -0,0 +1,402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable.Stack\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.util.InfoTheory\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import scala.collection.mutable\n+\n+\n+/**\n+ * This class contains methods to discretize continuous values with the method proposed in\n+ * [Fayyad and Irani, Multi-Interval Discretization of Continuous-Valued Attributes, 1993]\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    @transient var data: RDD[LabeledPoint],"
  }],
  "prId": 216
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Could you explain the reason of setting `18000` as default? \n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-03-25T17:26:55Z",
    "diffHunk": "@@ -0,0 +1,402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable.Stack\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.util.InfoTheory\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import scala.collection.mutable\n+\n+\n+/**\n+ * This class contains methods to discretize continuous values with the method proposed in\n+ * [Fayyad and Irani, Multi-Interval Discretization of Continuous-Valued Attributes, 1993]\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    @transient var data: RDD[LabeledPoint],\n+    @transient var continousFeatures: Seq[Int],\n+    var elementsPerPartition: Int = 18000,"
  }, {
    "body": "I've made a really simple test to determine which ratio was faster in my case. But I think this would be different in each cluster and, therefore, it's configurable.\n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-03-27T12:16:49Z",
    "diffHunk": "@@ -0,0 +1,402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable.Stack\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.util.InfoTheory\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import scala.collection.mutable\n+\n+\n+/**\n+ * This class contains methods to discretize continuous values with the method proposed in\n+ * [Fayyad and Irani, Multi-Interval Discretization of Continuous-Valued Attributes, 1993]\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    @transient var data: RDD[LabeledPoint],\n+    @transient var continousFeatures: Seq[Int],\n+    var elementsPerPartition: Int = 18000,"
  }],
  "prId": 216
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Better use `mutable.Map` instead of `Map` to be explicit.\n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-03-25T18:44:38Z",
    "diffHunk": "@@ -0,0 +1,402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable.Stack\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.util.InfoTheory\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import scala.collection.mutable\n+\n+\n+/**\n+ * This class contains methods to discretize continuous values with the method proposed in\n+ * [Fayyad and Irani, Multi-Interval Discretization of Continuous-Valued Attributes, 1993]\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    @transient var data: RDD[LabeledPoint],\n+    @transient var continousFeatures: Seq[Int],\n+    var elementsPerPartition: Int = 18000,\n+    var maxBins: Int = Int.MaxValue)\n+  extends DiscretizerModel {\n+\n+  private var thresholds = Map.empty[Int, Seq[Double]]"
  }],
  "prId": 216
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "camelCase for variable names.\n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-03-25T18:55:37Z",
    "diffHunk": "@@ -0,0 +1,402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable.Stack\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.util.InfoTheory\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import scala.collection.mutable\n+\n+\n+/**\n+ * This class contains methods to discretize continuous values with the method proposed in\n+ * [Fayyad and Irani, Multi-Interval Discretization of Continuous-Valued Attributes, 1993]\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    @transient var data: RDD[LabeledPoint],\n+    @transient var continousFeatures: Seq[Int],\n+    var elementsPerPartition: Int = 18000,\n+    var maxBins: Int = Int.MaxValue)\n+  extends DiscretizerModel {\n+\n+  private var thresholds = Map.empty[Int, Seq[Double]]\n+  private val partitions = { x : Long => math.ceil(x.toDouble / elementsPerPartition).toInt }\n+\n+  def this() = this(null, null)\n+\n+  /**\n+   * Sets the RDD[LabeledPoint] to be discretized\n+   *\n+   * @param data RDD[LabeledPoint] to be discretized\n+   */\n+  def setData(data: RDD[LabeledPoint]): EntropyMinimizationDiscretizer = {\n+    this.data = data\n+    this\n+  }\n+\n+  /**\n+   * Sets the indexes of the features to be discretized\n+   *\n+   * @param continuousFeatures Indexes of features to be discretized\n+   */\n+  def setContinuousFeatures(continuousFeatures: Seq[Int]): EntropyMinimizationDiscretizer = {\n+    this.continousFeatures = continuousFeatures\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of elements that a partition should have\n+   *\n+   * @param ratio Maximum number of elements for a partition\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setElementsPerPartition(ratio: Int): EntropyMinimizationDiscretizer = {\n+    this.elementsPerPartition = ratio\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of discrete values\n+   *\n+   * @param maxBins Maximum number of discrete values\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setMaxBins(maxBins: Int): EntropyMinimizationDiscretizer = {\n+    this.maxBins = maxBins\n+    this\n+  }\n+  \n+  /**\n+   * Returns the thresholds used to discretized the given feature\n+   *\n+   * @param feature The number of the feature to discretize\n+   */\n+  def getThresholdsForFeature(feature: Int): Seq[Double] = {\n+    thresholds.get(feature) match {\n+      case Some(a) => a\n+      case None =>\n+        val featureValues = data.map({\n+          case LabeledPoint(label, values) => (values(feature), label.toString.trim)\n+        })\n+        val sortedValues = featureValues.sortByKey()\n+        val initial_candidates = initialThresholds(sortedValues)"
  }],
  "prId": 216
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`zipWithIndex map` to `zipWithIndex.map`. Please check\nhttps://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide\nfor infix usage.\n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-03-25T19:02:57Z",
    "diffHunk": "@@ -0,0 +1,402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable.Stack\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.util.InfoTheory\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import scala.collection.mutable\n+\n+\n+/**\n+ * This class contains methods to discretize continuous values with the method proposed in\n+ * [Fayyad and Irani, Multi-Interval Discretization of Continuous-Valued Attributes, 1993]\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    @transient var data: RDD[LabeledPoint],\n+    @transient var continousFeatures: Seq[Int],\n+    var elementsPerPartition: Int = 18000,\n+    var maxBins: Int = Int.MaxValue)\n+  extends DiscretizerModel {\n+\n+  private var thresholds = Map.empty[Int, Seq[Double]]\n+  private val partitions = { x : Long => math.ceil(x.toDouble / elementsPerPartition).toInt }\n+\n+  def this() = this(null, null)\n+\n+  /**\n+   * Sets the RDD[LabeledPoint] to be discretized\n+   *\n+   * @param data RDD[LabeledPoint] to be discretized\n+   */\n+  def setData(data: RDD[LabeledPoint]): EntropyMinimizationDiscretizer = {\n+    this.data = data\n+    this\n+  }\n+\n+  /**\n+   * Sets the indexes of the features to be discretized\n+   *\n+   * @param continuousFeatures Indexes of features to be discretized\n+   */\n+  def setContinuousFeatures(continuousFeatures: Seq[Int]): EntropyMinimizationDiscretizer = {\n+    this.continousFeatures = continuousFeatures\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of elements that a partition should have\n+   *\n+   * @param ratio Maximum number of elements for a partition\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setElementsPerPartition(ratio: Int): EntropyMinimizationDiscretizer = {\n+    this.elementsPerPartition = ratio\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of discrete values\n+   *\n+   * @param maxBins Maximum number of discrete values\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setMaxBins(maxBins: Int): EntropyMinimizationDiscretizer = {\n+    this.maxBins = maxBins\n+    this\n+  }\n+  \n+  /**\n+   * Returns the thresholds used to discretized the given feature\n+   *\n+   * @param feature The number of the feature to discretize\n+   */\n+  def getThresholdsForFeature(feature: Int): Seq[Double] = {\n+    thresholds.get(feature) match {\n+      case Some(a) => a\n+      case None =>\n+        val featureValues = data.map({\n+          case LabeledPoint(label, values) => (values(feature), label.toString.trim)\n+        })\n+        val sortedValues = featureValues.sortByKey()\n+        val initial_candidates = initialThresholds(sortedValues)\n+        val thresholdsForFeature = this.getThresholds(initial_candidates)\n+        this.thresholds += ((feature, thresholdsForFeature))\n+        thresholdsForFeature\n+    }\n+  }\n+\n+  /**\n+   * Returns the thresholds used to discretized the given features\n+   *\n+   * @param features The number of the feature to discretize\n+   */\n+  def getThresholdsForFeature(features: Seq[Int]): Map[Int, Seq[Double]] = {\n+    for (feature <- features diff this.thresholds.keys.toSeq) {\n+      getThresholdsForFeature(feature)\n+    }\n+\n+    this.thresholds.filter({ features.contains(_) })\n+  }\n+\n+  /**\n+   * Returns the thresholds used to discretized the continuous features\n+   */\n+  def getThresholdsForContinuousFeatures: Map[Int, Seq[Double]] = {\n+    for (feature <- continousFeatures diff this.thresholds.keys.toSeq) {\n+      getThresholdsForFeature(feature)\n+    }\n+\n+    this.thresholds\n+  }\n+\n+  /**\n+   * Calculates the initial candidate treholds for a feature\n+   * @param data RDD of (value, label) pairs\n+   * @return RDD of (candidate, class frequencies between last and current candidate) pairs\n+   */\n+  private def initialThresholds(data: RDD[(Double, String)]): RDD[(Double, Map[String,Int])] = {\n+    data.mapPartitions({ it =>\n+      var lastX = Double.NegativeInfinity\n+      var lastY = \"\"\n+      var result = Seq.empty[(Double, Map[String, Int])]\n+      var freqs = Map.empty[String, Int]\n+\n+      for ((x, y) <- it) {\n+        if (x != lastX && y != lastY && lastX != Double.NegativeInfinity) {\n+          // new candidate and interval\n+          result = ((x + lastX) / 2, freqs) +: result\n+          freqs = freqs.empty + ((y, 1))\n+        } else {\n+          // we continue on the same interval\n+          freqs = freqs.updated(y, freqs.getOrElse(y, 0) + 1)\n+        }\n+        lastX = x\n+        lastY = y\n+      }\n+\n+      // we add last element as a candidate threshold for convenience \n+      result = (lastX, freqs) +: result\n+\n+      result.reverse.toIterator\n+    }).persist(StorageLevel.MEMORY_AND_DISK)\n+  }\n+  \n+  /**\n+   * Returns a sequence of doubles that define the intervals to make the discretization.\n+   *\n+   * @param candidates RDD of (value, label) pairs\n+   */\n+  private def getThresholds(candidates: RDD[(Double, Map[String,Int])]): Seq[Double] = {\n+\n+    //Create queue\n+    val stack = new mutable.Queue[((Double, Double), Option[Double])]\n+\n+    //Insert first in the stack\n+    stack.enqueue(((Double.NegativeInfinity, Double.PositiveInfinity), None))\n+    var result = Seq(Double.NegativeInfinity)\n+\n+    // While more elements to eval, continue\n+    while(stack.length > 0 && result.size < this.maxBins){\n+\n+      val (bounds, lastThresh) = stack.dequeue\n+\n+      var cands = candidates.filter({ case (th, _) => th > bounds._1 && th <= bounds._2 })\n+      val nCands = cands.count\n+      if (nCands > 0) {\n+        cands = cands.coalesce(partitions(nCands))\n+\n+        evalThresholds(cands, lastThresh) match {\n+          case Some(th) =>\n+            result = th +: result\n+            stack.enqueue(((bounds._1, th), Some(th)))\n+            stack.enqueue(((th, bounds._2), Some(th)))\n+          case None => /* criteria not fulfilled, finish */\n+        }\n+      }\n+    }\n+    (Double.PositiveInfinity +: result).sorted\n+  }\n+\n+  /**\n+   * Selects one final thresholds among the candidates and returns two partitions to recurse\n+   *\n+   * @param candidates RDD of (candidate, class frequencies between last and current candidate)\n+   * @param lastSelected last selected threshold to avoid selecting it again\n+   */\n+  private def evalThresholds(\n+      candidates: RDD[(Double, Map[String, Int])],\n+      lastSelected : Option[Double]) = {\n+\n+    var result = candidates.map({\n+      case (cand, freqs) =>\n+        (cand, freqs, Seq.empty[Int], Seq.empty[Int])\n+    }).cache\n+\n+    val numPartitions = candidates.partitions.size\n+    val bc_numPartitions = candidates.context.broadcast(numPartitions)\n+\n+    // stores accumulated freqs from left to right\n+    val l_total = candidates.context.accumulator(Map.empty[String, Int])(MapAccumulator)\n+    // stores accumulated freqs from right to left\n+    val r_total = candidates.context.accumulator(Map.empty[String, Int])(MapAccumulator)\n+\n+    // calculates accumulated frequencies for each candidate\n+    (0 until numPartitions) foreach { l2r_i =>\n+\n+      val bc_l_total = l_total.value\n+      val bc_r_total = r_total.value\n+\n+      result =\n+        result.mapPartitionsWithIndex({ (slice, it) =>\n+\n+          val l2r = slice == l2r_i\n+          val r2l = slice == bc_numPartitions.value - 1 - l2r_i\n+\n+          if (l2r && r2l) {\n+\n+            // accumulate both from left to right and right to left\n+            var partialResult = Seq.empty[(Double, Map[String, Int], Seq[Int], Seq[Int])]\n+            var accum = Map.empty[String, Int]\n+\n+            for ((cand, freqs, _, r_freqs) <- it) {\n+              accum = Utils.sumFreqMaps(accum, freqs)\n+              val l_freqs = Utils.sumFreqMaps(accum, bc_l_total).values.toList\n+              partialResult = (cand, freqs, l_freqs, r_freqs) +: partialResult\n+            }\n+\n+            l_total += accum\n+\n+            val r2lIt = partialResult.iterator\n+            partialResult = Seq.empty[(Double, Map[String, Int], Seq[Int], Seq[Int])]\n+            accum = Map.empty[String, Int]\n+            for ((cand, freqs, l_freqs, _) <- r2lIt) {\n+              val r_freqs = Utils.sumFreqMaps(accum, bc_r_total).values.toList\n+              accum = Utils.sumFreqMaps(accum, freqs)\n+              partialResult = (cand, freqs, l_freqs, r_freqs) +: partialResult\n+            }\n+            r_total += accum\n+\n+            partialResult.iterator\n+\n+          } else if (l2r) {\n+\n+            // accumulate freqs from left to right\n+            var partialResult = Seq.empty[(Double, Map[String, Int], Seq[Int], Seq[Int])]\n+            var accum = Map.empty[String, Int]\n+\n+            for ((cand, freqs, _, r_freqs) <- it) {\n+              accum = Utils.sumFreqMaps(accum, freqs)\n+              val l_freqs = Utils.sumFreqMaps(accum, bc_l_total).values.toList\n+              partialResult = (cand, freqs, l_freqs, r_freqs) +: partialResult\n+            }\n+\n+            l_total += accum\n+            partialResult.reverseIterator\n+\n+          } else if (r2l) {\n+\n+            // accumulate freqs from right to left\n+            var partialResult = Seq.empty[(Double, Map[String, Int], Seq[Int], Seq[Int])]\n+            var accum = Map.empty[String, Int]\n+            val r2lIt = it.toSeq.reverseIterator\n+\n+            for ((cand, freqs, l_freqs, _) <- r2lIt) {\n+              val r_freqs = Utils.sumFreqMaps(accum, bc_r_total).values.toList\n+              accum = Utils.sumFreqMaps(accum, freqs)\n+              partialResult = (cand, freqs, l_freqs, r_freqs) +: partialResult\n+            }\n+\n+            r_total += accum\n+\n+            partialResult.iterator\n+\n+          } else {\n+            // do nothing in this iteration\n+            it\n+          }\n+        }, true) // important to maintain partitions within the loop\n+        .persist(StorageLevel.MEMORY_AND_DISK) // needed, otherwise spark repeats calculations\n+\n+      result.foreachPartition({ _ => }) // Forces the iteration to be calculated\n+    }\n+\n+    // calculate h(S)\n+    // s: number of elements\n+    // k: number of distinct classes\n+    // hs: entropy\n+\n+    val s  = l_total.value.values.reduce(_ + _)\n+    val hs = InfoTheory.entropy(l_total.value.values.toSeq, s)\n+    val k  = l_total.value.values.size\n+\n+    // select best threshold according to the criteria\n+    val final_candidates =\n+      result.flatMap({\n+        case (cand, _, l_freqs, r_freqs) =>\n+\n+          val k1  = l_freqs.size\n+          val s1  = if (k1 > 0) l_freqs.reduce(_ + _) else 0\n+          val hs1 = InfoTheory.entropy(l_freqs, s1)\n+\n+          val k2  = r_freqs.size\n+          val s2  = if (k2 > 0) r_freqs.reduce(_ + _) else 0\n+          val hs2 = InfoTheory.entropy(r_freqs, s2)\n+\n+          val weighted_hs = (s1 * hs1 + s2 * hs2) / s\n+          val gain        = hs - weighted_hs\n+          val delta       = Utils.log2(3 ^ k - 2) - (k * hs - k1 * hs1 - k2 * hs2)\n+          var criterion   = (gain - (Utils.log2(s - 1) + delta) / s) > -1e-5\n+\n+          lastSelected match {\n+              case None =>\n+              case Some(last) => criterion = criterion && (cand != last)\n+          }\n+\n+          if (criterion) {\n+            Seq((weighted_hs, cand))\n+          } else {\n+            Seq.empty[(Double, Double)]\n+          }\n+      })\n+\n+    // choose best candidates and partition data to make recursive calls\n+    if (final_candidates.count > 0) {\n+      val selected_threshold = final_candidates.reduce({ case ((whs1, cand1), (whs2, cand2)) =>\n+        if (whs1 < whs2) (whs1, cand1) else (whs2, cand2)\n+      })._2;\n+      Some(selected_threshold)\n+    } else {\n+      None\n+    }\n+\n+  }\n+\n+  /**\n+   * Discretizes a value with a set of intervals.\n+   *\n+   * @param value The value to be discretized\n+   * @param thresholds Thresholds used to asign a discrete value\n+   */\n+  private def assignDiscreteValue(value: Double, thresholds: Seq[Double]) = {\n+    var aux = thresholds.zipWithIndex\n+    while (value > aux.head._1) aux = aux.tail\n+    aux.head._2\n+  }\n+\n+  /**\n+   * Discretizes an RDD of (label, array of values) pairs.\n+   */\n+  def discretize: RDD[LabeledPoint] = {\n+    // calculate thresholds that aren't already calculated\n+    getThresholdsForContinuousFeatures\n+\n+    val bc_thresholds = this.data.context.broadcast(this.thresholds)\n+\n+    // applies thresholds to discretize every continuous feature\n+    data.map {\n+      case LabeledPoint(label, values) =>\n+        LabeledPoint(label,\n+          values.zipWithIndex map {"
  }],
  "prId": 216
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "move this line to the one above\n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-03-25T19:03:29Z",
    "diffHunk": "@@ -0,0 +1,402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable.Stack\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.util.InfoTheory\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import scala.collection.mutable\n+\n+\n+/**\n+ * This class contains methods to discretize continuous values with the method proposed in\n+ * [Fayyad and Irani, Multi-Interval Discretization of Continuous-Valued Attributes, 1993]\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    @transient var data: RDD[LabeledPoint],\n+    @transient var continousFeatures: Seq[Int],\n+    var elementsPerPartition: Int = 18000,\n+    var maxBins: Int = Int.MaxValue)\n+  extends DiscretizerModel {\n+\n+  private var thresholds = Map.empty[Int, Seq[Double]]\n+  private val partitions = { x : Long => math.ceil(x.toDouble / elementsPerPartition).toInt }\n+\n+  def this() = this(null, null)\n+\n+  /**\n+   * Sets the RDD[LabeledPoint] to be discretized\n+   *\n+   * @param data RDD[LabeledPoint] to be discretized\n+   */\n+  def setData(data: RDD[LabeledPoint]): EntropyMinimizationDiscretizer = {\n+    this.data = data\n+    this\n+  }\n+\n+  /**\n+   * Sets the indexes of the features to be discretized\n+   *\n+   * @param continuousFeatures Indexes of features to be discretized\n+   */\n+  def setContinuousFeatures(continuousFeatures: Seq[Int]): EntropyMinimizationDiscretizer = {\n+    this.continousFeatures = continuousFeatures\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of elements that a partition should have\n+   *\n+   * @param ratio Maximum number of elements for a partition\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setElementsPerPartition(ratio: Int): EntropyMinimizationDiscretizer = {\n+    this.elementsPerPartition = ratio\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of discrete values\n+   *\n+   * @param maxBins Maximum number of discrete values\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setMaxBins(maxBins: Int): EntropyMinimizationDiscretizer = {\n+    this.maxBins = maxBins\n+    this\n+  }\n+  \n+  /**\n+   * Returns the thresholds used to discretized the given feature\n+   *\n+   * @param feature The number of the feature to discretize\n+   */\n+  def getThresholdsForFeature(feature: Int): Seq[Double] = {\n+    thresholds.get(feature) match {\n+      case Some(a) => a\n+      case None =>\n+        val featureValues = data.map({\n+          case LabeledPoint(label, values) => (values(feature), label.toString.trim)\n+        })\n+        val sortedValues = featureValues.sortByKey()\n+        val initial_candidates = initialThresholds(sortedValues)\n+        val thresholdsForFeature = this.getThresholds(initial_candidates)\n+        this.thresholds += ((feature, thresholdsForFeature))\n+        thresholdsForFeature\n+    }\n+  }\n+\n+  /**\n+   * Returns the thresholds used to discretized the given features\n+   *\n+   * @param features The number of the feature to discretize\n+   */\n+  def getThresholdsForFeature(features: Seq[Int]): Map[Int, Seq[Double]] = {\n+    for (feature <- features diff this.thresholds.keys.toSeq) {\n+      getThresholdsForFeature(feature)\n+    }\n+\n+    this.thresholds.filter({ features.contains(_) })\n+  }\n+\n+  /**\n+   * Returns the thresholds used to discretized the continuous features\n+   */\n+  def getThresholdsForContinuousFeatures: Map[Int, Seq[Double]] = {\n+    for (feature <- continousFeatures diff this.thresholds.keys.toSeq) {\n+      getThresholdsForFeature(feature)\n+    }\n+\n+    this.thresholds\n+  }\n+\n+  /**\n+   * Calculates the initial candidate treholds for a feature\n+   * @param data RDD of (value, label) pairs\n+   * @return RDD of (candidate, class frequencies between last and current candidate) pairs\n+   */\n+  private def initialThresholds(data: RDD[(Double, String)]): RDD[(Double, Map[String,Int])] = {\n+    data.mapPartitions({ it =>\n+      var lastX = Double.NegativeInfinity\n+      var lastY = \"\"\n+      var result = Seq.empty[(Double, Map[String, Int])]\n+      var freqs = Map.empty[String, Int]\n+\n+      for ((x, y) <- it) {\n+        if (x != lastX && y != lastY && lastX != Double.NegativeInfinity) {\n+          // new candidate and interval\n+          result = ((x + lastX) / 2, freqs) +: result\n+          freqs = freqs.empty + ((y, 1))\n+        } else {\n+          // we continue on the same interval\n+          freqs = freqs.updated(y, freqs.getOrElse(y, 0) + 1)\n+        }\n+        lastX = x\n+        lastY = y\n+      }\n+\n+      // we add last element as a candidate threshold for convenience \n+      result = (lastX, freqs) +: result\n+\n+      result.reverse.toIterator\n+    }).persist(StorageLevel.MEMORY_AND_DISK)\n+  }\n+  \n+  /**\n+   * Returns a sequence of doubles that define the intervals to make the discretization.\n+   *\n+   * @param candidates RDD of (value, label) pairs\n+   */\n+  private def getThresholds(candidates: RDD[(Double, Map[String,Int])]): Seq[Double] = {\n+\n+    //Create queue\n+    val stack = new mutable.Queue[((Double, Double), Option[Double])]\n+\n+    //Insert first in the stack\n+    stack.enqueue(((Double.NegativeInfinity, Double.PositiveInfinity), None))\n+    var result = Seq(Double.NegativeInfinity)\n+\n+    // While more elements to eval, continue\n+    while(stack.length > 0 && result.size < this.maxBins){\n+\n+      val (bounds, lastThresh) = stack.dequeue\n+\n+      var cands = candidates.filter({ case (th, _) => th > bounds._1 && th <= bounds._2 })\n+      val nCands = cands.count\n+      if (nCands > 0) {\n+        cands = cands.coalesce(partitions(nCands))\n+\n+        evalThresholds(cands, lastThresh) match {\n+          case Some(th) =>\n+            result = th +: result\n+            stack.enqueue(((bounds._1, th), Some(th)))\n+            stack.enqueue(((th, bounds._2), Some(th)))\n+          case None => /* criteria not fulfilled, finish */\n+        }\n+      }\n+    }\n+    (Double.PositiveInfinity +: result).sorted\n+  }\n+\n+  /**\n+   * Selects one final thresholds among the candidates and returns two partitions to recurse\n+   *\n+   * @param candidates RDD of (candidate, class frequencies between last and current candidate)\n+   * @param lastSelected last selected threshold to avoid selecting it again\n+   */\n+  private def evalThresholds(\n+      candidates: RDD[(Double, Map[String, Int])],\n+      lastSelected : Option[Double]) = {\n+\n+    var result = candidates.map({\n+      case (cand, freqs) =>\n+        (cand, freqs, Seq.empty[Int], Seq.empty[Int])\n+    }).cache\n+\n+    val numPartitions = candidates.partitions.size\n+    val bc_numPartitions = candidates.context.broadcast(numPartitions)\n+\n+    // stores accumulated freqs from left to right\n+    val l_total = candidates.context.accumulator(Map.empty[String, Int])(MapAccumulator)\n+    // stores accumulated freqs from right to left\n+    val r_total = candidates.context.accumulator(Map.empty[String, Int])(MapAccumulator)\n+\n+    // calculates accumulated frequencies for each candidate\n+    (0 until numPartitions) foreach { l2r_i =>\n+\n+      val bc_l_total = l_total.value\n+      val bc_r_total = r_total.value\n+\n+      result =\n+        result.mapPartitionsWithIndex({ (slice, it) =>\n+\n+          val l2r = slice == l2r_i\n+          val r2l = slice == bc_numPartitions.value - 1 - l2r_i\n+\n+          if (l2r && r2l) {\n+\n+            // accumulate both from left to right and right to left\n+            var partialResult = Seq.empty[(Double, Map[String, Int], Seq[Int], Seq[Int])]\n+            var accum = Map.empty[String, Int]\n+\n+            for ((cand, freqs, _, r_freqs) <- it) {\n+              accum = Utils.sumFreqMaps(accum, freqs)\n+              val l_freqs = Utils.sumFreqMaps(accum, bc_l_total).values.toList\n+              partialResult = (cand, freqs, l_freqs, r_freqs) +: partialResult\n+            }\n+\n+            l_total += accum\n+\n+            val r2lIt = partialResult.iterator\n+            partialResult = Seq.empty[(Double, Map[String, Int], Seq[Int], Seq[Int])]\n+            accum = Map.empty[String, Int]\n+            for ((cand, freqs, l_freqs, _) <- r2lIt) {\n+              val r_freqs = Utils.sumFreqMaps(accum, bc_r_total).values.toList\n+              accum = Utils.sumFreqMaps(accum, freqs)\n+              partialResult = (cand, freqs, l_freqs, r_freqs) +: partialResult\n+            }\n+            r_total += accum\n+\n+            partialResult.iterator\n+\n+          } else if (l2r) {\n+\n+            // accumulate freqs from left to right\n+            var partialResult = Seq.empty[(Double, Map[String, Int], Seq[Int], Seq[Int])]\n+            var accum = Map.empty[String, Int]\n+\n+            for ((cand, freqs, _, r_freqs) <- it) {\n+              accum = Utils.sumFreqMaps(accum, freqs)\n+              val l_freqs = Utils.sumFreqMaps(accum, bc_l_total).values.toList\n+              partialResult = (cand, freqs, l_freqs, r_freqs) +: partialResult\n+            }\n+\n+            l_total += accum\n+            partialResult.reverseIterator\n+\n+          } else if (r2l) {\n+\n+            // accumulate freqs from right to left\n+            var partialResult = Seq.empty[(Double, Map[String, Int], Seq[Int], Seq[Int])]\n+            var accum = Map.empty[String, Int]\n+            val r2lIt = it.toSeq.reverseIterator\n+\n+            for ((cand, freqs, l_freqs, _) <- r2lIt) {\n+              val r_freqs = Utils.sumFreqMaps(accum, bc_r_total).values.toList\n+              accum = Utils.sumFreqMaps(accum, freqs)\n+              partialResult = (cand, freqs, l_freqs, r_freqs) +: partialResult\n+            }\n+\n+            r_total += accum\n+\n+            partialResult.iterator\n+\n+          } else {\n+            // do nothing in this iteration\n+            it\n+          }\n+        }, true) // important to maintain partitions within the loop\n+        .persist(StorageLevel.MEMORY_AND_DISK) // needed, otherwise spark repeats calculations\n+\n+      result.foreachPartition({ _ => }) // Forces the iteration to be calculated\n+    }\n+\n+    // calculate h(S)\n+    // s: number of elements\n+    // k: number of distinct classes\n+    // hs: entropy\n+\n+    val s  = l_total.value.values.reduce(_ + _)\n+    val hs = InfoTheory.entropy(l_total.value.values.toSeq, s)\n+    val k  = l_total.value.values.size\n+\n+    // select best threshold according to the criteria\n+    val final_candidates =\n+      result.flatMap({\n+        case (cand, _, l_freqs, r_freqs) =>\n+\n+          val k1  = l_freqs.size\n+          val s1  = if (k1 > 0) l_freqs.reduce(_ + _) else 0\n+          val hs1 = InfoTheory.entropy(l_freqs, s1)\n+\n+          val k2  = r_freqs.size\n+          val s2  = if (k2 > 0) r_freqs.reduce(_ + _) else 0\n+          val hs2 = InfoTheory.entropy(r_freqs, s2)\n+\n+          val weighted_hs = (s1 * hs1 + s2 * hs2) / s\n+          val gain        = hs - weighted_hs\n+          val delta       = Utils.log2(3 ^ k - 2) - (k * hs - k1 * hs1 - k2 * hs2)\n+          var criterion   = (gain - (Utils.log2(s - 1) + delta) / s) > -1e-5\n+\n+          lastSelected match {\n+              case None =>\n+              case Some(last) => criterion = criterion && (cand != last)\n+          }\n+\n+          if (criterion) {\n+            Seq((weighted_hs, cand))\n+          } else {\n+            Seq.empty[(Double, Double)]\n+          }\n+      })\n+\n+    // choose best candidates and partition data to make recursive calls\n+    if (final_candidates.count > 0) {\n+      val selected_threshold = final_candidates.reduce({ case ((whs1, cand1), (whs2, cand2)) =>\n+        if (whs1 < whs2) (whs1, cand1) else (whs2, cand2)\n+      })._2;\n+      Some(selected_threshold)\n+    } else {\n+      None\n+    }\n+\n+  }\n+\n+  /**\n+   * Discretizes a value with a set of intervals.\n+   *\n+   * @param value The value to be discretized\n+   * @param thresholds Thresholds used to asign a discrete value\n+   */\n+  private def assignDiscreteValue(value: Double, thresholds: Seq[Double]) = {\n+    var aux = thresholds.zipWithIndex\n+    while (value > aux.head._1) aux = aux.tail\n+    aux.head._2\n+  }\n+\n+  /**\n+   * Discretizes an RDD of (label, array of values) pairs.\n+   */\n+  def discretize: RDD[LabeledPoint] = {\n+    // calculate thresholds that aren't already calculated\n+    getThresholdsForContinuousFeatures\n+\n+    val bc_thresholds = this.data.context.broadcast(this.thresholds)\n+\n+    // applies thresholds to discretize every continuous feature\n+    data.map {\n+      case LabeledPoint(label, values) =>"
  }],
  "prId": 216
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Remove empty line.\n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-03-25T19:04:32Z",
    "diffHunk": "@@ -0,0 +1,402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable.Stack\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.util.InfoTheory\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import scala.collection.mutable\n+\n+\n+/**\n+ * This class contains methods to discretize continuous values with the method proposed in\n+ * [Fayyad and Irani, Multi-Interval Discretization of Continuous-Valued Attributes, 1993]\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    @transient var data: RDD[LabeledPoint],\n+    @transient var continousFeatures: Seq[Int],\n+    var elementsPerPartition: Int = 18000,\n+    var maxBins: Int = Int.MaxValue)\n+  extends DiscretizerModel {\n+\n+  private var thresholds = Map.empty[Int, Seq[Double]]\n+  private val partitions = { x : Long => math.ceil(x.toDouble / elementsPerPartition).toInt }\n+\n+  def this() = this(null, null)\n+\n+  /**\n+   * Sets the RDD[LabeledPoint] to be discretized\n+   *\n+   * @param data RDD[LabeledPoint] to be discretized\n+   */\n+  def setData(data: RDD[LabeledPoint]): EntropyMinimizationDiscretizer = {\n+    this.data = data\n+    this\n+  }\n+\n+  /**\n+   * Sets the indexes of the features to be discretized\n+   *\n+   * @param continuousFeatures Indexes of features to be discretized\n+   */\n+  def setContinuousFeatures(continuousFeatures: Seq[Int]): EntropyMinimizationDiscretizer = {\n+    this.continousFeatures = continuousFeatures\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of elements that a partition should have\n+   *\n+   * @param ratio Maximum number of elements for a partition\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setElementsPerPartition(ratio: Int): EntropyMinimizationDiscretizer = {\n+    this.elementsPerPartition = ratio\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of discrete values\n+   *\n+   * @param maxBins Maximum number of discrete values\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setMaxBins(maxBins: Int): EntropyMinimizationDiscretizer = {\n+    this.maxBins = maxBins\n+    this\n+  }\n+  \n+  /**\n+   * Returns the thresholds used to discretized the given feature\n+   *\n+   * @param feature The number of the feature to discretize\n+   */\n+  def getThresholdsForFeature(feature: Int): Seq[Double] = {\n+    thresholds.get(feature) match {\n+      case Some(a) => a\n+      case None =>\n+        val featureValues = data.map({\n+          case LabeledPoint(label, values) => (values(feature), label.toString.trim)\n+        })\n+        val sortedValues = featureValues.sortByKey()\n+        val initial_candidates = initialThresholds(sortedValues)\n+        val thresholdsForFeature = this.getThresholds(initial_candidates)\n+        this.thresholds += ((feature, thresholdsForFeature))\n+        thresholdsForFeature\n+    }\n+  }\n+\n+  /**\n+   * Returns the thresholds used to discretized the given features\n+   *\n+   * @param features The number of the feature to discretize\n+   */\n+  def getThresholdsForFeature(features: Seq[Int]): Map[Int, Seq[Double]] = {\n+    for (feature <- features diff this.thresholds.keys.toSeq) {\n+      getThresholdsForFeature(feature)\n+    }\n+\n+    this.thresholds.filter({ features.contains(_) })\n+  }\n+\n+  /**\n+   * Returns the thresholds used to discretized the continuous features\n+   */\n+  def getThresholdsForContinuousFeatures: Map[Int, Seq[Double]] = {\n+    for (feature <- continousFeatures diff this.thresholds.keys.toSeq) {\n+      getThresholdsForFeature(feature)\n+    }\n+\n+    this.thresholds\n+  }\n+\n+  /**\n+   * Calculates the initial candidate treholds for a feature\n+   * @param data RDD of (value, label) pairs\n+   * @return RDD of (candidate, class frequencies between last and current candidate) pairs\n+   */\n+  private def initialThresholds(data: RDD[(Double, String)]): RDD[(Double, Map[String,Int])] = {\n+    data.mapPartitions({ it =>\n+      var lastX = Double.NegativeInfinity\n+      var lastY = \"\"\n+      var result = Seq.empty[(Double, Map[String, Int])]\n+      var freqs = Map.empty[String, Int]\n+\n+      for ((x, y) <- it) {\n+        if (x != lastX && y != lastY && lastX != Double.NegativeInfinity) {\n+          // new candidate and interval\n+          result = ((x + lastX) / 2, freqs) +: result\n+          freqs = freqs.empty + ((y, 1))\n+        } else {\n+          // we continue on the same interval\n+          freqs = freqs.updated(y, freqs.getOrElse(y, 0) + 1)\n+        }\n+        lastX = x\n+        lastY = y\n+      }\n+\n+      // we add last element as a candidate threshold for convenience \n+      result = (lastX, freqs) +: result\n+\n+      result.reverse.toIterator\n+    }).persist(StorageLevel.MEMORY_AND_DISK)\n+  }\n+  \n+  /**\n+   * Returns a sequence of doubles that define the intervals to make the discretization.\n+   *\n+   * @param candidates RDD of (value, label) pairs\n+   */\n+  private def getThresholds(candidates: RDD[(Double, Map[String,Int])]): Seq[Double] = {\n+\n+    //Create queue\n+    val stack = new mutable.Queue[((Double, Double), Option[Double])]\n+\n+    //Insert first in the stack\n+    stack.enqueue(((Double.NegativeInfinity, Double.PositiveInfinity), None))\n+    var result = Seq(Double.NegativeInfinity)\n+\n+    // While more elements to eval, continue\n+    while(stack.length > 0 && result.size < this.maxBins){\n+\n+      val (bounds, lastThresh) = stack.dequeue\n+\n+      var cands = candidates.filter({ case (th, _) => th > bounds._1 && th <= bounds._2 })\n+      val nCands = cands.count\n+      if (nCands > 0) {\n+        cands = cands.coalesce(partitions(nCands))\n+\n+        evalThresholds(cands, lastThresh) match {\n+          case Some(th) =>\n+            result = th +: result\n+            stack.enqueue(((bounds._1, th), Some(th)))\n+            stack.enqueue(((th, bounds._2), Some(th)))\n+          case None => /* criteria not fulfilled, finish */\n+        }\n+      }\n+    }\n+    (Double.PositiveInfinity +: result).sorted\n+  }\n+\n+  /**\n+   * Selects one final thresholds among the candidates and returns two partitions to recurse\n+   *\n+   * @param candidates RDD of (candidate, class frequencies between last and current candidate)\n+   * @param lastSelected last selected threshold to avoid selecting it again\n+   */\n+  private def evalThresholds(\n+      candidates: RDD[(Double, Map[String, Int])],\n+      lastSelected : Option[Double]) = {\n+\n+    var result = candidates.map({\n+      case (cand, freqs) =>\n+        (cand, freqs, Seq.empty[Int], Seq.empty[Int])\n+    }).cache\n+\n+    val numPartitions = candidates.partitions.size\n+    val bc_numPartitions = candidates.context.broadcast(numPartitions)\n+\n+    // stores accumulated freqs from left to right\n+    val l_total = candidates.context.accumulator(Map.empty[String, Int])(MapAccumulator)\n+    // stores accumulated freqs from right to left\n+    val r_total = candidates.context.accumulator(Map.empty[String, Int])(MapAccumulator)\n+\n+    // calculates accumulated frequencies for each candidate\n+    (0 until numPartitions) foreach { l2r_i =>\n+\n+      val bc_l_total = l_total.value\n+      val bc_r_total = r_total.value\n+\n+      result =\n+        result.mapPartitionsWithIndex({ (slice, it) =>\n+\n+          val l2r = slice == l2r_i\n+          val r2l = slice == bc_numPartitions.value - 1 - l2r_i\n+\n+          if (l2r && r2l) {\n+\n+            // accumulate both from left to right and right to left\n+            var partialResult = Seq.empty[(Double, Map[String, Int], Seq[Int], Seq[Int])]\n+            var accum = Map.empty[String, Int]\n+\n+            for ((cand, freqs, _, r_freqs) <- it) {\n+              accum = Utils.sumFreqMaps(accum, freqs)\n+              val l_freqs = Utils.sumFreqMaps(accum, bc_l_total).values.toList\n+              partialResult = (cand, freqs, l_freqs, r_freqs) +: partialResult\n+            }\n+\n+            l_total += accum\n+\n+            val r2lIt = partialResult.iterator\n+            partialResult = Seq.empty[(Double, Map[String, Int], Seq[Int], Seq[Int])]\n+            accum = Map.empty[String, Int]\n+            for ((cand, freqs, l_freqs, _) <- r2lIt) {\n+              val r_freqs = Utils.sumFreqMaps(accum, bc_r_total).values.toList\n+              accum = Utils.sumFreqMaps(accum, freqs)\n+              partialResult = (cand, freqs, l_freqs, r_freqs) +: partialResult\n+            }\n+            r_total += accum\n+\n+            partialResult.iterator\n+\n+          } else if (l2r) {\n+\n+            // accumulate freqs from left to right\n+            var partialResult = Seq.empty[(Double, Map[String, Int], Seq[Int], Seq[Int])]\n+            var accum = Map.empty[String, Int]\n+\n+            for ((cand, freqs, _, r_freqs) <- it) {\n+              accum = Utils.sumFreqMaps(accum, freqs)\n+              val l_freqs = Utils.sumFreqMaps(accum, bc_l_total).values.toList\n+              partialResult = (cand, freqs, l_freqs, r_freqs) +: partialResult\n+            }\n+\n+            l_total += accum\n+            partialResult.reverseIterator\n+\n+          } else if (r2l) {\n+\n+            // accumulate freqs from right to left\n+            var partialResult = Seq.empty[(Double, Map[String, Int], Seq[Int], Seq[Int])]\n+            var accum = Map.empty[String, Int]\n+            val r2lIt = it.toSeq.reverseIterator\n+\n+            for ((cand, freqs, l_freqs, _) <- r2lIt) {\n+              val r_freqs = Utils.sumFreqMaps(accum, bc_r_total).values.toList\n+              accum = Utils.sumFreqMaps(accum, freqs)\n+              partialResult = (cand, freqs, l_freqs, r_freqs) +: partialResult\n+            }\n+\n+            r_total += accum\n+\n+            partialResult.iterator\n+\n+          } else {\n+            // do nothing in this iteration\n+            it\n+          }\n+        }, true) // important to maintain partitions within the loop\n+        .persist(StorageLevel.MEMORY_AND_DISK) // needed, otherwise spark repeats calculations\n+\n+      result.foreachPartition({ _ => }) // Forces the iteration to be calculated\n+    }\n+\n+    // calculate h(S)\n+    // s: number of elements\n+    // k: number of distinct classes\n+    // hs: entropy\n+\n+    val s  = l_total.value.values.reduce(_ + _)\n+    val hs = InfoTheory.entropy(l_total.value.values.toSeq, s)\n+    val k  = l_total.value.values.size\n+\n+    // select best threshold according to the criteria\n+    val final_candidates =\n+      result.flatMap({\n+        case (cand, _, l_freqs, r_freqs) =>\n+\n+          val k1  = l_freqs.size\n+          val s1  = if (k1 > 0) l_freqs.reduce(_ + _) else 0\n+          val hs1 = InfoTheory.entropy(l_freqs, s1)\n+\n+          val k2  = r_freqs.size\n+          val s2  = if (k2 > 0) r_freqs.reduce(_ + _) else 0\n+          val hs2 = InfoTheory.entropy(r_freqs, s2)\n+\n+          val weighted_hs = (s1 * hs1 + s2 * hs2) / s\n+          val gain        = hs - weighted_hs\n+          val delta       = Utils.log2(3 ^ k - 2) - (k * hs - k1 * hs1 - k2 * hs2)\n+          var criterion   = (gain - (Utils.log2(s - 1) + delta) / s) > -1e-5\n+\n+          lastSelected match {\n+              case None =>\n+              case Some(last) => criterion = criterion && (cand != last)\n+          }\n+\n+          if (criterion) {\n+            Seq((weighted_hs, cand))\n+          } else {\n+            Seq.empty[(Double, Double)]\n+          }\n+      })\n+\n+    // choose best candidates and partition data to make recursive calls\n+    if (final_candidates.count > 0) {\n+      val selected_threshold = final_candidates.reduce({ case ((whs1, cand1), (whs2, cand2)) =>\n+        if (whs1 < whs2) (whs1, cand1) else (whs2, cand2)\n+      })._2;\n+      Some(selected_threshold)\n+    } else {\n+      None\n+    }\n+\n+  }\n+\n+  /**\n+   * Discretizes a value with a set of intervals.\n+   *\n+   * @param value The value to be discretized\n+   * @param thresholds Thresholds used to asign a discrete value\n+   */\n+  private def assignDiscreteValue(value: Double, thresholds: Seq[Double]) = {\n+    var aux = thresholds.zipWithIndex\n+    while (value > aux.head._1) aux = aux.tail\n+    aux.head._2\n+  }\n+\n+  /**\n+   * Discretizes an RDD of (label, array of values) pairs.\n+   */\n+  def discretize: RDD[LabeledPoint] = {\n+    // calculate thresholds that aren't already calculated\n+    getThresholdsForContinuousFeatures\n+\n+    val bc_thresholds = this.data.context.broadcast(this.thresholds)\n+\n+    // applies thresholds to discretize every continuous feature\n+    data.map {\n+      case LabeledPoint(label, values) =>\n+        LabeledPoint(label,\n+          values.zipWithIndex map {\n+            case (value, i) =>\n+              if (bc_thresholds.value.keySet contains i) {\n+                assignDiscreteValue(value, bc_thresholds.value(i))\n+              } else {\n+                value\n+              }\n+          })\n+    }\n+  }\n+"
  }],
  "prId": 216
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Why do we need conversion to String?\n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-03-25T20:36:28Z",
    "diffHunk": "@@ -0,0 +1,402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable.Stack\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.util.InfoTheory\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import scala.collection.mutable\n+\n+\n+/**\n+ * This class contains methods to discretize continuous values with the method proposed in\n+ * [Fayyad and Irani, Multi-Interval Discretization of Continuous-Valued Attributes, 1993]\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    @transient var data: RDD[LabeledPoint],\n+    @transient var continousFeatures: Seq[Int],\n+    var elementsPerPartition: Int = 18000,\n+    var maxBins: Int = Int.MaxValue)\n+  extends DiscretizerModel {\n+\n+  private var thresholds = Map.empty[Int, Seq[Double]]\n+  private val partitions = { x : Long => math.ceil(x.toDouble / elementsPerPartition).toInt }\n+\n+  def this() = this(null, null)\n+\n+  /**\n+   * Sets the RDD[LabeledPoint] to be discretized\n+   *\n+   * @param data RDD[LabeledPoint] to be discretized\n+   */\n+  def setData(data: RDD[LabeledPoint]): EntropyMinimizationDiscretizer = {\n+    this.data = data\n+    this\n+  }\n+\n+  /**\n+   * Sets the indexes of the features to be discretized\n+   *\n+   * @param continuousFeatures Indexes of features to be discretized\n+   */\n+  def setContinuousFeatures(continuousFeatures: Seq[Int]): EntropyMinimizationDiscretizer = {\n+    this.continousFeatures = continuousFeatures\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of elements that a partition should have\n+   *\n+   * @param ratio Maximum number of elements for a partition\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setElementsPerPartition(ratio: Int): EntropyMinimizationDiscretizer = {\n+    this.elementsPerPartition = ratio\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of discrete values\n+   *\n+   * @param maxBins Maximum number of discrete values\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setMaxBins(maxBins: Int): EntropyMinimizationDiscretizer = {\n+    this.maxBins = maxBins\n+    this\n+  }\n+  \n+  /**\n+   * Returns the thresholds used to discretized the given feature\n+   *\n+   * @param feature The number of the feature to discretize\n+   */\n+  def getThresholdsForFeature(feature: Int): Seq[Double] = {\n+    thresholds.get(feature) match {\n+      case Some(a) => a\n+      case None =>\n+        val featureValues = data.map({\n+          case LabeledPoint(label, values) => (values(feature), label.toString.trim)"
  }],
  "prId": 216
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "The implementation here is really hard to understand. It seems to me that you need \n\n```\nSparkContext#runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] => U): Array[U]\n```\n\nto get the sum per partition, then compute partition-wise cumsum on the master, then each partition knows the number to start. Maybe I understand this block wrongly.\n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-03-25T21:12:14Z",
    "diffHunk": "@@ -0,0 +1,402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable.Stack\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.util.InfoTheory\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import scala.collection.mutable\n+\n+\n+/**\n+ * This class contains methods to discretize continuous values with the method proposed in\n+ * [Fayyad and Irani, Multi-Interval Discretization of Continuous-Valued Attributes, 1993]\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    @transient var data: RDD[LabeledPoint],\n+    @transient var continousFeatures: Seq[Int],\n+    var elementsPerPartition: Int = 18000,\n+    var maxBins: Int = Int.MaxValue)\n+  extends DiscretizerModel {\n+\n+  private var thresholds = Map.empty[Int, Seq[Double]]\n+  private val partitions = { x : Long => math.ceil(x.toDouble / elementsPerPartition).toInt }\n+\n+  def this() = this(null, null)\n+\n+  /**\n+   * Sets the RDD[LabeledPoint] to be discretized\n+   *\n+   * @param data RDD[LabeledPoint] to be discretized\n+   */\n+  def setData(data: RDD[LabeledPoint]): EntropyMinimizationDiscretizer = {\n+    this.data = data\n+    this\n+  }\n+\n+  /**\n+   * Sets the indexes of the features to be discretized\n+   *\n+   * @param continuousFeatures Indexes of features to be discretized\n+   */\n+  def setContinuousFeatures(continuousFeatures: Seq[Int]): EntropyMinimizationDiscretizer = {\n+    this.continousFeatures = continuousFeatures\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of elements that a partition should have\n+   *\n+   * @param ratio Maximum number of elements for a partition\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setElementsPerPartition(ratio: Int): EntropyMinimizationDiscretizer = {\n+    this.elementsPerPartition = ratio\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of discrete values\n+   *\n+   * @param maxBins Maximum number of discrete values\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setMaxBins(maxBins: Int): EntropyMinimizationDiscretizer = {\n+    this.maxBins = maxBins\n+    this\n+  }\n+  \n+  /**\n+   * Returns the thresholds used to discretized the given feature\n+   *\n+   * @param feature The number of the feature to discretize\n+   */\n+  def getThresholdsForFeature(feature: Int): Seq[Double] = {\n+    thresholds.get(feature) match {\n+      case Some(a) => a\n+      case None =>\n+        val featureValues = data.map({\n+          case LabeledPoint(label, values) => (values(feature), label.toString.trim)\n+        })\n+        val sortedValues = featureValues.sortByKey()\n+        val initial_candidates = initialThresholds(sortedValues)\n+        val thresholdsForFeature = this.getThresholds(initial_candidates)\n+        this.thresholds += ((feature, thresholdsForFeature))\n+        thresholdsForFeature\n+    }\n+  }\n+\n+  /**\n+   * Returns the thresholds used to discretized the given features\n+   *\n+   * @param features The number of the feature to discretize\n+   */\n+  def getThresholdsForFeature(features: Seq[Int]): Map[Int, Seq[Double]] = {\n+    for (feature <- features diff this.thresholds.keys.toSeq) {\n+      getThresholdsForFeature(feature)\n+    }\n+\n+    this.thresholds.filter({ features.contains(_) })\n+  }\n+\n+  /**\n+   * Returns the thresholds used to discretized the continuous features\n+   */\n+  def getThresholdsForContinuousFeatures: Map[Int, Seq[Double]] = {\n+    for (feature <- continousFeatures diff this.thresholds.keys.toSeq) {\n+      getThresholdsForFeature(feature)\n+    }\n+\n+    this.thresholds\n+  }\n+\n+  /**\n+   * Calculates the initial candidate treholds for a feature\n+   * @param data RDD of (value, label) pairs\n+   * @return RDD of (candidate, class frequencies between last and current candidate) pairs\n+   */\n+  private def initialThresholds(data: RDD[(Double, String)]): RDD[(Double, Map[String,Int])] = {\n+    data.mapPartitions({ it =>\n+      var lastX = Double.NegativeInfinity\n+      var lastY = \"\"\n+      var result = Seq.empty[(Double, Map[String, Int])]\n+      var freqs = Map.empty[String, Int]\n+\n+      for ((x, y) <- it) {\n+        if (x != lastX && y != lastY && lastX != Double.NegativeInfinity) {\n+          // new candidate and interval\n+          result = ((x + lastX) / 2, freqs) +: result\n+          freqs = freqs.empty + ((y, 1))\n+        } else {\n+          // we continue on the same interval\n+          freqs = freqs.updated(y, freqs.getOrElse(y, 0) + 1)\n+        }\n+        lastX = x\n+        lastY = y\n+      }\n+\n+      // we add last element as a candidate threshold for convenience \n+      result = (lastX, freqs) +: result\n+\n+      result.reverse.toIterator\n+    }).persist(StorageLevel.MEMORY_AND_DISK)\n+  }\n+  \n+  /**\n+   * Returns a sequence of doubles that define the intervals to make the discretization.\n+   *\n+   * @param candidates RDD of (value, label) pairs\n+   */\n+  private def getThresholds(candidates: RDD[(Double, Map[String,Int])]): Seq[Double] = {\n+\n+    //Create queue\n+    val stack = new mutable.Queue[((Double, Double), Option[Double])]\n+\n+    //Insert first in the stack\n+    stack.enqueue(((Double.NegativeInfinity, Double.PositiveInfinity), None))\n+    var result = Seq(Double.NegativeInfinity)\n+\n+    // While more elements to eval, continue\n+    while(stack.length > 0 && result.size < this.maxBins){\n+\n+      val (bounds, lastThresh) = stack.dequeue\n+\n+      var cands = candidates.filter({ case (th, _) => th > bounds._1 && th <= bounds._2 })\n+      val nCands = cands.count\n+      if (nCands > 0) {\n+        cands = cands.coalesce(partitions(nCands))\n+\n+        evalThresholds(cands, lastThresh) match {\n+          case Some(th) =>\n+            result = th +: result\n+            stack.enqueue(((bounds._1, th), Some(th)))\n+            stack.enqueue(((th, bounds._2), Some(th)))\n+          case None => /* criteria not fulfilled, finish */\n+        }\n+      }\n+    }\n+    (Double.PositiveInfinity +: result).sorted\n+  }\n+\n+  /**\n+   * Selects one final thresholds among the candidates and returns two partitions to recurse\n+   *\n+   * @param candidates RDD of (candidate, class frequencies between last and current candidate)\n+   * @param lastSelected last selected threshold to avoid selecting it again\n+   */\n+  private def evalThresholds(\n+      candidates: RDD[(Double, Map[String, Int])],\n+      lastSelected : Option[Double]) = {\n+\n+    var result = candidates.map({\n+      case (cand, freqs) =>\n+        (cand, freqs, Seq.empty[Int], Seq.empty[Int])\n+    }).cache\n+\n+    val numPartitions = candidates.partitions.size\n+    val bc_numPartitions = candidates.context.broadcast(numPartitions)\n+\n+    // stores accumulated freqs from left to right\n+    val l_total = candidates.context.accumulator(Map.empty[String, Int])(MapAccumulator)\n+    // stores accumulated freqs from right to left\n+    val r_total = candidates.context.accumulator(Map.empty[String, Int])(MapAccumulator)\n+\n+    // calculates accumulated frequencies for each candidate\n+    (0 until numPartitions) foreach { l2r_i =>"
  }],
  "prId": 216
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Could you comment on what this is for?\n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-03-25T21:18:30Z",
    "diffHunk": "@@ -0,0 +1,402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable.Stack\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.util.InfoTheory\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import scala.collection.mutable\n+\n+\n+/**\n+ * This class contains methods to discretize continuous values with the method proposed in\n+ * [Fayyad and Irani, Multi-Interval Discretization of Continuous-Valued Attributes, 1993]\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    @transient var data: RDD[LabeledPoint],\n+    @transient var continousFeatures: Seq[Int],\n+    var elementsPerPartition: Int = 18000,\n+    var maxBins: Int = Int.MaxValue)\n+  extends DiscretizerModel {\n+\n+  private var thresholds = Map.empty[Int, Seq[Double]]\n+  private val partitions = { x : Long => math.ceil(x.toDouble / elementsPerPartition).toInt }\n+\n+  def this() = this(null, null)\n+\n+  /**\n+   * Sets the RDD[LabeledPoint] to be discretized\n+   *\n+   * @param data RDD[LabeledPoint] to be discretized\n+   */\n+  def setData(data: RDD[LabeledPoint]): EntropyMinimizationDiscretizer = {\n+    this.data = data\n+    this\n+  }\n+\n+  /**\n+   * Sets the indexes of the features to be discretized\n+   *\n+   * @param continuousFeatures Indexes of features to be discretized\n+   */\n+  def setContinuousFeatures(continuousFeatures: Seq[Int]): EntropyMinimizationDiscretizer = {\n+    this.continousFeatures = continuousFeatures\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of elements that a partition should have\n+   *\n+   * @param ratio Maximum number of elements for a partition\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setElementsPerPartition(ratio: Int): EntropyMinimizationDiscretizer = {\n+    this.elementsPerPartition = ratio\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of discrete values\n+   *\n+   * @param maxBins Maximum number of discrete values\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setMaxBins(maxBins: Int): EntropyMinimizationDiscretizer = {\n+    this.maxBins = maxBins\n+    this\n+  }\n+  \n+  /**\n+   * Returns the thresholds used to discretized the given feature\n+   *\n+   * @param feature The number of the feature to discretize\n+   */\n+  def getThresholdsForFeature(feature: Int): Seq[Double] = {\n+    thresholds.get(feature) match {\n+      case Some(a) => a\n+      case None =>\n+        val featureValues = data.map({\n+          case LabeledPoint(label, values) => (values(feature), label.toString.trim)\n+        })\n+        val sortedValues = featureValues.sortByKey()\n+        val initial_candidates = initialThresholds(sortedValues)\n+        val thresholdsForFeature = this.getThresholds(initial_candidates)\n+        this.thresholds += ((feature, thresholdsForFeature))\n+        thresholdsForFeature\n+    }\n+  }\n+\n+  /**\n+   * Returns the thresholds used to discretized the given features\n+   *\n+   * @param features The number of the feature to discretize\n+   */\n+  def getThresholdsForFeature(features: Seq[Int]): Map[Int, Seq[Double]] = {\n+    for (feature <- features diff this.thresholds.keys.toSeq) {\n+      getThresholdsForFeature(feature)\n+    }\n+\n+    this.thresholds.filter({ features.contains(_) })\n+  }\n+\n+  /**\n+   * Returns the thresholds used to discretized the continuous features\n+   */\n+  def getThresholdsForContinuousFeatures: Map[Int, Seq[Double]] = {\n+    for (feature <- continousFeatures diff this.thresholds.keys.toSeq) {\n+      getThresholdsForFeature(feature)\n+    }\n+\n+    this.thresholds\n+  }\n+\n+  /**\n+   * Calculates the initial candidate treholds for a feature\n+   * @param data RDD of (value, label) pairs\n+   * @return RDD of (candidate, class frequencies between last and current candidate) pairs\n+   */\n+  private def initialThresholds(data: RDD[(Double, String)]): RDD[(Double, Map[String,Int])] = {\n+    data.mapPartitions({ it =>\n+      var lastX = Double.NegativeInfinity\n+      var lastY = \"\"\n+      var result = Seq.empty[(Double, Map[String, Int])]\n+      var freqs = Map.empty[String, Int]\n+\n+      for ((x, y) <- it) {\n+        if (x != lastX && y != lastY && lastX != Double.NegativeInfinity) {\n+          // new candidate and interval\n+          result = ((x + lastX) / 2, freqs) +: result\n+          freqs = freqs.empty + ((y, 1))\n+        } else {\n+          // we continue on the same interval\n+          freqs = freqs.updated(y, freqs.getOrElse(y, 0) + 1)\n+        }\n+        lastX = x\n+        lastY = y\n+      }\n+\n+      // we add last element as a candidate threshold for convenience \n+      result = (lastX, freqs) +: result\n+\n+      result.reverse.toIterator\n+    }).persist(StorageLevel.MEMORY_AND_DISK)\n+  }\n+  \n+  /**\n+   * Returns a sequence of doubles that define the intervals to make the discretization.\n+   *\n+   * @param candidates RDD of (value, label) pairs\n+   */\n+  private def getThresholds(candidates: RDD[(Double, Map[String,Int])]): Seq[Double] = {\n+\n+    //Create queue\n+    val stack = new mutable.Queue[((Double, Double), Option[Double])]\n+\n+    //Insert first in the stack\n+    stack.enqueue(((Double.NegativeInfinity, Double.PositiveInfinity), None))\n+    var result = Seq(Double.NegativeInfinity)\n+\n+    // While more elements to eval, continue\n+    while(stack.length > 0 && result.size < this.maxBins){\n+\n+      val (bounds, lastThresh) = stack.dequeue\n+\n+      var cands = candidates.filter({ case (th, _) => th > bounds._1 && th <= bounds._2 })\n+      val nCands = cands.count\n+      if (nCands > 0) {\n+        cands = cands.coalesce(partitions(nCands))",
    "line": 124
  }, {
    "body": "This tries to avoid having lots of partitions for just a few items. Since the nature of the algorithm is analyzing subsets of the initial candidates set, each time the RDD has fewer elements. If you don't coalesce the RDD you end up with an RDD with as many blocks as the original data and very few items.\n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-03-27T12:14:47Z",
    "diffHunk": "@@ -0,0 +1,402 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable.Stack\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.util.InfoTheory\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import scala.collection.mutable\n+\n+\n+/**\n+ * This class contains methods to discretize continuous values with the method proposed in\n+ * [Fayyad and Irani, Multi-Interval Discretization of Continuous-Valued Attributes, 1993]\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    @transient var data: RDD[LabeledPoint],\n+    @transient var continousFeatures: Seq[Int],\n+    var elementsPerPartition: Int = 18000,\n+    var maxBins: Int = Int.MaxValue)\n+  extends DiscretizerModel {\n+\n+  private var thresholds = Map.empty[Int, Seq[Double]]\n+  private val partitions = { x : Long => math.ceil(x.toDouble / elementsPerPartition).toInt }\n+\n+  def this() = this(null, null)\n+\n+  /**\n+   * Sets the RDD[LabeledPoint] to be discretized\n+   *\n+   * @param data RDD[LabeledPoint] to be discretized\n+   */\n+  def setData(data: RDD[LabeledPoint]): EntropyMinimizationDiscretizer = {\n+    this.data = data\n+    this\n+  }\n+\n+  /**\n+   * Sets the indexes of the features to be discretized\n+   *\n+   * @param continuousFeatures Indexes of features to be discretized\n+   */\n+  def setContinuousFeatures(continuousFeatures: Seq[Int]): EntropyMinimizationDiscretizer = {\n+    this.continousFeatures = continuousFeatures\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of elements that a partition should have\n+   *\n+   * @param ratio Maximum number of elements for a partition\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setElementsPerPartition(ratio: Int): EntropyMinimizationDiscretizer = {\n+    this.elementsPerPartition = ratio\n+    this\n+  }\n+\n+  /**\n+   * Sets the maximum number of discrete values\n+   *\n+   * @param maxBins Maximum number of discrete values\n+   * @return The Discretizer with the parameter changed\n+   */\n+  def setMaxBins(maxBins: Int): EntropyMinimizationDiscretizer = {\n+    this.maxBins = maxBins\n+    this\n+  }\n+  \n+  /**\n+   * Returns the thresholds used to discretized the given feature\n+   *\n+   * @param feature The number of the feature to discretize\n+   */\n+  def getThresholdsForFeature(feature: Int): Seq[Double] = {\n+    thresholds.get(feature) match {\n+      case Some(a) => a\n+      case None =>\n+        val featureValues = data.map({\n+          case LabeledPoint(label, values) => (values(feature), label.toString.trim)\n+        })\n+        val sortedValues = featureValues.sortByKey()\n+        val initial_candidates = initialThresholds(sortedValues)\n+        val thresholdsForFeature = this.getThresholds(initial_candidates)\n+        this.thresholds += ((feature, thresholdsForFeature))\n+        thresholdsForFeature\n+    }\n+  }\n+\n+  /**\n+   * Returns the thresholds used to discretized the given features\n+   *\n+   * @param features The number of the feature to discretize\n+   */\n+  def getThresholdsForFeature(features: Seq[Int]): Map[Int, Seq[Double]] = {\n+    for (feature <- features diff this.thresholds.keys.toSeq) {\n+      getThresholdsForFeature(feature)\n+    }\n+\n+    this.thresholds.filter({ features.contains(_) })\n+  }\n+\n+  /**\n+   * Returns the thresholds used to discretized the continuous features\n+   */\n+  def getThresholdsForContinuousFeatures: Map[Int, Seq[Double]] = {\n+    for (feature <- continousFeatures diff this.thresholds.keys.toSeq) {\n+      getThresholdsForFeature(feature)\n+    }\n+\n+    this.thresholds\n+  }\n+\n+  /**\n+   * Calculates the initial candidate treholds for a feature\n+   * @param data RDD of (value, label) pairs\n+   * @return RDD of (candidate, class frequencies between last and current candidate) pairs\n+   */\n+  private def initialThresholds(data: RDD[(Double, String)]): RDD[(Double, Map[String,Int])] = {\n+    data.mapPartitions({ it =>\n+      var lastX = Double.NegativeInfinity\n+      var lastY = \"\"\n+      var result = Seq.empty[(Double, Map[String, Int])]\n+      var freqs = Map.empty[String, Int]\n+\n+      for ((x, y) <- it) {\n+        if (x != lastX && y != lastY && lastX != Double.NegativeInfinity) {\n+          // new candidate and interval\n+          result = ((x + lastX) / 2, freqs) +: result\n+          freqs = freqs.empty + ((y, 1))\n+        } else {\n+          // we continue on the same interval\n+          freqs = freqs.updated(y, freqs.getOrElse(y, 0) + 1)\n+        }\n+        lastX = x\n+        lastY = y\n+      }\n+\n+      // we add last element as a candidate threshold for convenience \n+      result = (lastX, freqs) +: result\n+\n+      result.reverse.toIterator\n+    }).persist(StorageLevel.MEMORY_AND_DISK)\n+  }\n+  \n+  /**\n+   * Returns a sequence of doubles that define the intervals to make the discretization.\n+   *\n+   * @param candidates RDD of (value, label) pairs\n+   */\n+  private def getThresholds(candidates: RDD[(Double, Map[String,Int])]): Seq[Double] = {\n+\n+    //Create queue\n+    val stack = new mutable.Queue[((Double, Double), Option[Double])]\n+\n+    //Insert first in the stack\n+    stack.enqueue(((Double.NegativeInfinity, Double.PositiveInfinity), None))\n+    var result = Seq(Double.NegativeInfinity)\n+\n+    // While more elements to eval, continue\n+    while(stack.length > 0 && result.size < this.maxBins){\n+\n+      val (bounds, lastThresh) = stack.dequeue\n+\n+      var cands = candidates.filter({ case (th, _) => th > bounds._1 && th <= bounds._2 })\n+      val nCands = cands.count\n+      if (nCands > 0) {\n+        cands = cands.coalesce(partitions(nCands))",
    "line": 124
  }],
  "prId": 216
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "This block is still quite complicated. Let's say you have partitioned frequency sequence\n\n```\n1 2 1 3 || 1 1 1 1 || 1 2 3 2\n```\n\nFrom left to right, you want\n\n```\n1 3 4 7 || 8 9 10 11 || 12 14 17 19\n```\n\nFrom right to left, you want\n\n```\n19 18 16 15 || 12 11 10 9 || 8 7 5 2\n```\n\nThis can be done by first computing partition-wise sums:\n\n```\n7 || 4 || 8\n```\n\nThen compute cumsums from left to right\n\n```\n0 || 7 || 11 || 19\n```\n\nand from right to left\n\n```\n19 || 12 || 8 || 0\n```\n\nThe global cumsum becomes, from left to right\n\n```\n0 + cumsum(1, 2, 1, 3) || 7 + cumsum(1, 1, 1, 1) || 11 + cumsum(1, 2, 3, 2)\n```\n\nfrom right to left\n\n```\n19 - cumsum(0, 1, 2, 1) || 12 - cumsum(0, 1, 1, 1) || 8 - cumsum(0, 1, 2, 3)\n```\n\nThere are some details I may miss. But two jobs should be sufficient for this computation. \n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-04-01T16:35:58Z",
    "diffHunk": "@@ -0,0 +1,317 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+\n+/**\n+ * This class contains methods to calculate thresholds to discretize continuous values with the\n+ * method proposed by Fayyad and Irani in Multi-Interval Discretization of Continuous-Valued\n+ * Attributes (1993).\n+ *\n+ * @param continuousFeaturesIndexes Indexes of features to be discretized.\n+ * @param elementsPerPartition Maximum number of thresholds to treat in each RDD partition.\n+ * @param maxBins Maximum number of bins for each discretized feature.\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    val continuousFeaturesIndexes: Seq[Int],\n+    val elementsPerPartition: Int,\n+    val maxBins: Int)\n+  extends Serializable {\n+\n+  private val partitions = { x: Long => math.ceil(x.toDouble / elementsPerPartition).toInt }\n+  private val log2 = { x: Double => math.log(x) / math.log(2) }\n+\n+  /**\n+   * Run the algorithm with the configured parameters on an input.\n+   * @param data RDD of LabeledPoint's.\n+   * @return A EntropyMinimizationDiscretizerModel with the thresholds to discretize.\n+   */\n+  def run(data: RDD[LabeledPoint]) = {\n+    val labels2Int = data.context.broadcast(data.map(_.label).distinct.collect.zipWithIndex.toMap)\n+    val nLabels = labels2Int.value.size\n+\n+    var thresholds = Map.empty[Int, Seq[Double]]\n+    for (i <- continuousFeaturesIndexes) {\n+      val featureValues = data.map({\n+        case LabeledPoint(label, values) => (values(i), labels2Int.value(label))\n+      })\n+      val sortedValues = featureValues.sortByKey()\n+      val initialCandidates = initialThresholds(sortedValues, nLabels)\n+      val thresholdsForFeature = this.getThresholds(initialCandidates, nLabels)\n+      thresholds += ((i, thresholdsForFeature))\n+    }\n+\n+    new EntropyMinimizationDiscretizerModel(thresholds)\n+\n+  }\n+\n+  /**\n+   * Calculates the initial candidate treholds for a feature\n+   * @param data RDD of (value, label) pairs.\n+   * @param nLabels Number of distinct labels in the dataset.\n+   * @return RDD of (candidate, class frequencies between last and current candidate) pairs.\n+   */\n+  private def initialThresholds(data: RDD[(Double, Int)], nLabels: Int) = {\n+    data.mapPartitions({ it =>\n+      var lastX = Double.NegativeInfinity\n+      var lastY = Int.MinValue\n+      var result = Seq.empty[(Double, Array[Long])]\n+      var freqs = Array.fill[Long](nLabels)(0L)\n+\n+      for ((x, y) <- it) {\n+        if (x != lastX && y != lastY && lastX != Double.NegativeInfinity) {\n+          // new candidate and interval\n+          result = ((x + lastX) / 2, freqs) +: result\n+          freqs = Array.fill[Long](nLabels)(0L)\n+          freqs(y) = 1L\n+        } else {\n+          // we continue on the same interval\n+          freqs(y) += 1\n+        }\n+        lastX = x\n+        lastY = y\n+      }\n+\n+      // we add last element as a candidate threshold for convenience\n+      result = (lastX, freqs) +: result\n+\n+      result.reverse.toIterator\n+    }).persist(StorageLevel.MEMORY_AND_DISK)\n+  }\n+  \n+  /**\n+   * Returns a sequence of doubles that define the intervals to make the discretization.\n+   *\n+   * @param candidates RDD of (value, label) pairs\n+   */\n+  private def getThresholds(candidates: RDD[(Double, Array[Long])], nLabels: Int): Seq[Double] = {\n+\n+    // Create queue\n+    val stack = new mutable.Queue[((Double, Double), Option[Double])]\n+\n+    // Insert first in the stack\n+    stack.enqueue(((Double.NegativeInfinity, Double.PositiveInfinity), None))\n+    var result = Seq(Double.NegativeInfinity)\n+\n+    // While more elements to eval, continue\n+    while(stack.length > 0 && result.size < this.maxBins){\n+\n+      val (bounds, lastThresh) = stack.dequeue\n+\n+      var cands = candidates.filter({ case (th, _) => th > bounds._1 && th <= bounds._2 })\n+      val nCands = cands.count\n+      if (nCands > 0) {\n+        cands = cands.coalesce(partitions(nCands))\n+\n+        evalThresholds(cands, lastThresh, nLabels) match {\n+          case Some(th) =>\n+            result = th +: result\n+            stack.enqueue(((bounds._1, th), Some(th)))\n+            stack.enqueue(((th, bounds._2), Some(th)))\n+          case None => /* criteria not fulfilled, finish */\n+        }\n+      }\n+    }\n+    (Double.PositiveInfinity +: result).sorted\n+  }\n+\n+  /**\n+   * Selects one final thresholds among the candidates and returns two partitions to recurse\n+   *\n+   * @param candidates RDD of (candidate, class frequencies between last and current candidate)\n+   * @param lastSelected last selected threshold to avoid selecting it again\n+   */\n+  private def evalThresholds(\n+      candidates: RDD[(Double, Array[Long])],\n+      lastSelected : Option[Double],\n+      nLabels: Int) = {\n+\n+    var result = candidates.map({\n+      case (cand, freqs) =>\n+        (cand, freqs, Array.empty[Long], Array.empty[Long])\n+    }).cache\n+\n+    val numPartitions = candidates.partitions.size\n+    val bcNumPartitions = candidates.context.broadcast(numPartitions)\n+\n+    // stores accumulated freqs from left to right\n+    val bcLeftTotal = candidates.context.accumulator(Array.fill(nLabels)(0L))(ArrayAccumulator)\n+    // stores accumulated freqs from right to left\n+    val bcRightTotal = candidates.context.accumulator(Array.fill(nLabels)(0L))(ArrayAccumulator)\n+\n+    // calculates accumulated frequencies for each candidate\n+    (0 until numPartitions) foreach { l2rIndex =>"
  }],
  "prId": 216
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`^` means binary XOR in Scala. I assume that you need `math.pow` here. But if I'm wrong, please put a comment before this line and explain the bitwise operation.\n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-04-01T16:43:08Z",
    "diffHunk": "@@ -0,0 +1,317 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+\n+/**\n+ * This class contains methods to calculate thresholds to discretize continuous values with the\n+ * method proposed by Fayyad and Irani in Multi-Interval Discretization of Continuous-Valued\n+ * Attributes (1993).\n+ *\n+ * @param continuousFeaturesIndexes Indexes of features to be discretized.\n+ * @param elementsPerPartition Maximum number of thresholds to treat in each RDD partition.\n+ * @param maxBins Maximum number of bins for each discretized feature.\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    val continuousFeaturesIndexes: Seq[Int],\n+    val elementsPerPartition: Int,\n+    val maxBins: Int)\n+  extends Serializable {\n+\n+  private val partitions = { x: Long => math.ceil(x.toDouble / elementsPerPartition).toInt }\n+  private val log2 = { x: Double => math.log(x) / math.log(2) }\n+\n+  /**\n+   * Run the algorithm with the configured parameters on an input.\n+   * @param data RDD of LabeledPoint's.\n+   * @return A EntropyMinimizationDiscretizerModel with the thresholds to discretize.\n+   */\n+  def run(data: RDD[LabeledPoint]) = {\n+    val labels2Int = data.context.broadcast(data.map(_.label).distinct.collect.zipWithIndex.toMap)\n+    val nLabels = labels2Int.value.size\n+\n+    var thresholds = Map.empty[Int, Seq[Double]]\n+    for (i <- continuousFeaturesIndexes) {\n+      val featureValues = data.map({\n+        case LabeledPoint(label, values) => (values(i), labels2Int.value(label))\n+      })\n+      val sortedValues = featureValues.sortByKey()\n+      val initialCandidates = initialThresholds(sortedValues, nLabels)\n+      val thresholdsForFeature = this.getThresholds(initialCandidates, nLabels)\n+      thresholds += ((i, thresholdsForFeature))\n+    }\n+\n+    new EntropyMinimizationDiscretizerModel(thresholds)\n+\n+  }\n+\n+  /**\n+   * Calculates the initial candidate treholds for a feature\n+   * @param data RDD of (value, label) pairs.\n+   * @param nLabels Number of distinct labels in the dataset.\n+   * @return RDD of (candidate, class frequencies between last and current candidate) pairs.\n+   */\n+  private def initialThresholds(data: RDD[(Double, Int)], nLabels: Int) = {\n+    data.mapPartitions({ it =>\n+      var lastX = Double.NegativeInfinity\n+      var lastY = Int.MinValue\n+      var result = Seq.empty[(Double, Array[Long])]\n+      var freqs = Array.fill[Long](nLabels)(0L)\n+\n+      for ((x, y) <- it) {\n+        if (x != lastX && y != lastY && lastX != Double.NegativeInfinity) {\n+          // new candidate and interval\n+          result = ((x + lastX) / 2, freqs) +: result\n+          freqs = Array.fill[Long](nLabels)(0L)\n+          freqs(y) = 1L\n+        } else {\n+          // we continue on the same interval\n+          freqs(y) += 1\n+        }\n+        lastX = x\n+        lastY = y\n+      }\n+\n+      // we add last element as a candidate threshold for convenience\n+      result = (lastX, freqs) +: result\n+\n+      result.reverse.toIterator\n+    }).persist(StorageLevel.MEMORY_AND_DISK)\n+  }\n+  \n+  /**\n+   * Returns a sequence of doubles that define the intervals to make the discretization.\n+   *\n+   * @param candidates RDD of (value, label) pairs\n+   */\n+  private def getThresholds(candidates: RDD[(Double, Array[Long])], nLabels: Int): Seq[Double] = {\n+\n+    // Create queue\n+    val stack = new mutable.Queue[((Double, Double), Option[Double])]\n+\n+    // Insert first in the stack\n+    stack.enqueue(((Double.NegativeInfinity, Double.PositiveInfinity), None))\n+    var result = Seq(Double.NegativeInfinity)\n+\n+    // While more elements to eval, continue\n+    while(stack.length > 0 && result.size < this.maxBins){\n+\n+      val (bounds, lastThresh) = stack.dequeue\n+\n+      var cands = candidates.filter({ case (th, _) => th > bounds._1 && th <= bounds._2 })\n+      val nCands = cands.count\n+      if (nCands > 0) {\n+        cands = cands.coalesce(partitions(nCands))\n+\n+        evalThresholds(cands, lastThresh, nLabels) match {\n+          case Some(th) =>\n+            result = th +: result\n+            stack.enqueue(((bounds._1, th), Some(th)))\n+            stack.enqueue(((th, bounds._2), Some(th)))\n+          case None => /* criteria not fulfilled, finish */\n+        }\n+      }\n+    }\n+    (Double.PositiveInfinity +: result).sorted\n+  }\n+\n+  /**\n+   * Selects one final thresholds among the candidates and returns two partitions to recurse\n+   *\n+   * @param candidates RDD of (candidate, class frequencies between last and current candidate)\n+   * @param lastSelected last selected threshold to avoid selecting it again\n+   */\n+  private def evalThresholds(\n+      candidates: RDD[(Double, Array[Long])],\n+      lastSelected : Option[Double],\n+      nLabels: Int) = {\n+\n+    var result = candidates.map({\n+      case (cand, freqs) =>\n+        (cand, freqs, Array.empty[Long], Array.empty[Long])\n+    }).cache\n+\n+    val numPartitions = candidates.partitions.size\n+    val bcNumPartitions = candidates.context.broadcast(numPartitions)\n+\n+    // stores accumulated freqs from left to right\n+    val bcLeftTotal = candidates.context.accumulator(Array.fill(nLabels)(0L))(ArrayAccumulator)\n+    // stores accumulated freqs from right to left\n+    val bcRightTotal = candidates.context.accumulator(Array.fill(nLabels)(0L))(ArrayAccumulator)\n+\n+    // calculates accumulated frequencies for each candidate\n+    (0 until numPartitions) foreach { l2rIndex =>\n+\n+      val leftTotal = bcLeftTotal.value\n+      val rightTotal = bcRightTotal.value\n+\n+      result =\n+        result.mapPartitionsWithIndex({ (slice, it) =>\n+\n+          val l2r = slice == l2rIndex\n+          val r2l = slice == bcNumPartitions.value - 1 - l2rIndex\n+\n+          if (l2r && r2l) {\n+\n+            // accumulate both from left to right and right to left\n+            var partialResult = Seq.empty[(Double, Array[Long], Array[Long], Array[Long])]\n+            var accum = leftTotal\n+\n+            for ((cand, freqs, _, rightFreqs) <- it) {\n+              for (i <- 0 until nLabels) accum(i) += freqs(i)\n+              partialResult = (cand, freqs, accum.clone, rightFreqs) +: partialResult\n+            }\n+\n+            bcLeftTotal += accum\n+\n+            val r2lIt = partialResult.iterator\n+            partialResult = Seq.empty[(Double, Array[Long], Array[Long], Array[Long])]\n+            accum = Array.fill[Long](nLabels)(0L)\n+\n+            for ((cand, freqs, leftFreqs, _) <- r2lIt) {\n+              partialResult = (cand, freqs, leftFreqs, accum.clone) +: partialResult\n+              for (i <- 0 until nLabels) accum(i) += freqs(i)\n+            }\n+\n+            bcRightTotal += accum\n+\n+            partialResult.iterator\n+\n+          } else if (l2r) {\n+\n+            // accumulate freqs from left to right\n+            var partialResult = Seq.empty[(Double, Array[Long], Array[Long], Array[Long])]\n+            val accum = leftTotal\n+\n+            for ((cand, freqs, _, rightFreqs) <- it) {\n+              for (i <- 0 until nLabels) accum(i) += freqs(i)\n+              partialResult = (cand, freqs, accum.clone, rightFreqs) +: partialResult\n+            }\n+\n+            bcLeftTotal += accum\n+            partialResult.reverseIterator\n+\n+          } else if (r2l) {\n+\n+            // accumulate freqs from right to left\n+            val r2lIt = it.toSeq.reverseIterator\n+\n+            var partialResult = Seq.empty[(Double, Array[Long], Array[Long], Array[Long])]\n+            val accum = rightTotal\n+\n+            for ((cand, freqs, leftFreqs, _) <- r2lIt) {\n+              partialResult = (cand, freqs, leftFreqs, accum.clone) +: partialResult\n+              for (i <- 0 until nLabels) accum(i) += freqs(i)\n+            }\n+\n+            bcRightTotal += accum\n+\n+            partialResult.iterator\n+\n+          } else {\n+            // do nothing in this iteration\n+            it\n+          }\n+        }, true) // important to maintain partitions within the loop\n+        .persist(StorageLevel.MEMORY_AND_DISK) // needed, otherwise spark repeats calculations\n+\n+      result.foreachPartition({ _ => }) // Forces the iteration to be calculated\n+\n+    }\n+\n+    // calculate h(S)\n+    // s: number of elements\n+    // k: number of distinct classes\n+    // hs: entropy\n+\n+    val s  = bcLeftTotal.value.reduce(_ + _)\n+    val hs = InfoTheory.entropy(bcLeftTotal.value.toSeq, s)\n+    val k  = bcLeftTotal.value.filter(_ != 0).size\n+\n+    // select best threshold according to the criteria\n+    val finalCandidates =\n+      result.flatMap({\n+        case (cand, _, leftFreqs, rightFreqs) =>\n+\n+          val k1  = leftFreqs.filter(_ != 0).size\n+          val s1  = if (k1 > 0) leftFreqs.reduce(_ + _) else 0\n+          val hs1 = InfoTheory.entropy(leftFreqs, s1)\n+\n+          val k2  = rightFreqs.filter(_ != 0).size\n+          val s2  = if (k2 > 0) rightFreqs.reduce(_ + _) else 0\n+          val hs2 = InfoTheory.entropy(rightFreqs, s2)\n+\n+          val weightedHs = (s1 * hs1 + s2 * hs2) / s\n+          val gain        = hs - weightedHs\n+          val delta       = log2(3 ^ k - 2) - (k * hs - k1 * hs1 - k2 * hs2)"
  }],
  "prId": 216
}, {
  "comments": [{
    "author": {
      "login": "avulanov"
    },
    "body": "values(i) -->> values.toArray(i)\n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-08-11T13:54:45Z",
    "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+\n+/**\n+ * This class contains methods to calculate thresholds to discretize continuous values with the\n+ * method proposed by Fayyad and Irani in Multi-Interval Discretization of Continuous-Valued\n+ * Attributes (1993).\n+ *\n+ * @param continuousFeaturesIndexes Indexes of features to be discretized.\n+ * @param elementsPerPartition Maximum number of thresholds to treat in each RDD partition.\n+ * @param maxBins Maximum number of bins for each discretized feature.\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    val continuousFeaturesIndexes: Seq[Int],\n+    val elementsPerPartition: Int,\n+    val maxBins: Int)\n+  extends Serializable {\n+\n+  private val partitions = { x: Long => math.ceil(x.toDouble / elementsPerPartition).toInt }\n+  private val log2 = { x: Double => math.log(x) / math.log(2) }\n+\n+  /**\n+   * Run the algorithm with the configured parameters on an input.\n+   * @param data RDD of LabeledPoint's.\n+   * @return A EntropyMinimizationDiscretizerModel with the thresholds to discretize.\n+   */\n+  def run(data: RDD[LabeledPoint]) = {\n+    val labels2Int = data.context.broadcast(data.map(_.label).distinct.collect.zipWithIndex.toMap)\n+    val nLabels = labels2Int.value.size\n+\n+    var thresholds = Map.empty[Int, Seq[Double]]\n+    for (i <- continuousFeaturesIndexes) {\n+      val featureValues = data.map({\n+        case LabeledPoint(label, values) => (values(i), labels2Int.value(label))",
    "line": 56
  }],
  "prId": 216
}, {
  "comments": [{
    "author": {
      "login": "leizongxiong"
    },
    "body": "leftTotal(j) += bcTotals.value(i)(j)? I think it should be  rightTotal(j) += bcTotals.value(i)(j)  @LIDIAgroup  and do you think about take one sample on candidate thresh value?\n",
    "commit": "70b63e42d81f9ca9f9c48bf8435f9620c6800b75",
    "createdAt": "2014-11-02T09:08:43Z",
    "diffHunk": "@@ -0,0 +1,276 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.discretization\n+\n+import scala.collection.mutable\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.regression.LabeledPoint\n+\n+/**\n+ * This class contains methods to calculate thresholds to discretize continuous values with the\n+ * method proposed by Fayyad and Irani in Multi-Interval Discretization of Continuous-Valued\n+ * Attributes (1993).\n+ *\n+ * @param continuousFeaturesIndexes Indexes of features to be discretized.\n+ * @param elementsPerPartition Maximum number of thresholds to treat in each RDD partition.\n+ * @param maxBins Maximum number of bins for each discretized feature.\n+ */\n+class EntropyMinimizationDiscretizer private (\n+    val continuousFeaturesIndexes: Seq[Int],\n+    val elementsPerPartition: Int,\n+    val maxBins: Int)\n+  extends Serializable {\n+\n+  private val partitions = { x: Long => math.ceil(x.toDouble / elementsPerPartition).toInt }\n+  private val log2 = { x: Double => math.log(x) / math.log(2) }\n+\n+  /**\n+   * Run the algorithm with the configured parameters on an input.\n+   * @param data RDD of LabeledPoint's.\n+   * @return A EntropyMinimizationDiscretizerModel with the thresholds to discretize.\n+   */\n+  def run(data: RDD[LabeledPoint]) = {\n+    val labels2Int = data.context.broadcast(data.map(_.label).distinct.collect.zipWithIndex.toMap)\n+    val nLabels = labels2Int.value.size\n+\n+    var thresholds = Map.empty[Int, Seq[Double]]\n+    for (i <- continuousFeaturesIndexes) {\n+      val featureValues = data.map({\n+        case LabeledPoint(label, values) => (values(i), labels2Int.value(label))\n+      })\n+      val sortedValues = featureValues.sortByKey()\n+      val initialCandidates = initialThresholds(sortedValues, nLabels)\n+      val thresholdsForFeature = this.getThresholds(initialCandidates, nLabels)\n+      thresholds += ((i, thresholdsForFeature))\n+    }\n+\n+    new EntropyMinimizationDiscretizerModel(thresholds)\n+\n+  }\n+\n+  /**\n+   * Calculates the initial candidate treholds for a feature\n+   * @param data RDD of (value, label) pairs.\n+   * @param nLabels Number of distinct labels in the dataset.\n+   * @return RDD of (candidate, class frequencies between last and current candidate) pairs.\n+   */\n+  private def initialThresholds(data: RDD[(Double, Int)], nLabels: Int) = {\n+    data.mapPartitions({ it =>\n+      var lastX = Double.NegativeInfinity\n+      var lastY = Int.MinValue\n+      var result = Seq.empty[(Double, Array[Long])]\n+      var freqs = Array.fill[Long](nLabels)(0L)\n+\n+      for ((x, y) <- it) {\n+        if (x != lastX && y != lastY && lastX != Double.NegativeInfinity) {\n+          // new candidate and interval\n+          result = ((x + lastX) / 2, freqs) +: result\n+          freqs = Array.fill[Long](nLabels)(0L)\n+          freqs(y) = 1L\n+        } else {\n+          // we continue on the same interval\n+          freqs(y) += 1\n+        }\n+        lastX = x\n+        lastY = y\n+      }\n+\n+      // we add last element as a candidate threshold for convenience\n+      result = (lastX, freqs) +: result\n+\n+      result.reverse.toIterator\n+    }).persist(StorageLevel.MEMORY_AND_DISK)\n+  }\n+  \n+  /**\n+   * Returns a sequence of doubles that define the intervals to make the discretization.\n+   *\n+   * @param candidates RDD of (value, label) pairs\n+   */\n+  private def getThresholds(candidates: RDD[(Double, Array[Long])], nLabels: Int): Seq[Double] = {\n+\n+    // Create queue\n+    val stack = new mutable.Queue[((Double, Double), Option[Double])]\n+\n+    // Insert first in the stack\n+    stack.enqueue(((Double.NegativeInfinity, Double.PositiveInfinity), None))\n+    var result = Seq(Double.NegativeInfinity)\n+\n+    // While more elements to eval, continue\n+    while(stack.length > 0 && result.size < this.maxBins){\n+\n+      val (bounds, lastThresh) = stack.dequeue\n+\n+      var cands = candidates.filter({ case (th, _) => th > bounds._1 && th <= bounds._2 })\n+      val nCands = cands.count\n+      if (nCands > 0) {\n+        cands = cands.coalesce(partitions(nCands))\n+\n+        evalThresholds(cands, lastThresh, nLabels) match {\n+          case Some(th) =>\n+            result = th +: result\n+            stack.enqueue(((bounds._1, th), Some(th)))\n+            stack.enqueue(((th, bounds._2), Some(th)))\n+          case None => /* criteria not fulfilled, finish */\n+        }\n+      }\n+    }\n+    (Double.PositiveInfinity +: result).sorted\n+  }\n+\n+  /**\n+   * Selects one final thresholds among the candidates and returns two partitions to recurse\n+   *\n+   * @param candidates RDD of (candidate, class frequencies between last and current candidate)\n+   * @param lastSelected last selected threshold to avoid selecting it again\n+   */\n+  private def evalThresholds(\n+      candidates: RDD[(Double, Array[Long])],\n+      lastSelected : Option[Double],\n+      nLabels: Int) = {\n+\n+    val numPartitions = candidates.partitions.size\n+\n+    val sc = candidates.sparkContext\n+\n+    // store total frequencies for each partition\n+    val totals = sc.runJob(candidates, { case it =>\n+      val accum = Array.fill(nLabels)(0L)\n+      for ((_, freqs) <- it) {\n+        for (i <- 0 until nLabels) accum(i) += freqs(i)\n+      }\n+      accum\n+    }: (Iterator[(Double, Array[Long])]) => Array[Long])\n+\n+    val bcTotals = sc.broadcast(totals)\n+\n+    val result =\n+      candidates.mapPartitionsWithIndex({ (slice, it) =>\n+\n+        // accumulate freqs from left to right\n+        val leftTotal = Array.fill(nLabels)(0L)\n+        for (i <- 0 until slice) {\n+          for (j <- 0 until nLabels) leftTotal(j) += bcTotals.value(i)(j)\n+        }\n+\n+        var leftAccum = Seq.empty[(Double, Array[Long], Array[Long])]\n+\n+        for ((cand, freqs) <- it) {\n+          for (i <- 0 until nLabels) leftTotal(i) += freqs(i)\n+          leftAccum = (cand, freqs, leftTotal.clone) +: leftAccum\n+        }\n+\n+        // accumulate freqs from right to left\n+        val rightTotal = Array.fill(nLabels)(0L)\n+        for (i <- slice + 1 until numPartitions) {\n+          for (j <- 0 until nLabels) leftTotal(j) += bcTotals.value(i)(j)",
    "line": 183
  }],
  "prId": 216
}]