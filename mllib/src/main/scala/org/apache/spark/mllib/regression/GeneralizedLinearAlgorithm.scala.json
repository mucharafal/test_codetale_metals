[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "should use `input` itself\n",
    "commit": "f19fc02418b7790103406bfb8f1c0e982abe17f2",
    "createdAt": "2014-08-12T07:35:29Z",
    "diffHunk": "@@ -137,11 +154,45 @@ abstract class GeneralizedLinearAlgorithm[M <: GeneralizedLinearModel]\n       throw new SparkException(\"Input validation failed.\")\n     }\n \n+    /**\n+     * Scaling to minimize the condition number:\n+     *\n+     * During the optimization process, the convergence (rate) depends on the condition number of\n+     * the training dataset. Scaling the variables often reduces this condition number, thus\n+     * improving the convergence rate dramatically. Without reducing the condition number,\n+     * some training datasets mixing the columns with different scales may not be able to converge.\n+     *\n+     * GLMNET and LIBSVM packages perform the scaling to reduce the condition number, and return\n+     * the weights in the original scale.\n+     * See page 9 in http://cran.r-project.org/web/packages/glmnet/glmnet.pdf\n+     *\n+     * Here, if useFeatureScaling is enabled, we will standardize the training features by dividing\n+     * the variance of each column (without subtracting the mean), and train the model in the\n+     * scaled space. Then we transform the coefficients from the scaled space to the original scale\n+     * as GLMNET and LIBSVM do.\n+     *\n+     * Currently, it's only enabled in LogisticRegressionWithLBFGS\n+     */\n+    val scaler = if (useFeatureScaling) {\n+      (new StandardScaler).fit(input.map(x => x.features))\n+    } else {\n+      null\n+    }\n+\n     // Prepend an extra variable consisting of all 1.0's for the intercept.\n     val data = if (addIntercept) {\n-      input.map(labeledPoint => (labeledPoint.label, appendBias(labeledPoint.features)))\n+      if(useFeatureScaling) {\n+        input.map(labeledPoint =>\n+          (labeledPoint.label, appendBias(scaler.transform(labeledPoint.features))))\n+      } else {\n+        input.map(labeledPoint => (labeledPoint.label, appendBias(labeledPoint.features)))\n+      }\n     } else {\n-      input.map(labeledPoint => (labeledPoint.label, labeledPoint.features))\n+      if (useFeatureScaling) {\n+        input.map(labeledPoint => (labeledPoint.label, scaler.transform(labeledPoint.features)))\n+      } else {\n+        input.map(labeledPoint => (labeledPoint.label, labeledPoint.features))",
    "line": 74
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "It's not identical map. It's converting labeledPoint to tuple of response and feature vector for optimizer. \n",
    "commit": "f19fc02418b7790103406bfb8f1c0e982abe17f2",
    "createdAt": "2014-08-13T01:26:25Z",
    "diffHunk": "@@ -137,11 +154,45 @@ abstract class GeneralizedLinearAlgorithm[M <: GeneralizedLinearModel]\n       throw new SparkException(\"Input validation failed.\")\n     }\n \n+    /**\n+     * Scaling to minimize the condition number:\n+     *\n+     * During the optimization process, the convergence (rate) depends on the condition number of\n+     * the training dataset. Scaling the variables often reduces this condition number, thus\n+     * improving the convergence rate dramatically. Without reducing the condition number,\n+     * some training datasets mixing the columns with different scales may not be able to converge.\n+     *\n+     * GLMNET and LIBSVM packages perform the scaling to reduce the condition number, and return\n+     * the weights in the original scale.\n+     * See page 9 in http://cran.r-project.org/web/packages/glmnet/glmnet.pdf\n+     *\n+     * Here, if useFeatureScaling is enabled, we will standardize the training features by dividing\n+     * the variance of each column (without subtracting the mean), and train the model in the\n+     * scaled space. Then we transform the coefficients from the scaled space to the original scale\n+     * as GLMNET and LIBSVM do.\n+     *\n+     * Currently, it's only enabled in LogisticRegressionWithLBFGS\n+     */\n+    val scaler = if (useFeatureScaling) {\n+      (new StandardScaler).fit(input.map(x => x.features))\n+    } else {\n+      null\n+    }\n+\n     // Prepend an extra variable consisting of all 1.0's for the intercept.\n     val data = if (addIntercept) {\n-      input.map(labeledPoint => (labeledPoint.label, appendBias(labeledPoint.features)))\n+      if(useFeatureScaling) {\n+        input.map(labeledPoint =>\n+          (labeledPoint.label, appendBias(scaler.transform(labeledPoint.features))))\n+      } else {\n+        input.map(labeledPoint => (labeledPoint.label, appendBias(labeledPoint.features)))\n+      }\n     } else {\n-      input.map(labeledPoint => (labeledPoint.label, labeledPoint.features))\n+      if (useFeatureScaling) {\n+        input.map(labeledPoint => (labeledPoint.label, scaler.transform(labeledPoint.features)))\n+      } else {\n+        input.map(labeledPoint => (labeledPoint.label, labeledPoint.features))",
    "line": 74
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Sorry, I didn't realize that.\n",
    "commit": "f19fc02418b7790103406bfb8f1c0e982abe17f2",
    "createdAt": "2014-08-14T05:32:44Z",
    "diffHunk": "@@ -137,11 +154,45 @@ abstract class GeneralizedLinearAlgorithm[M <: GeneralizedLinearModel]\n       throw new SparkException(\"Input validation failed.\")\n     }\n \n+    /**\n+     * Scaling to minimize the condition number:\n+     *\n+     * During the optimization process, the convergence (rate) depends on the condition number of\n+     * the training dataset. Scaling the variables often reduces this condition number, thus\n+     * improving the convergence rate dramatically. Without reducing the condition number,\n+     * some training datasets mixing the columns with different scales may not be able to converge.\n+     *\n+     * GLMNET and LIBSVM packages perform the scaling to reduce the condition number, and return\n+     * the weights in the original scale.\n+     * See page 9 in http://cran.r-project.org/web/packages/glmnet/glmnet.pdf\n+     *\n+     * Here, if useFeatureScaling is enabled, we will standardize the training features by dividing\n+     * the variance of each column (without subtracting the mean), and train the model in the\n+     * scaled space. Then we transform the coefficients from the scaled space to the original scale\n+     * as GLMNET and LIBSVM do.\n+     *\n+     * Currently, it's only enabled in LogisticRegressionWithLBFGS\n+     */\n+    val scaler = if (useFeatureScaling) {\n+      (new StandardScaler).fit(input.map(x => x.features))\n+    } else {\n+      null\n+    }\n+\n     // Prepend an extra variable consisting of all 1.0's for the intercept.\n     val data = if (addIntercept) {\n-      input.map(labeledPoint => (labeledPoint.label, appendBias(labeledPoint.features)))\n+      if(useFeatureScaling) {\n+        input.map(labeledPoint =>\n+          (labeledPoint.label, appendBias(scaler.transform(labeledPoint.features))))\n+      } else {\n+        input.map(labeledPoint => (labeledPoint.label, appendBias(labeledPoint.features)))\n+      }\n     } else {\n-      input.map(labeledPoint => (labeledPoint.label, labeledPoint.features))\n+      if (useFeatureScaling) {\n+        input.map(labeledPoint => (labeledPoint.label, scaler.transform(labeledPoint.features)))\n+      } else {\n+        input.map(labeledPoint => (labeledPoint.label, labeledPoint.features))",
    "line": 74
  }],
  "prId": 1897
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`minimize the condition number` is not accurate. We can say `scaling columns to unit variance as a heuristic to reduce the condition number`.\n",
    "commit": "f19fc02418b7790103406bfb8f1c0e982abe17f2",
    "createdAt": "2014-08-12T07:39:09Z",
    "diffHunk": "@@ -137,11 +154,45 @@ abstract class GeneralizedLinearAlgorithm[M <: GeneralizedLinearModel]\n       throw new SparkException(\"Input validation failed.\")\n     }\n \n+    /**\n+     * Scaling to minimize the condition number:"
  }],
  "prId": 1897
}]