[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Use `zipWithUniqueId`, which doesn't trigger a job.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-12T23:24:02Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "I need the exact index of each entry in order to compute accurate ranks in the case of duplicate values. Doesn't seem like zipWithUniqueId allows me to do that esp when duplicate values fall into different partitions. Suggestions on making it work with zipWithUniqueId?\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-14T19:45:53Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "FWIW I tried it with zipWithUniqueId, and, as expected, the results were wrong when more than 1 partition is used.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-16T18:31:42Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "The input `X` is not ordered. I think the purpose here is just to assign each row an id for joining back the elements after we obtain the rank for each column.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-16T18:54:35Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "you're right. sorry i was thinking about the 2nd zipWithIndex call (on the sorted RDD). That one absolutely cannot be replaced with zipWithUnqueId or the results are all wrong.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-16T19:06:53Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "If `zipWithUnqueId` is used, this block becomes unnecessary.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-12T23:25:33Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "you would still branch the lineage into numCols RDDs, which then get joined back into a single RDD. Does the joining back prevent recalculation of the common lineage prefix when it's branched?\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-14T19:24:48Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "See my previous comment about mentioning the cost in the doc. User should be responsible to cache the RDD. If user already cached the input RDD, we are duplicating the data here.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-16T18:58:26Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Please add a warning if `numCols` is large, say,  > 100. I'm not sure whether this is a good threshold. It would be great if you can try some examples and set a reasonable threshold.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-12T23:27:24Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "The code fits a single line: `val column = indexed.map { case (vector, index) => (vector(k), index) }`\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-12T23:30:32Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`sortByKey` is done through a range partitioner, which means records having the same key will be inside the same partition (please check the code and confirm). So `mapPartitions` should be sufficient to break ties.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-12T23:35:30Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "\"Breaking ties\" is a loose description of what happens. I actually need all items of the same value in the same partition in order to take the average of their positions in the sorted list. I'm open to suggestions on how to make it work with mapPartitions though.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-14T19:47:12Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "The `RangePartitioner` guarantees  same keys appear in the same partition. Then inside each partition, you go through an iterator of (value: Double, rank: Long). Do not output a value and its rank directly. Count records with the same value until a different value comes up, then flush out the value and the average rank.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-16T19:03:00Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "Hmm so one problem with the buffering approach for duplicates is that the uniqueIds for all the duplicate entries need to be saved in a list (since they're not necessarily sequential, so we can't do some kind of range thing). In the degenerate case where everything is a duplicate in a big RDD, this can be problematic (unless we want to get really fancy about saving ranges of IDs).\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-16T23:11:07Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Right, we also need to maintain the list of original row ids. If a column only has one distinct value, groupBy may not work well either. We use Long ids. With a 100MB buffer, we can handle 12M entries. I hope it is already large enough for practical use. The groupBy operator may trigger a global shuffle, which is slow. Please check whether we can use the `RangePartitioner` obtained from `sortByKey` in groupBy so it doesn't trigger a global shuffle.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-17T00:41:25Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "RangePartitioner should be fine since getPartition works on distinct keys (so like you said, duplicate values are assigned to the same partition). As with many other things, we could just add a warning in the docs saying that if there's too many duplicates, use with caution.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-17T00:50:51Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`ranks(i)` doesn't have a partitioner. So this should fall back to a default `HashPartitioner` with defaultParallelism. If that is the case, please use `HashPartitioner` directly to avoid confusion.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-12T23:41:41Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)\n+    val ranks = groupedByValue.flatMap[(Long, Double)] { item =>\n+      val duplicates = item._2\n+      if (duplicates.size > 1) {\n+        val averageRank = duplicates.foldLeft(0L) {_ + _._2 + 1} / duplicates.size.toDouble\n+        duplicates.map(entry => (entry._1._2, averageRank)).toSeq\n+      } else {\n+        duplicates.map(entry => (entry._1._2, entry._2.toDouble + 1)).toSeq\n+      }\n+    }\n+    ranks.sortByKey()\n+  }\n+\n+  private def makeRankMatrix(ranks: Array[RDD[(Long, Double)]]): RDD[Vector] = {\n+    val partitioner = Partitioner.defaultPartitioner(ranks(0), ranks.tail: _*)"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "That reminds me, changed `val column` to be created using `mapPartitions` instead in order to preserve original partitioning in order to avoid shuffling, so they should have partitioner inherited from `indexed`. In either case, I think it's actually cleaner to use the Partitioner.defaultPartitioner since it's exactly intended to create a partitions given a bunch of RDDs. (It does one more check if default.parallelism isn't set, and I don't think there's point in copying in the second half of the function since it's just bloating the codebase with duplicate logic.)\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-14T20:09:46Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)\n+    val ranks = groupedByValue.flatMap[(Long, Double)] { item =>\n+      val duplicates = item._2\n+      if (duplicates.size > 1) {\n+        val averageRank = duplicates.foldLeft(0L) {_ + _._2 + 1} / duplicates.size.toDouble\n+        duplicates.map(entry => (entry._1._2, averageRank)).toSeq\n+      } else {\n+        duplicates.map(entry => (entry._1._2, entry._2.toDouble + 1)).toSeq\n+      }\n+    }\n+    ranks.sortByKey()\n+  }\n+\n+  private def makeRankMatrix(ranks: Array[RDD[(Long, Double)]]): RDD[Vector] = {\n+    val partitioner = Partitioner.defaultPartitioner(ranks(0), ranks.tail: _*)"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "The code is weird because `ranks` do share any partitioner. It is cleaner to use HashPartitioner with the same number of partitions in the input RDD or simply `defaultParallelisim`.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-16T19:06:54Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)\n+    val ranks = groupedByValue.flatMap[(Long, Double)] { item =>\n+      val duplicates = item._2\n+      if (duplicates.size > 1) {\n+        val averageRank = duplicates.foldLeft(0L) {_ + _._2 + 1} / duplicates.size.toDouble\n+        duplicates.map(entry => (entry._1._2, averageRank)).toSeq\n+      } else {\n+        duplicates.map(entry => (entry._1._2, entry._2.toDouble + 1)).toSeq\n+      }\n+    }\n+    ranks.sortByKey()\n+  }\n+\n+  private def makeRankMatrix(ranks: Array[RDD[(Long, Double)]]): RDD[Vector] = {\n+    val partitioner = Partitioner.defaultPartitioner(ranks(0), ranks.tail: _*)"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "number of partitions in input RDD is better given the two choices since defaultParallelism can be unset (in which case we need a copypasta of the second half of Partitioner.defaultPartitioner)\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-16T21:25:25Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)\n+    val ranks = groupedByValue.flatMap[(Long, Double)] { item =>\n+      val duplicates = item._2\n+      if (duplicates.size > 1) {\n+        val averageRank = duplicates.foldLeft(0L) {_ + _._2 + 1} / duplicates.size.toDouble\n+        duplicates.map(entry => (entry._1._2, averageRank)).toSeq\n+      } else {\n+        duplicates.map(entry => (entry._1._2, entry._2.toDouble + 1)).toSeq\n+      }\n+    }\n+    ranks.sortByKey()\n+  }\n+\n+  private def makeRankMatrix(ranks: Array[RDD[(Long, Double)]]): RDD[Vector] = {\n+    val partitioner = Partitioner.defaultPartitioner(ranks(0), ranks.tail: _*)"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "sounds good.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-17T04:24:02Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)\n+    val ranks = groupedByValue.flatMap[(Long, Double)] { item =>\n+      val duplicates = item._2\n+      if (duplicates.size > 1) {\n+        val averageRank = duplicates.foldLeft(0L) {_ + _._2 + 1} / duplicates.size.toDouble\n+        duplicates.map(entry => (entry._1._2, averageRank)).toSeq\n+      } else {\n+        duplicates.map(entry => (entry._1._2, entry._2.toDouble + 1)).toSeq\n+      }\n+    }\n+    ranks.sortByKey()\n+  }\n+\n+  private def makeRankMatrix(ranks: Array[RDD[(Long, Double)]]): RDD[Vector] = {\n+    val partitioner = Partitioner.defaultPartitioner(ranks(0), ranks.tail: _*)"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "replace `(` by a space \n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-12T23:41:58Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)\n+    val ranks = groupedByValue.flatMap[(Long, Double)] { item =>\n+      val duplicates = item._2\n+      if (duplicates.size > 1) {\n+        val averageRank = duplicates.foldLeft(0L) {_ + _._2 + 1} / duplicates.size.toDouble\n+        duplicates.map(entry => (entry._1._2, averageRank)).toSeq\n+      } else {\n+        duplicates.map(entry => (entry._1._2, entry._2.toDouble + 1)).toSeq\n+      }\n+    }\n+    ranks.sortByKey()\n+  }\n+\n+  private def makeRankMatrix(ranks: Array[RDD[(Long, Double)]]): RDD[Vector] = {\n+    val partitioner = Partitioner.defaultPartitioner(ranks(0), ranks.tail: _*)\n+    val cogrouped = new CoGroupedRDD[Long](ranks, partitioner)\n+    cogrouped.mapPartitions({ iter =>"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "Missed the `preservesPartitioning = true`, which made the parenthesis necessary. Good catch.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-14T20:11:40Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)\n+    val ranks = groupedByValue.flatMap[(Long, Double)] { item =>\n+      val duplicates = item._2\n+      if (duplicates.size > 1) {\n+        val averageRank = duplicates.foldLeft(0L) {_ + _._2 + 1} / duplicates.size.toDouble\n+        duplicates.map(entry => (entry._1._2, averageRank)).toSeq\n+      } else {\n+        duplicates.map(entry => (entry._1._2, entry._2.toDouble + 1)).toSeq\n+      }\n+    }\n+    ranks.sortByKey()\n+  }\n+\n+  private def makeRankMatrix(ranks: Array[RDD[(Long, Double)]]): RDD[Vector] = {\n+    val partitioner = Partitioner.defaultPartitioner(ranks(0), ranks.tail: _*)\n+    val cogrouped = new CoGroupedRDD[Long](ranks, partitioner)\n+    cogrouped.mapPartitions({ iter =>"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Please check my comment above about `preservePartitioning` before adding `preservePartitioning = true`.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-16T19:08:04Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)\n+    val ranks = groupedByValue.flatMap[(Long, Double)] { item =>\n+      val duplicates = item._2\n+      if (duplicates.size > 1) {\n+        val averageRank = duplicates.foldLeft(0L) {_ + _._2 + 1} / duplicates.size.toDouble\n+        duplicates.map(entry => (entry._1._2, averageRank)).toSeq\n+      } else {\n+        duplicates.map(entry => (entry._1._2, entry._2.toDouble + 1)).toSeq\n+      }\n+    }\n+    ranks.sortByKey()\n+  }\n+\n+  private def makeRankMatrix(ranks: Array[RDD[(Long, Double)]]): RDD[Vector] = {\n+    val partitioner = Partitioner.defaultPartitioner(ranks(0), ranks.tail: _*)\n+    val cogrouped = new CoGroupedRDD[Long](ranks, partitioner)\n+    cogrouped.mapPartitions({ iter =>"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "pretty sure we want to preserve the partitioner here (o/w it's gets wiped in the map operation).\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-16T21:47:42Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)\n+    val ranks = groupedByValue.flatMap[(Long, Double)] { item =>\n+      val duplicates = item._2\n+      if (duplicates.size > 1) {\n+        val averageRank = duplicates.foldLeft(0L) {_ + _._2 + 1} / duplicates.size.toDouble\n+        duplicates.map(entry => (entry._1._2, averageRank)).toSeq\n+      } else {\n+        duplicates.map(entry => (entry._1._2, entry._2.toDouble + 1)).toSeq\n+      }\n+    }\n+    ranks.sortByKey()\n+  }\n+\n+  private def makeRankMatrix(ranks: Array[RDD[(Long, Double)]]): RDD[Vector] = {\n+    val partitioner = Partitioner.defaultPartitioner(ranks(0), ranks.tail: _*)\n+    val cogrouped = new CoGroupedRDD[Long](ranks, partitioner)\n+    cogrouped.mapPartitions({ iter =>"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "The output is sent to `Pearson.computeCorrelationMatrix`, which uses covariance to derive the correlation matrix. No `groupBy` or `join` is used. The partitioner has no function here and it shouldn't be carried over because the output is `RDD[Vector]` but the partitioner applies to `index`, which no longer exists in the data.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-17T04:27:02Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)\n+    val ranks = groupedByValue.flatMap[(Long, Double)] { item =>\n+      val duplicates = item._2\n+      if (duplicates.size > 1) {\n+        val averageRank = duplicates.foldLeft(0L) {_ + _._2 + 1} / duplicates.size.toDouble\n+        duplicates.map(entry => (entry._1._2, averageRank)).toSeq\n+      } else {\n+        duplicates.map(entry => (entry._1._2, entry._2.toDouble + 1)).toSeq\n+      }\n+    }\n+    ranks.sortByKey()\n+  }\n+\n+  private def makeRankMatrix(ranks: Array[RDD[(Long, Double)]]): RDD[Vector] = {\n+    val partitioner = Partitioner.defaultPartitioner(ranks(0), ranks.tail: _*)\n+    val cogrouped = new CoGroupedRDD[Long](ranks, partitioner)\n+    cogrouped.mapPartitions({ iter =>"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Add a space after `{`. If it hits the line width limit, move `DenseVector(values.flatten.toArray)` to the next line.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-12T23:42:55Z",
    "diffHunk": "@@ -0,0 +1,102 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import org.apache.spark.Partitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithIndex()\n+    // Attempt to checkpoint the RDD before splitting it into numCols RDD[Double]s to avoid\n+    // computing the lineage prefix multiple times.\n+    // If checkpoint directory not set, cache the RDD instead.\n+    try {\n+      indexed.checkpoint()\n+    } catch {\n+      case e: Exception => indexed.cache()\n+    }\n+\n+    val numCols = X.first.size\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map {case(vector, index) => {\n+        (vector(k), index)}\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+    val groupedByValue = sorted.groupBy(_._1._1)\n+    val ranks = groupedByValue.flatMap[(Long, Double)] { item =>\n+      val duplicates = item._2\n+      if (duplicates.size > 1) {\n+        val averageRank = duplicates.foldLeft(0L) {_ + _._2 + 1} / duplicates.size.toDouble\n+        duplicates.map(entry => (entry._1._2, averageRank)).toSeq\n+      } else {\n+        duplicates.map(entry => (entry._1._2, entry._2.toDouble + 1)).toSeq\n+      }\n+    }\n+    ranks.sortByKey()\n+  }\n+\n+  private def makeRankMatrix(ranks: Array[RDD[(Long, Double)]]): RDD[Vector] = {\n+    val partitioner = Partitioner.defaultPartitioner(ranks(0), ranks.tail: _*)\n+    val cogrouped = new CoGroupedRDD[Long](ranks, partitioner)\n+    cogrouped.mapPartitions({ iter =>\n+      iter.map {case (index, values:Seq[Seq[Double]]) => new DenseVector(values.flatten.toArray)}"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "It doesn't matter because it doesn't affect the variance and the correlation.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-18T06:42:16Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.HashPartitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   *\n+   * Input RDD[Vector] should be cached or checkpointed if possible since it would be split into\n+   * numCol RDD[Double]s, each of which sorted, and the joined back into a single RDD[Vector].\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithUniqueId()\n+\n+    val numCols = X.first.size\n+    // TODO add warning for too many columns\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.mapPartitions[(Double, Long)] { iter =>\n+        iter.map { case (vector, index) => (vector(k), index) }\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks, X)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   *\n+   * @param indexed RDD[(Double, Long)] containing pairs of the format (originalValue, uniqueId)\n+   * @return RDD[(Long, Double)] containing pairs of the format (uniqueId, rank), where uniqueId is\n+   *         copied from the input RDD.\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+\n+    val ranks: RDD[(Long, Double)] = sorted.mapPartitions { iter =>\n+      // add an extra element to signify the end of the list so that flatMap can flush the last\n+      // batch of duplicates\n+      val padded = iter ++\n+        Iterator[((Double, Long), Long)](((Double.NaN, Long.MinValue), Long.MinValue))\n+      var lastVal = 0.0\n+      var rankSum = 0.0\n+      val IDBuffer = new ArrayBuffer[Long]()\n+      padded.flatMap { item =>\n+        val rank = item._2 + 1 // zipWithIndex is 0-indexed but ranks are 1-indexed"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "Added for the sake of consistency with the definition (also in case someone wants to pull the ranks function out to be used somewhere else). But I can remove it with a note in the docs saying this isn't verbatim with the def in the wiki link.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-18T06:45:44Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.HashPartitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   *\n+   * Input RDD[Vector] should be cached or checkpointed if possible since it would be split into\n+   * numCol RDD[Double]s, each of which sorted, and the joined back into a single RDD[Vector].\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithUniqueId()\n+\n+    val numCols = X.first.size\n+    // TODO add warning for too many columns\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.mapPartitions[(Double, Long)] { iter =>\n+        iter.map { case (vector, index) => (vector(k), index) }\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks, X)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   *\n+   * @param indexed RDD[(Double, Long)] containing pairs of the format (originalValue, uniqueId)\n+   * @return RDD[(Long, Double)] containing pairs of the format (uniqueId, rank), where uniqueId is\n+   *         copied from the input RDD.\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+\n+    val ranks: RDD[(Long, Double)] = sorted.mapPartitions { iter =>\n+      // add an extra element to signify the end of the list so that flatMap can flush the last\n+      // batch of duplicates\n+      val padded = iter ++\n+        Iterator[((Double, Long), Long)](((Double.NaN, Long.MinValue), Long.MinValue))\n+      var lastVal = 0.0\n+      var rankSum = 0.0\n+      val IDBuffer = new ArrayBuffer[Long]()\n+      padded.flatMap { item =>\n+        val rank = item._2 + 1 // zipWithIndex is 0-indexed but ranks are 1-indexed"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Yes, please remove it and leave a note.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-18T07:27:28Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.HashPartitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   *\n+   * Input RDD[Vector] should be cached or checkpointed if possible since it would be split into\n+   * numCol RDD[Double]s, each of which sorted, and the joined back into a single RDD[Vector].\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithUniqueId()\n+\n+    val numCols = X.first.size\n+    // TODO add warning for too many columns\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.mapPartitions[(Double, Long)] { iter =>\n+        iter.map { case (vector, index) => (vector(k), index) }\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks, X)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   *\n+   * @param indexed RDD[(Double, Long)] containing pairs of the format (originalValue, uniqueId)\n+   * @return RDD[(Long, Double)] containing pairs of the format (uniqueId, rank), where uniqueId is\n+   *         copied from the input RDD.\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+\n+    val ranks: RDD[(Long, Double)] = sorted.mapPartitions { iter =>\n+      // add an extra element to signify the end of the list so that flatMap can flush the last\n+      // batch of duplicates\n+      val padded = iter ++\n+        Iterator[((Double, Long), Long)](((Double.NaN, Long.MinValue), Long.MinValue))\n+      var lastVal = 0.0\n+      var rankSum = 0.0\n+      val IDBuffer = new ArrayBuffer[Long]()\n+      padded.flatMap { item =>\n+        val rank = item._2 + 1 // zipWithIndex is 0-indexed but ranks are 1-indexed"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Storing the first rank is sufficient. The average rank can be derived from first rank and `IDBuffer.size`:\n\n```\nfirstRank + IDBuffer.size / 2.0\n```\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-18T06:42:17Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.HashPartitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   *\n+   * Input RDD[Vector] should be cached or checkpointed if possible since it would be split into\n+   * numCol RDD[Double]s, each of which sorted, and the joined back into a single RDD[Vector].\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithUniqueId()\n+\n+    val numCols = X.first.size\n+    // TODO add warning for too many columns\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.mapPartitions[(Double, Long)] { iter =>\n+        iter.map { case (vector, index) => (vector(k), index) }\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks, X)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   *\n+   * @param indexed RDD[(Double, Long)] containing pairs of the format (originalValue, uniqueId)\n+   * @return RDD[(Long, Double)] containing pairs of the format (uniqueId, rank), where uniqueId is\n+   *         copied from the input RDD.\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+\n+    val ranks: RDD[(Long, Double)] = sorted.mapPartitions { iter =>\n+      // add an extra element to signify the end of the list so that flatMap can flush the last\n+      // batch of duplicates\n+      val padded = iter ++\n+        Iterator[((Double, Long), Long)](((Double.NaN, Long.MinValue), Long.MinValue))\n+      var lastVal = 0.0\n+      var rankSum = 0.0\n+      val IDBuffer = new ArrayBuffer[Long]()\n+      padded.flatMap { item =>\n+        val rank = item._2 + 1 // zipWithIndex is 0-indexed but ranks are 1-indexed\n+        if (item._1._1  == lastVal && item._2 != Long.MinValue) {\n+          rankSum += rank"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Use `map` instead of `mapPartitions`.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-18T06:43:52Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.HashPartitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   *\n+   * Input RDD[Vector] should be cached or checkpointed if possible since it would be split into\n+   * numCol RDD[Double]s, each of which sorted, and the joined back into a single RDD[Vector].\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithUniqueId()\n+\n+    val numCols = X.first.size\n+    // TODO add warning for too many columns\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.mapPartitions[(Double, Long)] { iter =>\n+        iter.map { case (vector, index) => (vector(k), index) }\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks, X)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   *\n+   * @param indexed RDD[(Double, Long)] containing pairs of the format (originalValue, uniqueId)\n+   * @return RDD[(Long, Double)] containing pairs of the format (uniqueId, rank), where uniqueId is\n+   *         copied from the input RDD.\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+\n+    val ranks: RDD[(Long, Double)] = sorted.mapPartitions { iter =>\n+      // add an extra element to signify the end of the list so that flatMap can flush the last\n+      // batch of duplicates\n+      val padded = iter ++\n+        Iterator[((Double, Long), Long)](((Double.NaN, Long.MinValue), Long.MinValue))\n+      var lastVal = 0.0\n+      var rankSum = 0.0\n+      val IDBuffer = new ArrayBuffer[Long]()\n+      padded.flatMap { item =>\n+        val rank = item._2 + 1 // zipWithIndex is 0-indexed but ranks are 1-indexed\n+        if (item._1._1  == lastVal && item._2 != Long.MinValue) {\n+          rankSum += rank\n+          IDBuffer += item._1._2\n+          Iterator.empty\n+        } else {\n+          val entries = if (IDBuffer.size == 0) {\n+            Iterator.empty\n+          } else if (IDBuffer.size == 1) {\n+            Iterator((IDBuffer(0), rankSum))\n+          } else {\n+            IDBuffer.map(id => (id, rankSum / IDBuffer.size))\n+          }\n+          lastVal = item._1._1\n+          rankSum = rank\n+          IDBuffer.clear()\n+          IDBuffer += item._1._2\n+          entries\n+        }\n+      }\n+    }\n+    ranks\n+  }\n+\n+  private def makeRankMatrix(ranks: Array[RDD[(Long, Double)]], input: RDD[Vector]): RDD[Vector] = {\n+    val partitioner = new HashPartitioner(input.partitions.size)\n+    val cogrouped = new CoGroupedRDD[Long](ranks, partitioner)\n+    cogrouped.mapPartitions { iter =>"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "Care to edify me on the performance difference? Smaller closure to be serialized? What's a good rule of thumb for when to use `map` v `mapPartitions`?\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-18T06:50:04Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.HashPartitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   *\n+   * Input RDD[Vector] should be cached or checkpointed if possible since it would be split into\n+   * numCol RDD[Double]s, each of which sorted, and the joined back into a single RDD[Vector].\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithUniqueId()\n+\n+    val numCols = X.first.size\n+    // TODO add warning for too many columns\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.mapPartitions[(Double, Long)] { iter =>\n+        iter.map { case (vector, index) => (vector(k), index) }\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks, X)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   *\n+   * @param indexed RDD[(Double, Long)] containing pairs of the format (originalValue, uniqueId)\n+   * @return RDD[(Long, Double)] containing pairs of the format (uniqueId, rank), where uniqueId is\n+   *         copied from the input RDD.\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+\n+    val ranks: RDD[(Long, Double)] = sorted.mapPartitions { iter =>\n+      // add an extra element to signify the end of the list so that flatMap can flush the last\n+      // batch of duplicates\n+      val padded = iter ++\n+        Iterator[((Double, Long), Long)](((Double.NaN, Long.MinValue), Long.MinValue))\n+      var lastVal = 0.0\n+      var rankSum = 0.0\n+      val IDBuffer = new ArrayBuffer[Long]()\n+      padded.flatMap { item =>\n+        val rank = item._2 + 1 // zipWithIndex is 0-indexed but ranks are 1-indexed\n+        if (item._1._1  == lastVal && item._2 != Long.MinValue) {\n+          rankSum += rank\n+          IDBuffer += item._1._2\n+          Iterator.empty\n+        } else {\n+          val entries = if (IDBuffer.size == 0) {\n+            Iterator.empty\n+          } else if (IDBuffer.size == 1) {\n+            Iterator((IDBuffer(0), rankSum))\n+          } else {\n+            IDBuffer.map(id => (id, rankSum / IDBuffer.size))\n+          }\n+          lastVal = item._1._1\n+          rankSum = rank\n+          IDBuffer.clear()\n+          IDBuffer += item._1._2\n+          entries\n+        }\n+      }\n+    }\n+    ranks\n+  }\n+\n+  private def makeRankMatrix(ranks: Array[RDD[(Long, Double)]], input: RDD[Vector]): RDD[Vector] = {\n+    val partitioner = new HashPartitioner(input.partitions.size)\n+    val cogrouped = new CoGroupedRDD[Long](ranks, partitioner)\n+    cogrouped.mapPartitions { iter =>"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "I don't think there is performance difference. But `.map { ... }` is more concise than `.mapPartitions { iter => iter.map { ... } }`.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-18T07:27:12Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.HashPartitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   *\n+   * Input RDD[Vector] should be cached or checkpointed if possible since it would be split into\n+   * numCol RDD[Double]s, each of which sorted, and the joined back into a single RDD[Vector].\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithUniqueId()\n+\n+    val numCols = X.first.size\n+    // TODO add warning for too many columns\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.mapPartitions[(Double, Long)] { iter =>\n+        iter.map { case (vector, index) => (vector(k), index) }\n+      }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks, X)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [3.5, 2.0, 1.0, 3.5]\n+   *\n+   * @param indexed RDD[(Double, Long)] containing pairs of the format (originalValue, uniqueId)\n+   * @return RDD[(Long, Double)] containing pairs of the format (uniqueId, rank), where uniqueId is\n+   *         copied from the input RDD.\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+\n+    val ranks: RDD[(Long, Double)] = sorted.mapPartitions { iter =>\n+      // add an extra element to signify the end of the list so that flatMap can flush the last\n+      // batch of duplicates\n+      val padded = iter ++\n+        Iterator[((Double, Long), Long)](((Double.NaN, Long.MinValue), Long.MinValue))\n+      var lastVal = 0.0\n+      var rankSum = 0.0\n+      val IDBuffer = new ArrayBuffer[Long]()\n+      padded.flatMap { item =>\n+        val rank = item._2 + 1 // zipWithIndex is 0-indexed but ranks are 1-indexed\n+        if (item._1._1  == lastVal && item._2 != Long.MinValue) {\n+          rankSum += rank\n+          IDBuffer += item._1._2\n+          Iterator.empty\n+        } else {\n+          val entries = if (IDBuffer.size == 0) {\n+            Iterator.empty\n+          } else if (IDBuffer.size == 1) {\n+            Iterator((IDBuffer(0), rankSum))\n+          } else {\n+            IDBuffer.map(id => (id, rankSum / IDBuffer.size))\n+          }\n+          lastVal = item._1._1\n+          rankSum = rank\n+          IDBuffer.clear()\n+          IDBuffer += item._1._2\n+          entries\n+        }\n+      }\n+    }\n+    ranks\n+  }\n+\n+  private def makeRankMatrix(ranks: Array[RDD[(Long, Double)]], input: RDD[Vector]): RDD[Vector] = {\n+    val partitioner = new HashPartitioner(input.partitions.size)\n+    val cogrouped = new CoGroupedRDD[Long](ranks, partitioner)\n+    cogrouped.mapPartitions { iter =>"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Use `map` instead of `mapPartitions`.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-18T06:46:34Z",
    "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.HashPartitioner\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+object SpearmansCorrelation extends Correlation {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   *\n+   * Input RDD[Vector] should be cached or checkpointed if possible since it would be split into\n+   * numCol RDD[Double]s, each of which sorted, and the joined back into a single RDD[Vector].\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithUniqueId()\n+\n+    val numCols = X.first.size\n+    // TODO add warning for too many columns\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.mapPartitions[(Double, Long)] { iter =>"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Should it be called `SpearmanCorrelation` instead of `SpearmansCorrelation`?\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-18T21:30:15Z",
    "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, HashPartitioner}\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+private[stat] object SpearmansCorrelation extends Correlation with Logging {"
  }, {
    "author": {
      "login": "dorx"
    },
    "body": "Spearman's correlation is a lot more prevalent (whereas Pearson correlation is much more common), but changing it to SpearmanCorrelation for consistency. This was why I wanted to provide fuzzy string matching.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-18T21:48:06Z",
    "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, HashPartitioner}\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+private[stat] object SpearmansCorrelation extends Correlation with Logging {"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Should be `firstRank + (IDBuffer.size - 1)  / 2.0`. I was wrong in my previous comment.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-18T21:39:26Z",
    "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, HashPartitioner}\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+private[stat] object SpearmansCorrelation extends Correlation with Logging {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   *\n+   * Input RDD[Vector] should be cached or checkpointed if possible since it would be split into\n+   * numCol RDD[Double]s, each of which sorted, and the joined back into a single RDD[Vector].\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithUniqueId()\n+\n+    val numCols = X.first.size\n+    if (numCols > 50) {\n+      logWarning(\"Computing the Spearman correlation matrix can be slow for large RDDs with more\"\n+        + \" than 50 columns.\")\n+    }\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map { case (vector, index) => (vector(k), index) }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks, X)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [2.5, 1.0, 0.0, 2.5]\n+   * Note that positions here are 0-indexed, instead of the 1-indexed as in the definition for\n+   * ranks in the standard definition for Spearman's correlation. This does not affect the final\n+   * results and is slightly more performant.\n+   *\n+   * @param indexed RDD[(Double, Long)] containing pairs of the format (originalValue, uniqueId)\n+   * @return RDD[(Long, Double)] containing pairs of the format (uniqueId, rank), where uniqueId is\n+   *         copied from the input RDD.\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+\n+    val ranks: RDD[(Long, Double)] = sorted.mapPartitions { iter =>\n+      // add an extra element to signify the end of the list so that flatMap can flush the last\n+      // batch of duplicates\n+      val padded = iter ++\n+        Iterator[((Double, Long), Long)](((Double.NaN, Long.MinValue), Long.MinValue))\n+      var lastVal = 0.0\n+      var firstRank = 0.0\n+      val IDBuffer = new ArrayBuffer[Long]()\n+      padded.flatMap { item =>\n+        val rank = item._2\n+        if (item._1._1  == lastVal && item._2 != Long.MinValue) {\n+          IDBuffer += item._1._2\n+          Iterator.empty\n+        } else {\n+          val entries = if (IDBuffer.size == 0) {\n+            Iterator.empty\n+          } else if (IDBuffer.size == 1) {\n+            Iterator((IDBuffer(0), firstRank))\n+          } else {\n+            // averageRank = ((firstRank + IDBuffer.size) / 2.0) / IDBuffer.size\n+            val averageRank = firstRank / (2 * IDBuffer.size) + 0.5"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Use camelCase for normal variables. We did use capital letters for matrices in MLlib but not for normal Java objects.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-18T21:40:13Z",
    "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, HashPartitioner}\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+private[stat] object SpearmansCorrelation extends Correlation with Logging {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   *\n+   * Input RDD[Vector] should be cached or checkpointed if possible since it would be split into\n+   * numCol RDD[Double]s, each of which sorted, and the joined back into a single RDD[Vector].\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithUniqueId()\n+\n+    val numCols = X.first.size\n+    if (numCols > 50) {\n+      logWarning(\"Computing the Spearman correlation matrix can be slow for large RDDs with more\"\n+        + \" than 50 columns.\")\n+    }\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map { case (vector, index) => (vector(k), index) }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks, X)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [2.5, 1.0, 0.0, 2.5]\n+   * Note that positions here are 0-indexed, instead of the 1-indexed as in the definition for\n+   * ranks in the standard definition for Spearman's correlation. This does not affect the final\n+   * results and is slightly more performant.\n+   *\n+   * @param indexed RDD[(Double, Long)] containing pairs of the format (originalValue, uniqueId)\n+   * @return RDD[(Long, Double)] containing pairs of the format (uniqueId, rank), where uniqueId is\n+   *         copied from the input RDD.\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+\n+    val ranks: RDD[(Long, Double)] = sorted.mapPartitions { iter =>\n+      // add an extra element to signify the end of the list so that flatMap can flush the last\n+      // batch of duplicates\n+      val padded = iter ++\n+        Iterator[((Double, Long), Long)](((Double.NaN, Long.MinValue), Long.MinValue))\n+      var lastVal = 0.0\n+      var firstRank = 0.0\n+      val IDBuffer = new ArrayBuffer[Long]()"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Using `case ((v, id), rank) =>` here can improve the readability of the code block.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-18T22:24:28Z",
    "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, HashPartitioner}\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+private[stat] object SpearmansCorrelation extends Correlation with Logging {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   *\n+   * Input RDD[Vector] should be cached or checkpointed if possible since it would be split into\n+   * numCol RDD[Double]s, each of which sorted, and the joined back into a single RDD[Vector].\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithUniqueId()\n+\n+    val numCols = X.first.size\n+    if (numCols > 50) {\n+      logWarning(\"Computing the Spearman correlation matrix can be slow for large RDDs with more\"\n+        + \" than 50 columns.\")\n+    }\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map { case (vector, index) => (vector(k), index) }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks, X)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [2.5, 1.0, 0.0, 2.5]\n+   * Note that positions here are 0-indexed, instead of the 1-indexed as in the definition for\n+   * ranks in the standard definition for Spearman's correlation. This does not affect the final\n+   * results and is slightly more performant.\n+   *\n+   * @param indexed RDD[(Double, Long)] containing pairs of the format (originalValue, uniqueId)\n+   * @return RDD[(Long, Double)] containing pairs of the format (uniqueId, rank), where uniqueId is\n+   *         copied from the input RDD.\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+\n+    val ranks: RDD[(Long, Double)] = sorted.mapPartitions { iter =>\n+      // add an extra element to signify the end of the list so that flatMap can flush the last\n+      // batch of duplicates\n+      val padded = iter ++\n+        Iterator[((Double, Long), Long)](((Double.NaN, Long.MinValue), Long.MinValue))\n+      var lastVal = 0.0\n+      var firstRank = 0.0\n+      val IDBuffer = new ArrayBuffer[Long]()\n+      padded.flatMap { item =>"
  }],
  "prId": 1367
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "I think this comment could be removed since the line below if not very hard to understand.\n",
    "commit": "c0dd7dc1851c2f3c33c7c76ecf235f5eaf289b10",
    "createdAt": "2014-07-18T22:28:38Z",
    "diffHunk": "@@ -0,0 +1,128 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.correlation\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{Logging, HashPartitioner}\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{DenseVector, Matrix, Vector}\n+import org.apache.spark.rdd.{CoGroupedRDD, RDD}\n+\n+/**\n+ * Compute Spearman's correlation for two RDDs of the type RDD[Double] or the correlation matrix\n+ * for an RDD of the type RDD[Vector].\n+ *\n+ * Definition of Spearman's correlation can be found at\n+ * http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient\n+ */\n+private[stat] object SpearmansCorrelation extends Correlation with Logging {\n+\n+  /**\n+   * Compute Spearman's correlation for two datasets.\n+   */\n+  override def computeCorrelation(x: RDD[Double], y: RDD[Double]): Double = {\n+    computeCorrelationWithMatrixImpl(x, y)\n+  }\n+\n+  /**\n+   * Compute Spearman's correlation matrix S, for the input matrix, where S(i, j) is the\n+   * correlation between column i and j.\n+   *\n+   * Input RDD[Vector] should be cached or checkpointed if possible since it would be split into\n+   * numCol RDD[Double]s, each of which sorted, and the joined back into a single RDD[Vector].\n+   */\n+  override def computeCorrelationMatrix(X: RDD[Vector]): Matrix = {\n+    val indexed = X.zipWithUniqueId()\n+\n+    val numCols = X.first.size\n+    if (numCols > 50) {\n+      logWarning(\"Computing the Spearman correlation matrix can be slow for large RDDs with more\"\n+        + \" than 50 columns.\")\n+    }\n+    val ranks = new Array[RDD[(Long, Double)]](numCols)\n+\n+    // Note: we use a for loop here instead of a while loop with a single index variable\n+    // to avoid race condition caused by closure serialization\n+    for (k <- 0 until numCols) {\n+      val column = indexed.map { case (vector, index) => (vector(k), index) }\n+      ranks(k) = getRanks(column)\n+    }\n+\n+    val ranksMat: RDD[Vector] = makeRankMatrix(ranks, X)\n+    PearsonCorrelation.computeCorrelationMatrix(ranksMat)\n+  }\n+\n+  /**\n+   * Compute the ranks for elements in the input RDD, using the average method for ties.\n+   *\n+   * With the average method, elements with the same value receive the same rank that's computed\n+   * by taking the average of their positions in the sorted list.\n+   * e.g. ranks([2, 1, 0, 2]) = [2.5, 1.0, 0.0, 2.5]\n+   * Note that positions here are 0-indexed, instead of the 1-indexed as in the definition for\n+   * ranks in the standard definition for Spearman's correlation. This does not affect the final\n+   * results and is slightly more performant.\n+   *\n+   * @param indexed RDD[(Double, Long)] containing pairs of the format (originalValue, uniqueId)\n+   * @return RDD[(Long, Double)] containing pairs of the format (uniqueId, rank), where uniqueId is\n+   *         copied from the input RDD.\n+   */\n+  private def getRanks(indexed: RDD[(Double, Long)]): RDD[(Long, Double)] = {\n+    // Get elements' positions in the sorted list for computing average rank for duplicate values\n+    val sorted = indexed.sortByKey().zipWithIndex()\n+\n+    val ranks: RDD[(Long, Double)] = sorted.mapPartitions { iter =>\n+      // add an extra element to signify the end of the list so that flatMap can flush the last\n+      // batch of duplicates\n+      val padded = iter ++\n+        Iterator[((Double, Long), Long)](((Double.NaN, Long.MinValue), Long.MinValue))\n+      var lastVal = 0.0\n+      var firstRank = 0.0\n+      val IDBuffer = new ArrayBuffer[Long]()\n+      padded.flatMap { item =>\n+        val rank = item._2\n+        if (item._1._1  == lastVal && item._2 != Long.MinValue) {\n+          IDBuffer += item._1._2\n+          Iterator.empty\n+        } else {\n+          val entries = if (IDBuffer.size == 0) {\n+            Iterator.empty\n+          } else if (IDBuffer.size == 1) {\n+            Iterator((IDBuffer(0), firstRank))\n+          } else {\n+            // averageRank = ((firstRank + IDBuffer.size) / 2.0) / IDBuffer.size"
  }],
  "prId": 1367
}]