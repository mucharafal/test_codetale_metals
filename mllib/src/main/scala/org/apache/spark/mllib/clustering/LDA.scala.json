[{
  "comments": [{
    "author": {
      "login": "hhbyyh"
    },
    "body": "I'm thinking we should move all the parameter check into run, since user may set parameters in different orders.",
    "commit": "31cd11b94c751c42347e1c0e5a2cb707d1cd907c",
    "createdAt": "2017-04-11T07:06:57Z",
    "diffHunk": "@@ -315,6 +315,27 @@ class LDA private (\n     this\n   }\n \n+  // Initial LDAModel can be provided rather than using random initialization\n+  private var initialModel: Option[LDAModel] = None\n+\n+  /**\n+   * Set the initial starting point, bypassing the random initialization.\n+   * This can be used for incremental learning.\n+   * This is supported only for online optimizer, and the condition model.k == this.k must be met,\n+   * failure results in an IllegalArgumentException.\n+   */\n+  @Since(\"2.2.0\")\n+  def setInitialModel(model: LDAModel): this.type = {\n+    require(model.k == k, \"mismatched number of topics\")\n+    this.ldaOptimizer match {\n+      case _: OnlineLDAOptimizer =>\n+        initialModel = Some(model)\n+        this\n+      case _ => throw new IllegalArgumentException(\n+        \"Only online optimizer supports initialization with a previous model.\")\n+    }\n+  }"
  }],
  "prId": 17461
}]