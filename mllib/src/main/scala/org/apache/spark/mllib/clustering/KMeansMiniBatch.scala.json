[{
  "comments": [{
    "author": {
      "login": "dorx"
    },
    "body": "sample actually takes a Double as the second argument for the sampling rate (between 0.0, and 1.0). I think you want takeSample here in order to get the exact batch size (but be warned that takeSample actually needs to collect the sample to the driver). Or you could compute the sampling rate and use sample instead, for an approximate sample size (you can only do this for free if the size of the RDD is already known, o/w you need to do a count, which requires a pass over the entire RDD).\n",
    "commit": "cfff6ff1bda4715694c34534e93c24c3f542ea84",
    "createdAt": "2014-07-09T02:10:10Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{DenseVector => BDV, Vector => BV, norm => breezeNorm}\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+/**\n+ * K-means clustering with support for multiple parallel runs, a k-means++ like initialization\n+ * mode (the k-means|| algorithm by Bahmani et al), and randomly-sampled mini-batches of points\n+ * in each iteration instead of all points for speed (Web-Scale K-Means Clustering by Sculley).\n+ * \n+ * This is an iterative algorithm that will make multiple passes over the data, so any RDDs given\n+ * to it should be cached by the user.\n+ */\n+class KMeansMiniBatch private (\n+    private var k: Int,\n+    private var maxIterations: Int,\n+    private var batchSize: Int,\n+    private var runs: Int,\n+    private var initializationMode: String,\n+    private var initializationSteps: Int) extends Serializable with KMeansCommons with Logging {\n+ \n+  /**\n+   * Constructs a KMeans instance with default parameters: {k: 2, maxIterations: 20, runs: 1,\n+   * batchSize: 1000, initializationMode: \"k-means||\", initializationSteps: 5}.\n+   */\n+  def this() = this(2, 20, 1, 1000, KMeansMiniBatch.K_MEANS_PARALLEL, 5)\n+\n+  def setBatchSize(batchSize: Int): KMeansMiniBatch = {\n+    this.batchSize = batchSize\n+    this\n+  }\n+  \n+  /** Set the number of clusters to create (k). Default: 2. */\n+  def setK(k: Int): KMeansMiniBatch = {\n+    this.k = k\n+    this\n+  }\n+\n+  /** Set maximum number of iterations to run. Default: 20. */\n+  def setMaxIterations(maxIterations: Int): KMeansMiniBatch = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /**\n+   * Set the initialization algorithm. This can be either \"random\" to choose random points as\n+   * initial cluster centers, or \"k-means||\" to use a parallel variant of k-means++\n+   * (Bahmani et al., Scalable K-Means++, VLDB 2012). Default: k-means||.\n+   */\n+  def setInitializationMode(initializationMode: String): KMeansMiniBatch = {\n+    if (initializationMode != KMeansMiniBatch.RANDOM && initializationMode != KMeansMiniBatch.K_MEANS_PARALLEL) {\n+      throw new IllegalArgumentException(\"Invalid initialization mode: \" + initializationMode)\n+    }\n+    this.initializationMode = initializationMode\n+    this\n+  }\n+\n+  /**\n+   * :: Experimental ::\n+   * Set the number of runs of the algorithm to execute in parallel. We initialize the algorithm\n+   * this many times with random starting conditions (configured by the initialization mode), then\n+   * return the best clustering found over any run. Default: 1.\n+   */\n+  @Experimental\n+  def setRuns(runs: Int): KMeansMiniBatch = {\n+    if (runs <= 0) {\n+      throw new IllegalArgumentException(\"Number of runs must be positive\")\n+    }\n+    this.runs = runs\n+    this\n+  }\n+\n+  /**\n+   * Set the number of steps for the k-means|| initialization mode. This is an advanced\n+   * setting -- the default of 5 is almost always enough. Default: 5.\n+   */\n+  def setInitializationSteps(initializationSteps: Int): KMeansMiniBatch = {\n+    if (initializationSteps <= 0) {\n+      throw new IllegalArgumentException(\"Number of initialization steps must be positive\")\n+    }\n+    this.initializationSteps = initializationSteps\n+    this\n+  }\n+\n+  /**\n+   * Train a K-means model on the given set of points; `data` should be cached for high\n+   * performance, because this is an iterative algorithm.\n+   */\n+  def run(data: RDD[Vector]): KMeansModel = {\n+    // Compute squared norms and cache them.\n+    val norms = data.map(v => breezeNorm(v.toBreeze, 2.0))\n+    norms.persist()\n+    val breezeData = data.map(_.toBreeze).zip(norms).map { case (v, norm) =>\n+      new BreezeVectorWithNorm(v, norm)\n+    }\n+    \n+    val runModels = (0 until runs).map { _ =>\n+      runBreeze(breezeData)\n+    }\n+    \n+    val bestModel = runModels.minBy(t => t._2)._1\n+    \n+    norms.unpersist()\n+    bestModel\n+  }\n+\n+  /**\n+   * Implementation of K-Means using breeze.\n+   */\n+  private def runBreeze(data: RDD[BreezeVectorWithNorm]): (KMeansModel, Double) = {\n+\n+    val sc = data.sparkContext\n+\n+    val initStartTime = System.nanoTime()\n+\n+    val centers = if (initializationMode == KMeansMiniBatch.RANDOM) {\n+      initRandom(data, k)\n+    } else {\n+      initParallel(data, k, initializationSteps)\n+    }\n+    \n+    val centerCounts = Array.fill(centers.length){0}\n+    \n+    val initTimeInSeconds = (System.nanoTime() - initStartTime) / 1e9\n+    logInfo(s\"Initialization with $initializationMode took \" + \"%.3f\".format(initTimeInSeconds) +\n+      \" seconds.\")\n+\n+    var costs = 0.0\n+    var iteration = 0\n+    val iterationStartTime = System.nanoTime()\n+\n+    // Execute iterations of Lloyd's algorithm until all runs have converged\n+    while (iteration < maxIterations) {\n+\n+      val sampledPoints = data.sample(false, batchSize)"
  }, {
    "author": {
      "login": "rnowling"
    },
    "body": "Thanks, dorx!  I'm surprised this didn't result in a compilation or run-time error.  I'll update the code.\n",
    "commit": "cfff6ff1bda4715694c34534e93c24c3f542ea84",
    "createdAt": "2014-07-09T13:00:26Z",
    "diffHunk": "@@ -0,0 +1,263 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{DenseVector => BDV, Vector => BV, norm => breezeNorm}\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+/**\n+ * K-means clustering with support for multiple parallel runs, a k-means++ like initialization\n+ * mode (the k-means|| algorithm by Bahmani et al), and randomly-sampled mini-batches of points\n+ * in each iteration instead of all points for speed (Web-Scale K-Means Clustering by Sculley).\n+ * \n+ * This is an iterative algorithm that will make multiple passes over the data, so any RDDs given\n+ * to it should be cached by the user.\n+ */\n+class KMeansMiniBatch private (\n+    private var k: Int,\n+    private var maxIterations: Int,\n+    private var batchSize: Int,\n+    private var runs: Int,\n+    private var initializationMode: String,\n+    private var initializationSteps: Int) extends Serializable with KMeansCommons with Logging {\n+ \n+  /**\n+   * Constructs a KMeans instance with default parameters: {k: 2, maxIterations: 20, runs: 1,\n+   * batchSize: 1000, initializationMode: \"k-means||\", initializationSteps: 5}.\n+   */\n+  def this() = this(2, 20, 1, 1000, KMeansMiniBatch.K_MEANS_PARALLEL, 5)\n+\n+  def setBatchSize(batchSize: Int): KMeansMiniBatch = {\n+    this.batchSize = batchSize\n+    this\n+  }\n+  \n+  /** Set the number of clusters to create (k). Default: 2. */\n+  def setK(k: Int): KMeansMiniBatch = {\n+    this.k = k\n+    this\n+  }\n+\n+  /** Set maximum number of iterations to run. Default: 20. */\n+  def setMaxIterations(maxIterations: Int): KMeansMiniBatch = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /**\n+   * Set the initialization algorithm. This can be either \"random\" to choose random points as\n+   * initial cluster centers, or \"k-means||\" to use a parallel variant of k-means++\n+   * (Bahmani et al., Scalable K-Means++, VLDB 2012). Default: k-means||.\n+   */\n+  def setInitializationMode(initializationMode: String): KMeansMiniBatch = {\n+    if (initializationMode != KMeansMiniBatch.RANDOM && initializationMode != KMeansMiniBatch.K_MEANS_PARALLEL) {\n+      throw new IllegalArgumentException(\"Invalid initialization mode: \" + initializationMode)\n+    }\n+    this.initializationMode = initializationMode\n+    this\n+  }\n+\n+  /**\n+   * :: Experimental ::\n+   * Set the number of runs of the algorithm to execute in parallel. We initialize the algorithm\n+   * this many times with random starting conditions (configured by the initialization mode), then\n+   * return the best clustering found over any run. Default: 1.\n+   */\n+  @Experimental\n+  def setRuns(runs: Int): KMeansMiniBatch = {\n+    if (runs <= 0) {\n+      throw new IllegalArgumentException(\"Number of runs must be positive\")\n+    }\n+    this.runs = runs\n+    this\n+  }\n+\n+  /**\n+   * Set the number of steps for the k-means|| initialization mode. This is an advanced\n+   * setting -- the default of 5 is almost always enough. Default: 5.\n+   */\n+  def setInitializationSteps(initializationSteps: Int): KMeansMiniBatch = {\n+    if (initializationSteps <= 0) {\n+      throw new IllegalArgumentException(\"Number of initialization steps must be positive\")\n+    }\n+    this.initializationSteps = initializationSteps\n+    this\n+  }\n+\n+  /**\n+   * Train a K-means model on the given set of points; `data` should be cached for high\n+   * performance, because this is an iterative algorithm.\n+   */\n+  def run(data: RDD[Vector]): KMeansModel = {\n+    // Compute squared norms and cache them.\n+    val norms = data.map(v => breezeNorm(v.toBreeze, 2.0))\n+    norms.persist()\n+    val breezeData = data.map(_.toBreeze).zip(norms).map { case (v, norm) =>\n+      new BreezeVectorWithNorm(v, norm)\n+    }\n+    \n+    val runModels = (0 until runs).map { _ =>\n+      runBreeze(breezeData)\n+    }\n+    \n+    val bestModel = runModels.minBy(t => t._2)._1\n+    \n+    norms.unpersist()\n+    bestModel\n+  }\n+\n+  /**\n+   * Implementation of K-Means using breeze.\n+   */\n+  private def runBreeze(data: RDD[BreezeVectorWithNorm]): (KMeansModel, Double) = {\n+\n+    val sc = data.sparkContext\n+\n+    val initStartTime = System.nanoTime()\n+\n+    val centers = if (initializationMode == KMeansMiniBatch.RANDOM) {\n+      initRandom(data, k)\n+    } else {\n+      initParallel(data, k, initializationSteps)\n+    }\n+    \n+    val centerCounts = Array.fill(centers.length){0}\n+    \n+    val initTimeInSeconds = (System.nanoTime() - initStartTime) / 1e9\n+    logInfo(s\"Initialization with $initializationMode took \" + \"%.3f\".format(initTimeInSeconds) +\n+      \" seconds.\")\n+\n+    var costs = 0.0\n+    var iteration = 0\n+    val iterationStartTime = System.nanoTime()\n+\n+    // Execute iterations of Lloyd's algorithm until all runs have converged\n+    while (iteration < maxIterations) {\n+\n+      val sampledPoints = data.sample(false, batchSize)"
  }],
  "prId": 1248
}]