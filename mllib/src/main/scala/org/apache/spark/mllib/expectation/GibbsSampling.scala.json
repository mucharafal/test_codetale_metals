[{
  "comments": [{
    "author": {
      "login": "etrain"
    },
    "body": "Gibbs Sampling is a very useful general purpose tool to have. It's interface should be something more generic than RDD[Document], and the parameters should be amenable to domains other than text.\n",
    "commit": "45b157edfa4e6444809daca9b0b2d57e2b575e4b",
    "createdAt": "2014-04-30T01:48:43Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.expectation\n+\n+import java.util.Random\n+\n+import breeze.linalg.{DenseVector => BDV, sum}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.clustering.{Document, LDAParams}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+\n+/**\n+ * Gibbs sampling from a given dataset and org.apache.spark.mllib.model.\n+ * @param data Dataset, such as corpus.\n+ * @param numOuterIterations Number of outer iteration.\n+ * @param numInnerIterations Number of inner iteration, used in each partition.\n+ * @param docTopicSmoothing Document-topic smoothing.\n+ * @param topicTermSmoothing Topic-term smoothing.\n+ */\n+class GibbsSampling(",
    "line": 37
  }],
  "prId": 476
}, {
  "comments": [{
    "author": {
      "login": "etrain"
    },
    "body": "Why an accumulator and not an .aggregate()?\n",
    "commit": "45b157edfa4e6444809daca9b0b2d57e2b575e4b",
    "createdAt": "2014-04-30T01:49:45Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.expectation\n+\n+import java.util.Random\n+\n+import breeze.linalg.{DenseVector => BDV, sum}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.clustering.{Document, LDAParams}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+\n+/**\n+ * Gibbs sampling from a given dataset and org.apache.spark.mllib.model.\n+ * @param data Dataset, such as corpus.\n+ * @param numOuterIterations Number of outer iteration.\n+ * @param numInnerIterations Number of inner iteration, used in each partition.\n+ * @param docTopicSmoothing Document-topic smoothing.\n+ * @param topicTermSmoothing Topic-term smoothing.\n+ */\n+class GibbsSampling(\n+    data: RDD[Document],\n+    numOuterIterations: Int,\n+    numInnerIterations: Int,\n+    docTopicSmoothing: Double,\n+    topicTermSmoothing: Double)\n+  extends Logging with Serializable {\n+\n+  import GibbsSampling._\n+\n+  /**\n+   * Main function of running a Gibbs sampling method. It contains two phases of total Gibbs\n+   * sampling: first is initialization, second is real sampling.\n+   */\n+  def runGibbsSampling(\n+      initParams: LDAParams,\n+      data: RDD[Document] = data,\n+      numOuterIterations: Int = numOuterIterations,\n+      numInnerIterations: Int = numInnerIterations,\n+      docTopicSmoothing: Double = docTopicSmoothing,\n+      topicTermSmoothing: Double = topicTermSmoothing): LDAParams = {\n+\n+    val numTerms = initParams.topicTermCounts.head.size\n+    val numDocs = initParams.docCounts.size\n+    val numTopics = initParams.topicCounts.size\n+\n+    // Construct topic assignment RDD\n+    logInfo(\"Start initialization\")\n+\n+    val cpInterval = System.getProperty(\"spark.gibbsSampling.checkPointInterval\", \"10\").toInt\n+    val sc = data.context\n+    val (initialParams, initialChosenTopics) = sampleTermAssignment(initParams, data)\n+\n+    // Gibbs sampling\n+    val (params, _, _) = Iterator.iterate((sc.accumulable(initialParams), initialChosenTopics, 0)) {",
    "line": 71
  }, {
    "author": {
      "login": "yinxusen"
    },
    "body": "Accumulator has the ability to do the fine-grained updating for LDA parameters. For aggregate, I have to use `mapPartitions` as shown in my previous version of LDA impl [here](https://github.com/yinxusen/lda_spark/blob/mappartition/src/main/scala/org.apache.spark.mllib.expectation/GibbsSampling.scala#L77). However, the previous impl is slower than current version, partly because the serialization of the huge parameters.\n",
    "commit": "45b157edfa4e6444809daca9b0b2d57e2b575e4b",
    "createdAt": "2014-04-30T08:23:57Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.expectation\n+\n+import java.util.Random\n+\n+import breeze.linalg.{DenseVector => BDV, sum}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.clustering.{Document, LDAParams}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+\n+/**\n+ * Gibbs sampling from a given dataset and org.apache.spark.mllib.model.\n+ * @param data Dataset, such as corpus.\n+ * @param numOuterIterations Number of outer iteration.\n+ * @param numInnerIterations Number of inner iteration, used in each partition.\n+ * @param docTopicSmoothing Document-topic smoothing.\n+ * @param topicTermSmoothing Topic-term smoothing.\n+ */\n+class GibbsSampling(\n+    data: RDD[Document],\n+    numOuterIterations: Int,\n+    numInnerIterations: Int,\n+    docTopicSmoothing: Double,\n+    topicTermSmoothing: Double)\n+  extends Logging with Serializable {\n+\n+  import GibbsSampling._\n+\n+  /**\n+   * Main function of running a Gibbs sampling method. It contains two phases of total Gibbs\n+   * sampling: first is initialization, second is real sampling.\n+   */\n+  def runGibbsSampling(\n+      initParams: LDAParams,\n+      data: RDD[Document] = data,\n+      numOuterIterations: Int = numOuterIterations,\n+      numInnerIterations: Int = numInnerIterations,\n+      docTopicSmoothing: Double = docTopicSmoothing,\n+      topicTermSmoothing: Double = topicTermSmoothing): LDAParams = {\n+\n+    val numTerms = initParams.topicTermCounts.head.size\n+    val numDocs = initParams.docCounts.size\n+    val numTopics = initParams.topicCounts.size\n+\n+    // Construct topic assignment RDD\n+    logInfo(\"Start initialization\")\n+\n+    val cpInterval = System.getProperty(\"spark.gibbsSampling.checkPointInterval\", \"10\").toInt\n+    val sc = data.context\n+    val (initialParams, initialChosenTopics) = sampleTermAssignment(initParams, data)\n+\n+    // Gibbs sampling\n+    val (params, _, _) = Iterator.iterate((sc.accumulable(initialParams), initialChosenTopics, 0)) {",
    "line": 71
  }],
  "prId": 476
}, {
  "comments": [{
    "author": {
      "login": "etrain"
    },
    "body": "Again, Phi and Theta might be too big.\n",
    "commit": "45b157edfa4e6444809daca9b0b2d57e2b575e4b",
    "createdAt": "2014-04-30T01:53:04Z",
    "diffHunk": "@@ -0,0 +1,219 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.expectation\n+\n+import java.util.Random\n+\n+import breeze.linalg.{DenseVector => BDV, sum}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.clustering.{Document, LDAParams}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+\n+/**\n+ * Gibbs sampling from a given dataset and org.apache.spark.mllib.model.\n+ * @param data Dataset, such as corpus.\n+ * @param numOuterIterations Number of outer iteration.\n+ * @param numInnerIterations Number of inner iteration, used in each partition.\n+ * @param docTopicSmoothing Document-topic smoothing.\n+ * @param topicTermSmoothing Topic-term smoothing.\n+ */\n+class GibbsSampling(\n+    data: RDD[Document],\n+    numOuterIterations: Int,\n+    numInnerIterations: Int,\n+    docTopicSmoothing: Double,\n+    topicTermSmoothing: Double)\n+  extends Logging with Serializable {\n+\n+  import GibbsSampling._\n+\n+  /**\n+   * Main function of running a Gibbs sampling method. It contains two phases of total Gibbs\n+   * sampling: first is initialization, second is real sampling.\n+   */\n+  def runGibbsSampling(\n+      initParams: LDAParams,\n+      data: RDD[Document] = data,\n+      numOuterIterations: Int = numOuterIterations,\n+      numInnerIterations: Int = numInnerIterations,\n+      docTopicSmoothing: Double = docTopicSmoothing,\n+      topicTermSmoothing: Double = topicTermSmoothing): LDAParams = {\n+\n+    val numTerms = initParams.topicTermCounts.head.size\n+    val numDocs = initParams.docCounts.size\n+    val numTopics = initParams.topicCounts.size\n+\n+    // Construct topic assignment RDD\n+    logInfo(\"Start initialization\")\n+\n+    val cpInterval = System.getProperty(\"spark.gibbsSampling.checkPointInterval\", \"10\").toInt\n+    val sc = data.context\n+    val (initialParams, initialChosenTopics) = sampleTermAssignment(initParams, data)\n+\n+    // Gibbs sampling\n+    val (params, _, _) = Iterator.iterate((sc.accumulable(initialParams), initialChosenTopics, 0)) {\n+      case (lastParams, lastChosenTopics, i) =>\n+        logInfo(\"Start Gibbs sampling\")\n+\n+        val rand = new Random(42 + i * i)\n+        val params = sc.accumulable(LDAParams(numDocs, numTopics, numTerms))\n+        val chosenTopics = data.zip(lastChosenTopics).map {\n+          case (Document(docId, content), topics) =>\n+            content.zip(topics).map { case (term, topic) =>\n+              lastParams += (docId, term, topic, -1)\n+\n+              val chosenTopic = lastParams.localValue.dropOneDistSampler(\n+                docTopicSmoothing, topicTermSmoothing, term, docId, rand)\n+\n+              lastParams += (docId, term, chosenTopic, 1)\n+              params += (docId, term, chosenTopic, 1)\n+\n+              chosenTopic\n+            }\n+        }.cache()\n+\n+        if (i + 1 % cpInterval == 0) {\n+          chosenTopics.checkpoint()\n+        }\n+\n+        // Trigger a job to collect accumulable LDA parameters.\n+        chosenTopics.count()\n+        lastChosenTopics.unpersist()\n+\n+        (params, chosenTopics, i + 1)\n+    }.drop(1 + numOuterIterations).next()\n+\n+    params.value\n+  }\n+\n+  /**\n+   * Model matrix Phi and Theta are inferred via LDAParams.\n+   */\n+  def solvePhiAndTheta(\n+      params: LDAParams,\n+      docTopicSmoothing: Double = docTopicSmoothing,\n+      topicTermSmoothing: Double = topicTermSmoothing): (Array[Vector], Array[Vector]) = {",
    "line": 112
  }],
  "prId": 476
}]