[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "You should pass the broadcast variables themselves; don't extract the value here since this happens before the mapPartitions().\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-02T21:44:28Z",
    "diffHunk": "@@ -143,17 +141,9 @@ class GradientBoostedTreesModel(\n     val broadcastWeights = sc.broadcast(treeWeights)\n \n     (1 until numIterations).map { nTree =>\n-      predictionAndError = remappedData.zip(predictionAndError).mapPartitions { iter =>\n-        val currentTree = broadcastTrees.value(nTree)\n-        val currentTreeWeight = broadcastWeights.value(nTree)\n-        iter.map {\n-          case (point, (pred, error)) => {\n-            val newPred = pred + currentTree.predict(point.features) * currentTreeWeight\n-            val newError = loss.computeError(newPred, point.label)\n-            (newPred, newError)\n-          }\n-        }\n-      }\n+      predictionAndError = GradientBoostedTreesModel.updatePredictionError(\n+        remappedData, predictionAndError, broadcastWeights.value(nTree),"
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Doc: \"Compute the initial predictions and errors for a dataset for the first iteration of gradient boosting.\"  No need to say it's a \"method.\"  This is a leftover problem from before, so don't follow previous examples.  : )\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-02T21:44:29Z",
    "diffHunk": "@@ -166,6 +156,56 @@ class GradientBoostedTreesModel(\n \n object GradientBoostedTreesModel extends Loader[GradientBoostedTreesModel] {\n \n+  /**\n+   * Method to compute initial error and prediction as a RDD for the first"
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Doc: \"Training data\"  The generated doc will already include the argument type.  Also a leftover problem from before...\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-02T21:44:30Z",
    "diffHunk": "@@ -166,6 +156,56 @@ class GradientBoostedTreesModel(\n \n object GradientBoostedTreesModel extends Loader[GradientBoostedTreesModel] {\n \n+  /**\n+   * Method to compute initial error and prediction as a RDD for the first\n+   * iteration of gradient boosting.\n+   * @param data RDD of [[org.apache.spark.mllib.regression.LabeledPoint]]"
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "style: 1 parameter per line\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-02T21:44:31Z",
    "diffHunk": "@@ -166,6 +156,56 @@ class GradientBoostedTreesModel(\n \n object GradientBoostedTreesModel extends Loader[GradientBoostedTreesModel] {\n \n+  /**\n+   * Method to compute initial error and prediction as a RDD for the first\n+   * iteration of gradient boosting.\n+   * @param data RDD of [[org.apache.spark.mllib.regression.LabeledPoint]]\n+   * @param initTreeWeight: learning rate assigned to the first tree.\n+   * @param initTree: first DecisionTreeModel\n+   * @param loss: evaluation metric\n+   * @return a RDD with each element being a zip of the prediction and error\n+   *         corresponding to every sample.\n+   */\n+  def computeInitialPredictionAndError(\n+      data: RDD[LabeledPoint],\n+      initTreeWeight: Double,\n+      initTree: DecisionTreeModel, loss: Loss): RDD[(Double, Double)] = {"
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "rename i -> \"lp\" or \"labeledPoint\"\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-02T21:44:32Z",
    "diffHunk": "@@ -166,6 +156,56 @@ class GradientBoostedTreesModel(\n \n object GradientBoostedTreesModel extends Loader[GradientBoostedTreesModel] {\n \n+  /**\n+   * Method to compute initial error and prediction as a RDD for the first\n+   * iteration of gradient boosting.\n+   * @param data RDD of [[org.apache.spark.mllib.regression.LabeledPoint]]\n+   * @param initTreeWeight: learning rate assigned to the first tree.\n+   * @param initTree: first DecisionTreeModel\n+   * @param loss: evaluation metric\n+   * @return a RDD with each element being a zip of the prediction and error\n+   *         corresponding to every sample.\n+   */\n+  def computeInitialPredictionAndError(\n+      data: RDD[LabeledPoint],\n+      initTreeWeight: Double,\n+      initTree: DecisionTreeModel, loss: Loss): RDD[(Double, Double)] = {\n+    data.map { i =>"
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "ditto\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-02T21:44:33Z",
    "diffHunk": "@@ -166,6 +156,56 @@ class GradientBoostedTreesModel(\n \n object GradientBoostedTreesModel extends Loader[GradientBoostedTreesModel] {\n \n+  /**\n+   * Method to compute initial error and prediction as a RDD for the first\n+   * iteration of gradient boosting.\n+   * @param data RDD of [[org.apache.spark.mllib.regression.LabeledPoint]]\n+   * @param initTreeWeight: learning rate assigned to the first tree.\n+   * @param initTree: first DecisionTreeModel\n+   * @param loss: evaluation metric\n+   * @return a RDD with each element being a zip of the prediction and error\n+   *         corresponding to every sample.\n+   */\n+  def computeInitialPredictionAndError(\n+      data: RDD[LabeledPoint],\n+      initTreeWeight: Double,\n+      initTree: DecisionTreeModel, loss: Loss): RDD[(Double, Double)] = {\n+    data.map { i =>\n+      val pred = initTreeWeight * initTree.predict(i.features)\n+      val error = loss.computeError(pred, i.label)\n+      (pred, error)\n+    }\n+  }\n+\n+  /**\n+   * Method to update a zipped predictionError RDD"
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "ditto\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-02T21:44:35Z",
    "diffHunk": "@@ -166,6 +156,56 @@ class GradientBoostedTreesModel(\n \n object GradientBoostedTreesModel extends Loader[GradientBoostedTreesModel] {\n \n+  /**\n+   * Method to compute initial error and prediction as a RDD for the first\n+   * iteration of gradient boosting.\n+   * @param data RDD of [[org.apache.spark.mllib.regression.LabeledPoint]]\n+   * @param initTreeWeight: learning rate assigned to the first tree.\n+   * @param initTree: first DecisionTreeModel\n+   * @param loss: evaluation metric\n+   * @return a RDD with each element being a zip of the prediction and error\n+   *         corresponding to every sample.\n+   */\n+  def computeInitialPredictionAndError(\n+      data: RDD[LabeledPoint],\n+      initTreeWeight: Double,\n+      initTree: DecisionTreeModel, loss: Loss): RDD[(Double, Double)] = {\n+    data.map { i =>\n+      val pred = initTreeWeight * initTree.predict(i.features)\n+      val error = loss.computeError(pred, i.label)\n+      (pred, error)\n+    }\n+  }\n+\n+  /**\n+   * Method to update a zipped predictionError RDD\n+   * (as obtained with computeInitialPredictionAndError)\n+   * @param data RDD of [[org.apache.spark.mllib.regression.LabeledPoint]]"
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "typo: \"corresponding\"\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-02T21:44:36Z",
    "diffHunk": "@@ -166,6 +156,56 @@ class GradientBoostedTreesModel(\n \n object GradientBoostedTreesModel extends Loader[GradientBoostedTreesModel] {\n \n+  /**\n+   * Method to compute initial error and prediction as a RDD for the first\n+   * iteration of gradient boosting.\n+   * @param data RDD of [[org.apache.spark.mllib.regression.LabeledPoint]]\n+   * @param initTreeWeight: learning rate assigned to the first tree.\n+   * @param initTree: first DecisionTreeModel\n+   * @param loss: evaluation metric\n+   * @return a RDD with each element being a zip of the prediction and error\n+   *         corresponding to every sample.\n+   */\n+  def computeInitialPredictionAndError(\n+      data: RDD[LabeledPoint],\n+      initTreeWeight: Double,\n+      initTree: DecisionTreeModel, loss: Loss): RDD[(Double, Double)] = {\n+    data.map { i =>\n+      val pred = initTreeWeight * initTree.predict(i.features)\n+      val error = loss.computeError(pred, i.label)\n+      (pred, error)\n+    }\n+  }\n+\n+  /**\n+   * Method to update a zipped predictionError RDD\n+   * (as obtained with computeInitialPredictionAndError)\n+   * @param data RDD of [[org.apache.spark.mllib.regression.LabeledPoint]]\n+   * @param predictionAndError: predictionError RDD\n+   * @param currentTreeWeight: learning rate.\n+   * @param currentTree: first DecisionTree\n+   * @param loss: evaluation metric\n+   * @return a RDD with each element being a zip of the prediction and error\n+   *         corresponing to each sample."
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "\"point\" -->  (Use the same variable name for a labeled point everywhere for consistency.  I tend to use \"lp\")\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-02T21:44:38Z",
    "diffHunk": "@@ -166,6 +156,56 @@ class GradientBoostedTreesModel(\n \n object GradientBoostedTreesModel extends Loader[GradientBoostedTreesModel] {\n \n+  /**\n+   * Method to compute initial error and prediction as a RDD for the first\n+   * iteration of gradient boosting.\n+   * @param data RDD of [[org.apache.spark.mllib.regression.LabeledPoint]]\n+   * @param initTreeWeight: learning rate assigned to the first tree.\n+   * @param initTree: first DecisionTreeModel\n+   * @param loss: evaluation metric\n+   * @return a RDD with each element being a zip of the prediction and error\n+   *         corresponding to every sample.\n+   */\n+  def computeInitialPredictionAndError(\n+      data: RDD[LabeledPoint],\n+      initTreeWeight: Double,\n+      initTree: DecisionTreeModel, loss: Loss): RDD[(Double, Double)] = {\n+    data.map { i =>\n+      val pred = initTreeWeight * initTree.predict(i.features)\n+      val error = loss.computeError(pred, i.label)\n+      (pred, error)\n+    }\n+  }\n+\n+  /**\n+   * Method to update a zipped predictionError RDD\n+   * (as obtained with computeInitialPredictionAndError)\n+   * @param data RDD of [[org.apache.spark.mllib.regression.LabeledPoint]]\n+   * @param predictionAndError: predictionError RDD\n+   * @param currentTreeWeight: learning rate.\n+   * @param currentTree: first DecisionTree\n+   * @param loss: evaluation metric\n+   * @return a RDD with each element being a zip of the prediction and error\n+   *         corresponing to each sample.\n+   */\n+  def updatePredictionError(\n+    data: RDD[LabeledPoint],\n+    predictionAndError: RDD[(Double, Double)],\n+    currentTreeWeight: Double,\n+    currentTree: DecisionTreeModel,\n+    loss: Loss): RDD[(Double, Double)] = {\n+\n+    data.zip(predictionAndError).mapPartitions { iter =>\n+      iter.map {\n+        case (point, (pred, error)) => {"
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "I realized that the broadcast variables can't be unpersisted here since they were used in a map for an RDD which has yet to be materialized.  (No action has been called on newPredError yet.)  We have 2 choices:\n- remove the unpersist, and let GC handle it once all of these RDDs go out of scope (at the end of training)\n  - Let's do this one for now since it's simpler and since we aren't broadcasting (persisting) too much data.\n- return the broadcast variables, and keep them around until the caller can unpersist them safely\n  - We can do this in the future if users ever encounter problems with too many broadcast variables.\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-11T15:55:13Z",
    "diffHunk": "@@ -166,6 +150,68 @@ class GradientBoostedTreesModel(\n \n object GradientBoostedTreesModel extends Loader[GradientBoostedTreesModel] {\n \n+  /**\n+   * Compute the initial predictions and errors for a dataset for the first\n+   * iteration of gradient boosting.\n+   * @param Training data.\n+   * @param initTreeWeight: learning rate assigned to the first tree.\n+   * @param initTree: first DecisionTreeModel.\n+   * @param loss: evaluation metric.\n+   * @return a RDD with each element being a zip of the prediction and error\n+   *         corresponding to every sample.\n+   */\n+  def computeInitialPredictionAndError(\n+      data: RDD[LabeledPoint],\n+      initTreeWeight: Double,\n+      initTree: DecisionTreeModel,\n+      loss: Loss): RDD[(Double, Double)] = {\n+    data.map { lp =>\n+      val pred = initTreeWeight * initTree.predict(lp.features)\n+      val error = loss.computeError(pred, lp.label)\n+      (pred, error)\n+    }\n+  }\n+\n+  /**\n+   * Update a zipped predictionError RDD\n+   * (as obtained with computeInitialPredictionAndError)\n+   * @param training data.\n+   * @param predictionAndError: predictionError RDD\n+   * @param nTree: tree index.\n+   * @param treeWeight: Learning rate.\n+   * @param tree: Tree using which the prediction and error should be updated.\n+   * @param loss: evaluation metric.\n+   * @return a RDD with each element being a zip of the prediction and error\n+   *         corresponding to each sample.\n+   */\n+  def updatePredictionError(\n+    data: RDD[LabeledPoint],\n+    predictionAndError: RDD[(Double, Double)],\n+    treeWeight: Double,\n+    tree: DecisionTreeModel,\n+    loss: Loss): RDD[(Double, Double)] = {\n+\n+    val sc = data.sparkContext\n+    val broadcastedTreeWeight = sc.broadcast(treeWeight)\n+    val broadcastedTree = sc.broadcast(tree)\n+\n+    val newPredError = data.zip(predictionAndError).mapPartitions { iter =>\n+      val currentTreeWeight = broadcastedTreeWeight.value\n+      val currentTree = broadcastedTree.value\n+      iter.map {\n+        case (lp, (pred, error)) => {\n+          val newPred = pred + currentTree.predict(lp.features) * currentTreeWeight\n+          val newError = loss.computeError(newPred, lp.label)\n+          (newPred, newError)\n+        }\n+      }\n+    }\n+\n+    broadcastedTreeWeight.unpersist()"
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "no longer needed\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-12T23:13:06Z",
    "diffHunk": "@@ -27,6 +27,7 @@ import org.json4s.jackson.JsonMethods._\n import org.apache.spark.{Logging, SparkContext}\n import org.apache.spark.annotation.Experimental\n import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.broadcast.Broadcast"
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "\"map\" --> \"foreach\"\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-12T23:13:08Z",
    "diffHunk": "@@ -131,34 +132,17 @@ class GradientBoostedTreesModel(\n     val numIterations = trees.length\n     val evaluationArray = Array.fill(numIterations)(0.0)\n \n-    var predictionAndError: RDD[(Double, Double)] = remappedData.map { i =>\n-      val pred = treeWeights(0) * trees(0).predict(i.features)\n-      val error = loss.computeError(pred, i.label)\n-      (pred, error)\n-    }\n-    evaluationArray(0) = predictionAndError.values.mean()\n+    var predictionAndError = GradientBoostedTreesModel.computeInitialPredictionAndError(\n+      remappedData, treeWeights(0), trees(0), loss)\n \n-    // Avoid the model being copied across numIterations.\n-    val broadcastTrees = sc.broadcast(trees)\n-    val broadcastWeights = sc.broadcast(treeWeights)\n+    evaluationArray(0) = predictionAndError.values.mean()\n \n     (1 until numIterations).map { nTree =>"
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "\"Training data\" --> \"data: training data\"  (need to specify parameter name \"data\")\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-12T23:13:10Z",
    "diffHunk": "@@ -166,6 +150,65 @@ class GradientBoostedTreesModel(\n \n object GradientBoostedTreesModel extends Loader[GradientBoostedTreesModel] {\n \n+  /**\n+   * Compute the initial predictions and errors for a dataset for the first\n+   * iteration of gradient boosting.\n+   * @param Training data."
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "\"Training data\" --> \"data: training data\"  (need to specify parameter name \"data\")\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-12T23:13:11Z",
    "diffHunk": "@@ -166,6 +150,65 @@ class GradientBoostedTreesModel(\n \n object GradientBoostedTreesModel extends Loader[GradientBoostedTreesModel] {\n \n+  /**\n+   * Compute the initial predictions and errors for a dataset for the first\n+   * iteration of gradient boosting.\n+   * @param Training data.\n+   * @param initTreeWeight: learning rate assigned to the first tree.\n+   * @param initTree: first DecisionTreeModel.\n+   * @param loss: evaluation metric.\n+   * @return a RDD with each element being a zip of the prediction and error\n+   *         corresponding to every sample.\n+   */\n+  def computeInitialPredictionAndError(\n+      data: RDD[LabeledPoint],\n+      initTreeWeight: Double,\n+      initTree: DecisionTreeModel,\n+      loss: Loss): RDD[(Double, Double)] = {\n+    data.map { lp =>\n+      val pred = initTreeWeight * initTree.predict(lp.features)\n+      val error = loss.computeError(pred, lp.label)\n+      (pred, error)\n+    }\n+  }\n+\n+  /**\n+   * Update a zipped predictionError RDD\n+   * (as obtained with computeInitialPredictionAndError)\n+   * @param training data."
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "\"nTree\" no longer exists\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-12T23:13:12Z",
    "diffHunk": "@@ -166,6 +150,65 @@ class GradientBoostedTreesModel(\n \n object GradientBoostedTreesModel extends Loader[GradientBoostedTreesModel] {\n \n+  /**\n+   * Compute the initial predictions and errors for a dataset for the first\n+   * iteration of gradient boosting.\n+   * @param Training data.\n+   * @param initTreeWeight: learning rate assigned to the first tree.\n+   * @param initTree: first DecisionTreeModel.\n+   * @param loss: evaluation metric.\n+   * @return a RDD with each element being a zip of the prediction and error\n+   *         corresponding to every sample.\n+   */\n+  def computeInitialPredictionAndError(\n+      data: RDD[LabeledPoint],\n+      initTreeWeight: Double,\n+      initTree: DecisionTreeModel,\n+      loss: Loss): RDD[(Double, Double)] = {\n+    data.map { lp =>\n+      val pred = initTreeWeight * initTree.predict(lp.features)\n+      val error = loss.computeError(pred, lp.label)\n+      (pred, error)\n+    }\n+  }\n+\n+  /**\n+   * Update a zipped predictionError RDD\n+   * (as obtained with computeInitialPredictionAndError)\n+   * @param training data.\n+   * @param predictionAndError: predictionError RDD\n+   * @param nTree: tree index."
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "We should make a local (shallow) copy of treeWeights before the map, within this method:\n\n```\nval localTreeWeights = treeWeights\n```\n\nReferencing treeWeights, a member of the class, will actually make the entire class get serialized by the ClosureCleaner.  Assigning it to a local val fixes that.\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-13T18:57:22Z",
    "diffHunk": "@@ -131,34 +131,26 @@ class GradientBoostedTreesModel(\n     val numIterations = trees.length\n     val evaluationArray = Array.fill(numIterations)(0.0)\n \n-    var predictionAndError: RDD[(Double, Double)] = remappedData.map { i =>\n-      val pred = treeWeights(0) * trees(0).predict(i.features)\n-      val error = loss.computeError(pred, i.label)\n-      (pred, error)\n-    }\n+    var predictionAndError = GradientBoostedTreesModel.computeInitialPredictionAndError(\n+      remappedData, treeWeights(0), trees(0), loss)\n+\n     evaluationArray(0) = predictionAndError.values.mean()\n \n-    // Avoid the model being copied across numIterations.\n     val broadcastTrees = sc.broadcast(trees)\n-    val broadcastWeights = sc.broadcast(treeWeights)\n-\n     (1 until numIterations).map { nTree =>\n       predictionAndError = remappedData.zip(predictionAndError).mapPartitions { iter =>\n         val currentTree = broadcastTrees.value(nTree)\n-        val currentTreeWeight = broadcastWeights.value(nTree)\n-        iter.map {\n-          case (point, (pred, error)) => {\n-            val newPred = pred + currentTree.predict(point.features) * currentTreeWeight\n-            val newError = loss.computeError(newPred, point.label)\n-            (newPred, newError)\n-          }\n+        val currentTreeWeight = treeWeights(nTree)"
  }],
  "prId": 5330
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "no longer needed\n",
    "commit": "0b5d65900b22db5a0a9047fd2c3a1010d9d8c36b",
    "createdAt": "2015-04-13T18:57:25Z",
    "diffHunk": "@@ -166,6 +158,60 @@ class GradientBoostedTreesModel(\n \n object GradientBoostedTreesModel extends Loader[GradientBoostedTreesModel] {\n \n+  /**\n+   * Compute the initial predictions and errors for a dataset for the first\n+   * iteration of gradient boosting.\n+   * @param data: training data.\n+   * @param initTreeWeight: learning rate assigned to the first tree.\n+   * @param initTree: first DecisionTreeModel.\n+   * @param loss: evaluation metric.\n+   * @return a RDD with each element being a zip of the prediction and error\n+   *         corresponding to every sample.\n+   */\n+  def computeInitialPredictionAndError(\n+      data: RDD[LabeledPoint],\n+      initTreeWeight: Double,\n+      initTree: DecisionTreeModel,\n+      loss: Loss): RDD[(Double, Double)] = {\n+    data.map { lp =>\n+      val pred = initTreeWeight * initTree.predict(lp.features)\n+      val error = loss.computeError(pred, lp.label)\n+      (pred, error)\n+    }\n+  }\n+\n+  /**\n+   * Update a zipped predictionError RDD\n+   * (as obtained with computeInitialPredictionAndError)\n+   * @param data: training data.\n+   * @param predictionAndError: predictionError RDD\n+   * @param treeWeight: Learning rate.\n+   * @param tree: Tree using which the prediction and error should be updated.\n+   * @param loss: evaluation metric.\n+   * @return a RDD with each element being a zip of the prediction and error\n+   *         corresponding to each sample.\n+   */\n+  def updatePredictionError(\n+    data: RDD[LabeledPoint],\n+    predictionAndError: RDD[(Double, Double)],\n+    treeWeight: Double,\n+    tree: DecisionTreeModel,\n+    loss: Loss): RDD[(Double, Double)] = {\n+\n+    val sc = data.sparkContext"
  }],
  "prId": 5330
}]