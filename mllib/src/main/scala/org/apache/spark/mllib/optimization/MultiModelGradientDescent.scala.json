[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Here and in the other methods, maybe append an \"s\" if it takes multiple parameter settings: \"setStepSizes\"\n",
    "commit": "2ea711c96990c24ed29a4b9476a3809ef26403d6",
    "createdAt": "2014-09-19T21:07:35Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{DenseVector => BDV}\n+\n+import org.apache.spark.annotation.{Experimental, DeveloperApi}\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.mllib.rdd.RDDFunctions._\n+\n+class MultiModelGradientDescent private[mllib] (\n+    private var gradient: MultiModelGradient,\n+    private var updater: Array[MultiModelUpdater]) extends Optimizer[Matrix] with Logging {\n+\n+  private var stepSize: Array[Double] = Array(1.0, 0.1)\n+  private var numIterations: Array[Int] = Array(100)\n+  private var regParam: Array[Double] = Array(0.0, 0.1, 1.0)\n+  private var miniBatchFraction: Double = 1.0\n+\n+  /**\n+   * Set the initial step size of SGD for the first step. Default (1.0, 0.1).\n+   * In subsequent steps, the step size will decrease with stepSize/sqrt(t)\n+   */\n+  def setStepSize(step: Array[Double]): this.type = {",
    "line": 43
  }],
  "prId": 2451
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Are we trying to keep things Java-friendly?  (The default param values won't be.)\n",
    "commit": "2ea711c96990c24ed29a4b9476a3809ef26403d6",
    "createdAt": "2014-09-19T21:10:41Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{DenseVector => BDV}\n+\n+import org.apache.spark.annotation.{Experimental, DeveloperApi}\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.mllib.rdd.RDDFunctions._\n+\n+class MultiModelGradientDescent private[mllib] (\n+    private var gradient: MultiModelGradient,\n+    private var updater: Array[MultiModelUpdater]) extends Optimizer[Matrix] with Logging {\n+\n+  private var stepSize: Array[Double] = Array(1.0, 0.1)\n+  private var numIterations: Array[Int] = Array(100)\n+  private var regParam: Array[Double] = Array(0.0, 0.1, 1.0)\n+  private var miniBatchFraction: Double = 1.0\n+\n+  /**\n+   * Set the initial step size of SGD for the first step. Default (1.0, 0.1).\n+   * In subsequent steps, the step size will decrease with stepSize/sqrt(t)\n+   */\n+  def setStepSize(step: Array[Double]): this.type = {\n+    this.stepSize = step\n+    this\n+  }\n+\n+  /**\n+   * :: Experimental ::\n+   * Set fraction of data to be used for each SGD iteration.\n+   * Default 1.0 (corresponding to deterministic/classical gradient descent)\n+   */\n+  @Experimental\n+  def setMiniBatchFraction(fraction: Double): this.type = {\n+    this.miniBatchFraction = fraction\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for SGD. Default 100.\n+   */\n+  def setNumIterations(iters: Array[Int]): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the regularization parameter. Default (0.0, 0.1, 1.0).\n+   */\n+  def setRegParam(regParam: Array[Double]): this.type = {\n+    this.regParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the gradient function (of the loss function of one single data example)\n+   * to be used for SGD.\n+   */\n+  def setGradient(gradient: MultiModelGradient): this.type = {\n+    this.gradient = gradient\n+    this\n+  }\n+\n+\n+  /**\n+   * Set the updater function to actually perform a gradient step in a given direction.\n+   * The updater is responsible to perform the update from the regularization term as well,\n+   * and therefore determines what kind or regularization is used, if any.\n+   */\n+  def setUpdater(updater: Array[MultiModelUpdater]): this.type = {\n+    this.updater = updater\n+    this\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   * Runs gradient descent on the given training data.\n+   * @param data training data\n+   * @param initialWeights initial weights\n+   * @return solution vector\n+   */\n+  @DeveloperApi\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Matrix = {\n+    val (weights, _) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      data,\n+      gradient,\n+      updater,\n+      stepSize,\n+      numIterations,\n+      regParam,\n+      miniBatchFraction,\n+      initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run gradient descent.\n+ */\n+@DeveloperApi\n+object MultiModelGradientDescent extends Logging {\n+  /**\n+   * Run stochastic gradient descent (SGD) in parallel using mini batches.\n+   * In each iteration, we sample a subset (fraction miniBatchFraction) of the total data\n+   * in order to compute a gradient estimate.\n+   * Sampling, and averaging the subgradients over this subset is performed using one standard\n+   * spark map-reduce in each iteration.\n+   *\n+   * @param data - Input data for SGD. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param gradient - Gradient object (used to compute the gradient of the loss function of\n+   *                   one single data example)\n+   * @param updater - Updater function to actually perform a gradient step in a given direction.\n+   * @param stepSize - initial step size for the first step\n+   * @param numIterations - number of iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the\n+   *         stochastic loss computed for every iteration.\n+   */\n+  def runMiniBatchMMSGD(",
    "line": 146
  }],
  "prId": 2451
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "spacing\n",
    "commit": "2ea711c96990c24ed29a4b9476a3809ef26403d6",
    "createdAt": "2014-09-19T21:27:52Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{DenseVector => BDV}\n+\n+import org.apache.spark.annotation.{Experimental, DeveloperApi}\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.mllib.rdd.RDDFunctions._\n+\n+class MultiModelGradientDescent private[mllib] (\n+    private var gradient: MultiModelGradient,\n+    private var updater: Array[MultiModelUpdater]) extends Optimizer[Matrix] with Logging {\n+\n+  private var stepSize: Array[Double] = Array(1.0, 0.1)\n+  private var numIterations: Array[Int] = Array(100)\n+  private var regParam: Array[Double] = Array(0.0, 0.1, 1.0)\n+  private var miniBatchFraction: Double = 1.0\n+\n+  /**\n+   * Set the initial step size of SGD for the first step. Default (1.0, 0.1).\n+   * In subsequent steps, the step size will decrease with stepSize/sqrt(t)\n+   */\n+  def setStepSize(step: Array[Double]): this.type = {\n+    this.stepSize = step\n+    this\n+  }\n+\n+  /**\n+   * :: Experimental ::\n+   * Set fraction of data to be used for each SGD iteration.\n+   * Default 1.0 (corresponding to deterministic/classical gradient descent)\n+   */\n+  @Experimental\n+  def setMiniBatchFraction(fraction: Double): this.type = {\n+    this.miniBatchFraction = fraction\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for SGD. Default 100.\n+   */\n+  def setNumIterations(iters: Array[Int]): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the regularization parameter. Default (0.0, 0.1, 1.0).\n+   */\n+  def setRegParam(regParam: Array[Double]): this.type = {\n+    this.regParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the gradient function (of the loss function of one single data example)\n+   * to be used for SGD.\n+   */\n+  def setGradient(gradient: MultiModelGradient): this.type = {\n+    this.gradient = gradient\n+    this\n+  }\n+\n+\n+  /**\n+   * Set the updater function to actually perform a gradient step in a given direction.\n+   * The updater is responsible to perform the update from the regularization term as well,\n+   * and therefore determines what kind or regularization is used, if any.\n+   */\n+  def setUpdater(updater: Array[MultiModelUpdater]): this.type = {\n+    this.updater = updater\n+    this\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   * Runs gradient descent on the given training data.\n+   * @param data training data\n+   * @param initialWeights initial weights\n+   * @return solution vector\n+   */\n+  @DeveloperApi\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Matrix = {\n+    val (weights, _) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      data,\n+      gradient,\n+      updater,\n+      stepSize,\n+      numIterations,\n+      regParam,\n+      miniBatchFraction,\n+      initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run gradient descent.\n+ */\n+@DeveloperApi\n+object MultiModelGradientDescent extends Logging {\n+  /**\n+   * Run stochastic gradient descent (SGD) in parallel using mini batches.\n+   * In each iteration, we sample a subset (fraction miniBatchFraction) of the total data\n+   * in order to compute a gradient estimate.\n+   * Sampling, and averaging the subgradients over this subset is performed using one standard\n+   * spark map-reduce in each iteration.\n+   *\n+   * @param data - Input data for SGD. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param gradient - Gradient object (used to compute the gradient of the loss function of\n+   *                   one single data example)\n+   * @param updater - Updater function to actually perform a gradient step in a given direction.\n+   * @param stepSize - initial step size for the first step\n+   * @param numIterations - number of iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the\n+   *         stochastic loss computed for every iteration.\n+   */\n+  def runMiniBatchMMSGD(\n+      data: RDD[(Double, Vector)],\n+      gradient: MultiModelGradient,\n+      updater: Array[MultiModelUpdater],\n+      stepSize: Array[Double],\n+      numIterations: Array[Int],\n+      regParam: Array[Double],\n+      miniBatchFraction: Double,\n+      initialWeights: Vector,\n+      batchSize: Int = 64,\n+      useSparse: Boolean = true,\n+      buildSparseThreshold: Double = 0.2): (Matrix, Array[Vector]) = {\n+\n+    val maxNumIter = numIterations.max\n+    val stochasticLossHistory = new ArrayBuffer[Vector](maxNumIter)\n+\n+    val numExamples = data.count()\n+    val miniBatchSize = numExamples * miniBatchFraction\n+    val numModels = stepSize.length * regParam.length\n+    val numFeatures = initialWeights.size\n+    val numRegularizers = updater.length\n+    val updaterCounter = 0 until numRegularizers\n+    // Initialize weights as a column vector\n+    var weights = updaterCounter.map { i =>\n+      new DenseMatrix(numFeatures, 1, initialWeights.toArray).\n+        multiply(DenseMatrix.ones(1, numModels))\n+    }\n+\n+    var finalWeights: Matrix = new DenseMatrix(numFeatures, 0, Array.empty[Double])\n+\n+    // if no data, return initial weights to avoid NaNs\n+    if (numExamples == 0) {\n+",
    "line": 178
  }],
  "prId": 2451
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "This merits explanation.\n",
    "commit": "2ea711c96990c24ed29a4b9476a3809ef26403d6",
    "createdAt": "2014-09-19T21:32:37Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{DenseVector => BDV}\n+\n+import org.apache.spark.annotation.{Experimental, DeveloperApi}\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.mllib.rdd.RDDFunctions._\n+\n+class MultiModelGradientDescent private[mllib] (\n+    private var gradient: MultiModelGradient,\n+    private var updater: Array[MultiModelUpdater]) extends Optimizer[Matrix] with Logging {\n+\n+  private var stepSize: Array[Double] = Array(1.0, 0.1)\n+  private var numIterations: Array[Int] = Array(100)\n+  private var regParam: Array[Double] = Array(0.0, 0.1, 1.0)\n+  private var miniBatchFraction: Double = 1.0\n+\n+  /**\n+   * Set the initial step size of SGD for the first step. Default (1.0, 0.1).\n+   * In subsequent steps, the step size will decrease with stepSize/sqrt(t)\n+   */\n+  def setStepSize(step: Array[Double]): this.type = {\n+    this.stepSize = step\n+    this\n+  }\n+\n+  /**\n+   * :: Experimental ::\n+   * Set fraction of data to be used for each SGD iteration.\n+   * Default 1.0 (corresponding to deterministic/classical gradient descent)\n+   */\n+  @Experimental\n+  def setMiniBatchFraction(fraction: Double): this.type = {\n+    this.miniBatchFraction = fraction\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for SGD. Default 100.\n+   */\n+  def setNumIterations(iters: Array[Int]): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the regularization parameter. Default (0.0, 0.1, 1.0).\n+   */\n+  def setRegParam(regParam: Array[Double]): this.type = {\n+    this.regParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the gradient function (of the loss function of one single data example)\n+   * to be used for SGD.\n+   */\n+  def setGradient(gradient: MultiModelGradient): this.type = {\n+    this.gradient = gradient\n+    this\n+  }\n+\n+\n+  /**\n+   * Set the updater function to actually perform a gradient step in a given direction.\n+   * The updater is responsible to perform the update from the regularization term as well,\n+   * and therefore determines what kind or regularization is used, if any.\n+   */\n+  def setUpdater(updater: Array[MultiModelUpdater]): this.type = {\n+    this.updater = updater\n+    this\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   * Runs gradient descent on the given training data.\n+   * @param data training data\n+   * @param initialWeights initial weights\n+   * @return solution vector\n+   */\n+  @DeveloperApi\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Matrix = {\n+    val (weights, _) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      data,\n+      gradient,\n+      updater,\n+      stepSize,\n+      numIterations,\n+      regParam,\n+      miniBatchFraction,\n+      initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run gradient descent.\n+ */\n+@DeveloperApi\n+object MultiModelGradientDescent extends Logging {\n+  /**\n+   * Run stochastic gradient descent (SGD) in parallel using mini batches.\n+   * In each iteration, we sample a subset (fraction miniBatchFraction) of the total data\n+   * in order to compute a gradient estimate.\n+   * Sampling, and averaging the subgradients over this subset is performed using one standard\n+   * spark map-reduce in each iteration.\n+   *\n+   * @param data - Input data for SGD. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param gradient - Gradient object (used to compute the gradient of the loss function of\n+   *                   one single data example)\n+   * @param updater - Updater function to actually perform a gradient step in a given direction.\n+   * @param stepSize - initial step size for the first step\n+   * @param numIterations - number of iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the\n+   *         stochastic loss computed for every iteration.\n+   */\n+  def runMiniBatchMMSGD(\n+      data: RDD[(Double, Vector)],\n+      gradient: MultiModelGradient,\n+      updater: Array[MultiModelUpdater],\n+      stepSize: Array[Double],\n+      numIterations: Array[Int],\n+      regParam: Array[Double],\n+      miniBatchFraction: Double,\n+      initialWeights: Vector,\n+      batchSize: Int = 64,\n+      useSparse: Boolean = true,\n+      buildSparseThreshold: Double = 0.2): (Matrix, Array[Vector]) = {\n+\n+    val maxNumIter = numIterations.max\n+    val stochasticLossHistory = new ArrayBuffer[Vector](maxNumIter)\n+\n+    val numExamples = data.count()\n+    val miniBatchSize = numExamples * miniBatchFraction\n+    val numModels = stepSize.length * regParam.length\n+    val numFeatures = initialWeights.size\n+    val numRegularizers = updater.length\n+    val updaterCounter = 0 until numRegularizers\n+    // Initialize weights as a column vector\n+    var weights = updaterCounter.map { i =>\n+      new DenseMatrix(numFeatures, 1, initialWeights.toArray).\n+        multiply(DenseMatrix.ones(1, numModels))\n+    }\n+\n+    var finalWeights: Matrix = new DenseMatrix(numFeatures, 0, Array.empty[Double])\n+\n+    // if no data, return initial weights to avoid NaNs\n+    if (numExamples == 0) {\n+\n+      logInfo(\"GradientDescent.runMiniBatchSGD returning initial weights, no data found\")\n+      return (Matrices.horzCat(weights), stochasticLossHistory.toArray)\n+\n+    }\n+    val stepSizeMatrix = new DenseMatrix(1, numModels,\n+      stepSize.flatMap{ ss =>\n+        for (i <- 1 to regParam.length) yield ss\n+      }\n+    )\n+    val regMatrix = SparseMatrix.diag(Vectors.dense(stepSize.flatMap{ ss =>",
    "line": 188
  }],
  "prId": 2451
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Add doc: \"We will concatenate results (weights) to finalWeights as we iterate.\"  (or something like that)\n",
    "commit": "2ea711c96990c24ed29a4b9476a3809ef26403d6",
    "createdAt": "2014-09-19T21:33:30Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{DenseVector => BDV}\n+\n+import org.apache.spark.annotation.{Experimental, DeveloperApi}\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.mllib.rdd.RDDFunctions._\n+\n+class MultiModelGradientDescent private[mllib] (\n+    private var gradient: MultiModelGradient,\n+    private var updater: Array[MultiModelUpdater]) extends Optimizer[Matrix] with Logging {\n+\n+  private var stepSize: Array[Double] = Array(1.0, 0.1)\n+  private var numIterations: Array[Int] = Array(100)\n+  private var regParam: Array[Double] = Array(0.0, 0.1, 1.0)\n+  private var miniBatchFraction: Double = 1.0\n+\n+  /**\n+   * Set the initial step size of SGD for the first step. Default (1.0, 0.1).\n+   * In subsequent steps, the step size will decrease with stepSize/sqrt(t)\n+   */\n+  def setStepSize(step: Array[Double]): this.type = {\n+    this.stepSize = step\n+    this\n+  }\n+\n+  /**\n+   * :: Experimental ::\n+   * Set fraction of data to be used for each SGD iteration.\n+   * Default 1.0 (corresponding to deterministic/classical gradient descent)\n+   */\n+  @Experimental\n+  def setMiniBatchFraction(fraction: Double): this.type = {\n+    this.miniBatchFraction = fraction\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for SGD. Default 100.\n+   */\n+  def setNumIterations(iters: Array[Int]): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the regularization parameter. Default (0.0, 0.1, 1.0).\n+   */\n+  def setRegParam(regParam: Array[Double]): this.type = {\n+    this.regParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the gradient function (of the loss function of one single data example)\n+   * to be used for SGD.\n+   */\n+  def setGradient(gradient: MultiModelGradient): this.type = {\n+    this.gradient = gradient\n+    this\n+  }\n+\n+\n+  /**\n+   * Set the updater function to actually perform a gradient step in a given direction.\n+   * The updater is responsible to perform the update from the regularization term as well,\n+   * and therefore determines what kind or regularization is used, if any.\n+   */\n+  def setUpdater(updater: Array[MultiModelUpdater]): this.type = {\n+    this.updater = updater\n+    this\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   * Runs gradient descent on the given training data.\n+   * @param data training data\n+   * @param initialWeights initial weights\n+   * @return solution vector\n+   */\n+  @DeveloperApi\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Matrix = {\n+    val (weights, _) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      data,\n+      gradient,\n+      updater,\n+      stepSize,\n+      numIterations,\n+      regParam,\n+      miniBatchFraction,\n+      initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run gradient descent.\n+ */\n+@DeveloperApi\n+object MultiModelGradientDescent extends Logging {\n+  /**\n+   * Run stochastic gradient descent (SGD) in parallel using mini batches.\n+   * In each iteration, we sample a subset (fraction miniBatchFraction) of the total data\n+   * in order to compute a gradient estimate.\n+   * Sampling, and averaging the subgradients over this subset is performed using one standard\n+   * spark map-reduce in each iteration.\n+   *\n+   * @param data - Input data for SGD. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param gradient - Gradient object (used to compute the gradient of the loss function of\n+   *                   one single data example)\n+   * @param updater - Updater function to actually perform a gradient step in a given direction.\n+   * @param stepSize - initial step size for the first step\n+   * @param numIterations - number of iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the\n+   *         stochastic loss computed for every iteration.\n+   */\n+  def runMiniBatchMMSGD(\n+      data: RDD[(Double, Vector)],\n+      gradient: MultiModelGradient,\n+      updater: Array[MultiModelUpdater],\n+      stepSize: Array[Double],\n+      numIterations: Array[Int],\n+      regParam: Array[Double],\n+      miniBatchFraction: Double,\n+      initialWeights: Vector,\n+      batchSize: Int = 64,\n+      useSparse: Boolean = true,\n+      buildSparseThreshold: Double = 0.2): (Matrix, Array[Vector]) = {\n+\n+    val maxNumIter = numIterations.max\n+    val stochasticLossHistory = new ArrayBuffer[Vector](maxNumIter)\n+\n+    val numExamples = data.count()\n+    val miniBatchSize = numExamples * miniBatchFraction\n+    val numModels = stepSize.length * regParam.length\n+    val numFeatures = initialWeights.size\n+    val numRegularizers = updater.length\n+    val updaterCounter = 0 until numRegularizers\n+    // Initialize weights as a column vector\n+    var weights = updaterCounter.map { i =>\n+      new DenseMatrix(numFeatures, 1, initialWeights.toArray).\n+        multiply(DenseMatrix.ones(1, numModels))\n+    }\n+\n+    var finalWeights: Matrix = new DenseMatrix(numFeatures, 0, Array.empty[Double])",
    "line": 174
  }],
  "prId": 2451
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Add punctuation; these 2 lines look like 1 sentence.\n",
    "commit": "2ea711c96990c24ed29a4b9476a3809ef26403d6",
    "createdAt": "2014-09-19T21:34:53Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{DenseVector => BDV}\n+\n+import org.apache.spark.annotation.{Experimental, DeveloperApi}\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.mllib.rdd.RDDFunctions._\n+\n+class MultiModelGradientDescent private[mllib] (\n+    private var gradient: MultiModelGradient,\n+    private var updater: Array[MultiModelUpdater]) extends Optimizer[Matrix] with Logging {\n+\n+  private var stepSize: Array[Double] = Array(1.0, 0.1)\n+  private var numIterations: Array[Int] = Array(100)\n+  private var regParam: Array[Double] = Array(0.0, 0.1, 1.0)\n+  private var miniBatchFraction: Double = 1.0\n+\n+  /**\n+   * Set the initial step size of SGD for the first step. Default (1.0, 0.1).\n+   * In subsequent steps, the step size will decrease with stepSize/sqrt(t)\n+   */\n+  def setStepSize(step: Array[Double]): this.type = {\n+    this.stepSize = step\n+    this\n+  }\n+\n+  /**\n+   * :: Experimental ::\n+   * Set fraction of data to be used for each SGD iteration.\n+   * Default 1.0 (corresponding to deterministic/classical gradient descent)\n+   */\n+  @Experimental\n+  def setMiniBatchFraction(fraction: Double): this.type = {\n+    this.miniBatchFraction = fraction\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for SGD. Default 100.\n+   */\n+  def setNumIterations(iters: Array[Int]): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the regularization parameter. Default (0.0, 0.1, 1.0).\n+   */\n+  def setRegParam(regParam: Array[Double]): this.type = {\n+    this.regParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the gradient function (of the loss function of one single data example)\n+   * to be used for SGD.\n+   */\n+  def setGradient(gradient: MultiModelGradient): this.type = {\n+    this.gradient = gradient\n+    this\n+  }\n+\n+\n+  /**\n+   * Set the updater function to actually perform a gradient step in a given direction.\n+   * The updater is responsible to perform the update from the regularization term as well,\n+   * and therefore determines what kind or regularization is used, if any.\n+   */\n+  def setUpdater(updater: Array[MultiModelUpdater]): this.type = {\n+    this.updater = updater\n+    this\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   * Runs gradient descent on the given training data.\n+   * @param data training data\n+   * @param initialWeights initial weights\n+   * @return solution vector\n+   */\n+  @DeveloperApi\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Matrix = {\n+    val (weights, _) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      data,\n+      gradient,\n+      updater,\n+      stepSize,\n+      numIterations,\n+      regParam,\n+      miniBatchFraction,\n+      initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run gradient descent.\n+ */\n+@DeveloperApi\n+object MultiModelGradientDescent extends Logging {\n+  /**\n+   * Run stochastic gradient descent (SGD) in parallel using mini batches.\n+   * In each iteration, we sample a subset (fraction miniBatchFraction) of the total data\n+   * in order to compute a gradient estimate.\n+   * Sampling, and averaging the subgradients over this subset is performed using one standard\n+   * spark map-reduce in each iteration.\n+   *\n+   * @param data - Input data for SGD. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param gradient - Gradient object (used to compute the gradient of the loss function of\n+   *                   one single data example)\n+   * @param updater - Updater function to actually perform a gradient step in a given direction.\n+   * @param stepSize - initial step size for the first step\n+   * @param numIterations - number of iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the\n+   *         stochastic loss computed for every iteration.\n+   */\n+  def runMiniBatchMMSGD(\n+      data: RDD[(Double, Vector)],\n+      gradient: MultiModelGradient,\n+      updater: Array[MultiModelUpdater],\n+      stepSize: Array[Double],\n+      numIterations: Array[Int],\n+      regParam: Array[Double],\n+      miniBatchFraction: Double,\n+      initialWeights: Vector,\n+      batchSize: Int = 64,\n+      useSparse: Boolean = true,\n+      buildSparseThreshold: Double = 0.2): (Matrix, Array[Vector]) = {\n+\n+    val maxNumIter = numIterations.max\n+    val stochasticLossHistory = new ArrayBuffer[Vector](maxNumIter)\n+\n+    val numExamples = data.count()\n+    val miniBatchSize = numExamples * miniBatchFraction\n+    val numModels = stepSize.length * regParam.length\n+    val numFeatures = initialWeights.size\n+    val numRegularizers = updater.length\n+    val updaterCounter = 0 until numRegularizers\n+    // Initialize weights as a column vector\n+    var weights = updaterCounter.map { i =>\n+      new DenseMatrix(numFeatures, 1, initialWeights.toArray).\n+        multiply(DenseMatrix.ones(1, numModels))\n+    }\n+\n+    var finalWeights: Matrix = new DenseMatrix(numFeatures, 0, Array.empty[Double])\n+\n+    // if no data, return initial weights to avoid NaNs\n+    if (numExamples == 0) {\n+\n+      logInfo(\"GradientDescent.runMiniBatchSGD returning initial weights, no data found\")\n+      return (Matrices.horzCat(weights), stochasticLossHistory.toArray)\n+\n+    }\n+    val stepSizeMatrix = new DenseMatrix(1, numModels,\n+      stepSize.flatMap{ ss =>\n+        for (i <- 1 to regParam.length) yield ss\n+      }\n+    )\n+    val regMatrix = SparseMatrix.diag(Vectors.dense(stepSize.flatMap{ ss =>\n+      for (reg <- regParam) yield reg\n+    }))\n+\n+    val bcMetaData =\n+      if (useSparse) {\n+        data.context.broadcast(Matrices.getSparsityData(data, batchSize))\n+      } else {\n+        val emptyData: Array[(Int, Int)] = (0 until data.partitions.length).map { i =>\n+          (i, -1)}.toArray\n+        data.context.broadcast(emptyData)\n+      }\n+    val points = Matrices.fromRDD(data, bcMetaData.value, batchSize, buildSparseThreshold)\n+\n+    /**\n+     * For the first iteration, the regVal will be initialized as sum of weight squares\n+     * if it's L2 updater; for L1 updater, the same logic is followed.\n+     */\n+    val updaterWithIndex = updater.zipWithIndex\n+\n+    var regVal = updaterWithIndex.map { case (u, ind) =>\n+      u.compute(weights(ind), DenseMatrix.zeros(numFeatures, numModels),\n+        DenseMatrix.zeros(1, numModels), 1, regMatrix)._2\n+    }\n+    val orderedIters = numIterations.sorted\n+    var iterIndexCounter = 0\n+    for (i <- 1 to maxNumIter) {\n+      val bcWeights = data.context.broadcast(weights)\n+      // Sample a subset (fraction miniBatchFraction) of the total data",
    "line": 216
  }],
  "prId": 2451
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "miniBatchSize is inexact.  We could avoid the initial count() and instead aggregate the minibatch size during the treeAggregate.\n",
    "commit": "2ea711c96990c24ed29a4b9476a3809ef26403d6",
    "createdAt": "2014-09-19T21:38:30Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{DenseVector => BDV}\n+\n+import org.apache.spark.annotation.{Experimental, DeveloperApi}\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.mllib.rdd.RDDFunctions._\n+\n+class MultiModelGradientDescent private[mllib] (\n+    private var gradient: MultiModelGradient,\n+    private var updater: Array[MultiModelUpdater]) extends Optimizer[Matrix] with Logging {\n+\n+  private var stepSize: Array[Double] = Array(1.0, 0.1)\n+  private var numIterations: Array[Int] = Array(100)\n+  private var regParam: Array[Double] = Array(0.0, 0.1, 1.0)\n+  private var miniBatchFraction: Double = 1.0\n+\n+  /**\n+   * Set the initial step size of SGD for the first step. Default (1.0, 0.1).\n+   * In subsequent steps, the step size will decrease with stepSize/sqrt(t)\n+   */\n+  def setStepSize(step: Array[Double]): this.type = {\n+    this.stepSize = step\n+    this\n+  }\n+\n+  /**\n+   * :: Experimental ::\n+   * Set fraction of data to be used for each SGD iteration.\n+   * Default 1.0 (corresponding to deterministic/classical gradient descent)\n+   */\n+  @Experimental\n+  def setMiniBatchFraction(fraction: Double): this.type = {\n+    this.miniBatchFraction = fraction\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for SGD. Default 100.\n+   */\n+  def setNumIterations(iters: Array[Int]): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the regularization parameter. Default (0.0, 0.1, 1.0).\n+   */\n+  def setRegParam(regParam: Array[Double]): this.type = {\n+    this.regParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the gradient function (of the loss function of one single data example)\n+   * to be used for SGD.\n+   */\n+  def setGradient(gradient: MultiModelGradient): this.type = {\n+    this.gradient = gradient\n+    this\n+  }\n+\n+\n+  /**\n+   * Set the updater function to actually perform a gradient step in a given direction.\n+   * The updater is responsible to perform the update from the regularization term as well,\n+   * and therefore determines what kind or regularization is used, if any.\n+   */\n+  def setUpdater(updater: Array[MultiModelUpdater]): this.type = {\n+    this.updater = updater\n+    this\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   * Runs gradient descent on the given training data.\n+   * @param data training data\n+   * @param initialWeights initial weights\n+   * @return solution vector\n+   */\n+  @DeveloperApi\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Matrix = {\n+    val (weights, _) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      data,\n+      gradient,\n+      updater,\n+      stepSize,\n+      numIterations,\n+      regParam,\n+      miniBatchFraction,\n+      initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run gradient descent.\n+ */\n+@DeveloperApi\n+object MultiModelGradientDescent extends Logging {\n+  /**\n+   * Run stochastic gradient descent (SGD) in parallel using mini batches.\n+   * In each iteration, we sample a subset (fraction miniBatchFraction) of the total data\n+   * in order to compute a gradient estimate.\n+   * Sampling, and averaging the subgradients over this subset is performed using one standard\n+   * spark map-reduce in each iteration.\n+   *\n+   * @param data - Input data for SGD. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param gradient - Gradient object (used to compute the gradient of the loss function of\n+   *                   one single data example)\n+   * @param updater - Updater function to actually perform a gradient step in a given direction.\n+   * @param stepSize - initial step size for the first step\n+   * @param numIterations - number of iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the\n+   *         stochastic loss computed for every iteration.\n+   */\n+  def runMiniBatchMMSGD(\n+      data: RDD[(Double, Vector)],\n+      gradient: MultiModelGradient,\n+      updater: Array[MultiModelUpdater],\n+      stepSize: Array[Double],\n+      numIterations: Array[Int],\n+      regParam: Array[Double],\n+      miniBatchFraction: Double,\n+      initialWeights: Vector,\n+      batchSize: Int = 64,\n+      useSparse: Boolean = true,\n+      buildSparseThreshold: Double = 0.2): (Matrix, Array[Vector]) = {\n+\n+    val maxNumIter = numIterations.max\n+    val stochasticLossHistory = new ArrayBuffer[Vector](maxNumIter)\n+\n+    val numExamples = data.count()\n+    val miniBatchSize = numExamples * miniBatchFraction\n+    val numModels = stepSize.length * regParam.length\n+    val numFeatures = initialWeights.size\n+    val numRegularizers = updater.length\n+    val updaterCounter = 0 until numRegularizers\n+    // Initialize weights as a column vector\n+    var weights = updaterCounter.map { i =>\n+      new DenseMatrix(numFeatures, 1, initialWeights.toArray).\n+        multiply(DenseMatrix.ones(1, numModels))\n+    }\n+\n+    var finalWeights: Matrix = new DenseMatrix(numFeatures, 0, Array.empty[Double])\n+\n+    // if no data, return initial weights to avoid NaNs\n+    if (numExamples == 0) {\n+\n+      logInfo(\"GradientDescent.runMiniBatchSGD returning initial weights, no data found\")\n+      return (Matrices.horzCat(weights), stochasticLossHistory.toArray)\n+\n+    }\n+    val stepSizeMatrix = new DenseMatrix(1, numModels,\n+      stepSize.flatMap{ ss =>\n+        for (i <- 1 to regParam.length) yield ss\n+      }\n+    )\n+    val regMatrix = SparseMatrix.diag(Vectors.dense(stepSize.flatMap{ ss =>\n+      for (reg <- regParam) yield reg\n+    }))\n+\n+    val bcMetaData =\n+      if (useSparse) {\n+        data.context.broadcast(Matrices.getSparsityData(data, batchSize))\n+      } else {\n+        val emptyData: Array[(Int, Int)] = (0 until data.partitions.length).map { i =>\n+          (i, -1)}.toArray\n+        data.context.broadcast(emptyData)\n+      }\n+    val points = Matrices.fromRDD(data, bcMetaData.value, batchSize, buildSparseThreshold)\n+\n+    /**\n+     * For the first iteration, the regVal will be initialized as sum of weight squares\n+     * if it's L2 updater; for L1 updater, the same logic is followed.\n+     */\n+    val updaterWithIndex = updater.zipWithIndex\n+\n+    var regVal = updaterWithIndex.map { case (u, ind) =>\n+      u.compute(weights(ind), DenseMatrix.zeros(numFeatures, numModels),\n+        DenseMatrix.zeros(1, numModels), 1, regMatrix)._2\n+    }\n+    val orderedIters = numIterations.sorted\n+    var iterIndexCounter = 0\n+    for (i <- 1 to maxNumIter) {\n+      val bcWeights = data.context.broadcast(weights)\n+      // Sample a subset (fraction miniBatchFraction) of the total data\n+      // compute and sum up the subgradients on this subset (this is one map-reduce)\n+      val (gradientSum, lossSum) = points.sample(false, miniBatchFraction, 42 + i)\n+        .treeAggregate(updaterCounter.map(j => Matrices.zeros(numFeatures, numModels)),\n+          updaterCounter.map(j => Matrices.zeros(1, numModels)))(\n+          seqOp = (c, v) => (c, v) match { case ((grad, loss), (label, features)) =>\n+            val l = updaterCounter.map { j =>\n+              gradient.compute(features, label, bcWeights.value(j),\n+                grad(j).asInstanceOf[DenseMatrix])\n+            }\n+            (grad, loss.zip(l).map(r => r._1.elementWiseOperateInPlace(_ + _, r._2)))\n+          },\n+          combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1), (grad2, loss2)) =>\n+            (grad1.zip(grad2).map(r => r._1.elementWiseOperateInPlace(_ + _, r._2)),\n+              loss1.zip(loss2).map(r => r._1.elementWiseOperateInPlace(_ + _, r._2)))\n+          })\n+      stochasticLossHistory.append(Vectors.dense(Matrices.horzCat(updaterCounter.map { i =>\n+        lossSum(i).elementWiseOperateScalarInPlace(_ / _, miniBatchSize)."
  }],
  "prId": 2451
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "It might be better to compute updaterCounter at nodes so that we broadcast a scalar instead of an array.  (not that the array is that big, but still)\n",
    "commit": "2ea711c96990c24ed29a4b9476a3809ef26403d6",
    "createdAt": "2014-09-19T21:41:31Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{DenseVector => BDV}\n+\n+import org.apache.spark.annotation.{Experimental, DeveloperApi}\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.mllib.rdd.RDDFunctions._\n+\n+class MultiModelGradientDescent private[mllib] (\n+    private var gradient: MultiModelGradient,\n+    private var updater: Array[MultiModelUpdater]) extends Optimizer[Matrix] with Logging {\n+\n+  private var stepSize: Array[Double] = Array(1.0, 0.1)\n+  private var numIterations: Array[Int] = Array(100)\n+  private var regParam: Array[Double] = Array(0.0, 0.1, 1.0)\n+  private var miniBatchFraction: Double = 1.0\n+\n+  /**\n+   * Set the initial step size of SGD for the first step. Default (1.0, 0.1).\n+   * In subsequent steps, the step size will decrease with stepSize/sqrt(t)\n+   */\n+  def setStepSize(step: Array[Double]): this.type = {\n+    this.stepSize = step\n+    this\n+  }\n+\n+  /**\n+   * :: Experimental ::\n+   * Set fraction of data to be used for each SGD iteration.\n+   * Default 1.0 (corresponding to deterministic/classical gradient descent)\n+   */\n+  @Experimental\n+  def setMiniBatchFraction(fraction: Double): this.type = {\n+    this.miniBatchFraction = fraction\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for SGD. Default 100.\n+   */\n+  def setNumIterations(iters: Array[Int]): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the regularization parameter. Default (0.0, 0.1, 1.0).\n+   */\n+  def setRegParam(regParam: Array[Double]): this.type = {\n+    this.regParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the gradient function (of the loss function of one single data example)\n+   * to be used for SGD.\n+   */\n+  def setGradient(gradient: MultiModelGradient): this.type = {\n+    this.gradient = gradient\n+    this\n+  }\n+\n+\n+  /**\n+   * Set the updater function to actually perform a gradient step in a given direction.\n+   * The updater is responsible to perform the update from the regularization term as well,\n+   * and therefore determines what kind or regularization is used, if any.\n+   */\n+  def setUpdater(updater: Array[MultiModelUpdater]): this.type = {\n+    this.updater = updater\n+    this\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   * Runs gradient descent on the given training data.\n+   * @param data training data\n+   * @param initialWeights initial weights\n+   * @return solution vector\n+   */\n+  @DeveloperApi\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Matrix = {\n+    val (weights, _) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      data,\n+      gradient,\n+      updater,\n+      stepSize,\n+      numIterations,\n+      regParam,\n+      miniBatchFraction,\n+      initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run gradient descent.\n+ */\n+@DeveloperApi\n+object MultiModelGradientDescent extends Logging {\n+  /**\n+   * Run stochastic gradient descent (SGD) in parallel using mini batches.\n+   * In each iteration, we sample a subset (fraction miniBatchFraction) of the total data\n+   * in order to compute a gradient estimate.\n+   * Sampling, and averaging the subgradients over this subset is performed using one standard\n+   * spark map-reduce in each iteration.\n+   *\n+   * @param data - Input data for SGD. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param gradient - Gradient object (used to compute the gradient of the loss function of\n+   *                   one single data example)\n+   * @param updater - Updater function to actually perform a gradient step in a given direction.\n+   * @param stepSize - initial step size for the first step\n+   * @param numIterations - number of iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the\n+   *         stochastic loss computed for every iteration.\n+   */\n+  def runMiniBatchMMSGD(\n+      data: RDD[(Double, Vector)],\n+      gradient: MultiModelGradient,\n+      updater: Array[MultiModelUpdater],\n+      stepSize: Array[Double],\n+      numIterations: Array[Int],\n+      regParam: Array[Double],\n+      miniBatchFraction: Double,\n+      initialWeights: Vector,\n+      batchSize: Int = 64,\n+      useSparse: Boolean = true,\n+      buildSparseThreshold: Double = 0.2): (Matrix, Array[Vector]) = {\n+\n+    val maxNumIter = numIterations.max\n+    val stochasticLossHistory = new ArrayBuffer[Vector](maxNumIter)\n+\n+    val numExamples = data.count()\n+    val miniBatchSize = numExamples * miniBatchFraction\n+    val numModels = stepSize.length * regParam.length\n+    val numFeatures = initialWeights.size\n+    val numRegularizers = updater.length\n+    val updaterCounter = 0 until numRegularizers\n+    // Initialize weights as a column vector\n+    var weights = updaterCounter.map { i =>\n+      new DenseMatrix(numFeatures, 1, initialWeights.toArray).\n+        multiply(DenseMatrix.ones(1, numModels))\n+    }\n+\n+    var finalWeights: Matrix = new DenseMatrix(numFeatures, 0, Array.empty[Double])\n+\n+    // if no data, return initial weights to avoid NaNs\n+    if (numExamples == 0) {\n+\n+      logInfo(\"GradientDescent.runMiniBatchSGD returning initial weights, no data found\")\n+      return (Matrices.horzCat(weights), stochasticLossHistory.toArray)\n+\n+    }\n+    val stepSizeMatrix = new DenseMatrix(1, numModels,\n+      stepSize.flatMap{ ss =>\n+        for (i <- 1 to regParam.length) yield ss\n+      }\n+    )\n+    val regMatrix = SparseMatrix.diag(Vectors.dense(stepSize.flatMap{ ss =>\n+      for (reg <- regParam) yield reg\n+    }))\n+\n+    val bcMetaData =\n+      if (useSparse) {\n+        data.context.broadcast(Matrices.getSparsityData(data, batchSize))\n+      } else {\n+        val emptyData: Array[(Int, Int)] = (0 until data.partitions.length).map { i =>\n+          (i, -1)}.toArray\n+        data.context.broadcast(emptyData)\n+      }\n+    val points = Matrices.fromRDD(data, bcMetaData.value, batchSize, buildSparseThreshold)\n+\n+    /**\n+     * For the first iteration, the regVal will be initialized as sum of weight squares\n+     * if it's L2 updater; for L1 updater, the same logic is followed.\n+     */\n+    val updaterWithIndex = updater.zipWithIndex\n+\n+    var regVal = updaterWithIndex.map { case (u, ind) =>\n+      u.compute(weights(ind), DenseMatrix.zeros(numFeatures, numModels),\n+        DenseMatrix.zeros(1, numModels), 1, regMatrix)._2\n+    }\n+    val orderedIters = numIterations.sorted\n+    var iterIndexCounter = 0\n+    for (i <- 1 to maxNumIter) {\n+      val bcWeights = data.context.broadcast(weights)\n+      // Sample a subset (fraction miniBatchFraction) of the total data\n+      // compute and sum up the subgradients on this subset (this is one map-reduce)\n+      val (gradientSum, lossSum) = points.sample(false, miniBatchFraction, 42 + i)\n+        .treeAggregate(updaterCounter.map(j => Matrices.zeros(numFeatures, numModels)),",
    "line": 219
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "In this treeAggregate, it might be nice to include some explicit types for clarity.\n",
    "commit": "2ea711c96990c24ed29a4b9476a3809ef26403d6",
    "createdAt": "2014-09-19T21:43:35Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{DenseVector => BDV}\n+\n+import org.apache.spark.annotation.{Experimental, DeveloperApi}\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.mllib.rdd.RDDFunctions._\n+\n+class MultiModelGradientDescent private[mllib] (\n+    private var gradient: MultiModelGradient,\n+    private var updater: Array[MultiModelUpdater]) extends Optimizer[Matrix] with Logging {\n+\n+  private var stepSize: Array[Double] = Array(1.0, 0.1)\n+  private var numIterations: Array[Int] = Array(100)\n+  private var regParam: Array[Double] = Array(0.0, 0.1, 1.0)\n+  private var miniBatchFraction: Double = 1.0\n+\n+  /**\n+   * Set the initial step size of SGD for the first step. Default (1.0, 0.1).\n+   * In subsequent steps, the step size will decrease with stepSize/sqrt(t)\n+   */\n+  def setStepSize(step: Array[Double]): this.type = {\n+    this.stepSize = step\n+    this\n+  }\n+\n+  /**\n+   * :: Experimental ::\n+   * Set fraction of data to be used for each SGD iteration.\n+   * Default 1.0 (corresponding to deterministic/classical gradient descent)\n+   */\n+  @Experimental\n+  def setMiniBatchFraction(fraction: Double): this.type = {\n+    this.miniBatchFraction = fraction\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for SGD. Default 100.\n+   */\n+  def setNumIterations(iters: Array[Int]): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the regularization parameter. Default (0.0, 0.1, 1.0).\n+   */\n+  def setRegParam(regParam: Array[Double]): this.type = {\n+    this.regParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the gradient function (of the loss function of one single data example)\n+   * to be used for SGD.\n+   */\n+  def setGradient(gradient: MultiModelGradient): this.type = {\n+    this.gradient = gradient\n+    this\n+  }\n+\n+\n+  /**\n+   * Set the updater function to actually perform a gradient step in a given direction.\n+   * The updater is responsible to perform the update from the regularization term as well,\n+   * and therefore determines what kind or regularization is used, if any.\n+   */\n+  def setUpdater(updater: Array[MultiModelUpdater]): this.type = {\n+    this.updater = updater\n+    this\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   * Runs gradient descent on the given training data.\n+   * @param data training data\n+   * @param initialWeights initial weights\n+   * @return solution vector\n+   */\n+  @DeveloperApi\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Matrix = {\n+    val (weights, _) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      data,\n+      gradient,\n+      updater,\n+      stepSize,\n+      numIterations,\n+      regParam,\n+      miniBatchFraction,\n+      initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run gradient descent.\n+ */\n+@DeveloperApi\n+object MultiModelGradientDescent extends Logging {\n+  /**\n+   * Run stochastic gradient descent (SGD) in parallel using mini batches.\n+   * In each iteration, we sample a subset (fraction miniBatchFraction) of the total data\n+   * in order to compute a gradient estimate.\n+   * Sampling, and averaging the subgradients over this subset is performed using one standard\n+   * spark map-reduce in each iteration.\n+   *\n+   * @param data - Input data for SGD. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param gradient - Gradient object (used to compute the gradient of the loss function of\n+   *                   one single data example)\n+   * @param updater - Updater function to actually perform a gradient step in a given direction.\n+   * @param stepSize - initial step size for the first step\n+   * @param numIterations - number of iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the\n+   *         stochastic loss computed for every iteration.\n+   */\n+  def runMiniBatchMMSGD(\n+      data: RDD[(Double, Vector)],\n+      gradient: MultiModelGradient,\n+      updater: Array[MultiModelUpdater],\n+      stepSize: Array[Double],\n+      numIterations: Array[Int],\n+      regParam: Array[Double],\n+      miniBatchFraction: Double,\n+      initialWeights: Vector,\n+      batchSize: Int = 64,\n+      useSparse: Boolean = true,\n+      buildSparseThreshold: Double = 0.2): (Matrix, Array[Vector]) = {\n+\n+    val maxNumIter = numIterations.max\n+    val stochasticLossHistory = new ArrayBuffer[Vector](maxNumIter)\n+\n+    val numExamples = data.count()\n+    val miniBatchSize = numExamples * miniBatchFraction\n+    val numModels = stepSize.length * regParam.length\n+    val numFeatures = initialWeights.size\n+    val numRegularizers = updater.length\n+    val updaterCounter = 0 until numRegularizers\n+    // Initialize weights as a column vector\n+    var weights = updaterCounter.map { i =>\n+      new DenseMatrix(numFeatures, 1, initialWeights.toArray).\n+        multiply(DenseMatrix.ones(1, numModels))\n+    }\n+\n+    var finalWeights: Matrix = new DenseMatrix(numFeatures, 0, Array.empty[Double])\n+\n+    // if no data, return initial weights to avoid NaNs\n+    if (numExamples == 0) {\n+\n+      logInfo(\"GradientDescent.runMiniBatchSGD returning initial weights, no data found\")\n+      return (Matrices.horzCat(weights), stochasticLossHistory.toArray)\n+\n+    }\n+    val stepSizeMatrix = new DenseMatrix(1, numModels,\n+      stepSize.flatMap{ ss =>\n+        for (i <- 1 to regParam.length) yield ss\n+      }\n+    )\n+    val regMatrix = SparseMatrix.diag(Vectors.dense(stepSize.flatMap{ ss =>\n+      for (reg <- regParam) yield reg\n+    }))\n+\n+    val bcMetaData =\n+      if (useSparse) {\n+        data.context.broadcast(Matrices.getSparsityData(data, batchSize))\n+      } else {\n+        val emptyData: Array[(Int, Int)] = (0 until data.partitions.length).map { i =>\n+          (i, -1)}.toArray\n+        data.context.broadcast(emptyData)\n+      }\n+    val points = Matrices.fromRDD(data, bcMetaData.value, batchSize, buildSparseThreshold)\n+\n+    /**\n+     * For the first iteration, the regVal will be initialized as sum of weight squares\n+     * if it's L2 updater; for L1 updater, the same logic is followed.\n+     */\n+    val updaterWithIndex = updater.zipWithIndex\n+\n+    var regVal = updaterWithIndex.map { case (u, ind) =>\n+      u.compute(weights(ind), DenseMatrix.zeros(numFeatures, numModels),\n+        DenseMatrix.zeros(1, numModels), 1, regMatrix)._2\n+    }\n+    val orderedIters = numIterations.sorted\n+    var iterIndexCounter = 0\n+    for (i <- 1 to maxNumIter) {\n+      val bcWeights = data.context.broadcast(weights)\n+      // Sample a subset (fraction miniBatchFraction) of the total data\n+      // compute and sum up the subgradients on this subset (this is one map-reduce)\n+      val (gradientSum, lossSum) = points.sample(false, miniBatchFraction, 42 + i)\n+        .treeAggregate(updaterCounter.map(j => Matrices.zeros(numFeatures, numModels)),",
    "line": 219
  }],
  "prId": 2451
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Move \"u.compute(\" to next line\n",
    "commit": "2ea711c96990c24ed29a4b9476a3809ef26403d6",
    "createdAt": "2014-09-19T21:44:17Z",
    "diffHunk": "@@ -0,0 +1,256 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{DenseVector => BDV}\n+\n+import org.apache.spark.annotation.{Experimental, DeveloperApi}\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.mllib.rdd.RDDFunctions._\n+\n+class MultiModelGradientDescent private[mllib] (\n+    private var gradient: MultiModelGradient,\n+    private var updater: Array[MultiModelUpdater]) extends Optimizer[Matrix] with Logging {\n+\n+  private var stepSize: Array[Double] = Array(1.0, 0.1)\n+  private var numIterations: Array[Int] = Array(100)\n+  private var regParam: Array[Double] = Array(0.0, 0.1, 1.0)\n+  private var miniBatchFraction: Double = 1.0\n+\n+  /**\n+   * Set the initial step size of SGD for the first step. Default (1.0, 0.1).\n+   * In subsequent steps, the step size will decrease with stepSize/sqrt(t)\n+   */\n+  def setStepSize(step: Array[Double]): this.type = {\n+    this.stepSize = step\n+    this\n+  }\n+\n+  /**\n+   * :: Experimental ::\n+   * Set fraction of data to be used for each SGD iteration.\n+   * Default 1.0 (corresponding to deterministic/classical gradient descent)\n+   */\n+  @Experimental\n+  def setMiniBatchFraction(fraction: Double): this.type = {\n+    this.miniBatchFraction = fraction\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for SGD. Default 100.\n+   */\n+  def setNumIterations(iters: Array[Int]): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the regularization parameter. Default (0.0, 0.1, 1.0).\n+   */\n+  def setRegParam(regParam: Array[Double]): this.type = {\n+    this.regParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the gradient function (of the loss function of one single data example)\n+   * to be used for SGD.\n+   */\n+  def setGradient(gradient: MultiModelGradient): this.type = {\n+    this.gradient = gradient\n+    this\n+  }\n+\n+\n+  /**\n+   * Set the updater function to actually perform a gradient step in a given direction.\n+   * The updater is responsible to perform the update from the regularization term as well,\n+   * and therefore determines what kind or regularization is used, if any.\n+   */\n+  def setUpdater(updater: Array[MultiModelUpdater]): this.type = {\n+    this.updater = updater\n+    this\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   * Runs gradient descent on the given training data.\n+   * @param data training data\n+   * @param initialWeights initial weights\n+   * @return solution vector\n+   */\n+  @DeveloperApi\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Matrix = {\n+    val (weights, _) = MultiModelGradientDescent.runMiniBatchMMSGD(\n+      data,\n+      gradient,\n+      updater,\n+      stepSize,\n+      numIterations,\n+      regParam,\n+      miniBatchFraction,\n+      initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run gradient descent.\n+ */\n+@DeveloperApi\n+object MultiModelGradientDescent extends Logging {\n+  /**\n+   * Run stochastic gradient descent (SGD) in parallel using mini batches.\n+   * In each iteration, we sample a subset (fraction miniBatchFraction) of the total data\n+   * in order to compute a gradient estimate.\n+   * Sampling, and averaging the subgradients over this subset is performed using one standard\n+   * spark map-reduce in each iteration.\n+   *\n+   * @param data - Input data for SGD. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param gradient - Gradient object (used to compute the gradient of the loss function of\n+   *                   one single data example)\n+   * @param updater - Updater function to actually perform a gradient step in a given direction.\n+   * @param stepSize - initial step size for the first step\n+   * @param numIterations - number of iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the\n+   *         stochastic loss computed for every iteration.\n+   */\n+  def runMiniBatchMMSGD(\n+      data: RDD[(Double, Vector)],\n+      gradient: MultiModelGradient,\n+      updater: Array[MultiModelUpdater],\n+      stepSize: Array[Double],\n+      numIterations: Array[Int],\n+      regParam: Array[Double],\n+      miniBatchFraction: Double,\n+      initialWeights: Vector,\n+      batchSize: Int = 64,\n+      useSparse: Boolean = true,\n+      buildSparseThreshold: Double = 0.2): (Matrix, Array[Vector]) = {\n+\n+    val maxNumIter = numIterations.max\n+    val stochasticLossHistory = new ArrayBuffer[Vector](maxNumIter)\n+\n+    val numExamples = data.count()\n+    val miniBatchSize = numExamples * miniBatchFraction\n+    val numModels = stepSize.length * regParam.length\n+    val numFeatures = initialWeights.size\n+    val numRegularizers = updater.length\n+    val updaterCounter = 0 until numRegularizers\n+    // Initialize weights as a column vector\n+    var weights = updaterCounter.map { i =>\n+      new DenseMatrix(numFeatures, 1, initialWeights.toArray).\n+        multiply(DenseMatrix.ones(1, numModels))\n+    }\n+\n+    var finalWeights: Matrix = new DenseMatrix(numFeatures, 0, Array.empty[Double])\n+\n+    // if no data, return initial weights to avoid NaNs\n+    if (numExamples == 0) {\n+\n+      logInfo(\"GradientDescent.runMiniBatchSGD returning initial weights, no data found\")\n+      return (Matrices.horzCat(weights), stochasticLossHistory.toArray)\n+\n+    }\n+    val stepSizeMatrix = new DenseMatrix(1, numModels,\n+      stepSize.flatMap{ ss =>\n+        for (i <- 1 to regParam.length) yield ss\n+      }\n+    )\n+    val regMatrix = SparseMatrix.diag(Vectors.dense(stepSize.flatMap{ ss =>\n+      for (reg <- regParam) yield reg\n+    }))\n+\n+    val bcMetaData =\n+      if (useSparse) {\n+        data.context.broadcast(Matrices.getSparsityData(data, batchSize))\n+      } else {\n+        val emptyData: Array[(Int, Int)] = (0 until data.partitions.length).map { i =>\n+          (i, -1)}.toArray\n+        data.context.broadcast(emptyData)\n+      }\n+    val points = Matrices.fromRDD(data, bcMetaData.value, batchSize, buildSparseThreshold)\n+\n+    /**\n+     * For the first iteration, the regVal will be initialized as sum of weight squares\n+     * if it's L2 updater; for L1 updater, the same logic is followed.\n+     */\n+    val updaterWithIndex = updater.zipWithIndex\n+\n+    var regVal = updaterWithIndex.map { case (u, ind) =>\n+      u.compute(weights(ind), DenseMatrix.zeros(numFeatures, numModels),\n+        DenseMatrix.zeros(1, numModels), 1, regMatrix)._2\n+    }\n+    val orderedIters = numIterations.sorted\n+    var iterIndexCounter = 0\n+    for (i <- 1 to maxNumIter) {\n+      val bcWeights = data.context.broadcast(weights)\n+      // Sample a subset (fraction miniBatchFraction) of the total data\n+      // compute and sum up the subgradients on this subset (this is one map-reduce)\n+      val (gradientSum, lossSum) = points.sample(false, miniBatchFraction, 42 + i)\n+        .treeAggregate(updaterCounter.map(j => Matrices.zeros(numFeatures, numModels)),\n+          updaterCounter.map(j => Matrices.zeros(1, numModels)))(\n+          seqOp = (c, v) => (c, v) match { case ((grad, loss), (label, features)) =>\n+            val l = updaterCounter.map { j =>\n+              gradient.compute(features, label, bcWeights.value(j),\n+                grad(j).asInstanceOf[DenseMatrix])\n+            }\n+            (grad, loss.zip(l).map(r => r._1.elementWiseOperateInPlace(_ + _, r._2)))\n+          },\n+          combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1), (grad2, loss2)) =>\n+            (grad1.zip(grad2).map(r => r._1.elementWiseOperateInPlace(_ + _, r._2)),\n+              loss1.zip(loss2).map(r => r._1.elementWiseOperateInPlace(_ + _, r._2)))\n+          })\n+      stochasticLossHistory.append(Vectors.dense(Matrices.horzCat(updaterCounter.map { i =>\n+        lossSum(i).elementWiseOperateScalarInPlace(_ / _, miniBatchSize).\n+        elementWiseOperateOnRowsInPlace(_ + _, regVal(i))\n+      }).toArray))\n+      val update = updaterWithIndex.map { case (u, ind) => u.compute("
  }],
  "prId": 2451
}]