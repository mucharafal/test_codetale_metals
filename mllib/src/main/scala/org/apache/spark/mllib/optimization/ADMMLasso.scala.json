[{
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "add a space after `}`\n",
    "commit": "8df01660cf6606780eb8b723cdc4d7166ce5ac18",
    "createdAt": "2014-04-22T00:12:00Z",
    "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV, DenseMatrix => BDM, cholesky, norm}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.{Partitioner, HashPartitioner, Logging}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.annotation.DeveloperApi\n+\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Class used to solve the optimization problem in ADMMLasso\n+ */\n+@DeveloperApi\n+class ADMMLasso\n+  extends Optimizer with Logging\n+{\n+  private var numPartitions: Int = 10\n+  private var numIterations: Int = 100\n+  private var l1RegParam: Double = 1.0\n+  private var l2RegParam: Double = .0\n+  private var penalty: Double = 10.0\n+\n+\n+  /**\n+   * Set the number of partitions for ADMM. Default 10\n+   */\n+  def setNumPartitions(parts: Int): this.type = {\n+    this.numPartitions = parts\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for ADMM. Default 100.\n+   */\n+  def setNumIterations(iters: Int): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the l1-regularization parameter. Default 1.0.\n+   */\n+  def setL1RegParam(regParam: Double): this.type = {\n+    this.l1RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the l2-regularization parameter. Default .0\n+   */\n+  def setL2RegParam(regParam: Double): this.type = {\n+    this.l2RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the penalty parameter. Default 10.0\n+   */\n+  def setPenalty(penalty: Double): this.type = {\n+    this.penalty = penalty\n+    this\n+  }\n+\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Vector = {\n+    val (weights, _) = ADMMLasso.runADMM(data, numPartitions, numIterations, l1RegParam,\n+      l2RegParam, penalty, initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run ADMMLasso.\n+ */\n+@DeveloperApi\n+object ADMMLasso extends Logging {\n+\n+  /**\n+   * @param data  Input data for ADMMLasso. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param numPartitions  number of data blocks to partition the RDD into\n+   * @param numIterations  number of iterations that ADMM should be run.\n+   * @param l1RegParam  l1-regularization parameter\n+   * @param l2RegParam  l2-regularization parameter\n+   * @param penalty  The penalty parameter in ADMM\n+   * @param initialWeights  Initial set of weights to be used. Array should be equal in size to\n+   *        the number of features in the data.\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the loss\n+   *         computed for every iteration.\n+   */\n+  def runADMM(\n+      data: RDD[(Double, Vector)],\n+      numPartitions: Int,\n+      numIterations: Int,\n+      l1RegParam: Double,\n+      l2RegParam: Double,\n+      penalty: Double,\n+      initialWeights: Vector): (Vector, Array[Double]) = {\n+\n+    val lossHistory = new ArrayBuffer[Double](numIterations)\n+\n+    // Initialize weights as a column vector\n+    val p = initialWeights.size\n+\n+    // Consensus variable\n+    var z =  BDV.zeros[Double](p)\n+\n+    // Transform the input data into ADMM format\n+    def collectBlock(it: Iterator[(Vector, Double)]):\n+        Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))] = {\n+      val lab = new ArrayBuffer[Double]()\n+      val features = new ArrayBuffer[Double]()\n+      var row = 0\n+      it.foreach {case (f, l) =>\n+        lab += l\n+        features ++= f.toArray\n+        row += 1\n+      }\n+      val col = features.length/row\n+\n+      val designMatrix = new BDM(col, features.toArray).t\n+\n+      // Precompute the cholesky decomposition for solving linear system inside each partition\n+      val chol = if (row >= col) {\n+        cholesky((designMatrix.t * designMatrix) + (BDM.eye[Double](col) :* penalty))\n+      }\n+      else cholesky(((designMatrix * designMatrix.t) :/ penalty) + BDM.eye[Double](row))\n+\n+      Iterator(((BDV(lab.toArray), designMatrix, chol),\n+        (BDV(initialWeights.toArray), BDV.zeros[Double](col))))\n+    }\n+\n+    val partitionedData = data.map{case (x, y) => (y, x)}\n+      .partitionBy(new HashPartitioner(numPartitions)).cache()\n+\n+    // ((lab, design, chol), (x, u))\n+    var dividedData = partitionedData.mapPartitions(collectBlock, true)\n+\n+    var iter = 1\n+    var minorChange: Boolean = false\n+    while (iter <= numIterations && !minorChange) {\n+      val zBroadcast = z\n+      def localUpdate(\n+           it: Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))]):\n+           Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))] = {\n+        if (it.hasNext) {\n+          val localData = it.next()\n+          val (x, u) = localData._2\n+          val updatedU = u + ((x - zBroadcast) :* penalty)\n+          // Update local x by solving linear system Ax = q\n+          val (lab, design, chol) = localData._1\n+          val (row, col) = (design.rows, design.cols)\n+          val q = (design.t * lab) + (zBroadcast :* penalty) - u\n+\n+          val updatedX = if (row >= col) {\n+            chol.t \\ (chol \\ q)\n+          }else {"
  }],
  "prId": 458
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "It is better to put the `):` in the next line, with the returned type.\n",
    "commit": "8df01660cf6606780eb8b723cdc4d7166ce5ac18",
    "createdAt": "2014-04-22T00:13:05Z",
    "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV, DenseMatrix => BDM, cholesky, norm}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.{Partitioner, HashPartitioner, Logging}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.annotation.DeveloperApi\n+\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Class used to solve the optimization problem in ADMMLasso\n+ */\n+@DeveloperApi\n+class ADMMLasso\n+  extends Optimizer with Logging\n+{\n+  private var numPartitions: Int = 10\n+  private var numIterations: Int = 100\n+  private var l1RegParam: Double = 1.0\n+  private var l2RegParam: Double = .0\n+  private var penalty: Double = 10.0\n+\n+\n+  /**\n+   * Set the number of partitions for ADMM. Default 10\n+   */\n+  def setNumPartitions(parts: Int): this.type = {\n+    this.numPartitions = parts\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for ADMM. Default 100.\n+   */\n+  def setNumIterations(iters: Int): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the l1-regularization parameter. Default 1.0.\n+   */\n+  def setL1RegParam(regParam: Double): this.type = {\n+    this.l1RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the l2-regularization parameter. Default .0\n+   */\n+  def setL2RegParam(regParam: Double): this.type = {\n+    this.l2RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the penalty parameter. Default 10.0\n+   */\n+  def setPenalty(penalty: Double): this.type = {\n+    this.penalty = penalty\n+    this\n+  }\n+\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Vector = {\n+    val (weights, _) = ADMMLasso.runADMM(data, numPartitions, numIterations, l1RegParam,\n+      l2RegParam, penalty, initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run ADMMLasso.\n+ */\n+@DeveloperApi\n+object ADMMLasso extends Logging {\n+\n+  /**\n+   * @param data  Input data for ADMMLasso. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param numPartitions  number of data blocks to partition the RDD into\n+   * @param numIterations  number of iterations that ADMM should be run.\n+   * @param l1RegParam  l1-regularization parameter\n+   * @param l2RegParam  l2-regularization parameter\n+   * @param penalty  The penalty parameter in ADMM\n+   * @param initialWeights  Initial set of weights to be used. Array should be equal in size to\n+   *        the number of features in the data.\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the loss\n+   *         computed for every iteration.\n+   */\n+  def runADMM(\n+      data: RDD[(Double, Vector)],\n+      numPartitions: Int,\n+      numIterations: Int,\n+      l1RegParam: Double,\n+      l2RegParam: Double,\n+      penalty: Double,\n+      initialWeights: Vector): (Vector, Array[Double]) = {\n+\n+    val lossHistory = new ArrayBuffer[Double](numIterations)\n+\n+    // Initialize weights as a column vector\n+    val p = initialWeights.size\n+\n+    // Consensus variable\n+    var z =  BDV.zeros[Double](p)\n+\n+    // Transform the input data into ADMM format\n+    def collectBlock(it: Iterator[(Vector, Double)]):\n+        Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))] = {\n+      val lab = new ArrayBuffer[Double]()\n+      val features = new ArrayBuffer[Double]()\n+      var row = 0\n+      it.foreach {case (f, l) =>\n+        lab += l\n+        features ++= f.toArray\n+        row += 1\n+      }\n+      val col = features.length/row\n+\n+      val designMatrix = new BDM(col, features.toArray).t\n+\n+      // Precompute the cholesky decomposition for solving linear system inside each partition\n+      val chol = if (row >= col) {\n+        cholesky((designMatrix.t * designMatrix) + (BDM.eye[Double](col) :* penalty))\n+      }\n+      else cholesky(((designMatrix * designMatrix.t) :/ penalty) + BDM.eye[Double](row))\n+\n+      Iterator(((BDV(lab.toArray), designMatrix, chol),\n+        (BDV(initialWeights.toArray), BDV.zeros[Double](col))))\n+    }\n+\n+    val partitionedData = data.map{case (x, y) => (y, x)}\n+      .partitionBy(new HashPartitioner(numPartitions)).cache()\n+\n+    // ((lab, design, chol), (x, u))\n+    var dividedData = partitionedData.mapPartitions(collectBlock, true)\n+\n+    var iter = 1\n+    var minorChange: Boolean = false\n+    while (iter <= numIterations && !minorChange) {\n+      val zBroadcast = z\n+      def localUpdate(\n+           it: Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))]):"
  }],
  "prId": 458
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "remove extra line\n",
    "commit": "8df01660cf6606780eb8b723cdc4d7166ce5ac18",
    "createdAt": "2014-04-26T06:51:37Z",
    "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV, DenseMatrix => BDM, cholesky, norm}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.{Partitioner, HashPartitioner, Logging}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.annotation.DeveloperApi\n+",
    "line": 32
  }],
  "prId": 458
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "improper indent\n",
    "commit": "8df01660cf6606780eb8b723cdc4d7166ce5ac18",
    "createdAt": "2014-04-26T06:54:57Z",
    "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV, DenseMatrix => BDM, cholesky, norm}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.{Partitioner, HashPartitioner, Logging}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.annotation.DeveloperApi\n+\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Class used to solve the optimization problem in ADMMLasso\n+ */\n+@DeveloperApi\n+class ADMMLasso\n+  extends Optimizer with Logging\n+{\n+  private var numPartitions: Int = 10\n+  private var numIterations: Int = 100\n+  private var l1RegParam: Double = 1.0\n+  private var l2RegParam: Double = .0\n+  private var penalty: Double = 10.0\n+\n+\n+  /**\n+   * Set the number of partitions for ADMM. Default 10\n+   */\n+  def setNumPartitions(parts: Int): this.type = {\n+    this.numPartitions = parts\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for ADMM. Default 100.\n+   */\n+  def setNumIterations(iters: Int): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the l1-regularization parameter. Default 1.0.\n+   */\n+  def setL1RegParam(regParam: Double): this.type = {\n+    this.l1RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the l2-regularization parameter. Default .0\n+   */\n+  def setL2RegParam(regParam: Double): this.type = {\n+    this.l2RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the penalty parameter. Default 10.0\n+   */\n+  def setPenalty(penalty: Double): this.type = {\n+    this.penalty = penalty\n+    this\n+  }\n+\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Vector = {\n+    val (weights, _) = ADMMLasso.runADMM(data, numPartitions, numIterations, l1RegParam,\n+      l2RegParam, penalty, initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run ADMMLasso.\n+ */\n+@DeveloperApi\n+object ADMMLasso extends Logging {\n+\n+  /**\n+   * @param data  Input data for ADMMLasso. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param numPartitions  number of data blocks to partition the RDD into\n+   * @param numIterations  number of iterations that ADMM should be run.\n+   * @param l1RegParam  l1-regularization parameter\n+   * @param l2RegParam  l2-regularization parameter\n+   * @param penalty  The penalty parameter in ADMM\n+   * @param initialWeights  Initial set of weights to be used. Array should be equal in size to\n+   *        the number of features in the data.\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the loss\n+   *         computed for every iteration.\n+   */\n+  def runADMM(\n+      data: RDD[(Double, Vector)],\n+      numPartitions: Int,\n+      numIterations: Int,\n+      l1RegParam: Double,\n+      l2RegParam: Double,\n+      penalty: Double,\n+      initialWeights: Vector): (Vector, Array[Double]) = {\n+\n+    val lossHistory = new ArrayBuffer[Double](numIterations)\n+\n+    // Initialize weights as a column vector\n+    val p = initialWeights.size\n+\n+    // Consensus variable\n+    var z =  BDV.zeros[Double](p)\n+\n+    // Transform the input data into ADMM format\n+    def collectBlock(it: Iterator[(Vector, Double)]):\n+        Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))] = {\n+      val lab = new ArrayBuffer[Double]()\n+      val features = new ArrayBuffer[Double]()\n+      var row = 0\n+      it.foreach {case (f, l) =>\n+        lab += l\n+        features ++= f.toArray\n+        row += 1\n+      }\n+      val col = features.length/row\n+\n+      val designMatrix = new BDM(col, features.toArray).t\n+\n+      // Precompute the cholesky decomposition for solving linear system inside each partition\n+      val chol = if (row >= col) {\n+        cholesky((designMatrix.t * designMatrix) + (BDM.eye[Double](col) :* penalty))\n+      }\n+      else cholesky(((designMatrix * designMatrix.t) :/ penalty) + BDM.eye[Double](row))\n+\n+      Iterator(((BDV(lab.toArray), designMatrix, chol),\n+        (BDV(initialWeights.toArray), BDV.zeros[Double](col))))\n+    }\n+\n+    val partitionedData = data.map{case (x, y) => (y, x)}\n+      .partitionBy(new HashPartitioner(numPartitions)).cache()\n+\n+    // ((lab, design, chol), (x, u))\n+    var dividedData = partitionedData.mapPartitions(collectBlock, true)\n+\n+    var iter = 1\n+    var minorChange: Boolean = false\n+    while (iter <= numIterations && !minorChange) {\n+      val zBroadcast = z\n+      def localUpdate(\n+           it: Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))]"
  }],
  "prId": 458
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "same here\n",
    "commit": "8df01660cf6606780eb8b723cdc4d7166ce5ac18",
    "createdAt": "2014-04-26T06:55:06Z",
    "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV, DenseMatrix => BDM, cholesky, norm}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.{Partitioner, HashPartitioner, Logging}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.annotation.DeveloperApi\n+\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Class used to solve the optimization problem in ADMMLasso\n+ */\n+@DeveloperApi\n+class ADMMLasso\n+  extends Optimizer with Logging\n+{\n+  private var numPartitions: Int = 10\n+  private var numIterations: Int = 100\n+  private var l1RegParam: Double = 1.0\n+  private var l2RegParam: Double = .0\n+  private var penalty: Double = 10.0\n+\n+\n+  /**\n+   * Set the number of partitions for ADMM. Default 10\n+   */\n+  def setNumPartitions(parts: Int): this.type = {\n+    this.numPartitions = parts\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for ADMM. Default 100.\n+   */\n+  def setNumIterations(iters: Int): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the l1-regularization parameter. Default 1.0.\n+   */\n+  def setL1RegParam(regParam: Double): this.type = {\n+    this.l1RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the l2-regularization parameter. Default .0\n+   */\n+  def setL2RegParam(regParam: Double): this.type = {\n+    this.l2RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the penalty parameter. Default 10.0\n+   */\n+  def setPenalty(penalty: Double): this.type = {\n+    this.penalty = penalty\n+    this\n+  }\n+\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Vector = {\n+    val (weights, _) = ADMMLasso.runADMM(data, numPartitions, numIterations, l1RegParam,\n+      l2RegParam, penalty, initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run ADMMLasso.\n+ */\n+@DeveloperApi\n+object ADMMLasso extends Logging {\n+\n+  /**\n+   * @param data  Input data for ADMMLasso. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param numPartitions  number of data blocks to partition the RDD into\n+   * @param numIterations  number of iterations that ADMM should be run.\n+   * @param l1RegParam  l1-regularization parameter\n+   * @param l2RegParam  l2-regularization parameter\n+   * @param penalty  The penalty parameter in ADMM\n+   * @param initialWeights  Initial set of weights to be used. Array should be equal in size to\n+   *        the number of features in the data.\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the loss\n+   *         computed for every iteration.\n+   */\n+  def runADMM(\n+      data: RDD[(Double, Vector)],\n+      numPartitions: Int,\n+      numIterations: Int,\n+      l1RegParam: Double,\n+      l2RegParam: Double,\n+      penalty: Double,\n+      initialWeights: Vector): (Vector, Array[Double]) = {\n+\n+    val lossHistory = new ArrayBuffer[Double](numIterations)\n+\n+    // Initialize weights as a column vector\n+    val p = initialWeights.size\n+\n+    // Consensus variable\n+    var z =  BDV.zeros[Double](p)\n+\n+    // Transform the input data into ADMM format\n+    def collectBlock(it: Iterator[(Vector, Double)]):\n+        Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))] = {\n+      val lab = new ArrayBuffer[Double]()\n+      val features = new ArrayBuffer[Double]()\n+      var row = 0\n+      it.foreach {case (f, l) =>\n+        lab += l\n+        features ++= f.toArray\n+        row += 1\n+      }\n+      val col = features.length/row\n+\n+      val designMatrix = new BDM(col, features.toArray).t\n+\n+      // Precompute the cholesky decomposition for solving linear system inside each partition\n+      val chol = if (row >= col) {\n+        cholesky((designMatrix.t * designMatrix) + (BDM.eye[Double](col) :* penalty))\n+      }\n+      else cholesky(((designMatrix * designMatrix.t) :/ penalty) + BDM.eye[Double](row))\n+\n+      Iterator(((BDV(lab.toArray), designMatrix, chol),\n+        (BDV(initialWeights.toArray), BDV.zeros[Double](col))))\n+    }\n+\n+    val partitionedData = data.map{case (x, y) => (y, x)}\n+      .partitionBy(new HashPartitioner(numPartitions)).cache()\n+\n+    // ((lab, design, chol), (x, u))\n+    var dividedData = partitionedData.mapPartitions(collectBlock, true)\n+\n+    var iter = 1\n+    var minorChange: Boolean = false\n+    while (iter <= numIterations && !minorChange) {\n+      val zBroadcast = z\n+      def localUpdate(\n+           it: Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))]\n+           ):Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))] = {"
  }],
  "prId": 458
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "How about moving this function definition out of while loop?\n",
    "commit": "8df01660cf6606780eb8b723cdc4d7166ce5ac18",
    "createdAt": "2014-04-26T06:56:38Z",
    "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV, DenseMatrix => BDM, cholesky, norm}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.{Partitioner, HashPartitioner, Logging}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.annotation.DeveloperApi\n+\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Class used to solve the optimization problem in ADMMLasso\n+ */\n+@DeveloperApi\n+class ADMMLasso\n+  extends Optimizer with Logging\n+{\n+  private var numPartitions: Int = 10\n+  private var numIterations: Int = 100\n+  private var l1RegParam: Double = 1.0\n+  private var l2RegParam: Double = .0\n+  private var penalty: Double = 10.0\n+\n+\n+  /**\n+   * Set the number of partitions for ADMM. Default 10\n+   */\n+  def setNumPartitions(parts: Int): this.type = {\n+    this.numPartitions = parts\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for ADMM. Default 100.\n+   */\n+  def setNumIterations(iters: Int): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the l1-regularization parameter. Default 1.0.\n+   */\n+  def setL1RegParam(regParam: Double): this.type = {\n+    this.l1RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the l2-regularization parameter. Default .0\n+   */\n+  def setL2RegParam(regParam: Double): this.type = {\n+    this.l2RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the penalty parameter. Default 10.0\n+   */\n+  def setPenalty(penalty: Double): this.type = {\n+    this.penalty = penalty\n+    this\n+  }\n+\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Vector = {\n+    val (weights, _) = ADMMLasso.runADMM(data, numPartitions, numIterations, l1RegParam,\n+      l2RegParam, penalty, initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run ADMMLasso.\n+ */\n+@DeveloperApi\n+object ADMMLasso extends Logging {\n+\n+  /**\n+   * @param data  Input data for ADMMLasso. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param numPartitions  number of data blocks to partition the RDD into\n+   * @param numIterations  number of iterations that ADMM should be run.\n+   * @param l1RegParam  l1-regularization parameter\n+   * @param l2RegParam  l2-regularization parameter\n+   * @param penalty  The penalty parameter in ADMM\n+   * @param initialWeights  Initial set of weights to be used. Array should be equal in size to\n+   *        the number of features in the data.\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the loss\n+   *         computed for every iteration.\n+   */\n+  def runADMM(\n+      data: RDD[(Double, Vector)],\n+      numPartitions: Int,\n+      numIterations: Int,\n+      l1RegParam: Double,\n+      l2RegParam: Double,\n+      penalty: Double,\n+      initialWeights: Vector): (Vector, Array[Double]) = {\n+\n+    val lossHistory = new ArrayBuffer[Double](numIterations)\n+\n+    // Initialize weights as a column vector\n+    val p = initialWeights.size\n+\n+    // Consensus variable\n+    var z =  BDV.zeros[Double](p)\n+\n+    // Transform the input data into ADMM format\n+    def collectBlock(it: Iterator[(Vector, Double)]):\n+        Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))] = {\n+      val lab = new ArrayBuffer[Double]()\n+      val features = new ArrayBuffer[Double]()\n+      var row = 0\n+      it.foreach {case (f, l) =>\n+        lab += l\n+        features ++= f.toArray\n+        row += 1\n+      }\n+      val col = features.length/row\n+\n+      val designMatrix = new BDM(col, features.toArray).t\n+\n+      // Precompute the cholesky decomposition for solving linear system inside each partition\n+      val chol = if (row >= col) {\n+        cholesky((designMatrix.t * designMatrix) + (BDM.eye[Double](col) :* penalty))\n+      }\n+      else cholesky(((designMatrix * designMatrix.t) :/ penalty) + BDM.eye[Double](row))\n+\n+      Iterator(((BDV(lab.toArray), designMatrix, chol),\n+        (BDV(initialWeights.toArray), BDV.zeros[Double](col))))\n+    }\n+\n+    val partitionedData = data.map{case (x, y) => (y, x)}\n+      .partitionBy(new HashPartitioner(numPartitions)).cache()\n+\n+    // ((lab, design, chol), (x, u))\n+    var dividedData = partitionedData.mapPartitions(collectBlock, true)\n+\n+    var iter = 1\n+    var minorChange: Boolean = false\n+    while (iter <= numIterations && !minorChange) {\n+      val zBroadcast = z\n+      def localUpdate(",
    "line": 168
  }],
  "prId": 458
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "`localData._1` is not modified in the function. How about using broadcast instead of variable serialization? The iterative useless ser/des slows down the procedure, especially when the local design matrix is too large.\n",
    "commit": "8df01660cf6606780eb8b723cdc4d7166ce5ac18",
    "createdAt": "2014-04-26T06:59:33Z",
    "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV, DenseMatrix => BDM, cholesky, norm}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.{Partitioner, HashPartitioner, Logging}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.annotation.DeveloperApi\n+\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Class used to solve the optimization problem in ADMMLasso\n+ */\n+@DeveloperApi\n+class ADMMLasso\n+  extends Optimizer with Logging\n+{\n+  private var numPartitions: Int = 10\n+  private var numIterations: Int = 100\n+  private var l1RegParam: Double = 1.0\n+  private var l2RegParam: Double = .0\n+  private var penalty: Double = 10.0\n+\n+\n+  /**\n+   * Set the number of partitions for ADMM. Default 10\n+   */\n+  def setNumPartitions(parts: Int): this.type = {\n+    this.numPartitions = parts\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for ADMM. Default 100.\n+   */\n+  def setNumIterations(iters: Int): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the l1-regularization parameter. Default 1.0.\n+   */\n+  def setL1RegParam(regParam: Double): this.type = {\n+    this.l1RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the l2-regularization parameter. Default .0\n+   */\n+  def setL2RegParam(regParam: Double): this.type = {\n+    this.l2RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the penalty parameter. Default 10.0\n+   */\n+  def setPenalty(penalty: Double): this.type = {\n+    this.penalty = penalty\n+    this\n+  }\n+\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Vector = {\n+    val (weights, _) = ADMMLasso.runADMM(data, numPartitions, numIterations, l1RegParam,\n+      l2RegParam, penalty, initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run ADMMLasso.\n+ */\n+@DeveloperApi\n+object ADMMLasso extends Logging {\n+\n+  /**\n+   * @param data  Input data for ADMMLasso. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param numPartitions  number of data blocks to partition the RDD into\n+   * @param numIterations  number of iterations that ADMM should be run.\n+   * @param l1RegParam  l1-regularization parameter\n+   * @param l2RegParam  l2-regularization parameter\n+   * @param penalty  The penalty parameter in ADMM\n+   * @param initialWeights  Initial set of weights to be used. Array should be equal in size to\n+   *        the number of features in the data.\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the loss\n+   *         computed for every iteration.\n+   */\n+  def runADMM(\n+      data: RDD[(Double, Vector)],\n+      numPartitions: Int,\n+      numIterations: Int,\n+      l1RegParam: Double,\n+      l2RegParam: Double,\n+      penalty: Double,\n+      initialWeights: Vector): (Vector, Array[Double]) = {\n+\n+    val lossHistory = new ArrayBuffer[Double](numIterations)\n+\n+    // Initialize weights as a column vector\n+    val p = initialWeights.size\n+\n+    // Consensus variable\n+    var z =  BDV.zeros[Double](p)\n+\n+    // Transform the input data into ADMM format\n+    def collectBlock(it: Iterator[(Vector, Double)]):\n+        Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))] = {\n+      val lab = new ArrayBuffer[Double]()\n+      val features = new ArrayBuffer[Double]()\n+      var row = 0\n+      it.foreach {case (f, l) =>\n+        lab += l\n+        features ++= f.toArray\n+        row += 1\n+      }\n+      val col = features.length/row\n+\n+      val designMatrix = new BDM(col, features.toArray).t\n+\n+      // Precompute the cholesky decomposition for solving linear system inside each partition\n+      val chol = if (row >= col) {\n+        cholesky((designMatrix.t * designMatrix) + (BDM.eye[Double](col) :* penalty))\n+      }\n+      else cholesky(((designMatrix * designMatrix.t) :/ penalty) + BDM.eye[Double](row))\n+\n+      Iterator(((BDV(lab.toArray), designMatrix, chol),\n+        (BDV(initialWeights.toArray), BDV.zeros[Double](col))))\n+    }\n+\n+    val partitionedData = data.map{case (x, y) => (y, x)}\n+      .partitionBy(new HashPartitioner(numPartitions)).cache()\n+\n+    // ((lab, design, chol), (x, u))\n+    var dividedData = partitionedData.mapPartitions(collectBlock, true)\n+\n+    var iter = 1\n+    var minorChange: Boolean = false\n+    while (iter <= numIterations && !minorChange) {\n+      val zBroadcast = z\n+      def localUpdate(\n+           it: Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))]\n+           ):Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))] = {\n+        if (it.hasNext) {\n+          val localData = it.next()\n+          val (x, u) = localData._2\n+          val updatedU = u + ((x - zBroadcast) :* penalty)\n+          // Update local x by solving linear system Ax = q\n+          val (lab, design, chol) = localData._1\n+          val (row, col) = (design.rows, design.cols)\n+          val q = (design.t * lab) + (zBroadcast :* penalty) - u\n+\n+          val updatedX = if (row >= col) {\n+            chol.t \\ (chol \\ q)\n+          } else {\n+            (q :/ penalty) - ((design.t *(chol.t\\(chol\\(design * q)))) :/ (penalty * penalty))\n+          }\n+          Iterator((localData._1, (updatedX, updatedU)))"
  }],
  "prId": 458
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "Using broadcast instead of ser/des, then the `cache()` is saved, since you will use `reduce` to aggregate `last` and `zSum` back to driver.\n",
    "commit": "8df01660cf6606780eb8b723cdc4d7166ce5ac18",
    "createdAt": "2014-04-26T07:02:26Z",
    "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV, DenseMatrix => BDM, cholesky, norm}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.{Partitioner, HashPartitioner, Logging}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.annotation.DeveloperApi\n+\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Class used to solve the optimization problem in ADMMLasso\n+ */\n+@DeveloperApi\n+class ADMMLasso\n+  extends Optimizer with Logging\n+{\n+  private var numPartitions: Int = 10\n+  private var numIterations: Int = 100\n+  private var l1RegParam: Double = 1.0\n+  private var l2RegParam: Double = .0\n+  private var penalty: Double = 10.0\n+\n+\n+  /**\n+   * Set the number of partitions for ADMM. Default 10\n+   */\n+  def setNumPartitions(parts: Int): this.type = {\n+    this.numPartitions = parts\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for ADMM. Default 100.\n+   */\n+  def setNumIterations(iters: Int): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the l1-regularization parameter. Default 1.0.\n+   */\n+  def setL1RegParam(regParam: Double): this.type = {\n+    this.l1RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the l2-regularization parameter. Default .0\n+   */\n+  def setL2RegParam(regParam: Double): this.type = {\n+    this.l2RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the penalty parameter. Default 10.0\n+   */\n+  def setPenalty(penalty: Double): this.type = {\n+    this.penalty = penalty\n+    this\n+  }\n+\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Vector = {\n+    val (weights, _) = ADMMLasso.runADMM(data, numPartitions, numIterations, l1RegParam,\n+      l2RegParam, penalty, initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run ADMMLasso.\n+ */\n+@DeveloperApi\n+object ADMMLasso extends Logging {\n+\n+  /**\n+   * @param data  Input data for ADMMLasso. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param numPartitions  number of data blocks to partition the RDD into\n+   * @param numIterations  number of iterations that ADMM should be run.\n+   * @param l1RegParam  l1-regularization parameter\n+   * @param l2RegParam  l2-regularization parameter\n+   * @param penalty  The penalty parameter in ADMM\n+   * @param initialWeights  Initial set of weights to be used. Array should be equal in size to\n+   *        the number of features in the data.\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the loss\n+   *         computed for every iteration.\n+   */\n+  def runADMM(\n+      data: RDD[(Double, Vector)],\n+      numPartitions: Int,\n+      numIterations: Int,\n+      l1RegParam: Double,\n+      l2RegParam: Double,\n+      penalty: Double,\n+      initialWeights: Vector): (Vector, Array[Double]) = {\n+\n+    val lossHistory = new ArrayBuffer[Double](numIterations)\n+\n+    // Initialize weights as a column vector\n+    val p = initialWeights.size\n+\n+    // Consensus variable\n+    var z =  BDV.zeros[Double](p)\n+\n+    // Transform the input data into ADMM format\n+    def collectBlock(it: Iterator[(Vector, Double)]):\n+        Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))] = {\n+      val lab = new ArrayBuffer[Double]()\n+      val features = new ArrayBuffer[Double]()\n+      var row = 0\n+      it.foreach {case (f, l) =>\n+        lab += l\n+        features ++= f.toArray\n+        row += 1\n+      }\n+      val col = features.length/row\n+\n+      val designMatrix = new BDM(col, features.toArray).t\n+\n+      // Precompute the cholesky decomposition for solving linear system inside each partition\n+      val chol = if (row >= col) {\n+        cholesky((designMatrix.t * designMatrix) + (BDM.eye[Double](col) :* penalty))\n+      }\n+      else cholesky(((designMatrix * designMatrix.t) :/ penalty) + BDM.eye[Double](row))\n+\n+      Iterator(((BDV(lab.toArray), designMatrix, chol),\n+        (BDV(initialWeights.toArray), BDV.zeros[Double](col))))\n+    }\n+\n+    val partitionedData = data.map{case (x, y) => (y, x)}\n+      .partitionBy(new HashPartitioner(numPartitions)).cache()\n+\n+    // ((lab, design, chol), (x, u))\n+    var dividedData = partitionedData.mapPartitions(collectBlock, true)\n+\n+    var iter = 1\n+    var minorChange: Boolean = false\n+    while (iter <= numIterations && !minorChange) {\n+      val zBroadcast = z\n+      def localUpdate(\n+           it: Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))]\n+           ):Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))] = {\n+        if (it.hasNext) {\n+          val localData = it.next()\n+          val (x, u) = localData._2\n+          val updatedU = u + ((x - zBroadcast) :* penalty)\n+          // Update local x by solving linear system Ax = q\n+          val (lab, design, chol) = localData._1\n+          val (row, col) = (design.rows, design.cols)\n+          val q = (design.t * lab) + (zBroadcast :* penalty) - u\n+\n+          val updatedX = if (row >= col) {\n+            chol.t \\ (chol \\ q)\n+          } else {\n+            (q :/ penalty) - ((design.t *(chol.t\\(chol\\(design * q)))) :/ (penalty * penalty))\n+          }\n+          Iterator((localData._1, (updatedX, updatedU)))\n+        }\n+        else {\n+          it\n+        }\n+      }\n+      dividedData = dividedData.mapPartitions(localUpdate).cache()",
    "line": 203
  }],
  "prId": 458
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "remove extra line.\n",
    "commit": "8df01660cf6606780eb8b723cdc4d7166ce5ac18",
    "createdAt": "2014-04-26T07:04:22Z",
    "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV, DenseMatrix => BDM, cholesky, norm}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.{Partitioner, HashPartitioner, Logging}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.annotation.DeveloperApi\n+\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Class used to solve the optimization problem in ADMMLasso\n+ */\n+@DeveloperApi\n+class ADMMLasso\n+  extends Optimizer with Logging\n+{\n+  private var numPartitions: Int = 10\n+  private var numIterations: Int = 100\n+  private var l1RegParam: Double = 1.0\n+  private var l2RegParam: Double = .0\n+  private var penalty: Double = 10.0\n+",
    "line": 46
  }],
  "prId": 458
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "Do you mean real broadcast variable in Spark? If not, the `zBroadcast` could be removed, since `=` here is a shallow copy.\n",
    "commit": "8df01660cf6606780eb8b723cdc4d7166ce5ac18",
    "createdAt": "2014-04-26T07:13:28Z",
    "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV, DenseMatrix => BDM, cholesky, norm}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.{Partitioner, HashPartitioner, Logging}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.annotation.DeveloperApi\n+\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Class used to solve the optimization problem in ADMMLasso\n+ */\n+@DeveloperApi\n+class ADMMLasso\n+  extends Optimizer with Logging\n+{\n+  private var numPartitions: Int = 10\n+  private var numIterations: Int = 100\n+  private var l1RegParam: Double = 1.0\n+  private var l2RegParam: Double = .0\n+  private var penalty: Double = 10.0\n+\n+\n+  /**\n+   * Set the number of partitions for ADMM. Default 10\n+   */\n+  def setNumPartitions(parts: Int): this.type = {\n+    this.numPartitions = parts\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for ADMM. Default 100.\n+   */\n+  def setNumIterations(iters: Int): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the l1-regularization parameter. Default 1.0.\n+   */\n+  def setL1RegParam(regParam: Double): this.type = {\n+    this.l1RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the l2-regularization parameter. Default .0\n+   */\n+  def setL2RegParam(regParam: Double): this.type = {\n+    this.l2RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the penalty parameter. Default 10.0\n+   */\n+  def setPenalty(penalty: Double): this.type = {\n+    this.penalty = penalty\n+    this\n+  }\n+\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Vector = {\n+    val (weights, _) = ADMMLasso.runADMM(data, numPartitions, numIterations, l1RegParam,\n+      l2RegParam, penalty, initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run ADMMLasso.\n+ */\n+@DeveloperApi\n+object ADMMLasso extends Logging {\n+\n+  /**\n+   * @param data  Input data for ADMMLasso. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param numPartitions  number of data blocks to partition the RDD into\n+   * @param numIterations  number of iterations that ADMM should be run.\n+   * @param l1RegParam  l1-regularization parameter\n+   * @param l2RegParam  l2-regularization parameter\n+   * @param penalty  The penalty parameter in ADMM\n+   * @param initialWeights  Initial set of weights to be used. Array should be equal in size to\n+   *        the number of features in the data.\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the loss\n+   *         computed for every iteration.\n+   */\n+  def runADMM(\n+      data: RDD[(Double, Vector)],\n+      numPartitions: Int,\n+      numIterations: Int,\n+      l1RegParam: Double,\n+      l2RegParam: Double,\n+      penalty: Double,\n+      initialWeights: Vector): (Vector, Array[Double]) = {\n+\n+    val lossHistory = new ArrayBuffer[Double](numIterations)\n+\n+    // Initialize weights as a column vector\n+    val p = initialWeights.size\n+\n+    // Consensus variable\n+    var z =  BDV.zeros[Double](p)\n+\n+    // Transform the input data into ADMM format\n+    def collectBlock(it: Iterator[(Vector, Double)]):\n+        Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))] = {\n+      val lab = new ArrayBuffer[Double]()\n+      val features = new ArrayBuffer[Double]()\n+      var row = 0\n+      it.foreach {case (f, l) =>\n+        lab += l\n+        features ++= f.toArray\n+        row += 1\n+      }\n+      val col = features.length/row\n+\n+      val designMatrix = new BDM(col, features.toArray).t\n+\n+      // Precompute the cholesky decomposition for solving linear system inside each partition\n+      val chol = if (row >= col) {\n+        cholesky((designMatrix.t * designMatrix) + (BDM.eye[Double](col) :* penalty))\n+      }\n+      else cholesky(((designMatrix * designMatrix.t) :/ penalty) + BDM.eye[Double](row))\n+\n+      Iterator(((BDV(lab.toArray), designMatrix, chol),\n+        (BDV(initialWeights.toArray), BDV.zeros[Double](col))))\n+    }\n+\n+    val partitionedData = data.map{case (x, y) => (y, x)}\n+      .partitionBy(new HashPartitioner(numPartitions)).cache()\n+\n+    // ((lab, design, chol), (x, u))\n+    var dividedData = partitionedData.mapPartitions(collectBlock, true)\n+\n+    var iter = 1\n+    var minorChange: Boolean = false\n+    while (iter <= numIterations && !minorChange) {\n+      val zBroadcast = z",
    "line": 167
  }],
  "prId": 458
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "Be careful with `reduce`, it will throw exception if some of the partitions are empty. Using `aggregate` is better.\n",
    "commit": "8df01660cf6606780eb8b723cdc4d7166ce5ac18",
    "createdAt": "2014-04-26T07:15:27Z",
    "diffHunk": "@@ -0,0 +1,217 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV, DenseMatrix => BDM, cholesky, norm}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.{Partitioner, HashPartitioner, Logging}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.annotation.DeveloperApi\n+\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Class used to solve the optimization problem in ADMMLasso\n+ */\n+@DeveloperApi\n+class ADMMLasso\n+  extends Optimizer with Logging\n+{\n+  private var numPartitions: Int = 10\n+  private var numIterations: Int = 100\n+  private var l1RegParam: Double = 1.0\n+  private var l2RegParam: Double = .0\n+  private var penalty: Double = 10.0\n+\n+\n+  /**\n+   * Set the number of partitions for ADMM. Default 10\n+   */\n+  def setNumPartitions(parts: Int): this.type = {\n+    this.numPartitions = parts\n+    this\n+  }\n+\n+  /**\n+   * Set the number of iterations for ADMM. Default 100.\n+   */\n+  def setNumIterations(iters: Int): this.type = {\n+    this.numIterations = iters\n+    this\n+  }\n+\n+  /**\n+   * Set the l1-regularization parameter. Default 1.0.\n+   */\n+  def setL1RegParam(regParam: Double): this.type = {\n+    this.l1RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the l2-regularization parameter. Default .0\n+   */\n+  def setL2RegParam(regParam: Double): this.type = {\n+    this.l2RegParam = regParam\n+    this\n+  }\n+\n+  /**\n+   * Set the penalty parameter. Default 10.0\n+   */\n+  def setPenalty(penalty: Double): this.type = {\n+    this.penalty = penalty\n+    this\n+  }\n+\n+  def optimize(data: RDD[(Double, Vector)], initialWeights: Vector): Vector = {\n+    val (weights, _) = ADMMLasso.runADMM(data, numPartitions, numIterations, l1RegParam,\n+      l2RegParam, penalty, initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Top-level method to run ADMMLasso.\n+ */\n+@DeveloperApi\n+object ADMMLasso extends Logging {\n+\n+  /**\n+   * @param data  Input data for ADMMLasso. RDD of the set of data examples, each of\n+   *               the form (label, [feature values]).\n+   * @param numPartitions  number of data blocks to partition the RDD into\n+   * @param numIterations  number of iterations that ADMM should be run.\n+   * @param l1RegParam  l1-regularization parameter\n+   * @param l2RegParam  l2-regularization parameter\n+   * @param penalty  The penalty parameter in ADMM\n+   * @param initialWeights  Initial set of weights to be used. Array should be equal in size to\n+   *        the number of features in the data.\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the loss\n+   *         computed for every iteration.\n+   */\n+  def runADMM(\n+      data: RDD[(Double, Vector)],\n+      numPartitions: Int,\n+      numIterations: Int,\n+      l1RegParam: Double,\n+      l2RegParam: Double,\n+      penalty: Double,\n+      initialWeights: Vector): (Vector, Array[Double]) = {\n+\n+    val lossHistory = new ArrayBuffer[Double](numIterations)\n+\n+    // Initialize weights as a column vector\n+    val p = initialWeights.size\n+\n+    // Consensus variable\n+    var z =  BDV.zeros[Double](p)\n+\n+    // Transform the input data into ADMM format\n+    def collectBlock(it: Iterator[(Vector, Double)]):\n+        Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))] = {\n+      val lab = new ArrayBuffer[Double]()\n+      val features = new ArrayBuffer[Double]()\n+      var row = 0\n+      it.foreach {case (f, l) =>\n+        lab += l\n+        features ++= f.toArray\n+        row += 1\n+      }\n+      val col = features.length/row\n+\n+      val designMatrix = new BDM(col, features.toArray).t\n+\n+      // Precompute the cholesky decomposition for solving linear system inside each partition\n+      val chol = if (row >= col) {\n+        cholesky((designMatrix.t * designMatrix) + (BDM.eye[Double](col) :* penalty))\n+      }\n+      else cholesky(((designMatrix * designMatrix.t) :/ penalty) + BDM.eye[Double](row))\n+\n+      Iterator(((BDV(lab.toArray), designMatrix, chol),\n+        (BDV(initialWeights.toArray), BDV.zeros[Double](col))))\n+    }\n+\n+    val partitionedData = data.map{case (x, y) => (y, x)}\n+      .partitionBy(new HashPartitioner(numPartitions)).cache()\n+\n+    // ((lab, design, chol), (x, u))\n+    var dividedData = partitionedData.mapPartitions(collectBlock, true)\n+\n+    var iter = 1\n+    var minorChange: Boolean = false\n+    while (iter <= numIterations && !minorChange) {\n+      val zBroadcast = z\n+      def localUpdate(\n+           it: Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))]\n+           ):Iterator[((BDV[Double], BDM[Double], BDM[Double]), (BDV[Double], BDV[Double]))] = {\n+        if (it.hasNext) {\n+          val localData = it.next()\n+          val (x, u) = localData._2\n+          val updatedU = u + ((x - zBroadcast) :* penalty)\n+          // Update local x by solving linear system Ax = q\n+          val (lab, design, chol) = localData._1\n+          val (row, col) = (design.rows, design.cols)\n+          val q = (design.t * lab) + (zBroadcast :* penalty) - u\n+\n+          val updatedX = if (row >= col) {\n+            chol.t \\ (chol \\ q)\n+          } else {\n+            (q :/ penalty) - ((design.t *(chol.t\\(chol\\(design * q)))) :/ (penalty * penalty))\n+          }\n+          Iterator((localData._1, (updatedX, updatedU)))\n+        }\n+        else {\n+          it\n+        }\n+      }\n+      dividedData = dividedData.mapPartitions(localUpdate).cache()\n+      val (last, zSum) = dividedData.map{case (u, v) =>\n+        val (lab, design, chol) = u\n+        val residual = design * zBroadcast - lab\n+        (0.5 * residual.dot(residual), (v._2 :/ penalty) + v._1)\n+      }.reduce{case (x, y) => (x._1 + y._1, x._2 + y._2)}",
    "line": 208
  }],
  "prId": 458
}]