[{
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "I understand this must be >1 for `idx` to not be filtered, but I'm not too clear on what the `if (..) 0 else 1` is doing. Can you add some comments to describe your logic.\n",
    "commit": "3a34b56392a549708b9655b2f6ec68d49d7ecc9d",
    "createdAt": "2015-07-03T04:49:09Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{DenseVector, SparseVector, Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ * Model to extract significant indices from vector.\n+ *\n+ * Significant indices is vector's index that has different value for different vectors.\n+ *\n+ * For example, when you use HashingTF they create big sparse vector,\n+ * and this code convert to smallest vector that don't include same values indices for all vectors.\n+ *\n+ * @param indices array of significant indices.\n+ */\n+@Experimental\n+class SignificantSelectorModel(val indices: Array[Int]) extends VectorTransformer {\n+\n+  /**\n+   * Applies transformation on a vector.\n+   *\n+   * @param vector vector to be transformed.\n+   * @return transformed vector.\n+   */\n+  override def transform(vector: Vector): Vector = vector match {\n+    case DenseVector(vs) =>\n+      Vectors.dense(indices.map(vs))\n+\n+    case SparseVector(s, ids, vs) =>\n+      var sv_idx = 0\n+      var new_idx = 0\n+      val elements = new mutable.ListBuffer[(Int, Double)]()\n+      \n+      for (idx <- indices) {\n+        while (sv_idx < ids.length && ids(sv_idx) < idx) {\n+          sv_idx += 1\n+        }\n+        if (sv_idx < ids.length && ids(sv_idx) == idx) {\n+          elements += ((new_idx, vs(sv_idx)))\n+          sv_idx += 1\n+        }\n+        new_idx += 1\n+      }\n+      \n+      Vectors.sparse(indices.length, elements)\n+\n+    case v =>\n+      throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Specialized model for equivalent vectors\n+ */\n+@Experimental\n+class SignificantSelectorEmptyModel extends SignificantSelectorModel(Array[Int]()) {\n+  \n+  val empty_vector = Vectors.dense(Array[Double]())\n+  \n+  override def transform(vector: Vector): Vector = empty_vector\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Create Significant selector.\n+ */\n+@Experimental\n+class SignificantSelector() {\n+\n+  /**\n+   * Returns a significant vector indices selector.\n+   *\n+   * @param sources an `RDD[Vector]` containing the vectors.\n+   */\n+  def fit(sources: RDD[Vector]): SignificantSelectorModel = {\n+    val sources_count = sources.count()\n+    val significant_indices = sources.flatMap {\n+        case DenseVector(vs) =>\n+          vs.zipWithIndex\n+        case SparseVector(_, ids, vs) =>\n+          vs.zip(ids)\n+        case v =>\n+          throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+      }\n+      .map(e => (e.swap, 1))\n+      .reduceByKey(_ + _)\n+      .map { case ((idx, value), count) => (idx, (value, count))}\n+      .groupByKey()\n+      .mapValues { e =>\n+        val values = e.groupBy(_._1)\n+        val sum = e.map(_._2).sum\n+\n+        values.size + (if (sum == sources_count || values.contains(0.0)) 0 else 1)",
    "line": 115
  }, {
    "author": {
      "login": "catap"
    },
    "body": "Oh, this is hack for case when you has RDD what include dense and sparse vector.\n\nSparse vector hasn't got zero elements (`0d`) and `values.size` for sparse vector has count only for different non zero value.\n\nIf you have in RDD sparse and dense vector and last one has zero element where sparse vectro hasn't got element significant understand it's different values but isn't it.\n\nFor example, let's see following code:\n\n``` scala\n    val vectors = sc.parallelize(List(\n      Vectors.dense(0.0, 2.0, 3.0, 4.0),\n      Vectors.dense(0.0, 2.0, 3.0, 4.0),\n      Vectors.dense(0.0, 2.0, 3.0, 4.0),\n      Vectors.sparse(4, Seq((1, 3.0), (2, 4.0))),\n      Vectors.dense(0.0, 3.0, 5.0, 4.0),\n      Vectors.dense(0.0, 3.0, 7.0, 4.0)\n    ))\n```\n\nfirst element of each vector is zero for dense and empty for sparse. With out this hack significant induces will have first element, because they has different values (zero and empty), but if you convert sparse vector to dense vector significant induces hasn't got first element.\n\nIt is clear now? \n",
    "commit": "3a34b56392a549708b9655b2f6ec68d49d7ecc9d",
    "createdAt": "2015-07-03T05:27:59Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{DenseVector, SparseVector, Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ * Model to extract significant indices from vector.\n+ *\n+ * Significant indices is vector's index that has different value for different vectors.\n+ *\n+ * For example, when you use HashingTF they create big sparse vector,\n+ * and this code convert to smallest vector that don't include same values indices for all vectors.\n+ *\n+ * @param indices array of significant indices.\n+ */\n+@Experimental\n+class SignificantSelectorModel(val indices: Array[Int]) extends VectorTransformer {\n+\n+  /**\n+   * Applies transformation on a vector.\n+   *\n+   * @param vector vector to be transformed.\n+   * @return transformed vector.\n+   */\n+  override def transform(vector: Vector): Vector = vector match {\n+    case DenseVector(vs) =>\n+      Vectors.dense(indices.map(vs))\n+\n+    case SparseVector(s, ids, vs) =>\n+      var sv_idx = 0\n+      var new_idx = 0\n+      val elements = new mutable.ListBuffer[(Int, Double)]()\n+      \n+      for (idx <- indices) {\n+        while (sv_idx < ids.length && ids(sv_idx) < idx) {\n+          sv_idx += 1\n+        }\n+        if (sv_idx < ids.length && ids(sv_idx) == idx) {\n+          elements += ((new_idx, vs(sv_idx)))\n+          sv_idx += 1\n+        }\n+        new_idx += 1\n+      }\n+      \n+      Vectors.sparse(indices.length, elements)\n+\n+    case v =>\n+      throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Specialized model for equivalent vectors\n+ */\n+@Experimental\n+class SignificantSelectorEmptyModel extends SignificantSelectorModel(Array[Int]()) {\n+  \n+  val empty_vector = Vectors.dense(Array[Double]())\n+  \n+  override def transform(vector: Vector): Vector = empty_vector\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Create Significant selector.\n+ */\n+@Experimental\n+class SignificantSelector() {\n+\n+  /**\n+   * Returns a significant vector indices selector.\n+   *\n+   * @param sources an `RDD[Vector]` containing the vectors.\n+   */\n+  def fit(sources: RDD[Vector]): SignificantSelectorModel = {\n+    val sources_count = sources.count()\n+    val significant_indices = sources.flatMap {\n+        case DenseVector(vs) =>\n+          vs.zipWithIndex\n+        case SparseVector(_, ids, vs) =>\n+          vs.zip(ids)\n+        case v =>\n+          throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+      }\n+      .map(e => (e.swap, 1))\n+      .reduceByKey(_ + _)\n+      .map { case ((idx, value), count) => (idx, (value, count))}\n+      .groupByKey()\n+      .mapValues { e =>\n+        val values = e.groupBy(_._1)\n+        val sum = e.map(_._2).sum\n+\n+        values.size + (if (sum == sources_count || values.contains(0.0)) 0 else 1)",
    "line": 115
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "`SparseVector#size` should give you the total size of the sparse vector, while `SparseVector#numNonzeros` gives you the number of nonzero values.\n\nAlso, SparseVectors may contain zero elements (e.g. `Vectors.sparse(1, Seq((0, 0.0)))`); it's just that elements which are not active (in `values`) are assumed to be zero.\n\nI'm still not clear on what `(sum == sources_count || values.contains(0.0))` is testing: the first is true if all the vectors in the `RDD` were dense and the second is true if any of the vectors in the `RDD` were dense or if a `0.0` was present in a sparse vector. What are you trying to test here?\n\nI think that you could make the code better by making the handling of sparse/dense more uniform and explicit so other developers can more easily understand. Some suggestions:\n- Break up the chained transformations into some intermediate variables with more descriptive names\n- Add comments to describe what's happening at important parts of the code\n- Converting all vectors to some uniform type so the logic is explicit and the code is more uniform. If everything is converted to dense, then `sum == sources_count` will always be true. Right now the per-case logic of handling dense and sparse is buried in L115 and is not immediately apparent.\n",
    "commit": "3a34b56392a549708b9655b2f6ec68d49d7ecc9d",
    "createdAt": "2015-07-03T06:26:26Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{DenseVector, SparseVector, Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ * Model to extract significant indices from vector.\n+ *\n+ * Significant indices is vector's index that has different value for different vectors.\n+ *\n+ * For example, when you use HashingTF they create big sparse vector,\n+ * and this code convert to smallest vector that don't include same values indices for all vectors.\n+ *\n+ * @param indices array of significant indices.\n+ */\n+@Experimental\n+class SignificantSelectorModel(val indices: Array[Int]) extends VectorTransformer {\n+\n+  /**\n+   * Applies transformation on a vector.\n+   *\n+   * @param vector vector to be transformed.\n+   * @return transformed vector.\n+   */\n+  override def transform(vector: Vector): Vector = vector match {\n+    case DenseVector(vs) =>\n+      Vectors.dense(indices.map(vs))\n+\n+    case SparseVector(s, ids, vs) =>\n+      var sv_idx = 0\n+      var new_idx = 0\n+      val elements = new mutable.ListBuffer[(Int, Double)]()\n+      \n+      for (idx <- indices) {\n+        while (sv_idx < ids.length && ids(sv_idx) < idx) {\n+          sv_idx += 1\n+        }\n+        if (sv_idx < ids.length && ids(sv_idx) == idx) {\n+          elements += ((new_idx, vs(sv_idx)))\n+          sv_idx += 1\n+        }\n+        new_idx += 1\n+      }\n+      \n+      Vectors.sparse(indices.length, elements)\n+\n+    case v =>\n+      throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Specialized model for equivalent vectors\n+ */\n+@Experimental\n+class SignificantSelectorEmptyModel extends SignificantSelectorModel(Array[Int]()) {\n+  \n+  val empty_vector = Vectors.dense(Array[Double]())\n+  \n+  override def transform(vector: Vector): Vector = empty_vector\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Create Significant selector.\n+ */\n+@Experimental\n+class SignificantSelector() {\n+\n+  /**\n+   * Returns a significant vector indices selector.\n+   *\n+   * @param sources an `RDD[Vector]` containing the vectors.\n+   */\n+  def fit(sources: RDD[Vector]): SignificantSelectorModel = {\n+    val sources_count = sources.count()\n+    val significant_indices = sources.flatMap {\n+        case DenseVector(vs) =>\n+          vs.zipWithIndex\n+        case SparseVector(_, ids, vs) =>\n+          vs.zip(ids)\n+        case v =>\n+          throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+      }\n+      .map(e => (e.swap, 1))\n+      .reduceByKey(_ + _)\n+      .map { case ((idx, value), count) => (idx, (value, count))}\n+      .groupByKey()\n+      .mapValues { e =>\n+        val values = e.groupBy(_._1)\n+        val sum = e.map(_._2).sum\n+\n+        values.size + (if (sum == sources_count || values.contains(0.0)) 0 else 1)",
    "line": 115
  }, {
    "author": {
      "login": "catap"
    },
    "body": "Let's be clear about sparse vector before we will be on same way about this logic.\n\nIn my point of view: sparse vector is the vector where missing all zero elements for memory optimization. For example [here](http://www.cs.umd.edu/Outreach/hsContest99/questions/node3.html) you can found same definition:\n\n> A vector is a one-dimensional array of elements. The natural C++ implementation of a vector is as a one-dimensional array. However, in many applications, the elements of a vector have mostly zero values. Such a vector is said to be sparse. It is inefficient to use a one-dimensional array to store a sparse vector. It is also inefficient to add elements whose values are zero in forming sums of sparse vectors. Consequently, we should choose a different representation.\n",
    "commit": "3a34b56392a549708b9655b2f6ec68d49d7ecc9d",
    "createdAt": "2015-07-03T07:57:22Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{DenseVector, SparseVector, Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ * Model to extract significant indices from vector.\n+ *\n+ * Significant indices is vector's index that has different value for different vectors.\n+ *\n+ * For example, when you use HashingTF they create big sparse vector,\n+ * and this code convert to smallest vector that don't include same values indices for all vectors.\n+ *\n+ * @param indices array of significant indices.\n+ */\n+@Experimental\n+class SignificantSelectorModel(val indices: Array[Int]) extends VectorTransformer {\n+\n+  /**\n+   * Applies transformation on a vector.\n+   *\n+   * @param vector vector to be transformed.\n+   * @return transformed vector.\n+   */\n+  override def transform(vector: Vector): Vector = vector match {\n+    case DenseVector(vs) =>\n+      Vectors.dense(indices.map(vs))\n+\n+    case SparseVector(s, ids, vs) =>\n+      var sv_idx = 0\n+      var new_idx = 0\n+      val elements = new mutable.ListBuffer[(Int, Double)]()\n+      \n+      for (idx <- indices) {\n+        while (sv_idx < ids.length && ids(sv_idx) < idx) {\n+          sv_idx += 1\n+        }\n+        if (sv_idx < ids.length && ids(sv_idx) == idx) {\n+          elements += ((new_idx, vs(sv_idx)))\n+          sv_idx += 1\n+        }\n+        new_idx += 1\n+      }\n+      \n+      Vectors.sparse(indices.length, elements)\n+\n+    case v =>\n+      throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Specialized model for equivalent vectors\n+ */\n+@Experimental\n+class SignificantSelectorEmptyModel extends SignificantSelectorModel(Array[Int]()) {\n+  \n+  val empty_vector = Vectors.dense(Array[Double]())\n+  \n+  override def transform(vector: Vector): Vector = empty_vector\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Create Significant selector.\n+ */\n+@Experimental\n+class SignificantSelector() {\n+\n+  /**\n+   * Returns a significant vector indices selector.\n+   *\n+   * @param sources an `RDD[Vector]` containing the vectors.\n+   */\n+  def fit(sources: RDD[Vector]): SignificantSelectorModel = {\n+    val sources_count = sources.count()\n+    val significant_indices = sources.flatMap {\n+        case DenseVector(vs) =>\n+          vs.zipWithIndex\n+        case SparseVector(_, ids, vs) =>\n+          vs.zip(ids)\n+        case v =>\n+          throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+      }\n+      .map(e => (e.swap, 1))\n+      .reduceByKey(_ + _)\n+      .map { case ((idx, value), count) => (idx, (value, count))}\n+      .groupByKey()\n+      .mapValues { e =>\n+        val values = e.groupBy(_._1)\n+        val sum = e.map(_._2).sum\n+\n+        values.size + (if (sum == sources_count || values.contains(0.0)) 0 else 1)",
    "line": 115
  }],
  "prId": 6795
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "Can remove this, see line 125.\n",
    "commit": "3a34b56392a549708b9655b2f6ec68d49d7ecc9d",
    "createdAt": "2015-07-03T04:52:24Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{DenseVector, SparseVector, Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ * Model to extract significant indices from vector.\n+ *\n+ * Significant indices is vector's index that has different value for different vectors.\n+ *\n+ * For example, when you use HashingTF they create big sparse vector,\n+ * and this code convert to smallest vector that don't include same values indices for all vectors.\n+ *\n+ * @param indices array of significant indices.\n+ */\n+@Experimental\n+class SignificantSelectorModel(val indices: Array[Int]) extends VectorTransformer {\n+\n+  /**\n+   * Applies transformation on a vector.\n+   *\n+   * @param vector vector to be transformed.\n+   * @return transformed vector.\n+   */\n+  override def transform(vector: Vector): Vector = vector match {\n+    case DenseVector(vs) =>\n+      Vectors.dense(indices.map(vs))\n+\n+    case SparseVector(s, ids, vs) =>\n+      var sv_idx = 0\n+      var new_idx = 0\n+      val elements = new mutable.ListBuffer[(Int, Double)]()\n+      \n+      for (idx <- indices) {\n+        while (sv_idx < ids.length && ids(sv_idx) < idx) {\n+          sv_idx += 1\n+        }\n+        if (sv_idx < ids.length && ids(sv_idx) == idx) {\n+          elements += ((new_idx, vs(sv_idx)))\n+          sv_idx += 1\n+        }\n+        new_idx += 1\n+      }\n+      \n+      Vectors.sparse(indices.length, elements)\n+\n+    case v =>\n+      throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Specialized model for equivalent vectors\n+ */\n+@Experimental\n+class SignificantSelectorEmptyModel extends SignificantSelectorModel(Array[Int]()) {",
    "line": 78
  }],
  "prId": 6795
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "Can just construct a `SignificantSelectorModel(Array())` instead\n",
    "commit": "3a34b56392a549708b9655b2f6ec68d49d7ecc9d",
    "createdAt": "2015-07-03T04:52:28Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{DenseVector, SparseVector, Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ * Model to extract significant indices from vector.\n+ *\n+ * Significant indices is vector's index that has different value for different vectors.\n+ *\n+ * For example, when you use HashingTF they create big sparse vector,\n+ * and this code convert to smallest vector that don't include same values indices for all vectors.\n+ *\n+ * @param indices array of significant indices.\n+ */\n+@Experimental\n+class SignificantSelectorModel(val indices: Array[Int]) extends VectorTransformer {\n+\n+  /**\n+   * Applies transformation on a vector.\n+   *\n+   * @param vector vector to be transformed.\n+   * @return transformed vector.\n+   */\n+  override def transform(vector: Vector): Vector = vector match {\n+    case DenseVector(vs) =>\n+      Vectors.dense(indices.map(vs))\n+\n+    case SparseVector(s, ids, vs) =>\n+      var sv_idx = 0\n+      var new_idx = 0\n+      val elements = new mutable.ListBuffer[(Int, Double)]()\n+      \n+      for (idx <- indices) {\n+        while (sv_idx < ids.length && ids(sv_idx) < idx) {\n+          sv_idx += 1\n+        }\n+        if (sv_idx < ids.length && ids(sv_idx) == idx) {\n+          elements += ((new_idx, vs(sv_idx)))\n+          sv_idx += 1\n+        }\n+        new_idx += 1\n+      }\n+      \n+      Vectors.sparse(indices.length, elements)\n+\n+    case v =>\n+      throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Specialized model for equivalent vectors\n+ */\n+@Experimental\n+class SignificantSelectorEmptyModel extends SignificantSelectorModel(Array[Int]()) {\n+  \n+  val empty_vector = Vectors.dense(Array[Double]())\n+  \n+  override def transform(vector: Vector): Vector = empty_vector\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Create Significant selector.\n+ */\n+@Experimental\n+class SignificantSelector() {\n+\n+  /**\n+   * Returns a significant vector indices selector.\n+   *\n+   * @param sources an `RDD[Vector]` containing the vectors.\n+   */\n+  def fit(sources: RDD[Vector]): SignificantSelectorModel = {\n+    val sources_count = sources.count()\n+    val significant_indices = sources.flatMap {\n+        case DenseVector(vs) =>\n+          vs.zipWithIndex\n+        case SparseVector(_, ids, vs) =>\n+          vs.zip(ids)\n+        case v =>\n+          throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+      }\n+      .map(e => (e.swap, 1))\n+      .reduceByKey(_ + _)\n+      .map { case ((idx, value), count) => (idx, (value, count))}\n+      .groupByKey()\n+      .mapValues { e =>\n+        val values = e.groupBy(_._1)\n+        val sum = e.map(_._2).sum\n+\n+        values.size + (if (sum == sources_count || values.contains(0.0)) 0 else 1)\n+      }\n+      .filter(_._2 > 1)\n+      .keys\n+      .collect()\n+      .sorted\n+\n+    if (significant_indices.nonEmpty)\n+      new SignificantSelectorModel(significant_indices)\n+    else\n+      new SignificantSelectorEmptyModel()",
    "line": 125
  }, {
    "author": {
      "login": "catap"
    },
    "body": "No, because I can't create an empty sparse vector, only dense.\n\nHere has two options:\n- create different model for empty indices.\n- make check if indices is empty each transform.\n\nI think first way better, don't I?\n",
    "commit": "3a34b56392a549708b9655b2f6ec68d49d7ecc9d",
    "createdAt": "2015-07-03T05:30:52Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{DenseVector, SparseVector, Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ * Model to extract significant indices from vector.\n+ *\n+ * Significant indices is vector's index that has different value for different vectors.\n+ *\n+ * For example, when you use HashingTF they create big sparse vector,\n+ * and this code convert to smallest vector that don't include same values indices for all vectors.\n+ *\n+ * @param indices array of significant indices.\n+ */\n+@Experimental\n+class SignificantSelectorModel(val indices: Array[Int]) extends VectorTransformer {\n+\n+  /**\n+   * Applies transformation on a vector.\n+   *\n+   * @param vector vector to be transformed.\n+   * @return transformed vector.\n+   */\n+  override def transform(vector: Vector): Vector = vector match {\n+    case DenseVector(vs) =>\n+      Vectors.dense(indices.map(vs))\n+\n+    case SparseVector(s, ids, vs) =>\n+      var sv_idx = 0\n+      var new_idx = 0\n+      val elements = new mutable.ListBuffer[(Int, Double)]()\n+      \n+      for (idx <- indices) {\n+        while (sv_idx < ids.length && ids(sv_idx) < idx) {\n+          sv_idx += 1\n+        }\n+        if (sv_idx < ids.length && ids(sv_idx) == idx) {\n+          elements += ((new_idx, vs(sv_idx)))\n+          sv_idx += 1\n+        }\n+        new_idx += 1\n+      }\n+      \n+      Vectors.sparse(indices.length, elements)\n+\n+    case v =>\n+      throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Specialized model for equivalent vectors\n+ */\n+@Experimental\n+class SignificantSelectorEmptyModel extends SignificantSelectorModel(Array[Int]()) {\n+  \n+  val empty_vector = Vectors.dense(Array[Double]())\n+  \n+  override def transform(vector: Vector): Vector = empty_vector\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Create Significant selector.\n+ */\n+@Experimental\n+class SignificantSelector() {\n+\n+  /**\n+   * Returns a significant vector indices selector.\n+   *\n+   * @param sources an `RDD[Vector]` containing the vectors.\n+   */\n+  def fit(sources: RDD[Vector]): SignificantSelectorModel = {\n+    val sources_count = sources.count()\n+    val significant_indices = sources.flatMap {\n+        case DenseVector(vs) =>\n+          vs.zipWithIndex\n+        case SparseVector(_, ids, vs) =>\n+          vs.zip(ids)\n+        case v =>\n+          throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+      }\n+      .map(e => (e.swap, 1))\n+      .reduceByKey(_ + _)\n+      .map { case ((idx, value), count) => (idx, (value, count))}\n+      .groupByKey()\n+      .mapValues { e =>\n+        val values = e.groupBy(_._1)\n+        val sum = e.map(_._2).sum\n+\n+        values.size + (if (sum == sources_count || values.contains(0.0)) 0 else 1)\n+      }\n+      .filter(_._2 > 1)\n+      .keys\n+      .collect()\n+      .sorted\n+\n+    if (significant_indices.nonEmpty)\n+      new SignificantSelectorModel(significant_indices)\n+    else\n+      new SignificantSelectorEmptyModel()",
    "line": 125
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "I would prefer not to add a public class if all it's doing is handling a special case. Perhaps we should allow sparse vectors to be empty as well. @mengxr thoughts?\n",
    "commit": "3a34b56392a549708b9655b2f6ec68d49d7ecc9d",
    "createdAt": "2015-07-03T06:40:51Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{DenseVector, SparseVector, Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ * Model to extract significant indices from vector.\n+ *\n+ * Significant indices is vector's index that has different value for different vectors.\n+ *\n+ * For example, when you use HashingTF they create big sparse vector,\n+ * and this code convert to smallest vector that don't include same values indices for all vectors.\n+ *\n+ * @param indices array of significant indices.\n+ */\n+@Experimental\n+class SignificantSelectorModel(val indices: Array[Int]) extends VectorTransformer {\n+\n+  /**\n+   * Applies transformation on a vector.\n+   *\n+   * @param vector vector to be transformed.\n+   * @return transformed vector.\n+   */\n+  override def transform(vector: Vector): Vector = vector match {\n+    case DenseVector(vs) =>\n+      Vectors.dense(indices.map(vs))\n+\n+    case SparseVector(s, ids, vs) =>\n+      var sv_idx = 0\n+      var new_idx = 0\n+      val elements = new mutable.ListBuffer[(Int, Double)]()\n+      \n+      for (idx <- indices) {\n+        while (sv_idx < ids.length && ids(sv_idx) < idx) {\n+          sv_idx += 1\n+        }\n+        if (sv_idx < ids.length && ids(sv_idx) == idx) {\n+          elements += ((new_idx, vs(sv_idx)))\n+          sv_idx += 1\n+        }\n+        new_idx += 1\n+      }\n+      \n+      Vectors.sparse(indices.length, elements)\n+\n+    case v =>\n+      throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Specialized model for equivalent vectors\n+ */\n+@Experimental\n+class SignificantSelectorEmptyModel extends SignificantSelectorModel(Array[Int]()) {\n+  \n+  val empty_vector = Vectors.dense(Array[Double]())\n+  \n+  override def transform(vector: Vector): Vector = empty_vector\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Create Significant selector.\n+ */\n+@Experimental\n+class SignificantSelector() {\n+\n+  /**\n+   * Returns a significant vector indices selector.\n+   *\n+   * @param sources an `RDD[Vector]` containing the vectors.\n+   */\n+  def fit(sources: RDD[Vector]): SignificantSelectorModel = {\n+    val sources_count = sources.count()\n+    val significant_indices = sources.flatMap {\n+        case DenseVector(vs) =>\n+          vs.zipWithIndex\n+        case SparseVector(_, ids, vs) =>\n+          vs.zip(ids)\n+        case v =>\n+          throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+      }\n+      .map(e => (e.swap, 1))\n+      .reduceByKey(_ + _)\n+      .map { case ((idx, value), count) => (idx, (value, count))}\n+      .groupByKey()\n+      .mapValues { e =>\n+        val values = e.groupBy(_._1)\n+        val sum = e.map(_._2).sum\n+\n+        values.size + (if (sum == sources_count || values.contains(0.0)) 0 else 1)\n+      }\n+      .filter(_._2 > 1)\n+      .keys\n+      .collect()\n+      .sorted\n+\n+    if (significant_indices.nonEmpty)\n+      new SignificantSelectorModel(significant_indices)\n+    else\n+      new SignificantSelectorEmptyModel()",
    "line": 125
  }, {
    "author": {
      "login": "catap"
    },
    "body": "So, I can suggest to make both model classes as private[mllib].\n\nDo you agree with this?\n",
    "commit": "3a34b56392a549708b9655b2f6ec68d49d7ecc9d",
    "createdAt": "2015-07-03T08:03:33Z",
    "diffHunk": "@@ -0,0 +1,127 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{DenseVector, SparseVector, Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * :: Experimental ::\n+ * Model to extract significant indices from vector.\n+ *\n+ * Significant indices is vector's index that has different value for different vectors.\n+ *\n+ * For example, when you use HashingTF they create big sparse vector,\n+ * and this code convert to smallest vector that don't include same values indices for all vectors.\n+ *\n+ * @param indices array of significant indices.\n+ */\n+@Experimental\n+class SignificantSelectorModel(val indices: Array[Int]) extends VectorTransformer {\n+\n+  /**\n+   * Applies transformation on a vector.\n+   *\n+   * @param vector vector to be transformed.\n+   * @return transformed vector.\n+   */\n+  override def transform(vector: Vector): Vector = vector match {\n+    case DenseVector(vs) =>\n+      Vectors.dense(indices.map(vs))\n+\n+    case SparseVector(s, ids, vs) =>\n+      var sv_idx = 0\n+      var new_idx = 0\n+      val elements = new mutable.ListBuffer[(Int, Double)]()\n+      \n+      for (idx <- indices) {\n+        while (sv_idx < ids.length && ids(sv_idx) < idx) {\n+          sv_idx += 1\n+        }\n+        if (sv_idx < ids.length && ids(sv_idx) == idx) {\n+          elements += ((new_idx, vs(sv_idx)))\n+          sv_idx += 1\n+        }\n+        new_idx += 1\n+      }\n+      \n+      Vectors.sparse(indices.length, elements)\n+\n+    case v =>\n+      throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Specialized model for equivalent vectors\n+ */\n+@Experimental\n+class SignificantSelectorEmptyModel extends SignificantSelectorModel(Array[Int]()) {\n+  \n+  val empty_vector = Vectors.dense(Array[Double]())\n+  \n+  override def transform(vector: Vector): Vector = empty_vector\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Create Significant selector.\n+ */\n+@Experimental\n+class SignificantSelector() {\n+\n+  /**\n+   * Returns a significant vector indices selector.\n+   *\n+   * @param sources an `RDD[Vector]` containing the vectors.\n+   */\n+  def fit(sources: RDD[Vector]): SignificantSelectorModel = {\n+    val sources_count = sources.count()\n+    val significant_indices = sources.flatMap {\n+        case DenseVector(vs) =>\n+          vs.zipWithIndex\n+        case SparseVector(_, ids, vs) =>\n+          vs.zip(ids)\n+        case v =>\n+          throw new IllegalArgumentException(\"Don't support vector type \" + v.getClass)\n+      }\n+      .map(e => (e.swap, 1))\n+      .reduceByKey(_ + _)\n+      .map { case ((idx, value), count) => (idx, (value, count))}\n+      .groupByKey()\n+      .mapValues { e =>\n+        val values = e.groupBy(_._1)\n+        val sum = e.map(_._2).sum\n+\n+        values.size + (if (sum == sources_count || values.contains(0.0)) 0 else 1)\n+      }\n+      .filter(_._2 > 1)\n+      .keys\n+      .collect()\n+      .sorted\n+\n+    if (significant_indices.nonEmpty)\n+      new SignificantSelectorModel(significant_indices)\n+    else\n+      new SignificantSelectorEmptyModel()",
    "line": 125
  }],
  "prId": 6795
}]