[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "This is a NOOP, but I'm OK if you feel it is clearer.\n",
    "commit": "f4b39d8b27def827b684f8e7704676ede72d5e6c",
    "createdAt": "2015-07-17T04:16:31Z",
    "diffHunk": "@@ -42,11 +42,11 @@ object SquaredError extends Loss {\n    * @return Loss gradient\n    */\n   override def gradient(prediction: Double, label: Double): Double = {\n-    2.0 * (prediction - label)\n+    - 2.0 * (label - prediction)",
    "line": 5
  }],
  "prId": 7435
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "ditto (noop)\n",
    "commit": "f4b39d8b27def827b684f8e7704676ede72d5e6c",
    "createdAt": "2015-07-17T04:16:35Z",
    "diffHunk": "@@ -42,11 +42,11 @@ object SquaredError extends Loss {\n    * @return Loss gradient\n    */\n   override def gradient(prediction: Double, label: Double): Double = {\n-    2.0 * (prediction - label)\n+    - 2.0 * (label - prediction)\n   }\n \n   override private[mllib] def computeError(prediction: Double, label: Double): Double = {\n-    val err = prediction - label\n+    val err = label - prediction",
    "line": 10
  }],
  "prId": 7435
}]