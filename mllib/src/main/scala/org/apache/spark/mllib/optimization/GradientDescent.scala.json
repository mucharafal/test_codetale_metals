[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`aggregate` -> `.treeAggregate`. We use a tree pattern to avoid sending too much data to the driver. Does it hurt streaming update performance? \n",
    "commit": "775ea29e53a7067ff6e143b455b49ddbb8553d94",
    "createdAt": "2014-08-01T22:56:30Z",
    "diffHunk": "@@ -162,45 +162,55 @@ object GradientDescent extends Logging {\n     val numExamples = data.count()\n     val miniBatchSize = numExamples * miniBatchFraction\n \n-    // Initialize weights as a column vector\n-    var weights = Vectors.dense(initialWeights.toArray)\n-    val n = weights.size\n-\n-    /**\n-     * For the first iteration, the regVal will be initialized as sum of weight squares\n-     * if it's L2 updater; for L1 updater, the same logic is followed.\n-     */\n-    var regVal = updater.compute(\n-      weights, Vectors.dense(new Array[Double](weights.size)), 0, 1, regParam)._2\n-\n-    for (i <- 1 to numIterations) {\n-      val bcWeights = data.context.broadcast(weights)\n-      // Sample a subset (fraction miniBatchFraction) of the total data\n-      // compute and sum up the subgradients on this subset (this is one map-reduce)\n-      val (gradientSum, lossSum) = data.sample(false, miniBatchFraction, 42 + i)\n-        .treeAggregate((BDV.zeros[Double](n), 0.0))(\n-          seqOp = (c, v) => (c, v) match { case ((grad, loss), (label, features)) =>\n-            val l = gradient.compute(features, label, bcWeights.value, Vectors.fromBreeze(grad))\n-            (grad, loss + l)\n-          },\n-          combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1), (grad2, loss2)) =>\n-            (grad1 += grad2, loss1 + loss2)\n-          })\n+    // if no data, return initial weights to avoid NaNs\n+    if (numExamples == 0) {\n+\n+      logInfo(\"GradientDescent.runMiniBatchSGD returning initial weights, no data found\")\n+      (initialWeights, stochasticLossHistory.toArray)\n+\n+    } else {\n+\n+      // Initialize weights as a column vector\n+      var weights = Vectors.dense(initialWeights.toArray)\n+      val n = weights.size\n \n       /**\n-       * NOTE(Xinghao): lossSum is computed using the weights from the previous iteration\n-       * and regVal is the regularization value computed in the previous iteration as well.\n+       * For the first iteration, the regVal will be initialized as sum of weight squares\n+       * if it's L2 updater; for L1 updater, the same logic is followed.\n        */\n-      stochasticLossHistory.append(lossSum / miniBatchSize + regVal)\n-      val update = updater.compute(\n-        weights, Vectors.fromBreeze(gradientSum / miniBatchSize), stepSize, i, regParam)\n-      weights = update._1\n-      regVal = update._2\n+      var regVal = updater.compute(\n+        weights, Vectors.dense(new Array[Double](weights.size)), 0, 1, regParam)._2\n+\n+      for (i <- 1 to numIterations) {\n+        // Sample a subset (fraction miniBatchFraction) of the total data\n+        // compute and sum up the subgradients on this subset (this is one map-reduce)\n+        val (gradientSum, lossSum) = data.sample(false, miniBatchFraction, 42 + i)\n+          .aggregate((BDV.zeros[Double](weights.size), 0.0))("
  }, {
    "author": {
      "login": "freeman-lab"
    },
    "body": "It's totally fine, I might have lost it in the merge, put it back.\n",
    "commit": "775ea29e53a7067ff6e143b455b49ddbb8553d94",
    "createdAt": "2014-08-01T23:33:44Z",
    "diffHunk": "@@ -162,45 +162,55 @@ object GradientDescent extends Logging {\n     val numExamples = data.count()\n     val miniBatchSize = numExamples * miniBatchFraction\n \n-    // Initialize weights as a column vector\n-    var weights = Vectors.dense(initialWeights.toArray)\n-    val n = weights.size\n-\n-    /**\n-     * For the first iteration, the regVal will be initialized as sum of weight squares\n-     * if it's L2 updater; for L1 updater, the same logic is followed.\n-     */\n-    var regVal = updater.compute(\n-      weights, Vectors.dense(new Array[Double](weights.size)), 0, 1, regParam)._2\n-\n-    for (i <- 1 to numIterations) {\n-      val bcWeights = data.context.broadcast(weights)\n-      // Sample a subset (fraction miniBatchFraction) of the total data\n-      // compute and sum up the subgradients on this subset (this is one map-reduce)\n-      val (gradientSum, lossSum) = data.sample(false, miniBatchFraction, 42 + i)\n-        .treeAggregate((BDV.zeros[Double](n), 0.0))(\n-          seqOp = (c, v) => (c, v) match { case ((grad, loss), (label, features)) =>\n-            val l = gradient.compute(features, label, bcWeights.value, Vectors.fromBreeze(grad))\n-            (grad, loss + l)\n-          },\n-          combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1), (grad2, loss2)) =>\n-            (grad1 += grad2, loss1 + loss2)\n-          })\n+    // if no data, return initial weights to avoid NaNs\n+    if (numExamples == 0) {\n+\n+      logInfo(\"GradientDescent.runMiniBatchSGD returning initial weights, no data found\")\n+      (initialWeights, stochasticLossHistory.toArray)\n+\n+    } else {\n+\n+      // Initialize weights as a column vector\n+      var weights = Vectors.dense(initialWeights.toArray)\n+      val n = weights.size\n \n       /**\n-       * NOTE(Xinghao): lossSum is computed using the weights from the previous iteration\n-       * and regVal is the regularization value computed in the previous iteration as well.\n+       * For the first iteration, the regVal will be initialized as sum of weight squares\n+       * if it's L2 updater; for L1 updater, the same logic is followed.\n        */\n-      stochasticLossHistory.append(lossSum / miniBatchSize + regVal)\n-      val update = updater.compute(\n-        weights, Vectors.fromBreeze(gradientSum / miniBatchSize), stepSize, i, regParam)\n-      weights = update._1\n-      regVal = update._2\n+      var regVal = updater.compute(\n+        weights, Vectors.dense(new Array[Double](weights.size)), 0, 1, regParam)._2\n+\n+      for (i <- 1 to numIterations) {\n+        // Sample a subset (fraction miniBatchFraction) of the total data\n+        // compute and sum up the subgradients on this subset (this is one map-reduce)\n+        val (gradientSum, lossSum) = data.sample(false, miniBatchFraction, 42 + i)\n+          .aggregate((BDV.zeros[Double](weights.size), 0.0))("
  }, {
    "author": {
      "login": "freeman-lab"
    },
    "body": "Same for broadcasting, sorry, fixing...\n",
    "commit": "775ea29e53a7067ff6e143b455b49ddbb8553d94",
    "createdAt": "2014-08-01T23:43:12Z",
    "diffHunk": "@@ -162,45 +162,55 @@ object GradientDescent extends Logging {\n     val numExamples = data.count()\n     val miniBatchSize = numExamples * miniBatchFraction\n \n-    // Initialize weights as a column vector\n-    var weights = Vectors.dense(initialWeights.toArray)\n-    val n = weights.size\n-\n-    /**\n-     * For the first iteration, the regVal will be initialized as sum of weight squares\n-     * if it's L2 updater; for L1 updater, the same logic is followed.\n-     */\n-    var regVal = updater.compute(\n-      weights, Vectors.dense(new Array[Double](weights.size)), 0, 1, regParam)._2\n-\n-    for (i <- 1 to numIterations) {\n-      val bcWeights = data.context.broadcast(weights)\n-      // Sample a subset (fraction miniBatchFraction) of the total data\n-      // compute and sum up the subgradients on this subset (this is one map-reduce)\n-      val (gradientSum, lossSum) = data.sample(false, miniBatchFraction, 42 + i)\n-        .treeAggregate((BDV.zeros[Double](n), 0.0))(\n-          seqOp = (c, v) => (c, v) match { case ((grad, loss), (label, features)) =>\n-            val l = gradient.compute(features, label, bcWeights.value, Vectors.fromBreeze(grad))\n-            (grad, loss + l)\n-          },\n-          combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1), (grad2, loss2)) =>\n-            (grad1 += grad2, loss1 + loss2)\n-          })\n+    // if no data, return initial weights to avoid NaNs\n+    if (numExamples == 0) {\n+\n+      logInfo(\"GradientDescent.runMiniBatchSGD returning initial weights, no data found\")\n+      (initialWeights, stochasticLossHistory.toArray)\n+\n+    } else {\n+\n+      // Initialize weights as a column vector\n+      var weights = Vectors.dense(initialWeights.toArray)\n+      val n = weights.size\n \n       /**\n-       * NOTE(Xinghao): lossSum is computed using the weights from the previous iteration\n-       * and regVal is the regularization value computed in the previous iteration as well.\n+       * For the first iteration, the regVal will be initialized as sum of weight squares\n+       * if it's L2 updater; for L1 updater, the same logic is followed.\n        */\n-      stochasticLossHistory.append(lossSum / miniBatchSize + regVal)\n-      val update = updater.compute(\n-        weights, Vectors.fromBreeze(gradientSum / miniBatchSize), stepSize, i, regParam)\n-      weights = update._1\n-      regVal = update._2\n+      var regVal = updater.compute(\n+        weights, Vectors.dense(new Array[Double](weights.size)), 0, 1, regParam)._2\n+\n+      for (i <- 1 to numIterations) {\n+        // Sample a subset (fraction miniBatchFraction) of the total data\n+        // compute and sum up the subgradients on this subset (this is one map-reduce)\n+        val (gradientSum, lossSum) = data.sample(false, miniBatchFraction, 42 + i)\n+          .aggregate((BDV.zeros[Double](weights.size), 0.0))("
  }],
  "prId": 1361
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "it may be better to use `return (initialWeights, stochasticLossHistory.toArray)` here to avoid having extra indentation for the main block.\n",
    "commit": "775ea29e53a7067ff6e143b455b49ddbb8553d94",
    "createdAt": "2014-08-01T22:57:48Z",
    "diffHunk": "@@ -162,45 +162,55 @@ object GradientDescent extends Logging {\n     val numExamples = data.count()\n     val miniBatchSize = numExamples * miniBatchFraction\n \n-    // Initialize weights as a column vector\n-    var weights = Vectors.dense(initialWeights.toArray)\n-    val n = weights.size\n-\n-    /**\n-     * For the first iteration, the regVal will be initialized as sum of weight squares\n-     * if it's L2 updater; for L1 updater, the same logic is followed.\n-     */\n-    var regVal = updater.compute(\n-      weights, Vectors.dense(new Array[Double](weights.size)), 0, 1, regParam)._2\n-\n-    for (i <- 1 to numIterations) {\n-      val bcWeights = data.context.broadcast(weights)\n-      // Sample a subset (fraction miniBatchFraction) of the total data\n-      // compute and sum up the subgradients on this subset (this is one map-reduce)\n-      val (gradientSum, lossSum) = data.sample(false, miniBatchFraction, 42 + i)\n-        .treeAggregate((BDV.zeros[Double](n), 0.0))(\n-          seqOp = (c, v) => (c, v) match { case ((grad, loss), (label, features)) =>\n-            val l = gradient.compute(features, label, bcWeights.value, Vectors.fromBreeze(grad))\n-            (grad, loss + l)\n-          },\n-          combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1), (grad2, loss2)) =>\n-            (grad1 += grad2, loss1 + loss2)\n-          })\n+    // if no data, return initial weights to avoid NaNs\n+    if (numExamples == 0) {\n+\n+      logInfo(\"GradientDescent.runMiniBatchSGD returning initial weights, no data found\")\n+      (initialWeights, stochasticLossHistory.toArray)"
  }, {
    "author": {
      "login": "freeman-lab"
    },
    "body": "Nice idea, made the change.\n",
    "commit": "775ea29e53a7067ff6e143b455b49ddbb8553d94",
    "createdAt": "2014-08-01T23:33:48Z",
    "diffHunk": "@@ -162,45 +162,55 @@ object GradientDescent extends Logging {\n     val numExamples = data.count()\n     val miniBatchSize = numExamples * miniBatchFraction\n \n-    // Initialize weights as a column vector\n-    var weights = Vectors.dense(initialWeights.toArray)\n-    val n = weights.size\n-\n-    /**\n-     * For the first iteration, the regVal will be initialized as sum of weight squares\n-     * if it's L2 updater; for L1 updater, the same logic is followed.\n-     */\n-    var regVal = updater.compute(\n-      weights, Vectors.dense(new Array[Double](weights.size)), 0, 1, regParam)._2\n-\n-    for (i <- 1 to numIterations) {\n-      val bcWeights = data.context.broadcast(weights)\n-      // Sample a subset (fraction miniBatchFraction) of the total data\n-      // compute and sum up the subgradients on this subset (this is one map-reduce)\n-      val (gradientSum, lossSum) = data.sample(false, miniBatchFraction, 42 + i)\n-        .treeAggregate((BDV.zeros[Double](n), 0.0))(\n-          seqOp = (c, v) => (c, v) match { case ((grad, loss), (label, features)) =>\n-            val l = gradient.compute(features, label, bcWeights.value, Vectors.fromBreeze(grad))\n-            (grad, loss + l)\n-          },\n-          combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1), (grad2, loss2)) =>\n-            (grad1 += grad2, loss1 + loss2)\n-          })\n+    // if no data, return initial weights to avoid NaNs\n+    if (numExamples == 0) {\n+\n+      logInfo(\"GradientDescent.runMiniBatchSGD returning initial weights, no data found\")\n+      (initialWeights, stochasticLossHistory.toArray)"
  }],
  "prId": 1361
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Broadcasting the weights is actually important for performance. Did you experience any problem with it? It may be an orthogonal issue. Maybe we should keep this code block unchanged.\n",
    "commit": "775ea29e53a7067ff6e143b455b49ddbb8553d94",
    "createdAt": "2014-08-02T00:00:42Z",
    "diffHunk": "@@ -174,17 +182,18 @@ object GradientDescent extends Logging {\n       weights, Vectors.dense(new Array[Double](weights.size)), 0, 1, regParam)._2\n \n     for (i <- 1 to numIterations) {\n-      val bcWeights = data.context.broadcast(weights)"
  }, {
    "author": {
      "login": "freeman-lab"
    },
    "body": "This was my mistake, should be fixed now.\n",
    "commit": "775ea29e53a7067ff6e143b455b49ddbb8553d94",
    "createdAt": "2014-08-02T00:46:03Z",
    "diffHunk": "@@ -174,17 +182,18 @@ object GradientDescent extends Logging {\n       weights, Vectors.dense(new Array[Double](weights.size)), 0, 1, regParam)._2\n \n     for (i <- 1 to numIterations) {\n-      val bcWeights = data.context.broadcast(weights)"
  }],
  "prId": 1361
}]