[{
  "comments": [{
    "author": {
      "login": "jackylk"
    },
    "body": "Can you make it generic type so that it can accept not only Array[Int] as input?\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T08:24:16Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "The Int data type is small than some other data types, such as String etc. So, inside the algorithm, we use Int data type to get the better performance.\n\nIf user use other data type, like String, maybe he will code String to Int, and decode the result to String. The following:\n\n```\nval sequences = Array(\n  Array(\"3\", \"1\", \"3\", \"4\", \"5\"),\n  Array(\"2\", \"3\", \"1\"),\n  Array(\"3\", \"4\", \"4\", \"3\"),\n  Array(\"1\", \"3\", \"4\", \"5\"),\n  Array(\"2\", \"4\", \"1\"),\n  Array(\"6\", \"5\", \"3\"))\nval rdd = sc.parallelize(sequences, 2).cache()\n\n// create coder and decoder\nval letterMap = rdd.flatMap(x => x.distinct).distinct().zipWithIndex().mapValues(_.toInt).collect\nval coder = letterMap.toMap\nval decoder = letterMap.map(x => (x._2, x._1)).toMap\n\n// code\nval intRdd = rdd.map(x => x.map(y => coder(y)))\n\nval prefixspan1 = new Prefixspan(intRdd, 2, 50)\nval result = prefixspan1.run()\n\n// decode\nval stringResult = result.map(x => (x._1.map(y => decoder(y)), x._2))\n```\n\nI think this is a general job, some other algorithms maybe need this function too, so can we add the coder and decoder as a separate model for all other algorithms ? \n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T09:18:42Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "`FPGrowth` accepts generic item type and encode items into indices after the first step. We can do that in a follow-up PR.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T14:36:53Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "The problem definition in the referenced paper suggests this should actually be `Array[Array[Item]]` for `Item` a generic type. I agree that this extension can be deferred to a later PR and the current code is fine as is.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T20:02:53Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Actually can the change `Array[Int]` -> `Array[Array[Int]]` be made in this PR since it affects the kind of a parameter exposed in a public API and will be more easy to generalize (for now we can just flatten the array and use the existing implementation)?\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:02:06Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "@feynmanliang The generalization for non-temporal items would happen in a follow-up PR before 1.5. This is discussed on the JIRA page.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T22:39:05Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "@mengxr This PR actually implements non-temporal (by temporal you mean consecutive correct?) sequential pattern mining, see the L86 and L149. My suggestion was to make the definition of a sequence consistent with the Han et al paper (An **itemset** is a subset of items. A **sequence** is an ordered list of itemsets) since this PR currently defines a sequence to be a ordered list of singleton itemsets.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T22:53:49Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Synced offline with @mengxr, `Array[Int]` OK for this PR.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T23:27:14Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "OK, It would be fixed in a follow-up PR before 1.5. It's soon.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T03:44:04Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Add `::Experimental::` here and `@Experimental` to `class PrefixSpan`.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T15:01:36Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T02:26:05Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "use DOI link: http://doi.org/10.1109/ICDE.2001.914830\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T15:01:38Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]]."
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Document that this implementation only covers itemsets of size 1 (Han et al define an itemset to be a subset of all items and a sequential pattern to be an ordered sequence of itemsets, Section 2: Def of **itemset** and **sequence**).\n\nAlso document whether this implementation supports non-contiguous (subsequences which skip items from the original sequence between each item) sequential patterns since Han et al define sequential patterns to be non-contiguous subsequences (Section 2: Def of **subsequence**) and if I recall correctly from our phone conversation this implementation will only support contiguous subsequences.\n\n**Edit: After reading the code (L86 and L149) it seems like you are indeed mining for non-contiguous subsequences!\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T20:00:42Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]]."
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "@mengxr Fixed the link.\n@feynmanliang You are right, the code supports non-contiguous.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T02:44:48Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]]."
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Move this to `run` as an input argument.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T15:01:40Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T02:47:47Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`Prefixscan` -> `PrefixScan`\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T15:01:42Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan("
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed, change Prefixspan to PrefixSpan.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T03:32:53Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan("
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Is `50` too big as the default? What is the default value SPMF and R?\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T15:01:45Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "R's [arulesSequence](http://cran.r-project.org/web/packages/arulesSequences/arulesSequences.pdf) package defaults `maxlen = 10` and `support = 0.1` (with support defined as proportion of transactions in database containing subsequence)\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T20:05:15Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed, changed maxPatternLength to 10, changed minSupport to 0.1.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T06:26:38Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Blank lines are used to separate code blocks but not individual statements in Spark.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T15:01:47Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T06:32:47Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "newline at the end\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T15:01:51Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {\n+    val elements = data\n+      .map(x => x.distinct)\n+      .flatMap(x => x)\n+      .groupBy(x => x)\n+      .map(x => (x._1, x._2.length))\n+\n+    val selectedSingleElements = elements.filter(x => x._2 >= minSupport)\n+\n+    val selectedElements = selectedSingleElements\n+      .map(x => (prefix ++ Seq(x._1), x._2))\n+      .toArray\n+\n+    val cleanedSearchSpace = data\n+      .map(x => x.filter(y => selectedSingleElements.contains(y)))\n+\n+    val newSearchSpace = selectedSingleElements.map { x =>\n+      val sub = cleanedSearchSpace.map(y => getSuffix(x._1, y)).filter(_.nonEmpty)\n+      (prefix ++ Seq(x._1), sub)\n+    }.filter(x => x._2.nonEmpty)\n+      .toArray\n+\n+    val continueProcess = newSearchSpace.nonEmpty && prefix.length + 1 < maxPatternLength\n+\n+    if (continueProcess) {\n+      val nextPatterns = newSearchSpace\n+        .map(x => getPatternsWithPrefix(x._1, x._2))\n+        .reduce(_ ++ _)\n+      selectedElements ++ nextPatterns\n+    } else {\n+      selectedElements\n+    }\n+  }\n+\n+  /**\n+   * calculate suffix sequence following a prefix in a sequence\n+   * @param prefix prefix\n+   * @param sequence original sequence\n+   * @return suffix sequence\n+   */\n+  private def getSuffix(prefix: Int, sequence: Array[Int]): Array[Int] = {\n+    val index = sequence.indexOf(prefix)\n+    if (index == -1) {\n+      Array()\n+    } else {\n+      sequence.takeRight(sequence.length - index - 1)\n+    }\n+  }\n+}"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T07:31:18Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {\n+    val elements = data\n+      .map(x => x.distinct)\n+      .flatMap(x => x)\n+      .groupBy(x => x)\n+      .map(x => (x._1, x._2.length))\n+\n+    val selectedSingleElements = elements.filter(x => x._2 >= minSupport)\n+\n+    val selectedElements = selectedSingleElements\n+      .map(x => (prefix ++ Seq(x._1), x._2))\n+      .toArray\n+\n+    val cleanedSearchSpace = data\n+      .map(x => x.filter(y => selectedSingleElements.contains(y)))\n+\n+    val newSearchSpace = selectedSingleElements.map { x =>\n+      val sub = cleanedSearchSpace.map(y => getSuffix(x._1, y)).filter(_.nonEmpty)\n+      (prefix ++ Seq(x._1), sub)\n+    }.filter(x => x._2.nonEmpty)\n+      .toArray\n+\n+    val continueProcess = newSearchSpace.nonEmpty && prefix.length + 1 < maxPatternLength\n+\n+    if (continueProcess) {\n+      val nextPatterns = newSearchSpace\n+        .map(x => getPatternsWithPrefix(x._1, x._2))\n+        .reduce(_ ++ _)\n+      selectedElements ++ nextPatterns\n+    } else {\n+      selectedElements\n+    }\n+  }\n+\n+  /**\n+   * calculate suffix sequence following a prefix in a sequence\n+   * @param prefix prefix\n+   * @param sequence original sequence\n+   * @return suffix sequence\n+   */\n+  private def getSuffix(prefix: Int, sequence: Array[Int]): Array[Int] = {\n+    val index = sequence.indexOf(prefix)\n+    if (index == -1) {\n+      Array()\n+    } else {\n+      sequence.takeRight(sequence.length - index - 1)\n+    }\n+  }\n+}"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "We should be consistent with `FPGrowth` and define `minSupport` to be a double in [0,1] indicating the proportion of transactions containing the sequential pattern (I realize that Han et al define it as the total count but [Wikipedia](https://en.wikipedia.org/wiki/Association_rule_learning#Support), which is referenced below, follows the same definition I am suggesting).\n\nThis just means that we will need to multiply by `sequences.count()` before doing any of the filtering by `minSupport`.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T19:57:27Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed, change minSupport data type to double in [0,1]\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T03:00:28Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "`sequences`should be an argument to `run()` rather than a constructor parameter.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T20:11:16Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T06:27:43Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "These should be `private var`s with public getters and setters to decouple interface from implementation (see [FPGrowth](https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/fpm/FPGrowth.scala#L76))\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T20:12:28Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Changed these input parameters to private var s, and add getters and setters.\n\n@feynmanliang \nBut I have a question, why don't we use the public input parameters ?\nThe following code, the class A is smaller than class B, Why is class B code style better ?\n\n```\nclass A (var x: Int = 12) {\n  def f() = {\n    println(x)\n  }\n}\n\nclass B (private var x: Int) {\n  def this() = this(12)\n  def setX(x: Int): this.type = {\n    this.x=x\n    this\n  }\n  def f() = {\n    println(x)\n  }\n}\n\nval a = new A()\na.f()\na.x = 15\na.f()\n\nval b = new B()\nb.f()\nb.setX(15)\nb.f()\n```\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T06:25:06Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Suppose you wanted to ensure that `x` is always `>=0`. In `B`, you would just redefine\n\n``` scala\ndef setX(x: Int): this.type = {\n  require(x >= 0)\n  ...\n}\n```\n\nwhereas I'm not too sure how to get that done easily in `A`.\n\nAlso, you can now chain method calls to `B`, for example:\n\n``` scala\nval b = new B()\nb.setX(15).f()\n```\n\nIn general, accessors help keep classes encapsulated and decouple classes by offering an additional level of indirection. [Wikipedia](https://en.wikipedia.org/wiki/Mutator_method) has a more lengthy explanation than what I've given.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T17:41:42Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "nit: `patternsOneLength`->`lengthOnePatternsAndCounts`\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:05:01Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T06:28:53Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "Combine L65-67 into `.flatMap(_.distinct.map((_, 1)))`\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:05:48Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T06:30:36Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "nit: `removedElements` -> `infrequentLengthOnePatterns`\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:08:39Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T06:34:20Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "nit: `savedElements` -> `frequentLengthOnePatterns`\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:10:47Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T06:37:10Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "nit: `sequence.drop(index+1)`\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:19:28Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {\n+    val elements = data\n+      .map(x => x.distinct)\n+      .flatMap(x => x)\n+      .groupBy(x => x)\n+      .map(x => (x._1, x._2.length))\n+\n+    val selectedSingleElements = elements.filter(x => x._2 >= minSupport)\n+\n+    val selectedElements = selectedSingleElements\n+      .map(x => (prefix ++ Seq(x._1), x._2))\n+      .toArray\n+\n+    val cleanedSearchSpace = data\n+      .map(x => x.filter(y => selectedSingleElements.contains(y)))\n+\n+    val newSearchSpace = selectedSingleElements.map { x =>\n+      val sub = cleanedSearchSpace.map(y => getSuffix(x._1, y)).filter(_.nonEmpty)\n+      (prefix ++ Seq(x._1), sub)\n+    }.filter(x => x._2.nonEmpty)\n+      .toArray\n+\n+    val continueProcess = newSearchSpace.nonEmpty && prefix.length + 1 < maxPatternLength\n+\n+    if (continueProcess) {\n+      val nextPatterns = newSearchSpace\n+        .map(x => getPatternsWithPrefix(x._1, x._2))\n+        .reduce(_ ++ _)\n+      selectedElements ++ nextPatterns\n+    } else {\n+      selectedElements\n+    }\n+  }\n+\n+  /**\n+   * calculate suffix sequence following a prefix in a sequence\n+   * @param prefix prefix\n+   * @param sequence original sequence\n+   * @return suffix sequence\n+   */\n+  private def getSuffix(prefix: Int, sequence: Array[Int]): Array[Int] = {\n+    val index = sequence.indexOf(prefix)\n+    if (index == -1) {\n+      Array()\n+    } else {\n+      sequence.takeRight(sequence.length - index - 1)"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T07:31:04Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {\n+    val elements = data\n+      .map(x => x.distinct)\n+      .flatMap(x => x)\n+      .groupBy(x => x)\n+      .map(x => (x._1, x._2.length))\n+\n+    val selectedSingleElements = elements.filter(x => x._2 >= minSupport)\n+\n+    val selectedElements = selectedSingleElements\n+      .map(x => (prefix ++ Seq(x._1), x._2))\n+      .toArray\n+\n+    val cleanedSearchSpace = data\n+      .map(x => x.filter(y => selectedSingleElements.contains(y)))\n+\n+    val newSearchSpace = selectedSingleElements.map { x =>\n+      val sub = cleanedSearchSpace.map(y => getSuffix(x._1, y)).filter(_.nonEmpty)\n+      (prefix ++ Seq(x._1), sub)\n+    }.filter(x => x._2.nonEmpty)\n+      .toArray\n+\n+    val continueProcess = newSearchSpace.nonEmpty && prefix.length + 1 < maxPatternLength\n+\n+    if (continueProcess) {\n+      val nextPatterns = newSearchSpace\n+        .map(x => getPatternsWithPrefix(x._1, x._2))\n+        .reduce(_ ++ _)\n+      selectedElements ++ nextPatterns\n+    } else {\n+      selectedElements\n+    }\n+  }\n+\n+  /**\n+   * calculate suffix sequence following a prefix in a sequence\n+   * @param prefix prefix\n+   * @param sequence original sequence\n+   * @return suffix sequence\n+   */\n+  private def getSuffix(prefix: Int, sequence: Array[Int]): Array[Int] = {\n+    val index = sequence.indexOf(prefix)\n+    if (index == -1) {\n+      Array()\n+    } else {\n+      sequence.takeRight(sequence.length - index - 1)"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "Why not just do the filter up on L95 when creating `prefixAndCandidates` to localize the logic?\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:23:11Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed. Moved the filter to L95. \n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T06:44:15Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "`.map(x => ...)` -> `.mapValues(_.toArray)`\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:24:32Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T06:45:42Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "This isn't really a performance-oriented repartition in [the sense Spark has defined repartition](http://spark.apache.org/docs/1.2.2/api/java/org/apache/spark/rdd/RDD.html#repartition%28int, scala.math.Ordering%29).\n\nHow about calling it `makePrefixProjectedDatabases` make it clear that this the projected database construction step in PrefixScan?\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:28:18Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Changed the function name to makePrefixProjectedDatabases. It's better than old one.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T06:40:25Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "!!!\n@mengxr could collecting the entire projected database overload a worker (e.g. imagine if a prefix was contained in every transaction, then the entire transaction database would be grouped under that key)?\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:34:12Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Synced offline with @mengxr, OK for this PR.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T23:27:39Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "nit: `data` -> `projectedDatabase`\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:36:29Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Also, this assumes that the entire `projectedDatabase` will fit on a single worker. In the worst case (e.g. if a prefix is in every transaction in the database), this assumes that the entire dataset will fit on a single worker.\n\nI wonder if there is a way to keep `data` as an `RDD` and make the iterations after the first iteration distributed as well.\n\n@mengxr \n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:47:30Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Synced offline with @mengxr, OK for this PR.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T23:27:55Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "L136 - 155 is very similar to `findPatternsLengthOne` except operating locally on `Array[Array[_]]` instead of an `RDD[_]` (which may be problematic). I wonder if we can refactor into a common method operating on `RDD[_]`?\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:42:56Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {\n+    val elements = data\n+      .map(x => x.distinct)\n+      .flatMap(x => x)\n+      .groupBy(x => x)\n+      .map(x => (x._1, x._2.length))\n+\n+    val selectedSingleElements = elements.filter(x => x._2 >= minSupport)\n+\n+    val selectedElements = selectedSingleElements\n+      .map(x => (prefix ++ Seq(x._1), x._2))\n+      .toArray\n+\n+    val cleanedSearchSpace = data\n+      .map(x => x.filter(y => selectedSingleElements.contains(y)))\n+\n+    val newSearchSpace = selectedSingleElements.map { x =>\n+      val sub = cleanedSearchSpace.map(y => getSuffix(x._1, y)).filter(_.nonEmpty)\n+      (prefix ++ Seq(x._1), sub)\n+    }.filter(x => x._2.nonEmpty)\n+      .toArray"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "@feynmanliang  findPatternsLengthOne and getPatternsWithPrefix do the similar work, but the input data type is different. findPatternsLengthOne faces the entire RDD data, getPatternsWithPrefix faces part of RDD data, but the content of the data is similar. Before shuffle, the prefix and the projected database distributed in all the partitions. After shuffle, one partition only including one prefix and its projected database. In other words, after shuffle, one partition of the new RDD is same as the old entire RDD. So, findPatternsLengthOne is different from getPatternsWithPrefix, eg. findPatternsLengthOne has some collect methods.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T08:47:49Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {\n+    val elements = data\n+      .map(x => x.distinct)\n+      .flatMap(x => x)\n+      .groupBy(x => x)\n+      .map(x => (x._1, x._2.length))\n+\n+    val selectedSingleElements = elements.filter(x => x._2 >= minSupport)\n+\n+    val selectedElements = selectedSingleElements\n+      .map(x => (prefix ++ Seq(x._1), x._2))\n+      .toArray\n+\n+    val cleanedSearchSpace = data\n+      .map(x => x.filter(y => selectedSingleElements.contains(y)))\n+\n+    val newSearchSpace = selectedSingleElements.map { x =>\n+      val sub = cleanedSearchSpace.map(y => getSuffix(x._1, y)).filter(_.nonEmpty)\n+      (prefix ++ Seq(x._1), sub)\n+    }.filter(x => x._2.nonEmpty)\n+      .toArray"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Gotcha :smile:. This is fine for this PR.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T17:53:21Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {\n+    val elements = data\n+      .map(x => x.distinct)\n+      .flatMap(x => x)\n+      .groupBy(x => x)\n+      .map(x => (x._1, x._2.length))\n+\n+    val selectedSingleElements = elements.filter(x => x._2 >= minSupport)\n+\n+    val selectedElements = selectedSingleElements\n+      .map(x => (prefix ++ Seq(x._1), x._2))\n+      .toArray\n+\n+    val cleanedSearchSpace = data\n+      .map(x => x.filter(y => selectedSingleElements.contains(y)))\n+\n+    val newSearchSpace = selectedSingleElements.map { x =>\n+      val sub = cleanedSearchSpace.map(y => getSuffix(x._1, y)).filter(_.nonEmpty)\n+      (prefix ++ Seq(x._1), sub)\n+    }.filter(x => x._2.nonEmpty)\n+      .toArray"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "nit: `newSearchSpace` -> `prefixProjectedDatabases`\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:50:13Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {\n+    val elements = data\n+      .map(x => x.distinct)\n+      .flatMap(x => x)\n+      .groupBy(x => x)\n+      .map(x => (x._1, x._2.length))\n+\n+    val selectedSingleElements = elements.filter(x => x._2 >= minSupport)\n+\n+    val selectedElements = selectedSingleElements\n+      .map(x => (prefix ++ Seq(x._1), x._2))\n+      .toArray\n+\n+    val cleanedSearchSpace = data\n+      .map(x => x.filter(y => selectedSingleElements.contains(y)))\n+\n+    val newSearchSpace = selectedSingleElements.map { x =>"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "fixed.\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T08:48:45Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {\n+    val elements = data\n+      .map(x => x.distinct)\n+      .flatMap(x => x)\n+      .groupBy(x => x)\n+      .map(x => (x._1, x._2.length))\n+\n+    val selectedSingleElements = elements.filter(x => x._2 >= minSupport)\n+\n+    val selectedElements = selectedSingleElements\n+      .map(x => (prefix ++ Seq(x._1), x._2))\n+      .toArray\n+\n+    val cleanedSearchSpace = data\n+      .map(x => x.filter(y => selectedSingleElements.contains(y)))\n+\n+    val newSearchSpace = selectedSingleElements.map { x =>"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "nit: `selectedElements` -> `frequentPrefixesAndCounts`\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:51:28Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {\n+    val elements = data\n+      .map(x => x.distinct)\n+      .flatMap(x => x)\n+      .groupBy(x => x)\n+      .map(x => (x._1, x._2.length))\n+\n+    val selectedSingleElements = elements.filter(x => x._2 >= minSupport)\n+\n+    val selectedElements = selectedSingleElements\n+      .map(x => (prefix ++ Seq(x._1), x._2))\n+      .toArray"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "fixed\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T07:20:38Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {\n+    val elements = data\n+      .map(x => x.distinct)\n+      .flatMap(x => x)\n+      .groupBy(x => x)\n+      .map(x => (x._1, x._2.length))\n+\n+    val selectedSingleElements = elements.filter(x => x._2 >= minSupport)\n+\n+    val selectedElements = selectedSingleElements\n+      .map(x => (prefix ++ Seq(x._1), x._2))\n+      .toArray"
  }],
  "prId": 7258
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "Only the keys of this `Map` are used later, how about calling `.keys()` here to avoid all the `._1`s later and to make logic clearer?\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:54:20Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {\n+    val elements = data\n+      .map(x => x.distinct)\n+      .flatMap(x => x)\n+      .groupBy(x => x)\n+      .map(x => (x._1, x._2.length))\n+\n+    val selectedSingleElements = elements.filter(x => x._2 >= minSupport)"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "nit: `selectedSingleElements` -> `frequentPrefixExtensions`\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-07T21:55:35Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {\n+    val elements = data\n+      .map(x => x.distinct)\n+      .flatMap(x => x)\n+      .groupBy(x => x)\n+      .map(x => (x._1, x._2.length))\n+\n+    val selectedSingleElements = elements.filter(x => x._2 >= minSupport)"
  }, {
    "author": {
      "login": "zhangjiajin"
    },
    "body": "Fixed\n",
    "commit": "078d4101f56c68c6f191de57f9e542a80f2c89b5",
    "createdAt": "2015-07-08T07:19:11Z",
    "diffHunk": "@@ -0,0 +1,183 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.fpm\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ *\n+ * A parallel PrefixSpan algorithm to mine sequential pattern.\n+ * The PrefixSpan algorithm is described in\n+ * [[http://web.engr.illinois.edu/~hanj/pdf/span01.pdf]].\n+ *\n+ * @param sequences original sequences data\n+ * @param minSupport the minimal support level of the sequential pattern, any pattern appears\n+ *                   more than minSupport times will be output\n+ * @param maxPatternLength the maximal length of the sequential pattern, any pattern appears\n+ *                   less than maxPatternLength will be output\n+ *\n+ * @see [[https://en.wikipedia.org/wiki/Sequential_Pattern_Mining Sequential Pattern Mining\n+ *       (Wikipedia)]]\n+ */\n+class Prefixspan(\n+    val sequences: RDD[Array[Int]],\n+    val minSupport: Int = 2,\n+    val maxPatternLength: Int = 50) extends java.io.Serializable {\n+\n+  /**\n+   * Calculate sequential patterns:\n+   * a) find and collect length-one patterns\n+   * b) for each length-one patterns and each sequence,\n+   *    emit (pattern (prefix), suffix sequence) as key-value pairs\n+   * c) group by key and then map value iterator to array\n+   * d) local PrefixSpan on each prefix\n+   * @return sequential patterns\n+   */\n+  def run(): RDD[(Seq[Int], Int)] = {\n+    val (patternsOneLength, prefixAndCandidates) = findPatternsLengthOne()\n+    val repartitionedRdd = repartitionSequences(prefixAndCandidates)\n+    val nextPatterns = getPatternsInLocal(repartitionedRdd)\n+    val allPatterns = patternsOneLength.map(x => (Seq(x._1), x._2)) ++ nextPatterns\n+    allPatterns\n+  }\n+\n+  /**\n+   * Find the patterns that it's length is one\n+   * @return length-one patterns and projection table\n+   */\n+  private def findPatternsLengthOne(): (RDD[(Int, Int)], RDD[(Seq[Int], Array[Int])]) = {\n+    val patternsOneLength = sequences\n+      .map(_.distinct)\n+      .flatMap(p => p)\n+      .map((_, 1))\n+      .reduceByKey(_ + _)\n+\n+    val removedElements: Array[Int] = patternsOneLength\n+      .filter(_._2 < minSupport)\n+      .map(_._1)\n+      .collect()\n+\n+    val savedElements = patternsOneLength.filter(_._2 >= minSupport)\n+\n+    val savedElementsArray = savedElements\n+      .map(_._1)\n+      .collect()\n+\n+    val filteredSequences =\n+      if (removedElements.isEmpty) {\n+        sequences\n+      } else {\n+        sequences.map { p =>\n+          p.filter { x => !removedElements.contains(x) }\n+        }\n+      }\n+\n+    val prefixAndCandidates = filteredSequences.flatMap { x =>\n+      savedElementsArray.map { y =>\n+        val sub = getSuffix(y, x)\n+        (Seq(y), sub)\n+      }\n+    }\n+\n+    (savedElements, prefixAndCandidates)\n+  }\n+\n+  /**\n+   * Re-partition the RDD data, to get better balance and performance.\n+   * @param data patterns and projected sequences data before re-partition\n+   * @return patterns and projected sequences data after re-partition\n+   */\n+  private def repartitionSequences(\n+      data: RDD[(Seq[Int], Array[Int])]): RDD[(Seq[Int], Array[Array[Int]])] = {\n+    val dataRemovedEmptyLine = data.filter(x => x._2.nonEmpty)\n+    val dataMerged = dataRemovedEmptyLine\n+      .groupByKey()\n+      .map(x => (x._1, x._2.toArray))\n+    dataMerged\n+  }\n+\n+  /**\n+   * calculate the patterns in local.\n+   * @param data patterns and projected sequences data data\n+   * @return patterns\n+   */\n+  private def getPatternsInLocal(\n+      data: RDD[(Seq[Int], Array[Array[Int]])]): RDD[(Seq[Int], Int)] = {\n+    val result = data.flatMap { x =>\n+      getPatternsWithPrefix(x._1, x._2)\n+    }\n+    result\n+  }\n+\n+  /**\n+   * calculate the patterns with one prefix in local.\n+   * @param prefix prefix\n+   * @param data patterns and projected sequences data\n+   * @return patterns\n+   */\n+  private def getPatternsWithPrefix(\n+      prefix: Seq[Int],\n+      data: Array[Array[Int]]): Array[(Seq[Int], Int)] = {\n+    val elements = data\n+      .map(x => x.distinct)\n+      .flatMap(x => x)\n+      .groupBy(x => x)\n+      .map(x => (x._1, x._2.length))\n+\n+    val selectedSingleElements = elements.filter(x => x._2 >= minSupport)"
  }],
  "prId": 7258
}]