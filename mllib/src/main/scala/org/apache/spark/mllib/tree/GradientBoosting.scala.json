[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "\"strategy\" is not used\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-17T23:39:42Z",
    "diffHunk": "@@ -0,0 +1,480 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import org.apache.spark.SparkContext._\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.{QuantileStrategy, BoostingStrategy}\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.impurity.Impurities\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{GradientBoostingModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): GradientBoostingModel = {\n+    val strategy = boostingStrategy.strategy"
  }],
  "prId": 2607
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "newline needed\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-17T23:39:47Z",
    "diffHunk": "@@ -0,0 +1,480 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import org.apache.spark.SparkContext._\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.{QuantileStrategy, BoostingStrategy}\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.impurity.Impurities\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{GradientBoostingModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): GradientBoostingModel = {\n+    val strategy = boostingStrategy.strategy\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): GradientBoostingModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param impurity Criterion used for information gain calculation.\n+   *                 Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]],\n+   *                  [[org.apache.spark.mllib.tree.impurity.Entropy]].\n+   *                 Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]].\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      impurity: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): GradientBoostingModel = {\n+    val lossType = Losses.fromString(loss)\n+    val impurityType = Impurities.fromString(impurity)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      impurityType, maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param impurity Criterion used for information gain calculation.\n+   *                 Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]],\n+   *                  [[org.apache.spark.mllib.tree.impurity.Entropy]].\n+   *                 Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]].\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      impurity: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): GradientBoostingModel = {\n+    val lossType = Losses.fromString(loss)\n+    val impurityType = Impurities.fromString(impurity)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      impurityType, maxDepth, learningRate, subsample, checkpointPeriod,\n+      numClassesForClassification, categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+  /**"
  }],
  "prId": 2607
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "trainClassification --> trainClassifier\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-17T23:39:49Z",
    "diffHunk": "@@ -0,0 +1,480 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import org.apache.spark.SparkContext._\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.{QuantileStrategy, BoostingStrategy}\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.impurity.Impurities\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{GradientBoostingModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): GradientBoostingModel = {\n+    val strategy = boostingStrategy.strategy\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): GradientBoostingModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param impurity Criterion used for information gain calculation.\n+   *                 Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]],\n+   *                  [[org.apache.spark.mllib.tree.impurity.Entropy]].\n+   *                 Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]].\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      impurity: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): GradientBoostingModel = {\n+    val lossType = Losses.fromString(loss)\n+    val impurityType = Impurities.fromString(impurity)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      impurityType, maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param impurity Criterion used for information gain calculation.\n+   *                 Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]],\n+   *                  [[org.apache.spark.mllib.tree.impurity.Entropy]].\n+   *                 Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]].\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      impurity: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): GradientBoostingModel = {\n+    val lossType = Losses.fromString(loss)\n+    val impurityType = Impurities.fromString(impurity)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      impurityType, maxDepth, learningRate, subsample, checkpointPeriod,\n+      numClassesForClassification, categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param impurity Criterion used for information gain calculation.\n+   *                 Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]],\n+   *                  [[org.apache.spark.mllib.tree.impurity.Entropy]].\n+   *                 Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]].\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      impurity: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : GradientBoostingModel = {\n+    val lossType = Losses.fromString(loss)\n+    val impurityType = Impurities.fromString(impurity)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      impurityType, maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int,\n+        Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param impurity Criterion used for information gain calculation.\n+   *                 Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]],\n+   *                  [[org.apache.spark.mllib.tree.impurity.Entropy]].\n+   *                 Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]].\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      impurity: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : GradientBoostingModel = {\n+    val lossType = Losses.fromString(loss)\n+    val impurityType = Impurities.fromString(impurity)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      impurityType, maxDepth, learningRate, subsample, checkpointPeriod,\n+      numClassesForClassification, categoricalFeaturesInfo =\n+      categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param impurity Criterion used for information gain calculation.\n+   *                 Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]],\n+   *                  [[org.apache.spark.mllib.tree.impurity.Entropy]].\n+   *                 Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]].\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      impurity: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): GradientBoostingModel = {\n+    val lossType = Losses.fromString(loss)\n+    val impurityType = Impurities.fromString(impurity)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      impurityType, maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param impurity Criterion used for information gain calculation.\n+   *                 Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]],\n+   *                  [[org.apache.spark.mllib.tree.impurity.Entropy]].\n+   *                 Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]].\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      impurity: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): GradientBoostingModel = {\n+    val lossType = Losses.fromString(loss)\n+    val impurityType = Impurities.fromString(impurity)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      impurityType, maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): GradientBoostingModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassification("
  }],
  "prId": 2607
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "This should probably just use the given loss function.  That way, if I'm trying to minimize say L1 error, this will track exactly that error.\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-17T23:39:52Z",
    "diffHunk": "@@ -0,0 +1,480 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import org.apache.spark.SparkContext._\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.{QuantileStrategy, BoostingStrategy}\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.impurity.Impurities\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{GradientBoostingModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): GradientBoostingModel = {\n+    val strategy = boostingStrategy.strategy\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): GradientBoostingModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param impurity Criterion used for information gain calculation.\n+   *                 Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]],\n+   *                  [[org.apache.spark.mllib.tree.impurity.Entropy]].\n+   *                 Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]].\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      impurity: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): GradientBoostingModel = {\n+    val lossType = Losses.fromString(loss)\n+    val impurityType = Impurities.fromString(impurity)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      impurityType, maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param impurity Criterion used for information gain calculation.\n+   *                 Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]],\n+   *                  [[org.apache.spark.mllib.tree.impurity.Entropy]].\n+   *                 Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]].\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      impurity: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): GradientBoostingModel = {\n+    val lossType = Losses.fromString(loss)\n+    val impurityType = Impurities.fromString(impurity)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      impurityType, maxDepth, learningRate, subsample, checkpointPeriod,\n+      numClassesForClassification, categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param impurity Criterion used for information gain calculation.\n+   *                 Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]],\n+   *                  [[org.apache.spark.mllib.tree.impurity.Entropy]].\n+   *                 Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]].\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      impurity: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : GradientBoostingModel = {\n+    val lossType = Losses.fromString(loss)\n+    val impurityType = Impurities.fromString(impurity)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      impurityType, maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int,\n+        Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param impurity Criterion used for information gain calculation.\n+   *                 Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]],\n+   *                  [[org.apache.spark.mllib.tree.impurity.Entropy]].\n+   *                 Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]].\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      impurity: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : GradientBoostingModel = {\n+    val lossType = Losses.fromString(loss)\n+    val impurityType = Impurities.fromString(impurity)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      impurityType, maxDepth, learningRate, subsample, checkpointPeriod,\n+      numClassesForClassification, categoricalFeaturesInfo =\n+      categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param impurity Criterion used for information gain calculation.\n+   *                 Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]],\n+   *                  [[org.apache.spark.mllib.tree.impurity.Entropy]].\n+   *                 Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]].\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      impurity: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): GradientBoostingModel = {\n+    val lossType = Losses.fromString(loss)\n+    val impurityType = Impurities.fromString(impurity)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      impurityType, maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param impurity Criterion used for information gain calculation.\n+   *                 Supported for Classification: [[org.apache.spark.mllib.tree.impurity.Gini]],\n+   *                  [[org.apache.spark.mllib.tree.impurity.Entropy]].\n+   *                 Supported for Regression: [[org.apache.spark.mllib.tree.impurity.Variance]].\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      impurity: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): GradientBoostingModel = {\n+    val lossType = Losses.fromString(loss)\n+    val impurityType = Impurities.fromString(impurity)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      impurityType, maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): GradientBoostingModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassification(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): GradientBoostingModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Classification, s\"Only Classification algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+\n+  /**\n+   * Internal method for performing regression using trees as base learners.\n+   * @param input training dataset\n+   * @param boostingStrategy boosting parameters\n+   * @return\n+   */\n+  private def boost(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): GradientBoostingModel = {\n+\n+    val timer = new TimeTracker()\n+\n+    timer.start(\"total\")\n+    timer.start(\"init\")\n+\n+\n+    // Initialize gradient boosting parameters\n+    val numEstimators = boostingStrategy.numEstimators\n+    val trees = new Array[DecisionTreeModel](numEstimators + 1)\n+    val loss = boostingStrategy.loss\n+    val learningRate = boostingStrategy.learningRate\n+    val subsample = boostingStrategy.subsample\n+    val checkpointingPeriod = boostingStrategy.checkpointPeriod\n+    val strategy = boostingStrategy.strategy\n+\n+    // Cache input\n+    input.persist(StorageLevel.MEMORY_AND_DISK)\n+    var lastCachedData = input\n+\n+    timer.stop(\"init\")\n+\n+    logDebug(\"##########\")\n+    logDebug(\"Building tree 0\")\n+    logDebug(\"##########\")\n+    var data = input\n+\n+    // 1. Initialize tree\n+    timer.start(\"building tree 0\")\n+    val firstModel = new DecisionTree(strategy).train(data)\n+    timer.stop(\"building tree 0\")\n+    trees(0) = firstModel\n+    logDebug(\"error of tree = \" + meanSquaredError(firstModel, data))\n+\n+    // psuedo-residual for second iteration\n+    data = data.map(point => LabeledPoint(loss.lossGradient(firstModel, point,\n+      learningRate), point.features))\n+\n+\n+    var m = 1\n+    while (m <= numEstimators) {\n+      timer.start(s\"building tree $m\")\n+      logDebug(\"###################################################\")\n+      logDebug(\"Gradient boosting tree iteration \" + m)\n+      logDebug(\"###################################################\")\n+      val model = new DecisionTree(strategy).train(data)\n+      timer.stop(s\"building tree $m\")\n+      trees(m) = model\n+      logDebug(\"error of tree = \" + meanSquaredError(model, data))\n+      // Update data with pseudo-residuals\n+      data = data.map(point => LabeledPoint(loss.lossGradient(model, point, learningRate),\n+        point.features))\n+      if (m % checkpointingPeriod == 1 && m != 1) {\n+        lastCachedData.unpersist()\n+      }\n+      // Checkpoint\n+      if (m % checkpointingPeriod == 0) {\n+        data = data.persist(StorageLevel.MEMORY_AND_DISK)\n+        lastCachedData = data\n+      }\n+      m += 1\n+    }\n+\n+    timer.stop(\"total\")\n+\n+    logInfo(\"Internal timing for DecisionTree:\")\n+    logInfo(s\"$timer\")\n+\n+\n+    // 3. Output classifier\n+    new GradientBoostingModel(trees, strategy.algo)\n+\n+  }\n+\n+  /**\n+   * Calculates the mean squared error for regression.\n+   */\n+  private def meanSquaredError(tree: DecisionTreeModel, data: RDD[LabeledPoint]): Double = {"
  }],
  "prId": 2607
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "\"regression\" --> \"binary classification and regression\"\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-28T19:42:27Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems."
  }],
  "prId": 2607
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "GradientBoostingModel --> WeightedEnsembleModel\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-28T19:42:31Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction"
  }],
  "prId": 2607
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "extra newline\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-28T19:42:36Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2, categoricalFeaturesInfo =\n+        categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod,\n+      numClassesForClassification, categoricalFeaturesInfo =\n+      categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Classification, s\"Only Classification algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Internal method for performing regression using trees as base learners.\n+   * @param input training dataset\n+   * @param boostingStrategy boosting parameters\n+   * @return\n+   */\n+  private def boost(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+\n+    val timer = new TimeTracker()\n+\n+    timer.start(\"total\")\n+    timer.start(\"init\")\n+"
  }],
  "prId": 2607
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Use same name: \"checkpointingPeriod\" or \"checkpointPeriod\"\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-28T19:42:38Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2, categoricalFeaturesInfo =\n+        categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod,\n+      numClassesForClassification, categoricalFeaturesInfo =\n+      categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Classification, s\"Only Classification algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Internal method for performing regression using trees as base learners.\n+   * @param input training dataset\n+   * @param boostingStrategy boosting parameters\n+   * @return\n+   */\n+  private def boost(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+\n+    val timer = new TimeTracker()\n+\n+    timer.start(\"total\")\n+    timer.start(\"init\")\n+\n+\n+    // Initialize gradient boosting parameters\n+    val numEstimators = boostingStrategy.numEstimators\n+    val baseLearners = new Array[DecisionTreeModel](numEstimators)\n+    val baseLearnerWeights = new Array[Double](numEstimators)\n+    val loss = boostingStrategy.loss\n+    val learningRate = boostingStrategy.learningRate\n+    val checkpointingPeriod = boostingStrategy.checkpointPeriod"
  }],
  "prId": 2607
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Use \"GradientBoosting$#trainRegressor\" instead of \"GradientBoosting#trainRegressor\" so doc processor handles object correctly (using Java reflection syntax).  Please update elsewhere too\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-28T19:42:41Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]"
  }, {
    "author": {
      "login": "manishamde"
    },
    "body": "Will do.\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-28T20:11:13Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]"
  }],
  "prId": 2607
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Could this please be renamed to checkpointInterval everywhere?  I just noticed this name is used throughout other parts of Spark, so we can follow that precedent.\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-28T22:19:33Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains."
  }, {
    "author": {
      "login": "manishamde"
    },
    "body": "Sure.\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-28T22:56:17Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains."
  }],
  "prId": 2607
}, {
  "comments": [{
    "author": {
      "login": "codedeft"
    },
    "body": "It seems to me that this will result in repetitive sampling/ re-discretization and etc. of the entire data set every iteration. Additionally, repersisting the entire dataset seems very expensive, in particular if the dataset (LabeledPoint) is initially coming from the disk.\n\nI think that the optimal thing to do is:\n1. Discretize the features and persist the entire discretized features only once.\n2. Calculate the new labels after each iteration, and create a separate RDD of these new labels, and persist them.\n3. zip the new labels with the discretized features and reuse the DecisionTree's regression logic.\n\nThis will require some modifications of internal DecisionTree.train but it seems to me the better thing to do.\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-29T19:01:24Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2, categoricalFeaturesInfo =\n+        categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod,\n+      numClassesForClassification, categoricalFeaturesInfo =\n+      categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Classification, s\"Only Classification algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Internal method for performing regression using trees as base learners.\n+   * @param input training dataset\n+   * @param boostingStrategy boosting parameters\n+   * @return\n+   */\n+  private def boost(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+\n+    val timer = new TimeTracker()\n+\n+    timer.start(\"total\")\n+    timer.start(\"init\")\n+\n+\n+    // Initialize gradient boosting parameters\n+    val numEstimators = boostingStrategy.numEstimators\n+    val baseLearners = new Array[DecisionTreeModel](numEstimators)\n+    val baseLearnerWeights = new Array[Double](numEstimators)\n+    val loss = boostingStrategy.loss\n+    val learningRate = boostingStrategy.learningRate\n+    val checkpointingPeriod = boostingStrategy.checkpointPeriod\n+    val strategy = boostingStrategy.strategy\n+\n+    // Cache input\n+    input.persist(StorageLevel.MEMORY_AND_DISK)\n+    // Dataset reference for keeping track of last cached dataset in memory.\n+    var lastCachedData = input\n+    // Dataset reference for noting dataset marked for unpersisting.\n+    var unpersistData = lastCachedData\n+\n+    timer.stop(\"init\")\n+\n+    logDebug(\"##########\")\n+    logDebug(\"Building tree 0\")\n+    logDebug(\"##########\")\n+    var data = input\n+\n+    // 1. Initialize tree\n+    timer.start(\"building tree 0\")\n+    val firstModel = new DecisionTree(strategy).train(data)\n+    timer.stop(\"building tree 0\")\n+    baseLearners(0) = firstModel\n+    baseLearnerWeights(0) = 1.0\n+    logDebug(\"error of tree = \" + loss.computeError(firstModel, data))\n+\n+    // psuedo-residual for second iteration\n+    data = data.map(point => LabeledPoint(loss.lossGradient(firstModel, point,\n+      learningRate), point.features))\n+\n+    var m = 1\n+    while (m < numEstimators) {\n+      timer.start(s\"building tree $m\")\n+      logDebug(\"###################################################\")\n+      logDebug(\"Gradient boosting tree iteration \" + m)\n+      logDebug(\"###################################################\")\n+      val model = new DecisionTree(strategy).train(data)",
    "line": 288
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "Based on @manishamde 's PR description, I think the plan is to do this optimization later.  Keeping it a separate PR is helpful for reducing conflict with your node ID caching PR [https://github.com/apache/spark/pull/2868].  I feel like it is easier to break things into smaller PRs.  Also, since this type of optimization will likely be useful for other meta-algorithms, it will be good to think about a standard interface for getting a learning algorithm's internal data representation (and the related prediction methods which take that internal representation).\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-29T19:20:44Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2, categoricalFeaturesInfo =\n+        categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod,\n+      numClassesForClassification, categoricalFeaturesInfo =\n+      categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Classification, s\"Only Classification algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Internal method for performing regression using trees as base learners.\n+   * @param input training dataset\n+   * @param boostingStrategy boosting parameters\n+   * @return\n+   */\n+  private def boost(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+\n+    val timer = new TimeTracker()\n+\n+    timer.start(\"total\")\n+    timer.start(\"init\")\n+\n+\n+    // Initialize gradient boosting parameters\n+    val numEstimators = boostingStrategy.numEstimators\n+    val baseLearners = new Array[DecisionTreeModel](numEstimators)\n+    val baseLearnerWeights = new Array[Double](numEstimators)\n+    val loss = boostingStrategy.loss\n+    val learningRate = boostingStrategy.learningRate\n+    val checkpointingPeriod = boostingStrategy.checkpointPeriod\n+    val strategy = boostingStrategy.strategy\n+\n+    // Cache input\n+    input.persist(StorageLevel.MEMORY_AND_DISK)\n+    // Dataset reference for keeping track of last cached dataset in memory.\n+    var lastCachedData = input\n+    // Dataset reference for noting dataset marked for unpersisting.\n+    var unpersistData = lastCachedData\n+\n+    timer.stop(\"init\")\n+\n+    logDebug(\"##########\")\n+    logDebug(\"Building tree 0\")\n+    logDebug(\"##########\")\n+    var data = input\n+\n+    // 1. Initialize tree\n+    timer.start(\"building tree 0\")\n+    val firstModel = new DecisionTree(strategy).train(data)\n+    timer.stop(\"building tree 0\")\n+    baseLearners(0) = firstModel\n+    baseLearnerWeights(0) = 1.0\n+    logDebug(\"error of tree = \" + loss.computeError(firstModel, data))\n+\n+    // psuedo-residual for second iteration\n+    data = data.map(point => LabeledPoint(loss.lossGradient(firstModel, point,\n+      learningRate), point.features))\n+\n+    var m = 1\n+    while (m < numEstimators) {\n+      timer.start(s\"building tree $m\")\n+      logDebug(\"###################################################\")\n+      logDebug(\"Gradient boosting tree iteration \" + m)\n+      logDebug(\"###################################################\")\n+      val model = new DecisionTree(strategy).train(data)",
    "line": 288
  }, {
    "author": {
      "login": "codedeft"
    },
    "body": "Makes sense. So I suppose you want to provide the functionality first and then optimize later ;).\n\nI'm not sure though about whether this is going to result in re-reading from the disk the input at every iteration. Maybe I'm wrong. But a simple change could be simply persisting features all the time, and re-persisting newly calculated labels periodically.\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-29T20:02:45Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2, categoricalFeaturesInfo =\n+        categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod,\n+      numClassesForClassification, categoricalFeaturesInfo =\n+      categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Classification, s\"Only Classification algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Internal method for performing regression using trees as base learners.\n+   * @param input training dataset\n+   * @param boostingStrategy boosting parameters\n+   * @return\n+   */\n+  private def boost(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+\n+    val timer = new TimeTracker()\n+\n+    timer.start(\"total\")\n+    timer.start(\"init\")\n+\n+\n+    // Initialize gradient boosting parameters\n+    val numEstimators = boostingStrategy.numEstimators\n+    val baseLearners = new Array[DecisionTreeModel](numEstimators)\n+    val baseLearnerWeights = new Array[Double](numEstimators)\n+    val loss = boostingStrategy.loss\n+    val learningRate = boostingStrategy.learningRate\n+    val checkpointingPeriod = boostingStrategy.checkpointPeriod\n+    val strategy = boostingStrategy.strategy\n+\n+    // Cache input\n+    input.persist(StorageLevel.MEMORY_AND_DISK)\n+    // Dataset reference for keeping track of last cached dataset in memory.\n+    var lastCachedData = input\n+    // Dataset reference for noting dataset marked for unpersisting.\n+    var unpersistData = lastCachedData\n+\n+    timer.stop(\"init\")\n+\n+    logDebug(\"##########\")\n+    logDebug(\"Building tree 0\")\n+    logDebug(\"##########\")\n+    var data = input\n+\n+    // 1. Initialize tree\n+    timer.start(\"building tree 0\")\n+    val firstModel = new DecisionTree(strategy).train(data)\n+    timer.stop(\"building tree 0\")\n+    baseLearners(0) = firstModel\n+    baseLearnerWeights(0) = 1.0\n+    logDebug(\"error of tree = \" + loss.computeError(firstModel, data))\n+\n+    // psuedo-residual for second iteration\n+    data = data.map(point => LabeledPoint(loss.lossGradient(firstModel, point,\n+      learningRate), point.features))\n+\n+    var m = 1\n+    while (m < numEstimators) {\n+      timer.start(s\"building tree $m\")\n+      logDebug(\"###################################################\")\n+      logDebug(\"Gradient boosting tree iteration \" + m)\n+      logDebug(\"###################################################\")\n+      val model = new DecisionTree(strategy).train(data)",
    "line": 288
  }, {
    "author": {
      "login": "manishamde"
    },
    "body": "@codedeft Thanks for your comment. Your observation is correct. Conversion to the internal discretized/binned storage format will definitely lead to a faster implementation and lower memory consumption on the cluster. As @jkbradley mentioned, we decided to work on it after the generic MLlib API work has been completed. We can then use methods such as `trainUsingInternalFormat` and `predictUsingInternalFormat` if the underlying algo (in this case DecisionTree) supports it.\n\nWe won't be re-reading from disk at every iteration but caching the training data at the first iteration and checkpointing/persisting every few iterations to avoid long lineage chains. Will comment on the checkpointing further in the other thread.\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-29T20:18:14Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2, categoricalFeaturesInfo =\n+        categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod,\n+      numClassesForClassification, categoricalFeaturesInfo =\n+      categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Classification, s\"Only Classification algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Internal method for performing regression using trees as base learners.\n+   * @param input training dataset\n+   * @param boostingStrategy boosting parameters\n+   * @return\n+   */\n+  private def boost(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+\n+    val timer = new TimeTracker()\n+\n+    timer.start(\"total\")\n+    timer.start(\"init\")\n+\n+\n+    // Initialize gradient boosting parameters\n+    val numEstimators = boostingStrategy.numEstimators\n+    val baseLearners = new Array[DecisionTreeModel](numEstimators)\n+    val baseLearnerWeights = new Array[Double](numEstimators)\n+    val loss = boostingStrategy.loss\n+    val learningRate = boostingStrategy.learningRate\n+    val checkpointingPeriod = boostingStrategy.checkpointPeriod\n+    val strategy = boostingStrategy.strategy\n+\n+    // Cache input\n+    input.persist(StorageLevel.MEMORY_AND_DISK)\n+    // Dataset reference for keeping track of last cached dataset in memory.\n+    var lastCachedData = input\n+    // Dataset reference for noting dataset marked for unpersisting.\n+    var unpersistData = lastCachedData\n+\n+    timer.stop(\"init\")\n+\n+    logDebug(\"##########\")\n+    logDebug(\"Building tree 0\")\n+    logDebug(\"##########\")\n+    var data = input\n+\n+    // 1. Initialize tree\n+    timer.start(\"building tree 0\")\n+    val firstModel = new DecisionTree(strategy).train(data)\n+    timer.stop(\"building tree 0\")\n+    baseLearners(0) = firstModel\n+    baseLearnerWeights(0) = 1.0\n+    logDebug(\"error of tree = \" + loss.computeError(firstModel, data))\n+\n+    // psuedo-residual for second iteration\n+    data = data.map(point => LabeledPoint(loss.lossGradient(firstModel, point,\n+      learningRate), point.features))\n+\n+    var m = 1\n+    while (m < numEstimators) {\n+      timer.start(s\"building tree $m\")\n+      logDebug(\"###################################################\")\n+      logDebug(\"Gradient boosting tree iteration \" + m)\n+      logDebug(\"###################################################\")\n+      val model = new DecisionTree(strategy).train(data)",
    "line": 288
  }, {
    "author": {
      "login": "codedeft"
    },
    "body": "One more thing, I think that the decision tree itself does persisting of discretized data. So it seems that this could potentially require doubly persisted datasets (one LabeledPoint and the other one TreePoint)?\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-29T20:24:19Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2, categoricalFeaturesInfo =\n+        categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod,\n+      numClassesForClassification, categoricalFeaturesInfo =\n+      categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Classification, s\"Only Classification algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Internal method for performing regression using trees as base learners.\n+   * @param input training dataset\n+   * @param boostingStrategy boosting parameters\n+   * @return\n+   */\n+  private def boost(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+\n+    val timer = new TimeTracker()\n+\n+    timer.start(\"total\")\n+    timer.start(\"init\")\n+\n+\n+    // Initialize gradient boosting parameters\n+    val numEstimators = boostingStrategy.numEstimators\n+    val baseLearners = new Array[DecisionTreeModel](numEstimators)\n+    val baseLearnerWeights = new Array[Double](numEstimators)\n+    val loss = boostingStrategy.loss\n+    val learningRate = boostingStrategy.learningRate\n+    val checkpointingPeriod = boostingStrategy.checkpointPeriod\n+    val strategy = boostingStrategy.strategy\n+\n+    // Cache input\n+    input.persist(StorageLevel.MEMORY_AND_DISK)\n+    // Dataset reference for keeping track of last cached dataset in memory.\n+    var lastCachedData = input\n+    // Dataset reference for noting dataset marked for unpersisting.\n+    var unpersistData = lastCachedData\n+\n+    timer.stop(\"init\")\n+\n+    logDebug(\"##########\")\n+    logDebug(\"Building tree 0\")\n+    logDebug(\"##########\")\n+    var data = input\n+\n+    // 1. Initialize tree\n+    timer.start(\"building tree 0\")\n+    val firstModel = new DecisionTree(strategy).train(data)\n+    timer.stop(\"building tree 0\")\n+    baseLearners(0) = firstModel\n+    baseLearnerWeights(0) = 1.0\n+    logDebug(\"error of tree = \" + loss.computeError(firstModel, data))\n+\n+    // psuedo-residual for second iteration\n+    data = data.map(point => LabeledPoint(loss.lossGradient(firstModel, point,\n+      learningRate), point.features))\n+\n+    var m = 1\n+    while (m < numEstimators) {\n+      timer.start(s\"building tree $m\")\n+      logDebug(\"###################################################\")\n+      logDebug(\"Gradient boosting tree iteration \" + m)\n+      logDebug(\"###################################################\")\n+      val model = new DecisionTree(strategy).train(data)",
    "line": 288
  }, {
    "author": {
      "login": "manishamde"
    },
    "body": "Correct. That's the big disadvantage of not using the internal format. It won't affect other algos as much since there is no discretization.\n\nWe have a few options:\n1. Keep the implementation as is and inform the user about memory requirements.\n2. Persisting RDD[TreePoint] is essential since we perform multiple passes on it during each tree construction and reading RDD[LabeledPoint] from disk every time.\n3. Persisting RDD[LabeledPoint] and not caching RDD[TreePoint] during tree construction leading to repeated LabeledPoint -> TreePoint conversions for each NodeGroup.\n\nThoughts? cc: @jkbradley \n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-29T20:34:34Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2, categoricalFeaturesInfo =\n+        categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod,\n+      numClassesForClassification, categoricalFeaturesInfo =\n+      categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Classification, s\"Only Classification algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Internal method for performing regression using trees as base learners.\n+   * @param input training dataset\n+   * @param boostingStrategy boosting parameters\n+   * @return\n+   */\n+  private def boost(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+\n+    val timer = new TimeTracker()\n+\n+    timer.start(\"total\")\n+    timer.start(\"init\")\n+\n+\n+    // Initialize gradient boosting parameters\n+    val numEstimators = boostingStrategy.numEstimators\n+    val baseLearners = new Array[DecisionTreeModel](numEstimators)\n+    val baseLearnerWeights = new Array[Double](numEstimators)\n+    val loss = boostingStrategy.loss\n+    val learningRate = boostingStrategy.learningRate\n+    val checkpointingPeriod = boostingStrategy.checkpointPeriod\n+    val strategy = boostingStrategy.strategy\n+\n+    // Cache input\n+    input.persist(StorageLevel.MEMORY_AND_DISK)\n+    // Dataset reference for keeping track of last cached dataset in memory.\n+    var lastCachedData = input\n+    // Dataset reference for noting dataset marked for unpersisting.\n+    var unpersistData = lastCachedData\n+\n+    timer.stop(\"init\")\n+\n+    logDebug(\"##########\")\n+    logDebug(\"Building tree 0\")\n+    logDebug(\"##########\")\n+    var data = input\n+\n+    // 1. Initialize tree\n+    timer.start(\"building tree 0\")\n+    val firstModel = new DecisionTree(strategy).train(data)\n+    timer.stop(\"building tree 0\")\n+    baseLearners(0) = firstModel\n+    baseLearnerWeights(0) = 1.0\n+    logDebug(\"error of tree = \" + loss.computeError(firstModel, data))\n+\n+    // psuedo-residual for second iteration\n+    data = data.map(point => LabeledPoint(loss.lossGradient(firstModel, point,\n+      learningRate), point.features))\n+\n+    var m = 1\n+    while (m < numEstimators) {\n+      timer.start(s\"building tree $m\")\n+      logDebug(\"###################################################\")\n+      logDebug(\"Gradient boosting tree iteration \" + m)\n+      logDebug(\"###################################################\")\n+      val model = new DecisionTree(strategy).train(data)",
    "line": 288
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "Eventually, I envision:\n(1) GradientBoosting gets 1 copy of the data from the weak learner (RDD[TreePoint] for DecisionTree) and persists it.\n(2) DecisionTree persists the NodeIdCache (possibly storing 2 copies of the cache).  GradientBoosting tells DecisionTree not to serialize anything.\n(3) GradientBoosting persists the label (as @codedeft suggested) only, and periodically serializes it.\n\nFor now, I saw we either keep it as is and add a warning, or spend a little time refactoring to just persist the labels per the suggestion from @codedeft \n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-29T21:09:50Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod, 2, categoricalFeaturesInfo =\n+        categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @param checkpointPeriod Checkpointing the dataset in memory to avoid long lineage chains.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double,\n+      checkpointPeriod: Int,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample, checkpointPeriod,\n+      numClassesForClassification, categoricalFeaturesInfo =\n+      categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsample  Fraction of the training data used for learning the decision tree.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsample: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsample)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return GradientBoostingModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Classification, s\"Only Classification algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Internal method for performing regression using trees as base learners.\n+   * @param input training dataset\n+   * @param boostingStrategy boosting parameters\n+   * @return\n+   */\n+  private def boost(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+\n+    val timer = new TimeTracker()\n+\n+    timer.start(\"total\")\n+    timer.start(\"init\")\n+\n+\n+    // Initialize gradient boosting parameters\n+    val numEstimators = boostingStrategy.numEstimators\n+    val baseLearners = new Array[DecisionTreeModel](numEstimators)\n+    val baseLearnerWeights = new Array[Double](numEstimators)\n+    val loss = boostingStrategy.loss\n+    val learningRate = boostingStrategy.learningRate\n+    val checkpointingPeriod = boostingStrategy.checkpointPeriod\n+    val strategy = boostingStrategy.strategy\n+\n+    // Cache input\n+    input.persist(StorageLevel.MEMORY_AND_DISK)\n+    // Dataset reference for keeping track of last cached dataset in memory.\n+    var lastCachedData = input\n+    // Dataset reference for noting dataset marked for unpersisting.\n+    var unpersistData = lastCachedData\n+\n+    timer.stop(\"init\")\n+\n+    logDebug(\"##########\")\n+    logDebug(\"Building tree 0\")\n+    logDebug(\"##########\")\n+    var data = input\n+\n+    // 1. Initialize tree\n+    timer.start(\"building tree 0\")\n+    val firstModel = new DecisionTree(strategy).train(data)\n+    timer.stop(\"building tree 0\")\n+    baseLearners(0) = firstModel\n+    baseLearnerWeights(0) = 1.0\n+    logDebug(\"error of tree = \" + loss.computeError(firstModel, data))\n+\n+    // psuedo-residual for second iteration\n+    data = data.map(point => LabeledPoint(loss.lossGradient(firstModel, point,\n+      learningRate), point.features))\n+\n+    var m = 1\n+    while (m < numEstimators) {\n+      timer.start(s\"building tree $m\")\n+      logDebug(\"###################################################\")\n+      logDebug(\"Gradient boosting tree iteration \" + m)\n+      logDebug(\"###################################################\")\n+      val model = new DecisionTree(strategy).train(data)",
    "line": 288
  }],
  "prId": 2607
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "This should be learningRate too, right?\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-30T21:59:37Z",
    "diffHunk": "@@ -0,0 +1,412 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression and binary classification problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting$#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting$#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, 2, categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting$#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, 2, categoricalFeaturesInfo =\n+        categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting$#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int,\n+        Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Classification, s\"Only Classification algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Internal method for performing regression using trees as base learners.\n+   * @param input training dataset\n+   * @param boostingStrategy boosting parameters\n+   * @return\n+   */\n+  private def boost(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+\n+    val timer = new TimeTracker()\n+    timer.start(\"total\")\n+    timer.start(\"init\")\n+\n+    // Initialize gradient boosting parameters\n+    val numEstimators = boostingStrategy.numEstimators\n+    val baseLearners = new Array[DecisionTreeModel](numEstimators)\n+    val baseLearnerWeights = new Array[Double](numEstimators)\n+    val loss = boostingStrategy.loss\n+    val learningRate = boostingStrategy.learningRate\n+    val strategy = boostingStrategy.strategy\n+\n+    // Cache input\n+    input.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    timer.stop(\"init\")\n+\n+    logDebug(\"##########\")\n+    logDebug(\"Building tree 0\")\n+    logDebug(\"##########\")\n+    var data = input\n+\n+    // 1. Initialize tree\n+    timer.start(\"building tree 0\")\n+    val firstTreeModel = new DecisionTree(strategy).train(data)\n+    baseLearners(0) = firstTreeModel\n+    baseLearnerWeights(0) = 1.0",
    "line": 271
  }, {
    "author": {
      "login": "manishamde"
    },
    "body": "I think the learning rate is applied after the first model.\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-30T22:05:49Z",
    "diffHunk": "@@ -0,0 +1,412 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression and binary classification problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting$#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting$#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, 2, categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting$#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, 2, categoricalFeaturesInfo =\n+        categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting$#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int,\n+        Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Classification, s\"Only Classification algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Internal method for performing regression using trees as base learners.\n+   * @param input training dataset\n+   * @param boostingStrategy boosting parameters\n+   * @return\n+   */\n+  private def boost(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+\n+    val timer = new TimeTracker()\n+    timer.start(\"total\")\n+    timer.start(\"init\")\n+\n+    // Initialize gradient boosting parameters\n+    val numEstimators = boostingStrategy.numEstimators\n+    val baseLearners = new Array[DecisionTreeModel](numEstimators)\n+    val baseLearnerWeights = new Array[Double](numEstimators)\n+    val loss = boostingStrategy.loss\n+    val learningRate = boostingStrategy.learningRate\n+    val strategy = boostingStrategy.strategy\n+\n+    // Cache input\n+    input.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    timer.stop(\"init\")\n+\n+    logDebug(\"##########\")\n+    logDebug(\"Building tree 0\")\n+    logDebug(\"##########\")\n+    var data = input\n+\n+    // 1. Initialize tree\n+    timer.start(\"building tree 0\")\n+    val firstTreeModel = new DecisionTree(strategy).train(data)\n+    baseLearners(0) = firstTreeModel\n+    baseLearnerWeights(0) = 1.0",
    "line": 271
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "In the Friedman paper, the first \"model\" is just the average label (for squared error).  I think it's fine to keep it as is; that way, running for just 1 iteration will behave reasonably.\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-30T22:46:03Z",
    "diffHunk": "@@ -0,0 +1,412 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression and binary classification problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting$#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting$#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, 2, categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting$#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, 2, categoricalFeaturesInfo =\n+        categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting$#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int,\n+        Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Classification, s\"Only Classification algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Internal method for performing regression using trees as base learners.\n+   * @param input training dataset\n+   * @param boostingStrategy boosting parameters\n+   * @return\n+   */\n+  private def boost(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+\n+    val timer = new TimeTracker()\n+    timer.start(\"total\")\n+    timer.start(\"init\")\n+\n+    // Initialize gradient boosting parameters\n+    val numEstimators = boostingStrategy.numEstimators\n+    val baseLearners = new Array[DecisionTreeModel](numEstimators)\n+    val baseLearnerWeights = new Array[Double](numEstimators)\n+    val loss = boostingStrategy.loss\n+    val learningRate = boostingStrategy.learningRate\n+    val strategy = boostingStrategy.strategy\n+\n+    // Cache input\n+    input.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    timer.stop(\"init\")\n+\n+    logDebug(\"##########\")\n+    logDebug(\"Building tree 0\")\n+    logDebug(\"##########\")\n+    var data = input\n+\n+    // 1. Initialize tree\n+    timer.start(\"building tree 0\")\n+    val firstTreeModel = new DecisionTree(strategy).train(data)\n+    baseLearners(0) = firstTreeModel\n+    baseLearnerWeights(0) = 1.0",
    "line": 271
  }, {
    "author": {
      "login": "manishamde"
    },
    "body": "Yup.\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-31T00:18:12Z",
    "diffHunk": "@@ -0,0 +1,412 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression and binary classification problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting$#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting$#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, 2, categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting$#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, 2, categoricalFeaturesInfo =\n+        categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting$#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int,\n+        Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Classification, s\"Only Classification algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Internal method for performing regression using trees as base learners.\n+   * @param input training dataset\n+   * @param boostingStrategy boosting parameters\n+   * @return\n+   */\n+  private def boost(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+\n+    val timer = new TimeTracker()\n+    timer.start(\"total\")\n+    timer.start(\"init\")\n+\n+    // Initialize gradient boosting parameters\n+    val numEstimators = boostingStrategy.numEstimators\n+    val baseLearners = new Array[DecisionTreeModel](numEstimators)\n+    val baseLearnerWeights = new Array[Double](numEstimators)\n+    val loss = boostingStrategy.loss\n+    val learningRate = boostingStrategy.learningRate\n+    val strategy = boostingStrategy.strategy\n+\n+    // Cache input\n+    input.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    timer.stop(\"init\")\n+\n+    logDebug(\"##########\")\n+    logDebug(\"Building tree 0\")\n+    logDebug(\"##########\")\n+    var data = input\n+\n+    // 1. Initialize tree\n+    timer.start(\"building tree 0\")\n+    val firstTreeModel = new DecisionTree(strategy).train(data)\n+    baseLearners(0) = firstTreeModel\n+    baseLearnerWeights(0) = 1.0",
    "line": 271
  }],
  "prId": 2607
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "(learningRate)\n",
    "commit": "991c7b58f4648693e7b01ef756d032cc51980eec",
    "createdAt": "2014-10-30T21:59:59Z",
    "diffHunk": "@@ -0,0 +1,412 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.tree\n+\n+import scala.collection.JavaConverters._\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.api.java.JavaRDD\n+import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.tree.impl.TimeTracker\n+import org.apache.spark.mllib.tree.loss.Losses\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.tree.model.{WeightedEnsembleModel, DecisionTreeModel}\n+import org.apache.spark.mllib.tree.configuration.Algo._\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.tree.configuration.EnsembleCombiningStrategy.Sum\n+\n+/**\n+ * :: Experimental ::\n+ * A class that implements gradient boosting for regression and binary classification problems.\n+ * @param boostingStrategy Parameters for the gradient boosting algorithm\n+ */\n+@Experimental\n+class GradientBoosting (\n+    private val boostingStrategy: BoostingStrategy) extends Serializable with Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def train(input: RDD[LabeledPoint]): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    algo match {\n+      case Regression => GradientBoosting.boost(input, boostingStrategy)\n+      case Classification =>\n+        val remappedInput = input.map(x => new LabeledPoint((x.label * 2) - 1, x.features))\n+        GradientBoosting.boost(remappedInput, boostingStrategy)\n+      case _ =>\n+        throw new IllegalArgumentException(s\"$algo is not supported by the gradient boosting.\")\n+    }\n+  }\n+\n+}\n+\n+\n+object GradientBoosting extends Logging {\n+\n+  /**\n+   * Method to train a gradient boosting model.\n+   *\n+   * Note: Using [[org.apache.spark.mllib.tree.GradientBoosting$#trainRegressor]]\n+   *       is recommended to clearly specify regression.\n+   *       Using [[org.apache.spark.mllib.tree.GradientBoosting$#trainClassifier]]\n+   *       is recommended to clearly specify regression.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def train(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, 2, categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: Map[Int, Int]): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting$#trainRegressor]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, 2, categoricalFeaturesInfo =\n+        categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int, Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Java-friendly API for [[org.apache.spark.mllib.tree.GradientBoosting$#trainClassifier]]\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @param numClassesForClassification Number of classes for classification.\n+   *                                    (Ignored for regression.)\n+   *                                    Default value is 2 (binary classification).\n+   * @param categoricalFeaturesInfo A map storing information about the categorical variables and\n+   *                                the number of discrete values they take. For example,\n+   *                                an entry (n -> k) implies the feature n is categorical with k\n+   *                                categories 0, 1, 2, ... , k-1. It's important to note that\n+   *                                features are zero-indexed.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: JavaRDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double,\n+      numClassesForClassification: Int,\n+      categoricalFeaturesInfo: java.util.Map[java.lang.Integer, java.lang.Integer])\n+      : WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate, numClassesForClassification,\n+      categoricalFeaturesInfo = categoricalFeaturesInfo.asInstanceOf[java.util.Map[Int,\n+        Int]].asScala.toMap)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting binary classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param numEstimators Number of estimators used in boosting stages. In other words,\n+   *                      number of boosting iterations performed.\n+   * @param loss Loss function used for minimization during gradient boosting.\n+   * @param maxDepth Maximum depth of the tree.\n+   *                 E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\n+   * @param learningRate Learning rate for shrinking the contribution of each estimator. The\n+   *                     learning rate should be between in the interval (0, 1]\n+   * @param subsamplingRate  Fraction of the training data used for learning the decision tree.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      numEstimators: Int,\n+      loss: String,\n+      maxDepth: Int,\n+      learningRate: Double,\n+      subsamplingRate: Double): WeightedEnsembleModel = {\n+    val lossType = Losses.fromString(loss)\n+    val boostingStrategy = new BoostingStrategy(Regression, numEstimators, lossType,\n+      maxDepth, learningRate, subsamplingRate)\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting regression model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainRegressor(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Regression, s\"Only Regression algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Method to train a gradient boosting classification model.\n+   *\n+   * @param input Training dataset: RDD of [[org.apache.spark.mllib.regression.LabeledPoint]].\n+   *              For classification, labels should take values {0, 1, ..., numClasses-1}.\n+   *              For regression, labels are real numbers.\n+   * @param boostingStrategy Configuration options for the boosting algorithm.\n+   * @return WeightedEnsembleModel that can be used for prediction\n+   */\n+  def trainClassifier(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+    val algo = boostingStrategy.algo\n+    require(algo == Classification, s\"Only Classification algo supported. Provided algo is $algo.\")\n+    new GradientBoosting(boostingStrategy).train(input)\n+  }\n+\n+  /**\n+   * Internal method for performing regression using trees as base learners.\n+   * @param input training dataset\n+   * @param boostingStrategy boosting parameters\n+   * @return\n+   */\n+  private def boost(\n+      input: RDD[LabeledPoint],\n+      boostingStrategy: BoostingStrategy): WeightedEnsembleModel = {\n+\n+    val timer = new TimeTracker()\n+    timer.start(\"total\")\n+    timer.start(\"init\")\n+\n+    // Initialize gradient boosting parameters\n+    val numEstimators = boostingStrategy.numEstimators\n+    val baseLearners = new Array[DecisionTreeModel](numEstimators)\n+    val baseLearnerWeights = new Array[Double](numEstimators)\n+    val loss = boostingStrategy.loss\n+    val learningRate = boostingStrategy.learningRate\n+    val strategy = boostingStrategy.strategy\n+\n+    // Cache input\n+    input.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    timer.stop(\"init\")\n+\n+    logDebug(\"##########\")\n+    logDebug(\"Building tree 0\")\n+    logDebug(\"##########\")\n+    var data = input\n+\n+    // 1. Initialize tree\n+    timer.start(\"building tree 0\")\n+    val firstTreeModel = new DecisionTree(strategy).train(data)\n+    baseLearners(0) = firstTreeModel\n+    baseLearnerWeights(0) = 1.0\n+    val startingModel = new WeightedEnsembleModel(Array(firstTreeModel), Array(1.0), Regression,",
    "line": 272
  }],
  "prId": 2607
}]