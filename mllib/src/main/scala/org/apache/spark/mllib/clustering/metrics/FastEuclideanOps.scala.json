[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove extra empty lines\n",
    "commit": "35da8e9e188e669460000d5799d061ecc3ca150f",
    "createdAt": "2014-10-06T21:50:02Z",
    "diffHunk": "@@ -0,0 +1,80 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering.metrics\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV}\n+import org.apache.spark.mllib.base._\n+import org.apache.spark.mllib.linalg.{SparseVector, DenseVector, Vector}\n+\n+import org.apache.spark.mllib.base.{Centroid, FPoint, PointOps, Infinity, Zero}\n+\n+"
  }],
  "prId": 2634
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Should `weight` be only used in aggregation rather than distance computation?\n",
    "commit": "35da8e9e188e669460000d5799d061ecc3ca150f",
    "createdAt": "2015-01-08T23:56:38Z",
    "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering.metrics\n+\n+import breeze.linalg.{ DenseVector => BDV, SparseVector => BSV, Vector => BV }\n+\n+import org.apache.spark.mllib.base._\n+import org.apache.spark.mllib.linalg.{ SparseVector, DenseVector, Vector }\n+import org.apache.spark.mllib.base.{ Centroid, FPoint, PointOps, Infinity, Zero }\n+\n+class FastEUPoint(raw: BV[Double], weight: Double) extends FPoint(raw, weight) {\n+  val norm = if (weight == Zero) Zero else raw.dot(raw) / (weight * weight)",
    "line": 27
  }, {
    "author": {
      "login": "derrickburns"
    },
    "body": "It is most efficient to maintain the cluster centers in homogeneous\ncoordinates.\n\nOn Thu, Jan 8, 2015 at 3:56 PM, Xiangrui Meng notifications@github.com\nwrote:\n\n> In\n> mllib/src/main/scala/org/apache/spark/mllib/clustering/metrics/FastEuclideanOps.scala\n> https://github.com/apache/spark/pull/2634#discussion-diff-22693112:\n> \n> > - \\* distributed under the License is distributed on an \"AS IS\" BASIS,\n> > - \\* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n> > - \\* See the License for the specific language governing permissions and\n> > - \\* limitations under the License.\n> > - */\n> >   +\n> >   +package org.apache.spark.mllib.clustering.metrics\n> >   +\n> >   +import breeze.linalg.{ DenseVector => BDV, SparseVector => BSV, Vector => BV }\n> >   +\n> >   +import org.apache.spark.mllib.base._\n> >   +import org.apache.spark.mllib.linalg.{ SparseVector, DenseVector, Vector }\n> >   +import org.apache.spark.mllib.base.{ Centroid, FPoint, PointOps, Infinity, Zero }\n> >   +\n> >   +class FastEUPoint(raw: BV[Double], weight: Double) extends FPoint(raw, weight) {\n> > -  val norm = if (weight == Zero) Zero else raw.dot(raw) / (weight \\* weight)\n> \n> Should weight be only used in aggregation rather than distance\n> computation?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/spark/pull/2634/files#r22693112.\n",
    "commit": "35da8e9e188e669460000d5799d061ecc3ca150f",
    "createdAt": "2015-01-11T07:17:48Z",
    "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering.metrics\n+\n+import breeze.linalg.{ DenseVector => BDV, SparseVector => BSV, Vector => BV }\n+\n+import org.apache.spark.mllib.base._\n+import org.apache.spark.mllib.linalg.{ SparseVector, DenseVector, Vector }\n+import org.apache.spark.mllib.base.{ Centroid, FPoint, PointOps, Infinity, Zero }\n+\n+class FastEUPoint(raw: BV[Double], weight: Double) extends FPoint(raw, weight) {\n+  val norm = if (weight == Zero) Zero else raw.dot(raw) / (weight * weight)",
    "line": 27
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Could you elaborate more about the benefit of using homogeneous coordinates? It should be at least documented that what \"weight\" stands for here. If a cluster have 100 points, do you assign its center with weight 100? If so, how to compute the distance between this center and an input point?\n",
    "commit": "35da8e9e188e669460000d5799d061ecc3ca150f",
    "createdAt": "2015-01-20T01:02:32Z",
    "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering.metrics\n+\n+import breeze.linalg.{ DenseVector => BDV, SparseVector => BSV, Vector => BV }\n+\n+import org.apache.spark.mllib.base._\n+import org.apache.spark.mllib.linalg.{ SparseVector, DenseVector, Vector }\n+import org.apache.spark.mllib.base.{ Centroid, FPoint, PointOps, Infinity, Zero }\n+\n+class FastEUPoint(raw: BV[Double], weight: Double) extends FPoint(raw, weight) {\n+  val norm = if (weight == Zero) Zero else raw.dot(raw) / (weight * weight)",
    "line": 27
  }, {
    "author": {
      "login": "derrickburns"
    },
    "body": "Simply put, computing the distance (and the cached values per point and centered used in the distance) is faster in homogeneous coordinates. \n\nImagine computing dot(x,y), where x and y are vectors. If x and y are in homogeneous coordinates, then it is faster to perform the fit product in homogeneous coordinates then divide by the weights of the two vectors than it is to divide each homogenous coordinate by the weights first and the do the dot product.\n\nSent from my iPhone\n\n> On Jan 19, 2015, at 5:02 PM, Xiangrui Meng notifications@github.com wrote:\n> \n> In mllib/src/main/scala/org/apache/spark/mllib/clustering/metrics/FastEuclideanOps.scala:\n> \n> > - \\* distributed under the License is distributed on an \"AS IS\" BASIS,\n> > - \\* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n> > - \\* See the License for the specific language governing permissions and\n> > - \\* limitations under the License.\n> > - */\n> >   +\n> >   +package org.apache.spark.mllib.clustering.metrics\n> >   +\n> >   +import breeze.linalg.{ DenseVector => BDV, SparseVector => BSV, Vector => BV }\n> >   +\n> >   +import org.apache.spark.mllib.base._\n> >   +import org.apache.spark.mllib.linalg.{ SparseVector, DenseVector, Vector }\n> >   +import org.apache.spark.mllib.base.{ Centroid, FPoint, PointOps, Infinity, Zero }\n> >   +\n> >   +class FastEUPoint(raw: BV[Double], weight: Double) extends FPoint(raw, weight) {\n> > -  val norm = if (weight == Zero) Zero else raw.dot(raw) / (weight \\* weight)\n> >   Could you elaborate more about the benefit of using homogeneous coordinates? It should be at least documented that what \"weight\" stands for here. If a cluster have 100 points, do you assign its center with weight 100? If so, how to compute the distance between this center and an input point?\n> \n> —\n> Reply to this email directly or view it on GitHub.\n",
    "commit": "35da8e9e188e669460000d5799d061ecc3ca150f",
    "createdAt": "2015-01-20T01:23:41Z",
    "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering.metrics\n+\n+import breeze.linalg.{ DenseVector => BDV, SparseVector => BSV, Vector => BV }\n+\n+import org.apache.spark.mllib.base._\n+import org.apache.spark.mllib.linalg.{ SparseVector, DenseVector, Vector }\n+import org.apache.spark.mllib.base.{ Centroid, FPoint, PointOps, Infinity, Zero }\n+\n+class FastEUPoint(raw: BV[Double], weight: Double) extends FPoint(raw, weight) {\n+  val norm = if (weight == Zero) Zero else raw.dot(raw) / (weight * weight)",
    "line": 27
  }, {
    "author": {
      "login": "derrickburns"
    },
    "body": "For the squared Euclidean distance case, weights would naturally start at 1.0 for each input point, so that a cluster with 100 points would have weight 100.0.\n\nHowever, for the KL divergence where the points are derived from frequencies, the initial weight could be the sum of the frequencies. This puts the points on the simplex without having to perform the division.\n\nSent from my iPhone\n\n> On Jan 19, 2015, at 5:02 PM, Xiangrui Meng notifications@github.com wrote:\n> \n> In mllib/src/main/scala/org/apache/spark/mllib/clustering/metrics/FastEuclideanOps.scala:\n> \n> > - \\* distributed under the License is distributed on an \"AS IS\" BASIS,\n> > - \\* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n> > - \\* See the License for the specific language governing permissions and\n> > - \\* limitations under the License.\n> > - */\n> >   +\n> >   +package org.apache.spark.mllib.clustering.metrics\n> >   +\n> >   +import breeze.linalg.{ DenseVector => BDV, SparseVector => BSV, Vector => BV }\n> >   +\n> >   +import org.apache.spark.mllib.base._\n> >   +import org.apache.spark.mllib.linalg.{ SparseVector, DenseVector, Vector }\n> >   +import org.apache.spark.mllib.base.{ Centroid, FPoint, PointOps, Infinity, Zero }\n> >   +\n> >   +class FastEUPoint(raw: BV[Double], weight: Double) extends FPoint(raw, weight) {\n> > -  val norm = if (weight == Zero) Zero else raw.dot(raw) / (weight \\* weight)\n> >   Could you elaborate more about the benefit of using homogeneous coordinates? It should be at least documented that what \"weight\" stands for here. If a cluster have 100 points, do you assign its center with weight 100? If so, how to compute the distance between this center and an input point?\n> \n> —\n> Reply to this email directly or view it on GitHub.\n",
    "commit": "35da8e9e188e669460000d5799d061ecc3ca150f",
    "createdAt": "2015-01-20T01:36:37Z",
    "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering.metrics\n+\n+import breeze.linalg.{ DenseVector => BDV, SparseVector => BSV, Vector => BV }\n+\n+import org.apache.spark.mllib.base._\n+import org.apache.spark.mllib.linalg.{ SparseVector, DenseVector, Vector }\n+import org.apache.spark.mllib.base.{ Centroid, FPoint, PointOps, Infinity, Zero }\n+\n+class FastEUPoint(raw: BV[Double], weight: Double) extends FPoint(raw, weight) {\n+  val norm = if (weight == Zero) Zero else raw.dot(raw) / (weight * weight)",
    "line": 27
  }, {
    "author": {
      "login": "derrickburns"
    },
    "body": "@mengxr\n\nI spent some time thinking about clustering sparse data.  I realized that\nthere may be a need to embed the sparse data into lower dimensional\nspaces.  Then, I realized that one of the Bregman divergences (i.e. the\ngeneralized symmetrized Bregman divergence), also embeds data into a\ndifferent space before clustering.  This led me to the conclusion that\nembedding the data into a convenient space just prior to clustering is a\nvery useful generic feature.  Adding it was trivial.  See the README on my\nGitHub project for details.\n\nI implemented two generic embeddings: 1) a sparse to dense embedding that\nmaps points isomorphically and 2) random indexing\nhttps://www.sics.se/~mange/papers/RI_intro.pdf that maps high dimensional\ndata into lower dimension in a way that preserves similarity per the\nJohnson-Lindenstrauss\nlemma http://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma. I\nalso recoded the embedding that I did for a generalized symmetrized KL\ndivergence to use the new embedding design.\n\nI think that with these improvements one can check the \"clusterer supports\nsparse data\" feature box in a practical sense.\n\nI am about to being performance testing of the randoming indexing embedding\non large volumes of high dimensional sparse data.\n\nPlease take a look at the new PointOps trait definition.\n\nOn Mon, Jan 19, 2015 at 5:36 PM, Derrick Burns derrickrburns@gmail.com\nwrote:\n\n> For the squared Euclidean distance case, weights would naturally start at\n> 1.0 for each input point, so that a cluster with 100 points would have\n> weight 100.0.\n> \n> However, for the KL divergence where the points are derived from\n> frequencies, the initial weight could be the sum of the frequencies. This\n> puts the points on the simplex without having to perform the division.\n> \n> Sent from my iPhone\n> \n> On Jan 19, 2015, at 5:02 PM, Xiangrui Meng notifications@github.com\n> wrote:\n> \n> In\n> mllib/src/main/scala/org/apache/spark/mllib/clustering/metrics/FastEuclideanOps.scala\n> https://github.com/apache/spark/pull/2634#discussion-diff-23197316:\n> \n> > - \\* distributed under the License is distributed on an \"AS IS\" BASIS,\n> > - \\* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n> > - \\* See the License for the specific language governing permissions and\n> > - \\* limitations under the License.\n> > - */\n> >   +\n> >   +package org.apache.spark.mllib.clustering.metrics\n> >   +\n> >   +import breeze.linalg.{ DenseVector => BDV, SparseVector => BSV, Vector => BV }\n> >   +\n> >   +import org.apache.spark.mllib.base._\n> >   +import org.apache.spark.mllib.linalg.{ SparseVector, DenseVector, Vector }\n> >   +import org.apache.spark.mllib.base.{ Centroid, FPoint, PointOps, Infinity, Zero }\n> >   +\n> >   +class FastEUPoint(raw: BV[Double], weight: Double) extends FPoint(raw, weight) {\n> > -  val norm = if (weight == Zero) Zero else raw.dot(raw) / (weight \\* weight)\n> \n> Could you elaborate more about the benefit of using homogeneous\n> coordinates? It should be at least documented that what \"weight\" stands for\n> here. If a cluster have 100 points, do you assign its center with weight\n> 100? If so, how to compute the distance between this center and an input\n> point?\n> \n> —\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/spark/pull/2634/files#r23197316.\n",
    "commit": "35da8e9e188e669460000d5799d061ecc3ca150f",
    "createdAt": "2015-01-23T03:43:20Z",
    "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering.metrics\n+\n+import breeze.linalg.{ DenseVector => BDV, SparseVector => BSV, Vector => BV }\n+\n+import org.apache.spark.mllib.base._\n+import org.apache.spark.mllib.linalg.{ SparseVector, DenseVector, Vector }\n+import org.apache.spark.mllib.base.{ Centroid, FPoint, PointOps, Infinity, Zero }\n+\n+class FastEUPoint(raw: BV[Double], weight: Double) extends FPoint(raw, weight) {\n+  val norm = if (weight == Zero) Zero else raw.dot(raw) / (weight * weight)",
    "line": 27
  }, {
    "author": {
      "login": "derrickburns"
    },
    "body": "@mengxr\nI have rewritten the README file\nhttps://github.com/derrickburns/generalized-kmeans-clustering for my\nclustering GtHub project. I think that you may find it useful.\n\nOn Thu, Jan 22, 2015 at 7:43 PM, Derrick Burns derrickrburns@gmail.com\nwrote:\n\n> @mengxr\n> \n> I spent some time thinking about clustering sparse data.  I realized that\n> there may be a need to embed the sparse data into lower dimensional\n> spaces.  Then, I realized that one of the Bregman divergences (i.e. the\n> generalized symmetrized Bregman divergence), also embeds data into a\n> different space before clustering.  This led me to the conclusion that\n> embedding the data into a convenient space just prior to clustering is a\n> very useful generic feature.  Adding it was trivial.  See the README on my\n> GitHub project for details.\n> \n> I implemented two generic embeddings: 1) a sparse to dense embedding that\n> maps points isomorphically and 2) random indexing\n> https://www.sics.se/~mange/papers/RI_intro.pdf that maps high\n> dimensional data into lower dimension in a way that preserves similarity\n> per the Johnson-Lindenstrauss lemma\n> http://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma. I\n> also recoded the embedding that I did for a generalized symmetrized KL\n> divergence to use the new embedding design.\n> \n> I think that with these improvements one can check the \"clusterer supports\n> sparse data\" feature box in a practical sense.\n> \n> I am about to being performance testing of the randoming indexing\n> embedding on large volumes of high dimensional sparse data.\n> \n> Please take a look at the new PointOps trait definition.\n> \n> On Mon, Jan 19, 2015 at 5:36 PM, Derrick Burns derrickrburns@gmail.com\n> wrote:\n> \n> > For the squared Euclidean distance case, weights would naturally start at\n> > 1.0 for each input point, so that a cluster with 100 points would have\n> > weight 100.0.\n> > \n> > However, for the KL divergence where the points are derived from\n> > frequencies, the initial weight could be the sum of the frequencies. This\n> > puts the points on the simplex without having to perform the division.\n> > \n> > Sent from my iPhone\n> > \n> > On Jan 19, 2015, at 5:02 PM, Xiangrui Meng notifications@github.com\n> > wrote:\n> > \n> > In\n> > mllib/src/main/scala/org/apache/spark/mllib/clustering/metrics/FastEuclideanOps.scala\n> > https://github.com/apache/spark/pull/2634#discussion-diff-23197316:\n> > \n> > > - \\* distributed under the License is distributed on an \"AS IS\" BASIS,\n> > > - \\* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n> > > - \\* See the License for the specific language governing permissions and\n> > > - \\* limitations under the License.\n> > > - */\n> > >   +\n> > >   +package org.apache.spark.mllib.clustering.metrics\n> > >   +\n> > >   +import breeze.linalg.{ DenseVector => BDV, SparseVector => BSV, Vector => BV }\n> > >   +\n> > >   +import org.apache.spark.mllib.base._\n> > >   +import org.apache.spark.mllib.linalg.{ SparseVector, DenseVector, Vector }\n> > >   +import org.apache.spark.mllib.base.{ Centroid, FPoint, PointOps, Infinity, Zero }\n> > >   +\n> > >   +class FastEUPoint(raw: BV[Double], weight: Double) extends FPoint(raw, weight) {\n> > > -  val norm = if (weight == Zero) Zero else raw.dot(raw) / (weight \\* weight)\n> > \n> > Could you elaborate more about the benefit of using homogeneous\n> > coordinates? It should be at least documented that what \"weight\" stands for\n> > here. If a cluster have 100 points, do you assign its center with weight\n> > 100? If so, how to compute the distance between this center and an input\n> > point?\n> > \n> > —\n> > Reply to this email directly or view it on GitHub\n> > https://github.com/apache/spark/pull/2634/files#r23197316.\n",
    "commit": "35da8e9e188e669460000d5799d061ecc3ca150f",
    "createdAt": "2015-01-24T09:45:11Z",
    "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering.metrics\n+\n+import breeze.linalg.{ DenseVector => BDV, SparseVector => BSV, Vector => BV }\n+\n+import org.apache.spark.mllib.base._\n+import org.apache.spark.mllib.linalg.{ SparseVector, DenseVector, Vector }\n+import org.apache.spark.mllib.base.{ Centroid, FPoint, PointOps, Infinity, Zero }\n+\n+class FastEUPoint(raw: BV[Double], weight: Double) extends FPoint(raw, weight) {\n+  val norm = if (weight == Zero) Zero else raw.dot(raw) / (weight * weight)",
    "line": 27
  }, {
    "author": {
      "login": "derrickburns"
    },
    "body": "@mengxr\nBy separating the initialization of cluster centers from the clustering,\none may trivially implement several variants of the standard K-means\nclustering, including the \"Wavelet-Based Anytime Algorithm for K-Means\nClustering of Time Series\",\nhttp://www.cs.gmu.edu/~jessica/publications/ikmeans_sdm_workshop03.pdf\nwhich has been demonstrated to give better quality clusterings in less time\nthan the standard algorithm.\n\nTo demonstrate this, I have implemented the aforementioned algorithm in\nroughly 50 lines of new code. See\ncom.massivedatascience.clusterer.WaveletKMeans.\n\nOn Sat, Jan 24, 2015 at 1:45 AM, Derrick Burns derrickrburns@gmail.com\nwrote:\n\n> @mengxr\n> I have rewritten the README file\n> https://github.com/derrickburns/generalized-kmeans-clustering for my\n> clustering GtHub project. I think that you may find it useful.\n> \n> On Thu, Jan 22, 2015 at 7:43 PM, Derrick Burns derrickrburns@gmail.com\n> wrote:\n> \n> > @mengxr\n> > \n> > I spent some time thinking about clustering sparse data.  I realized that\n> > there may be a need to embed the sparse data into lower dimensional\n> > spaces.  Then, I realized that one of the Bregman divergences (i.e. the\n> > generalized symmetrized Bregman divergence), also embeds data into a\n> > different space before clustering.  This led me to the conclusion that\n> > embedding the data into a convenient space just prior to clustering is a\n> > very useful generic feature.  Adding it was trivial.  See the README on my\n> > GitHub project for details.\n> > \n> > I implemented two generic embeddings: 1) a sparse to dense embedding that\n> > maps points isomorphically and 2) random indexing\n> > https://www.sics.se/~mange/papers/RI_intro.pdf that maps high\n> > dimensional data into lower dimension in a way that preserves similarity\n> > per the Johnson-Lindenstrauss lemma\n> > http://en.wikipedia.org/wiki/Johnson%E2%80%93Lindenstrauss_lemma. I\n> > also recoded the embedding that I did for a generalized symmetrized KL\n> > divergence to use the new embedding design.\n> > \n> > I think that with these improvements one can check the \"clusterer\n> > supports sparse data\" feature box in a practical sense.\n> > \n> > I am about to being performance testing of the randoming indexing\n> > embedding on large volumes of high dimensional sparse data.\n> > \n> > Please take a look at the new PointOps trait definition.\n> > \n> > On Mon, Jan 19, 2015 at 5:36 PM, Derrick Burns derrickrburns@gmail.com\n> > wrote:\n> > \n> > > For the squared Euclidean distance case, weights would naturally start\n> > > at 1.0 for each input point, so that a cluster with 100 points would have\n> > > weight 100.0.\n> > > \n> > > However, for the KL divergence where the points are derived from\n> > > frequencies, the initial weight could be the sum of the frequencies. This\n> > > puts the points on the simplex without having to perform the division.\n> > > \n> > > Sent from my iPhone\n> > > \n> > > On Jan 19, 2015, at 5:02 PM, Xiangrui Meng notifications@github.com\n> > > wrote:\n> > > \n> > > In\n> > > mllib/src/main/scala/org/apache/spark/mllib/clustering/metrics/FastEuclideanOps.scala\n> > > https://github.com/apache/spark/pull/2634#discussion-diff-23197316:\n> > > \n> > > > - \\* distributed under the License is distributed on an \"AS IS\" BASIS,\n> > > > - \\* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n> > > > - \\* See the License for the specific language governing permissions and\n> > > > - \\* limitations under the License.\n> > > > - */\n> > > >   +\n> > > >   +package org.apache.spark.mllib.clustering.metrics\n> > > >   +\n> > > >   +import breeze.linalg.{ DenseVector => BDV, SparseVector => BSV, Vector => BV }\n> > > >   +\n> > > >   +import org.apache.spark.mllib.base._\n> > > >   +import org.apache.spark.mllib.linalg.{ SparseVector, DenseVector, Vector }\n> > > >   +import org.apache.spark.mllib.base.{ Centroid, FPoint, PointOps, Infinity, Zero }\n> > > >   +\n> > > >   +class FastEUPoint(raw: BV[Double], weight: Double) extends FPoint(raw, weight) {\n> > > > -  val norm = if (weight == Zero) Zero else raw.dot(raw) / (weight \\* weight)\n> > > \n> > > Could you elaborate more about the benefit of using homogeneous\n> > > coordinates? It should be at least documented that what \"weight\" stands for\n> > > here. If a cluster have 100 points, do you assign its center with weight\n> > > 100? If so, how to compute the distance between this center and an input\n> > > point?\n> > > \n> > > —\n> > > Reply to this email directly or view it on GitHub\n> > > https://github.com/apache/spark/pull/2634/files#r23197316.\n",
    "commit": "35da8e9e188e669460000d5799d061ecc3ca150f",
    "createdAt": "2015-01-25T20:46:21Z",
    "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering.metrics\n+\n+import breeze.linalg.{ DenseVector => BDV, SparseVector => BSV, Vector => BV }\n+\n+import org.apache.spark.mllib.base._\n+import org.apache.spark.mllib.linalg.{ SparseVector, DenseVector, Vector }\n+import org.apache.spark.mllib.base.{ Centroid, FPoint, PointOps, Infinity, Zero }\n+\n+class FastEUPoint(raw: BV[Double], weight: Double) extends FPoint(raw, weight) {\n+  val norm = if (weight == Zero) Zero else raw.dot(raw) / (weight * weight)",
    "line": 27
  }],
  "prId": 2634
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "same question about using `weight` in `distance`\n",
    "commit": "35da8e9e188e669460000d5799d061ecc3ca150f",
    "createdAt": "2015-01-08T23:56:58Z",
    "diffHunk": "@@ -0,0 +1,77 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering.metrics\n+\n+import breeze.linalg.{ DenseVector => BDV, SparseVector => BSV, Vector => BV }\n+\n+import org.apache.spark.mllib.base._\n+import org.apache.spark.mllib.linalg.{ SparseVector, DenseVector, Vector }\n+import org.apache.spark.mllib.base.{ Centroid, FPoint, PointOps, Infinity, Zero }\n+\n+class FastEUPoint(raw: BV[Double], weight: Double) extends FPoint(raw, weight) {\n+  val norm = if (weight == Zero) Zero else raw.dot(raw) / (weight * weight)\n+}\n+\n+/**\n+ * Euclidean distance measure, expedited by pre-computing vector norms\n+ */\n+class FastEuclideanOps extends PointOps[FastEUPoint, FastEUPoint] with Serializable {\n+\n+  type C = FastEUPoint\n+  type P = FastEUPoint\n+\n+  val epsilon = 1e-4\n+\n+  /* compute a lower bound on the euclidean distance distance */\n+\n+  def distance(p: P, c: C, upperBound: Double): Double = {\n+    val d = if (p.weight == Zero || c.weight == Zero) {\n+      p.norm + c.norm\n+    } else {\n+      val x = p.raw.dot(c.raw) / (p.weight * c.weight)",
    "line": 46
  }],
  "prId": 2634
}]