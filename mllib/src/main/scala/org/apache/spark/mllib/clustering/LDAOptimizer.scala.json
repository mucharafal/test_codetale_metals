[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Should this be a val since you use Breeze to mutate the internals?\n",
    "commit": "78b0f5a0655eff5f8462927546c8fe7134436b72",
    "createdAt": "2015-07-21T05:22:42Z",
    "diffHunk": "@@ -387,39 +387,32 @@ final class OnlineLDAOptimizer extends LDAOptimizer {\n         }\n \n         // Initialize the variational distribution q(theta|gamma) for the mini-batch\n-        var gammad = new Gamma(gammaShape, 1.0 / gammaShape).samplesVector(k).t // 1 * K\n-        var Elogthetad = digamma(gammad) - digamma(sum(gammad))     // 1 * K\n-        var expElogthetad = exp(Elogthetad)                         // 1 * K\n-        val expElogbetad = expElogbeta(::, ids).toDenseMatrix       // K * ids\n+        var gammad: BDV[Double] ="
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Yep, fixed.\n",
    "commit": "78b0f5a0655eff5f8462927546c8fe7134436b72",
    "createdAt": "2015-07-21T17:44:02Z",
    "diffHunk": "@@ -387,39 +387,32 @@ final class OnlineLDAOptimizer extends LDAOptimizer {\n         }\n \n         // Initialize the variational distribution q(theta|gamma) for the mini-batch\n-        var gammad = new Gamma(gammaShape, 1.0 / gammaShape).samplesVector(k).t // 1 * K\n-        var Elogthetad = digamma(gammad) - digamma(sum(gammad))     // 1 * K\n-        var expElogthetad = exp(Elogthetad)                         // 1 * K\n-        val expElogbetad = expElogbeta(::, ids).toDenseMatrix       // K * ids\n+        var gammad: BDV[Double] ="
  }],
  "prId": 7454
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Should this be a val since you use Breeze to mutate the internals?\n",
    "commit": "78b0f5a0655eff5f8462927546c8fe7134436b72",
    "createdAt": "2015-07-21T05:22:44Z",
    "diffHunk": "@@ -387,39 +387,32 @@ final class OnlineLDAOptimizer extends LDAOptimizer {\n         }\n \n         // Initialize the variational distribution q(theta|gamma) for the mini-batch\n-        var gammad = new Gamma(gammaShape, 1.0 / gammaShape).samplesVector(k).t // 1 * K\n-        var Elogthetad = digamma(gammad) - digamma(sum(gammad))     // 1 * K\n-        var expElogthetad = exp(Elogthetad)                         // 1 * K\n-        val expElogbetad = expElogbeta(::, ids).toDenseMatrix       // K * ids\n+        var gammad: BDV[Double] =\n+          new Gamma(gammaShape, 1.0 / gammaShape).samplesVector(k)                   // K\n+        val expElogthetad: BDV[Double] = exp(digamma(gammad) - digamma(sum(gammad))) // K\n+        val expElogbetad: BDM[Double] = expElogbeta(ids, ::).toDenseMatrix           // ids * K\n \n-        var phinorm = expElogthetad * expElogbetad + 1e-100         // 1 * ids\n+        var phinorm: BDV[Double] = expElogbetad * expElogthetad :+ 1e-100            // ids"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Yep, fixed.\n",
    "commit": "78b0f5a0655eff5f8462927546c8fe7134436b72",
    "createdAt": "2015-07-21T17:44:04Z",
    "diffHunk": "@@ -387,39 +387,32 @@ final class OnlineLDAOptimizer extends LDAOptimizer {\n         }\n \n         // Initialize the variational distribution q(theta|gamma) for the mini-batch\n-        var gammad = new Gamma(gammaShape, 1.0 / gammaShape).samplesVector(k).t // 1 * K\n-        var Elogthetad = digamma(gammad) - digamma(sum(gammad))     // 1 * K\n-        var expElogthetad = exp(Elogthetad)                         // 1 * K\n-        val expElogbetad = expElogbeta(::, ids).toDenseMatrix       // K * ids\n+        var gammad: BDV[Double] =\n+          new Gamma(gammaShape, 1.0 / gammaShape).samplesVector(k)                   // K\n+        val expElogthetad: BDV[Double] = exp(digamma(gammad) - digamma(sum(gammad))) // K\n+        val expElogbetad: BDM[Double] = expElogbeta(ids, ::).toDenseMatrix           // ids * K\n \n-        var phinorm = expElogthetad * expElogbetad + 1e-100         // 1 * ids\n+        var phinorm: BDV[Double] = expElogbetad * expElogthetad :+ 1e-100            // ids"
  }],
  "prId": 7454
}]