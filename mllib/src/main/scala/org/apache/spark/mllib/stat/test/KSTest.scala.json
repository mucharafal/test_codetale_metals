[{
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Alphabetize the imports in here\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-24T20:00:38Z",
    "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.NormalDistribution\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.{SparkException, Logging}"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Nit: replace dat with data\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-24T20:01:13Z",
    "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.NormalDistribution\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.{SparkException, Logging}\n+import org.apache.spark.rdd.RDD\n+\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ */\n+  private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Calculate empirical cumulative distribution values needed for KS statistic\n+   * @param dat `RDD[Double]` on which to calculate empirical cumulative distribution values\n+   * @return and RDD of (Double, Double, Double), where the first element in each tuple is the\n+   *         value, the second element is the ECDFV - 1 /n, and the third element is the ECDFV,\n+   *         where ECDF stands for empirical cumulative distribution function value\n+   *\n+   */\n+  def empirical(dat: RDD[Double]): RDD[(Double, Double, Double)] = {"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "indent this back two spaces\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-24T20:02:54Z",
    "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.NormalDistribution\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.{SparkException, Logging}\n+import org.apache.spark.rdd.RDD\n+\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ */\n+  private[stat] object KSTest {"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "extra newline\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-24T20:03:16Z",
    "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.NormalDistribution\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.{SparkException, Logging}\n+import org.apache.spark.rdd.RDD\n+\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ */\n+  private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Calculate empirical cumulative distribution values needed for KS statistic\n+   * @param dat `RDD[Double]` on which to calculate empirical cumulative distribution values\n+   * @return and RDD of (Double, Double, Double), where the first element in each tuple is the\n+   *         value, the second element is the ECDFV - 1 /n, and the third element is the ECDFV,\n+   *         where ECDF stands for empirical cumulative distribution function value\n+   *"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "`distCalc` should be indented four spaces past `def` and `: KSTestResult` should be indented two spaces past `def`\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-24T20:04:11Z",
    "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.NormalDistribution\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.{SparkException, Logging}\n+import org.apache.spark.rdd.RDD\n+\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ */\n+  private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Calculate empirical cumulative distribution values needed for KS statistic\n+   * @param dat `RDD[Double]` on which to calculate empirical cumulative distribution values\n+   * @return and RDD of (Double, Double, Double), where the first element in each tuple is the\n+   *         value, the second element is the ECDFV - 1 /n, and the third element is the ECDFV,\n+   *         where ECDF stands for empirical cumulative distribution function value\n+   *\n+   */\n+  def empirical(dat: RDD[Double]): RDD[(Double, Double, Double)] = {\n+    val n = dat.count().toDouble\n+    dat.sortBy(x => x).zipWithIndex().map { case (v, i) => (v, i / n, (i + 1) / n) }\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param dat `RDD[Double]` to evaluate\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return a KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(dat: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val empiriRDD = empirical(dat) // empirical distribution\n+    val distances = empiriRDD.map {\n+        case (v, dl, dp) =>\n+          val cdfVal = cdf(v)\n+          Math.max(cdfVal - dl, dp - cdfVal)\n+      }\n+    val ksStat = distances.max()\n+    evalOneSampleP(ksStat, distances.count())\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution. Optimized\n+   * such that each partition runs a separate mapping operation. This can help in cases where the\n+   * CDF calculation involves creating an object. By using this implementation we can make sure\n+   * only 1 object is created per partition, versus 1 per observation.\n+   * @param dat `RDD[Double]` to evaluate\n+   * @param distCalc a function to calculate the distance between the empirical values and the\n+   *                 theoretical value\n+   * @return a KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSampleOpt(dat: RDD[Double],\n+                       distCalc: Iterator[(Double, Double, Double)] => Iterator[Double])"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Give an informative error message here that names the available options\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-24T20:11:39Z",
    "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.NormalDistribution\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.{SparkException, Logging}\n+import org.apache.spark.rdd.RDD\n+\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ */\n+  private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Calculate empirical cumulative distribution values needed for KS statistic\n+   * @param dat `RDD[Double]` on which to calculate empirical cumulative distribution values\n+   * @return and RDD of (Double, Double, Double), where the first element in each tuple is the\n+   *         value, the second element is the ECDFV - 1 /n, and the third element is the ECDFV,\n+   *         where ECDF stands for empirical cumulative distribution function value\n+   *\n+   */\n+  def empirical(dat: RDD[Double]): RDD[(Double, Double, Double)] = {\n+    val n = dat.count().toDouble\n+    dat.sortBy(x => x).zipWithIndex().map { case (v, i) => (v, i / n, (i + 1) / n) }\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param dat `RDD[Double]` to evaluate\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return a KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(dat: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val empiriRDD = empirical(dat) // empirical distribution\n+    val distances = empiriRDD.map {\n+        case (v, dl, dp) =>\n+          val cdfVal = cdf(v)\n+          Math.max(cdfVal - dl, dp - cdfVal)\n+      }\n+    val ksStat = distances.max()\n+    evalOneSampleP(ksStat, distances.count())\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution. Optimized\n+   * such that each partition runs a separate mapping operation. This can help in cases where the\n+   * CDF calculation involves creating an object. By using this implementation we can make sure\n+   * only 1 object is created per partition, versus 1 per observation.\n+   * @param dat `RDD[Double]` to evaluate\n+   * @param distCalc a function to calculate the distance between the empirical values and the\n+   *                 theoretical value\n+   * @return a KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSampleOpt(dat: RDD[Double],\n+                       distCalc: Iterator[(Double, Double, Double)] => Iterator[Double])\n+  : KSTestResult = {\n+    val empiriRDD = empirical(dat) // empirical distribution information\n+    val distances = empiriRDD.mapPartitions(distCalc, false)\n+    val ksStat = distances.max\n+    evalOneSampleP(ksStat, distances.count())\n+  }\n+\n+  /**\n+   * Returns a function to calculate the KSTest with a standard normal distribution\n+   * to be used with testOneSampleOpt\n+   * @return Return a function that we can map over partitions to calculate the KS distance in each\n+   */\n+  def stdNormDistances(): (Iterator[(Double, Double, Double)]) => Iterator[Double] = {\n+    val dist = new NormalDistribution(0, 1)\n+    (part: Iterator[(Double, Double, Double)]) => part.map {\n+      case (v, dl, dp) =>\n+        val cdfVal = dist.cumulativeProbability(v)\n+        Math.max(cdfVal - dl, dp - cdfVal)\n+    }\n+  }\n+\n+  /**\n+   * A convenience function that allows running the KS test for 1 set of sample data against\n+   * a named distribution\n+   * @param dat the sample data that we wish to evaluate\n+   * @param distName the name of the theoretical distribution\n+   * @return The KS statistic and p-value associated with a two sided test\n+   */\n+  def testOneSample(dat: RDD[Double], distName: String): KSTestResult = {\n+    val distanceCalc =\n+      distName match {\n+        case \"stdnorm\" => stdNormDistances()\n+        case  _ => throw new UnsupportedOperationException()"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "nit: \"data\" instead of \"dat\"\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-25T05:43:54Z",
    "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.NormalDistribution\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Calculate empirical cumulative distribution values needed for KS statistic\n+   * @param data `RDD[Double]` on which to calculate empirical cumulative distribution values\n+   * @return and RDD of (Double, Double, Double), where the first element in each tuple is the\n+   *         value, the second element is the ECDFV - 1 /n, and the third element is the ECDFV,\n+   *         where ECDF stands for empirical cumulative distribution function value\n+   */\n+  def empirical(data: RDD[Double]): RDD[(Double, Double, Double)] = {\n+    val n = data.count().toDouble\n+    data.sortBy(x => x).zipWithIndex().map { case (v, i) => (v, i / n, (i + 1) / n) }\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param dat `RDD[Double]` to evaluate\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(dat: RDD[Double], cdf: Double => Double): KSTestResult = {"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Can we reuse the count that was computed in `empirical`?\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-25T05:47:12Z",
    "diffHunk": "@@ -0,0 +1,126 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.NormalDistribution\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Calculate empirical cumulative distribution values needed for KS statistic\n+   * @param data `RDD[Double]` on which to calculate empirical cumulative distribution values\n+   * @return and RDD of (Double, Double, Double), where the first element in each tuple is the\n+   *         value, the second element is the ECDFV - 1 /n, and the third element is the ECDFV,\n+   *         where ECDF stands for empirical cumulative distribution function value\n+   */\n+  def empirical(data: RDD[Double]): RDD[(Double, Double, Double)] = {\n+    val n = data.count().toDouble\n+    data.sortBy(x => x).zipWithIndex().map { case (v, i) => (v, i / n, (i + 1) / n) }\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param dat `RDD[Double]` to evaluate\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(dat: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val empiriRDD = empirical(dat) // empirical distribution\n+    val distances = empiriRDD.map {\n+        case (v, dl, dp) =>\n+          val cdfVal = cdf(v)\n+          Math.max(cdfVal - dl, dp - cdfVal)\n+      }\n+    val ksStat = distances.max()\n+    evalOneSampleP(ksStat, distances.count())"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "extra newline\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-26T00:55:48Z",
    "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+"
  }, {
    "author": {
      "login": "sujkh85"
    },
    "body": "## NAVER - http://www.naver.com/\n\nsujkh@naver.com 님께 보내신 메일 <Re: [spark] [SPARK-8598] [MLlib] Implementation of 1-sample, two-sided, Kolmogorov Smirnov Test for RDDs (#6994)> 이 다음과 같은 이유로 전송 실패했습니다.\n\n---\n\n받는 사람이 회원님의 메일을 수신차단 하였습니다. \n\n---\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-26T00:56:29Z",
    "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "put part on the line above and indent the rest back two spaces\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-26T00:56:19Z",
    "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions {\n+      part =>"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "When arguments don't fit on a single line, spread them out so each gets its own line\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-26T00:57:26Z",
    "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions {\n+      part =>\n+        val partDiffs = oneSampleDifferences(part, n, cdf) // local distances\n+        searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+        }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toInt)\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param createDist `Unit => RealDistribution` function to create a theoretical distribution\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], createDist: () => RealDistribution): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions {\n+      part =>\n+        val partDiffs = oneSampleDifferences(part, n, createDist) // local distances\n+        searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toInt)\n+  }\n+\n+  /**\n+   * Calculate unadjusted distances between the empirical CDF and the theoretical CDF in a\n+   * partition\n+   * @param partData `Iterator[Double]` 1 partition of a sorted RDD\n+   * @param n `Double` the total size of the RDD\n+   * @param cdf `Double => Double` a function the calculates the theoretical CDF of a value\n+   * @return `Iterator[Double] `Unadjusted (ie. off by a constant) differences between\n+   *        ECDF (empirical cumulative distribution function) and CDF. We subtract in such a way\n+   *        that when adjusted by the appropriate constant, the difference will be equivalent\n+   *        to the KS statistic calculation described in\n+   *        http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm\n+   *        where the difference is not exactly symmetric\n+   */\n+  private def oneSampleDifferences(partData: Iterator[Double], n: Double, cdf: Double => Double)\n+    : Iterator[Double] = {\n+    // zip data with index (within that partition)\n+    // calculate local (unadjusted) ECDF and subtract CDF\n+    partData.zipWithIndex.map {\n+      case (v, ix) =>\n+        // dp and dl are later adjusted by constant, when global info is available\n+        val dp = (ix + 1) / n\n+        val dl = ix / n\n+        val cdfVal = cdf(v)\n+        // if dp > cdfVal the adjusted dp is still above cdfVal, if dp < cdfVal\n+        // we want negative distance so that constant adjusted gives correct distance\n+        if (dp > cdfVal) dp - cdfVal else dl - cdfVal\n+    }\n+  }\n+\n+  private def oneSampleDifferences(partData: Iterator[Double], n: Double,"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "Can you include a note on how this is implemented?\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-26T01:09:53Z",
    "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Should also copy this paragraph to the public API doc. Otherwise, user won't see it. Use `@see` for the wiki link.\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-07-01T19:15:39Z",
    "diffHunk": "@@ -0,0 +1,181 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "indent this back two spaces\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-26T20:42:09Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, cdf) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+      }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param createDist `Unit => RealDistribution` function to create a theoretical distribution\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], createDist: () => RealDistribution): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, createDist) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+      }.collect()"
  }, {
    "author": {
      "login": "josepablocam"
    },
    "body": "got it, sorry, though that should be indented since it is in the mapPartitions lambda. I'll indent back so that it is lined up with the val of val localData\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-26T20:52:20Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, cdf) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+      }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param createDist `Unit => RealDistribution` function to create a theoretical distribution\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], createDist: () => RealDistribution): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, createDist) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+      }.collect()"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "indent this back two spaces\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-26T20:42:25Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, cdf) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+      }.collect()"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "indent this back two spaces\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-26T20:43:05Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, cdf) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+      }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param createDist `Unit => RealDistribution` function to create a theoretical distribution\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], createDist: () => RealDistribution): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, createDist) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+      }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Calculate unadjusted distances between the empirical CDF and the theoretical CDF in a\n+   * partition\n+   * @param partData `Iterator[Double]` 1 partition of a sorted RDD\n+   * @param n `Double` the total size of the RDD\n+   * @param cdf `Double => Double` a function the calculates the theoretical CDF of a value\n+   * @return `Iterator[Double] `Unadjusted (ie. off by a constant) differences between\n+   *        ECDF (empirical cumulative distribution function) and CDF. We subtract in such a way\n+   *        that when adjusted by the appropriate constant, the difference will be equivalent\n+   *        to the KS statistic calculation described in\n+   *        http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm\n+   *        where the difference is not exactly symmetric\n+   */\n+  private def oneSampleDifferences(partData: Iterator[Double], n: Double, cdf: Double => Double)\n+    : Iterator[Double] = {\n+    // zip data with index (within that partition)\n+    // calculate local (unadjusted) ECDF and subtract CDF\n+    partData.zipWithIndex.map { case (v, ix) =>\n+      // dp and dl are later adjusted by constant, when global info is available\n+      val dp = (ix + 1) / n\n+      val dl = ix / n\n+      val cdfVal = cdf(v)\n+      // if dp > cdfVal the adjusted dp is still above cdfVal, if dp < cdfVal\n+      // we want negative distance so that constant adjusted gives correct distance\n+      if (dp > cdfVal) dp - cdfVal else dl - cdfVal\n+      }\n+  }\n+\n+  private def oneSampleDifferences(\n+      partData: Iterator[Double],\n+      n: Double,\n+      createDist: () => RealDistribution)\n+    : Iterator[Double] = {\n+    val dist = createDist()\n+    oneSampleDifferences(partData, n, x => dist.cumulativeProbability(x))\n+  }\n+\n+  /**\n+   * Search the unadjusted differences between ECDF and CDF in a partition and return the\n+   * two extrema (furthest below and furthest above CDF), along with a count of elements in that\n+   * partition\n+   * @param partDiffs `Iterator[Double]` the unadjusted differences between ECDF and CDF in a\n+   *                 partition\n+   * @return `Iterator[(Double, Double, Double)]` the local extrema and a count of elements\n+   */\n+  private def searchOneSampleCandidates(partDiffs: Iterator[Double])\n+    : Iterator[(Double, Double, Double)] = {\n+    val initAcc = (Double.MaxValue, Double.MinValue, 0.0)\n+    val partResults = partDiffs.foldLeft(initAcc) { case ((pMin, pMax, pCt), currDiff) =>\n+      (Math.min(pMin, currDiff), Math.max(pMax, currDiff), pCt + 1)\n+      }"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "back two spaces\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-26T20:43:29Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, cdf) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+      }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param createDist `Unit => RealDistribution` function to create a theoretical distribution\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], createDist: () => RealDistribution): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, createDist) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+      }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Calculate unadjusted distances between the empirical CDF and the theoretical CDF in a\n+   * partition\n+   * @param partData `Iterator[Double]` 1 partition of a sorted RDD\n+   * @param n `Double` the total size of the RDD\n+   * @param cdf `Double => Double` a function the calculates the theoretical CDF of a value\n+   * @return `Iterator[Double] `Unadjusted (ie. off by a constant) differences between\n+   *        ECDF (empirical cumulative distribution function) and CDF. We subtract in such a way\n+   *        that when adjusted by the appropriate constant, the difference will be equivalent\n+   *        to the KS statistic calculation described in\n+   *        http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm\n+   *        where the difference is not exactly symmetric\n+   */\n+  private def oneSampleDifferences(partData: Iterator[Double], n: Double, cdf: Double => Double)\n+    : Iterator[Double] = {\n+    // zip data with index (within that partition)\n+    // calculate local (unadjusted) ECDF and subtract CDF\n+    partData.zipWithIndex.map { case (v, ix) =>\n+      // dp and dl are later adjusted by constant, when global info is available\n+      val dp = (ix + 1) / n\n+      val dl = ix / n\n+      val cdfVal = cdf(v)\n+      // if dp > cdfVal the adjusted dp is still above cdfVal, if dp < cdfVal\n+      // we want negative distance so that constant adjusted gives correct distance\n+      if (dp > cdfVal) dp - cdfVal else dl - cdfVal\n+      }\n+  }\n+\n+  private def oneSampleDifferences(\n+      partData: Iterator[Double],\n+      n: Double,\n+      createDist: () => RealDistribution)\n+    : Iterator[Double] = {\n+    val dist = createDist()\n+    oneSampleDifferences(partData, n, x => dist.cumulativeProbability(x))\n+  }\n+\n+  /**\n+   * Search the unadjusted differences between ECDF and CDF in a partition and return the\n+   * two extrema (furthest below and furthest above CDF), along with a count of elements in that\n+   * partition\n+   * @param partDiffs `Iterator[Double]` the unadjusted differences between ECDF and CDF in a\n+   *                 partition\n+   * @return `Iterator[(Double, Double, Double)]` the local extrema and a count of elements\n+   */\n+  private def searchOneSampleCandidates(partDiffs: Iterator[Double])\n+    : Iterator[(Double, Double, Double)] = {\n+    val initAcc = (Double.MaxValue, Double.MinValue, 0.0)\n+    val partResults = partDiffs.foldLeft(initAcc) { case ((pMin, pMax, pCt), currDiff) =>\n+      (Math.min(pMin, currDiff), Math.max(pMax, currDiff), pCt + 1)\n+      }\n+    Array(partResults).iterator\n+  }\n+\n+  /**\n+   * Find the global maximum distance between ECDF and CDF (i.e. the KS Statistic) after adjusting\n+   * local extrema estimates from individual partitions with the amount of elements in preceding\n+   * partitions\n+   * @param localData `Array[(Double, Double, Double)]` A local array containing the collected\n+   *                 results of `searchOneSampleCandidates` across all partitions\n+   * @param n `Double`The size of the RDD\n+   * @return The one-sample Kolmogorov Smirnov Statistic\n+   */\n+  private def searchOneSampleStatistic(localData: Array[(Double, Double, Double)], n: Double)\n+    : Double = {\n+    val initAcc = (Double.MinValue, 0.0)\n+    // adjust differences based on the # of elements preceding it, which should provide\n+    // the correct distance between ECDF and CDF\n+    val results = localData.foldLeft(initAcc) { case ((prevMax, prevCt), (minCand, maxCand, ct)) =>\n+      val adjConst = prevCt / n\n+      val pdist1 = minCand + adjConst\n+      val pdist2 = maxCand + adjConst\n+      // adjust by 1 / N if pre-constant the value is less than cdf and post-constant\n+      // it is greater than or equal to the cdf\n+      val dist1 = if (pdist1 >= 0 && minCand < 0) pdist1 + 1 / n else Math.abs(pdist1)\n+      val dist2 = if (pdist2 >= 0 && maxCand < 0) pdist2 + 1 / n else Math.abs(pdist2)\n+      val maxVal = Array(prevMax, dist1, dist2).max\n+      (maxVal, prevCt + ct)\n+      }"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "sryza"
    },
    "body": "indent these two spaces. general rule is:\n- when you have an open curly brace, all following lines are indented two spaces until ending curly brace\n- ending curly brace is at the same level of indentation as the first non-space character on the line that has the starting curly brace\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-29T21:26:02Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+    val partDiffs = oneSampleDifferences(part, n, cdf) // local distances"
  }, {
    "author": {
      "login": "josepablocam"
    },
    "body": "done. Sorry, I misunderstood the comment before and thought you meant to un-indent the entire thing, not just the last line. Fixed now\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-06-29T22:32:49Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+    val partDiffs = oneSampleDifferences(part, n, cdf) // local distances"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "I like the algorithm:) Is there a reference, or is it original?\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-07-01T19:58:13Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we"
  }, {
    "author": {
      "login": "josepablocam"
    },
    "body": "Credit goes to @syrza for the algorithm\n\nIn mllib/src/main/scala/org/apache/spark/mllib/stat/test/KSTest.scala\nhttps://github.com/apache/spark/pull/6994#discussion_r33717824:\n\n> +\n> +/**\n> - \\* Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n> - \\* continuous distribution. By comparing the largest difference between the empirical cumulative\n> - \\* distribution of the sample data and the theoretical distribution we can provide a test for the\n> - \\* the null hypothesis that the sample data comes from that theoretical distribution.\n> - \\* For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n> - *\n> - \\* Implementation note: We seek to implement the KS test with a minimal number of distributed\n> - \\* passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n> - \\* calculate an empirical cumulative distribution value for each observation, and a theoretical\n> - \\* cumulative distribution value. We know the latter to be correct, while the former will be off by\n> - \\* a constant (how large the constant is depends on how many values precede it in other partitions).\n> - \\* However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n> - \\* and furthermore, that constant is the same within a given partition, we can pick 2 values\n> - \\* in each partition that can potentially resolve to the largest global distance. Namely, we\n\nI like the algorithm:) Is there a reference, or is it original?\n\n—\nReply to this email directly or view it on GitHub\nhttps://github.com/apache/spark/pull/6994/files#r33717824.\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-07-02T04:30:01Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "In theory, we want to output `max(cdfVal - (dl + offset), (dp + offset) - cdfVal)`, where `offset` is nonnegative. Even when `dp > cdfVal`, it is still possible that `cdfVal - (dl + offset) > (dp + offset) - cdfVal`. It might be simpler to output `(dl - cdfVal, dp - cdfVal)` pair and find min from the first and max from the second in `searchOneSampleCandidates`.\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-07-01T19:58:16Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, cdf) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param createDist `Unit => RealDistribution` function to create a theoretical distribution\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], createDist: () => RealDistribution): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, createDist) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Calculate unadjusted distances between the empirical CDF and the theoretical CDF in a\n+   * partition\n+   * @param partData `Iterator[Double]` 1 partition of a sorted RDD\n+   * @param n `Double` the total size of the RDD\n+   * @param cdf `Double => Double` a function the calculates the theoretical CDF of a value\n+   * @return `Iterator[Double] `Unadjusted (ie. off by a constant) differences between\n+   *        ECDF (empirical cumulative distribution function) and CDF. We subtract in such a way\n+   *        that when adjusted by the appropriate constant, the difference will be equivalent\n+   *        to the KS statistic calculation described in\n+   *        http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm\n+   *        where the difference is not exactly symmetric\n+   */\n+  private def oneSampleDifferences(partData: Iterator[Double], n: Double, cdf: Double => Double)\n+    : Iterator[Double] = {\n+    // zip data with index (within that partition)\n+    // calculate local (unadjusted) ECDF and subtract CDF\n+    partData.zipWithIndex.map { case (v, ix) =>\n+      // dp and dl are later adjusted by constant, when global info is available\n+      val dp = (ix + 1) / n\n+      val dl = ix / n\n+      val cdfVal = cdf(v)\n+      // if dp > cdfVal the adjusted dp is still above cdfVal, if dp < cdfVal\n+      // we want negative distance so that constant adjusted gives correct distance\n+      if (dp > cdfVal) dp - cdfVal else dl - cdfVal"
  }, {
    "author": {
      "login": "josepablocam"
    },
    "body": "Yes, you're absolutely right. I made this change, which also simplifies the searching of the global extrema, since we no longer have to adjust by 1/N. Also fixed an edge case for when a partition has no data.\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-07-07T19:00:24Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, cdf) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param createDist `Unit => RealDistribution` function to create a theoretical distribution\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], createDist: () => RealDistribution): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, createDist) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Calculate unadjusted distances between the empirical CDF and the theoretical CDF in a\n+   * partition\n+   * @param partData `Iterator[Double]` 1 partition of a sorted RDD\n+   * @param n `Double` the total size of the RDD\n+   * @param cdf `Double => Double` a function the calculates the theoretical CDF of a value\n+   * @return `Iterator[Double] `Unadjusted (ie. off by a constant) differences between\n+   *        ECDF (empirical cumulative distribution function) and CDF. We subtract in such a way\n+   *        that when adjusted by the appropriate constant, the difference will be equivalent\n+   *        to the KS statistic calculation described in\n+   *        http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm\n+   *        where the difference is not exactly symmetric\n+   */\n+  private def oneSampleDifferences(partData: Iterator[Double], n: Double, cdf: Double => Double)\n+    : Iterator[Double] = {\n+    // zip data with index (within that partition)\n+    // calculate local (unadjusted) ECDF and subtract CDF\n+    partData.zipWithIndex.map { case (v, ix) =>\n+      // dp and dl are later adjusted by constant, when global info is available\n+      val dp = (ix + 1) / n\n+      val dl = ix / n\n+      val cdfVal = cdf(v)\n+      // if dp > cdfVal the adjusted dp is still above cdfVal, if dp < cdfVal\n+      // we want negative distance so that constant adjusted gives correct distance\n+      if (dp > cdfVal) dp - cdfVal else dl - cdfVal"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`Math` -> `math`. Scala's `Math` is deprecated.\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-07-01T19:58:48Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test: https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, cdf) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param createDist `Unit => RealDistribution` function to create a theoretical distribution\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], createDist: () => RealDistribution): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, createDist) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Calculate unadjusted distances between the empirical CDF and the theoretical CDF in a\n+   * partition\n+   * @param partData `Iterator[Double]` 1 partition of a sorted RDD\n+   * @param n `Double` the total size of the RDD\n+   * @param cdf `Double => Double` a function the calculates the theoretical CDF of a value\n+   * @return `Iterator[Double] `Unadjusted (ie. off by a constant) differences between\n+   *        ECDF (empirical cumulative distribution function) and CDF. We subtract in such a way\n+   *        that when adjusted by the appropriate constant, the difference will be equivalent\n+   *        to the KS statistic calculation described in\n+   *        http://www.itl.nist.gov/div898/handbook/eda/section3/eda35g.htm\n+   *        where the difference is not exactly symmetric\n+   */\n+  private def oneSampleDifferences(partData: Iterator[Double], n: Double, cdf: Double => Double)\n+    : Iterator[Double] = {\n+    // zip data with index (within that partition)\n+    // calculate local (unadjusted) ECDF and subtract CDF\n+    partData.zipWithIndex.map { case (v, ix) =>\n+      // dp and dl are later adjusted by constant, when global info is available\n+      val dp = (ix + 1) / n\n+      val dl = ix / n\n+      val cdfVal = cdf(v)\n+      // if dp > cdfVal the adjusted dp is still above cdfVal, if dp < cdfVal\n+      // we want negative distance so that constant adjusted gives correct distance\n+      if (dp > cdfVal) dp - cdfVal else dl - cdfVal\n+    }\n+  }\n+\n+  private def oneSampleDifferences(\n+      partData: Iterator[Double],\n+      n: Double,\n+      createDist: () => RealDistribution)\n+    : Iterator[Double] = {\n+    val dist = createDist()\n+    oneSampleDifferences(partData, n, x => dist.cumulativeProbability(x))\n+  }\n+\n+  /**\n+   * Search the unadjusted differences between ECDF and CDF in a partition and return the\n+   * two extrema (furthest below and furthest above CDF), along with a count of elements in that\n+   * partition\n+   * @param partDiffs `Iterator[Double]` the unadjusted differences between ECDF and CDF in a\n+   *                 partition\n+   * @return `Iterator[(Double, Double, Double)]` the local extrema and a count of elements\n+   */\n+  private def searchOneSampleCandidates(partDiffs: Iterator[Double])\n+    : Iterator[(Double, Double, Double)] = {\n+    val initAcc = (Double.MaxValue, Double.MinValue, 0.0)\n+    val partResults = partDiffs.foldLeft(initAcc) { case ((pMin, pMax, pCt), currDiff) =>\n+      (Math.min(pMin, currDiff), Math.max(pMax, currDiff), pCt + 1)"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "If `params` is empty, shall we use standard normal instead of throwing an exception? We can generate a log message.\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-07-08T23:36:51Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test:\n+ * @see [[https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test]]\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, cdf) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param createDist `Unit => RealDistribution` function to create a theoretical distribution\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], createDist: () => RealDistribution): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, createDist) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Calculate unadjusted distances between the empirical CDF and the theoretical CDF in a\n+   * partition\n+   * @param partData `Iterator[Double]` 1 partition of a sorted RDD\n+   * @param n `Double` the total size of the RDD\n+   * @param cdf `Double => Double` a function the calculates the theoretical CDF of a value\n+   * @return `Iterator[(Double, Double)] `Unadjusted (ie. off by a constant) potential extrema\n+   *        in a partition. The first element corresponds to the (ECDF - 1/N) - CDF, the second\n+   *        element corresponds to ECDF - CDF.  We can then search the resulting iterator\n+   *        for the minimum of the first and the maximum of the second element, and provide this\n+   *        as a partition's candidate extrema\n+   */\n+  private def oneSampleDifferences(partData: Iterator[Double], n: Double, cdf: Double => Double)\n+    : Iterator[(Double, Double)] = {\n+    // zip data with index (within that partition)\n+    // calculate local (unadjusted) ECDF and subtract CDF\n+    partData.zipWithIndex.map { case (v, ix) =>\n+      // dp and dl are later adjusted by constant, when global info is available\n+      val dp = (ix + 1) / n\n+      val dl = ix / n\n+      val cdfVal = cdf(v)\n+      (dl - cdfVal, dp - cdfVal)\n+    }\n+  }\n+\n+  private def oneSampleDifferences(\n+      partData: Iterator[Double],\n+      n: Double,\n+      createDist: () => RealDistribution)\n+    : Iterator[(Double, Double)] = {\n+    val dist = createDist()\n+    oneSampleDifferences(partData, n, x => dist.cumulativeProbability(x))\n+  }\n+\n+  /**\n+   * Search the unadjusted differences in a partition and return the\n+   * two extrema (furthest below and furthest above CDF), along with a count of elements in that\n+   * partition\n+   * @param partDiffs `Iterator[(Double, Double)]` the unadjusted differences between ECDF and CDF\n+   *                 in a partition, which come as a tuple of (ECDF - 1/N - CDF, ECDF - CDF)\n+   * @return `Iterator[(Double, Double, Double)]` the local extrema and a count of elements\n+   */\n+  private def searchOneSampleCandidates(partDiffs: Iterator[(Double, Double)])\n+    : Iterator[(Double, Double, Double)] = {\n+    val initAcc = (Double.MaxValue, Double.MinValue, 0.0)\n+    val pResults = partDiffs.foldLeft(initAcc) { case ((pMin, pMax, pCt), (dl, dp)) =>\n+      (math.min(pMin, dl), math.max(pMax, dp), pCt + 1)\n+    }\n+    val results = if (pResults == initAcc) Array[(Double, Double, Double)]() else Array(pResults)\n+    results.iterator\n+  }\n+\n+  /**\n+   * Find the global maximum distance between ECDF and CDF (i.e. the KS Statistic) after adjusting\n+   * local extrema estimates from individual partitions with the amount of elements in preceding\n+   * partitions\n+   * @param localData `Array[(Double, Double, Double)]` A local array containing the collected\n+   *                 results of `searchOneSampleCandidates` across all partitions\n+   * @param n `Double`The size of the RDD\n+   * @return The one-sample Kolmogorov Smirnov Statistic\n+   */\n+  private def searchOneSampleStatistic(localData: Array[(Double, Double, Double)], n: Double)\n+    : Double = {\n+    val initAcc = (Double.MinValue, 0.0)\n+    // adjust differences based on the # of elements preceding it, which should provide\n+    // the correct distance between ECDF and CDF\n+    val results = localData.foldLeft(initAcc) { case ((prevMax, prevCt), (minCand, maxCand, ct)) =>\n+      val adjConst = prevCt / n\n+      val dist1 = math.abs(minCand + adjConst)\n+      val dist2 = math.abs(maxCand + adjConst)\n+      val maxVal = Array(prevMax, dist1, dist2).max\n+      (maxVal, prevCt + ct)\n+    }\n+    results._1\n+  }\n+\n+  /**\n+   * A convenience function that allows running the KS test for 1 set of sample data against\n+   * a named distribution\n+   * @param data the sample data that we wish to evaluate\n+   * @param distName the name of the theoretical distribution\n+   * @param params Variable length parameter for distribution's parameters\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], distName: String, params: Double*): KSTestResult = {\n+    val distanceCalc =\n+      distName match {\n+        case \"norm\" => () => {\n+          require(params.length == 2, \"Normal distribution requires mean and standard \" +"
  }, {
    "author": {
      "login": "josepablocam"
    },
    "body": "Added\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-07-09T01:13:03Z",
    "diffHunk": "@@ -0,0 +1,191 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test:\n+ * @see [[https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test]]\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution.\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, cdf) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param createDist `Unit => RealDistribution` function to create a theoretical distribution\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], createDist: () => RealDistribution): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, createDist) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Calculate unadjusted distances between the empirical CDF and the theoretical CDF in a\n+   * partition\n+   * @param partData `Iterator[Double]` 1 partition of a sorted RDD\n+   * @param n `Double` the total size of the RDD\n+   * @param cdf `Double => Double` a function the calculates the theoretical CDF of a value\n+   * @return `Iterator[(Double, Double)] `Unadjusted (ie. off by a constant) potential extrema\n+   *        in a partition. The first element corresponds to the (ECDF - 1/N) - CDF, the second\n+   *        element corresponds to ECDF - CDF.  We can then search the resulting iterator\n+   *        for the minimum of the first and the maximum of the second element, and provide this\n+   *        as a partition's candidate extrema\n+   */\n+  private def oneSampleDifferences(partData: Iterator[Double], n: Double, cdf: Double => Double)\n+    : Iterator[(Double, Double)] = {\n+    // zip data with index (within that partition)\n+    // calculate local (unadjusted) ECDF and subtract CDF\n+    partData.zipWithIndex.map { case (v, ix) =>\n+      // dp and dl are later adjusted by constant, when global info is available\n+      val dp = (ix + 1) / n\n+      val dl = ix / n\n+      val cdfVal = cdf(v)\n+      (dl - cdfVal, dp - cdfVal)\n+    }\n+  }\n+\n+  private def oneSampleDifferences(\n+      partData: Iterator[Double],\n+      n: Double,\n+      createDist: () => RealDistribution)\n+    : Iterator[(Double, Double)] = {\n+    val dist = createDist()\n+    oneSampleDifferences(partData, n, x => dist.cumulativeProbability(x))\n+  }\n+\n+  /**\n+   * Search the unadjusted differences in a partition and return the\n+   * two extrema (furthest below and furthest above CDF), along with a count of elements in that\n+   * partition\n+   * @param partDiffs `Iterator[(Double, Double)]` the unadjusted differences between ECDF and CDF\n+   *                 in a partition, which come as a tuple of (ECDF - 1/N - CDF, ECDF - CDF)\n+   * @return `Iterator[(Double, Double, Double)]` the local extrema and a count of elements\n+   */\n+  private def searchOneSampleCandidates(partDiffs: Iterator[(Double, Double)])\n+    : Iterator[(Double, Double, Double)] = {\n+    val initAcc = (Double.MaxValue, Double.MinValue, 0.0)\n+    val pResults = partDiffs.foldLeft(initAcc) { case ((pMin, pMax, pCt), (dl, dp)) =>\n+      (math.min(pMin, dl), math.max(pMax, dp), pCt + 1)\n+    }\n+    val results = if (pResults == initAcc) Array[(Double, Double, Double)]() else Array(pResults)\n+    results.iterator\n+  }\n+\n+  /**\n+   * Find the global maximum distance between ECDF and CDF (i.e. the KS Statistic) after adjusting\n+   * local extrema estimates from individual partitions with the amount of elements in preceding\n+   * partitions\n+   * @param localData `Array[(Double, Double, Double)]` A local array containing the collected\n+   *                 results of `searchOneSampleCandidates` across all partitions\n+   * @param n `Double`The size of the RDD\n+   * @return The one-sample Kolmogorov Smirnov Statistic\n+   */\n+  private def searchOneSampleStatistic(localData: Array[(Double, Double, Double)], n: Double)\n+    : Double = {\n+    val initAcc = (Double.MinValue, 0.0)\n+    // adjust differences based on the # of elements preceding it, which should provide\n+    // the correct distance between ECDF and CDF\n+    val results = localData.foldLeft(initAcc) { case ((prevMax, prevCt), (minCand, maxCand, ct)) =>\n+      val adjConst = prevCt / n\n+      val dist1 = math.abs(minCand + adjConst)\n+      val dist2 = math.abs(maxCand + adjConst)\n+      val maxVal = Array(prevMax, dist1, dist2).max\n+      (maxVal, prevCt + ct)\n+    }\n+    results._1\n+  }\n+\n+  /**\n+   * A convenience function that allows running the KS test for 1 set of sample data against\n+   * a named distribution\n+   * @param data the sample data that we wish to evaluate\n+   * @param distName the name of the theoretical distribution\n+   * @param params Variable length parameter for distribution's parameters\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], distName: String, params: Double*): KSTestResult = {\n+    val distanceCalc =\n+      distName match {\n+        case \"norm\" => () => {\n+          require(params.length == 2, \"Normal distribution requires mean and standard \" +"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "See my previous comments about the text.\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-07-09T06:30:36Z",
    "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import scala.annotation.varargs\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test:\n+ * @see [[https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test]]\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic."
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "minor: `oneSampleTwoSided` -> `OneSampleTwoSided`\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-07-09T06:30:37Z",
    "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import scala.annotation.varargs\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test:\n+ * @see [[https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test]]\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest extends Logging {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution\")"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Is it almost the same as `testOneSample(..., cdf)`? Should we reuse the code?\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-07-09T06:30:39Z",
    "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import scala.annotation.varargs\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test:\n+ * @see [[https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test]]\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest extends Logging {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, cdf) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param createDist `Unit => RealDistribution` function to create a theoretical distribution\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], createDist: () => RealDistribution): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, createDist) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)"
  }, {
    "author": {
      "login": "josepablocam"
    },
    "body": "@mengxr I've refactored the code, accounting that RealDistribution in math3 3.4.1 is serializable (which Sean had already pointed out to me). So that we now indeed use the testOneSample(..., cdf) and avoid duplication.\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-07-09T19:31:43Z",
    "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import scala.annotation.varargs\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test:\n+ * @see [[https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test]]\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest extends Logging {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, cdf) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param createDist `Unit => RealDistribution` function to create a theoretical distribution\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], createDist: () => RealDistribution): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, createDist) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`Normal` -> `normal`\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-07-09T06:30:45Z",
    "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import scala.annotation.varargs\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test:\n+ * @see [[https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test]]\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest extends Logging {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, cdf) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param createDist `Unit => RealDistribution` function to create a theoretical distribution\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], createDist: () => RealDistribution): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, createDist) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Calculate unadjusted distances between the empirical CDF and the theoretical CDF in a\n+   * partition\n+   * @param partData `Iterator[Double]` 1 partition of a sorted RDD\n+   * @param n `Double` the total size of the RDD\n+   * @param cdf `Double => Double` a function the calculates the theoretical CDF of a value\n+   * @return `Iterator[(Double, Double)] `Unadjusted (ie. off by a constant) potential extrema\n+   *        in a partition. The first element corresponds to the (ECDF - 1/N) - CDF, the second\n+   *        element corresponds to ECDF - CDF.  We can then search the resulting iterator\n+   *        for the minimum of the first and the maximum of the second element, and provide this\n+   *        as a partition's candidate extrema\n+   */\n+  private def oneSampleDifferences(partData: Iterator[Double], n: Double, cdf: Double => Double)\n+    : Iterator[(Double, Double)] = {\n+    // zip data with index (within that partition)\n+    // calculate local (unadjusted) ECDF and subtract CDF\n+    partData.zipWithIndex.map { case (v, ix) =>\n+      // dp and dl are later adjusted by constant, when global info is available\n+      val dp = (ix + 1) / n\n+      val dl = ix / n\n+      val cdfVal = cdf(v)\n+      (dl - cdfVal, dp - cdfVal)\n+    }\n+  }\n+\n+  private def oneSampleDifferences(\n+      partData: Iterator[Double],\n+      n: Double,\n+      createDist: () => RealDistribution)\n+    : Iterator[(Double, Double)] = {\n+    val dist = createDist()\n+    oneSampleDifferences(partData, n, x => dist.cumulativeProbability(x))\n+  }\n+\n+  /**\n+   * Search the unadjusted differences in a partition and return the\n+   * two extrema (furthest below and furthest above CDF), along with a count of elements in that\n+   * partition\n+   * @param partDiffs `Iterator[(Double, Double)]` the unadjusted differences between ECDF and CDF\n+   *                 in a partition, which come as a tuple of (ECDF - 1/N - CDF, ECDF - CDF)\n+   * @return `Iterator[(Double, Double, Double)]` the local extrema and a count of elements\n+   */\n+  private def searchOneSampleCandidates(partDiffs: Iterator[(Double, Double)])\n+    : Iterator[(Double, Double, Double)] = {\n+    val initAcc = (Double.MaxValue, Double.MinValue, 0.0)\n+    val pResults = partDiffs.foldLeft(initAcc) { case ((pMin, pMax, pCt), (dl, dp)) =>\n+      (math.min(pMin, dl), math.max(pMax, dp), pCt + 1)\n+    }\n+    val results = if (pResults == initAcc) Array[(Double, Double, Double)]() else Array(pResults)\n+    results.iterator\n+  }\n+\n+  /**\n+   * Find the global maximum distance between ECDF and CDF (i.e. the KS Statistic) after adjusting\n+   * local extrema estimates from individual partitions with the amount of elements in preceding\n+   * partitions\n+   * @param localData `Array[(Double, Double, Double)]` A local array containing the collected\n+   *                 results of `searchOneSampleCandidates` across all partitions\n+   * @param n `Double`The size of the RDD\n+   * @return The one-sample Kolmogorov Smirnov Statistic\n+   */\n+  private def searchOneSampleStatistic(localData: Array[(Double, Double, Double)], n: Double)\n+    : Double = {\n+    val initAcc = (Double.MinValue, 0.0)\n+    // adjust differences based on the # of elements preceding it, which should provide\n+    // the correct distance between ECDF and CDF\n+    val results = localData.foldLeft(initAcc) { case ((prevMax, prevCt), (minCand, maxCand, ct)) =>\n+      val adjConst = prevCt / n\n+      val dist1 = math.abs(minCand + adjConst)\n+      val dist2 = math.abs(maxCand + adjConst)\n+      val maxVal = Array(prevMax, dist1, dist2).max\n+      (maxVal, prevCt + ct)\n+    }\n+    results._1\n+  }\n+\n+  /**\n+   * A convenience function that allows running the KS test for 1 set of sample data against\n+   * a named distribution\n+   * @param data the sample data that we wish to evaluate\n+   * @param distName the name of the theoretical distribution\n+   * @param params Variable length parameter for distribution's parameters\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  @varargs\n+  def testOneSample(data: RDD[Double], distName: String, params: Double*): KSTestResult = {\n+    val distanceCalc =\n+      distName match {\n+        case \"norm\" => () => {\n+          if (params.nonEmpty) {\n+            // parameters are passed, then can only be 2\n+            require(params.length == 2, \"Normal distribution requires mean and standard \" +\n+              \"deviation as parameters\")\n+            new NormalDistribution(params(0), params(1))\n+          } else {\n+            // if no parameters passed in initializes to standard normal\n+            logInfo(\"No parameters specified for Normal distribution,\" +"
  }],
  "prId": 6994
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`are:[stdnorm]` -> `are: [norm]`\n",
    "commit": "08834f44cc2c2fee264931b618f5620b52026917",
    "createdAt": "2015-07-09T06:31:19Z",
    "diffHunk": "@@ -0,0 +1,203 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.stat.test\n+\n+import scala.annotation.varargs\n+\n+import org.apache.commons.math3.distribution.{NormalDistribution, RealDistribution}\n+import org.apache.commons.math3.stat.inference.KolmogorovSmirnovTest\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Conduct the two-sided Kolmogorov Smirnov test for data sampled from a\n+ * continuous distribution. By comparing the largest difference between the empirical cumulative\n+ * distribution of the sample data and the theoretical distribution we can provide a test for the\n+ * the null hypothesis that the sample data comes from that theoretical distribution.\n+ * For more information on KS Test:\n+ * @see [[https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test]]\n+ *\n+ * Implementation note: We seek to implement the KS test with a minimal number of distributed\n+ * passes. We sort the RDD, and then perform the following operations on a per-partition basis:\n+ * calculate an empirical cumulative distribution value for each observation, and a theoretical\n+ * cumulative distribution value. We know the latter to be correct, while the former will be off by\n+ * a constant (how large the constant is depends on how many values precede it in other partitions).\n+ * However, given that this constant simply shifts the ECDF upwards, but doesn't change its shape,\n+ * and furthermore, that constant is the same within a given partition, we can pick 2 values\n+ * in each partition that can potentially resolve to the largest global distance. Namely, we\n+ * pick the minimum distance and the maximum distance. Additionally, we keep track of how many\n+ * elements are in each partition. Once these three values have been returned for every partition,\n+ * we can collect and operate locally. Locally, we can now adjust each distance by the appropriate\n+ * constant (the cumulative sum of # of elements in the prior partitions divided by the data set\n+ * size). Finally, we take the maximum absolute value, and this is the statistic.\n+ */\n+private[stat] object KSTest extends Logging {\n+\n+  // Null hypothesis for the type of KS test to be included in the result.\n+  object NullHypothesis extends Enumeration {\n+    type NullHypothesis = Value\n+    val oneSampleTwoSided = Value(\"Sample follows theoretical distribution\")\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param cdf `Double => Double` function to calculate the theoretical CDF\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], cdf: Double => Double): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, cdf) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Runs a KS test for 1 set of sample data, comparing it to a theoretical distribution\n+   * @param data `RDD[Double]` data on which to run test\n+   * @param createDist `Unit => RealDistribution` function to create a theoretical distribution\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  def testOneSample(data: RDD[Double], createDist: () => RealDistribution): KSTestResult = {\n+    val n = data.count().toDouble\n+    val localData = data.sortBy(x => x).mapPartitions { part =>\n+      val partDiffs = oneSampleDifferences(part, n, createDist) // local distances\n+      searchOneSampleCandidates(partDiffs) // candidates: local extrema\n+    }.collect()\n+    val ksStat = searchOneSampleStatistic(localData, n) // result: global extreme\n+    evalOneSampleP(ksStat, n.toLong)\n+  }\n+\n+  /**\n+   * Calculate unadjusted distances between the empirical CDF and the theoretical CDF in a\n+   * partition\n+   * @param partData `Iterator[Double]` 1 partition of a sorted RDD\n+   * @param n `Double` the total size of the RDD\n+   * @param cdf `Double => Double` a function the calculates the theoretical CDF of a value\n+   * @return `Iterator[(Double, Double)] `Unadjusted (ie. off by a constant) potential extrema\n+   *        in a partition. The first element corresponds to the (ECDF - 1/N) - CDF, the second\n+   *        element corresponds to ECDF - CDF.  We can then search the resulting iterator\n+   *        for the minimum of the first and the maximum of the second element, and provide this\n+   *        as a partition's candidate extrema\n+   */\n+  private def oneSampleDifferences(partData: Iterator[Double], n: Double, cdf: Double => Double)\n+    : Iterator[(Double, Double)] = {\n+    // zip data with index (within that partition)\n+    // calculate local (unadjusted) ECDF and subtract CDF\n+    partData.zipWithIndex.map { case (v, ix) =>\n+      // dp and dl are later adjusted by constant, when global info is available\n+      val dp = (ix + 1) / n\n+      val dl = ix / n\n+      val cdfVal = cdf(v)\n+      (dl - cdfVal, dp - cdfVal)\n+    }\n+  }\n+\n+  private def oneSampleDifferences(\n+      partData: Iterator[Double],\n+      n: Double,\n+      createDist: () => RealDistribution)\n+    : Iterator[(Double, Double)] = {\n+    val dist = createDist()\n+    oneSampleDifferences(partData, n, x => dist.cumulativeProbability(x))\n+  }\n+\n+  /**\n+   * Search the unadjusted differences in a partition and return the\n+   * two extrema (furthest below and furthest above CDF), along with a count of elements in that\n+   * partition\n+   * @param partDiffs `Iterator[(Double, Double)]` the unadjusted differences between ECDF and CDF\n+   *                 in a partition, which come as a tuple of (ECDF - 1/N - CDF, ECDF - CDF)\n+   * @return `Iterator[(Double, Double, Double)]` the local extrema and a count of elements\n+   */\n+  private def searchOneSampleCandidates(partDiffs: Iterator[(Double, Double)])\n+    : Iterator[(Double, Double, Double)] = {\n+    val initAcc = (Double.MaxValue, Double.MinValue, 0.0)\n+    val pResults = partDiffs.foldLeft(initAcc) { case ((pMin, pMax, pCt), (dl, dp)) =>\n+      (math.min(pMin, dl), math.max(pMax, dp), pCt + 1)\n+    }\n+    val results = if (pResults == initAcc) Array[(Double, Double, Double)]() else Array(pResults)\n+    results.iterator\n+  }\n+\n+  /**\n+   * Find the global maximum distance between ECDF and CDF (i.e. the KS Statistic) after adjusting\n+   * local extrema estimates from individual partitions with the amount of elements in preceding\n+   * partitions\n+   * @param localData `Array[(Double, Double, Double)]` A local array containing the collected\n+   *                 results of `searchOneSampleCandidates` across all partitions\n+   * @param n `Double`The size of the RDD\n+   * @return The one-sample Kolmogorov Smirnov Statistic\n+   */\n+  private def searchOneSampleStatistic(localData: Array[(Double, Double, Double)], n: Double)\n+    : Double = {\n+    val initAcc = (Double.MinValue, 0.0)\n+    // adjust differences based on the # of elements preceding it, which should provide\n+    // the correct distance between ECDF and CDF\n+    val results = localData.foldLeft(initAcc) { case ((prevMax, prevCt), (minCand, maxCand, ct)) =>\n+      val adjConst = prevCt / n\n+      val dist1 = math.abs(minCand + adjConst)\n+      val dist2 = math.abs(maxCand + adjConst)\n+      val maxVal = Array(prevMax, dist1, dist2).max\n+      (maxVal, prevCt + ct)\n+    }\n+    results._1\n+  }\n+\n+  /**\n+   * A convenience function that allows running the KS test for 1 set of sample data against\n+   * a named distribution\n+   * @param data the sample data that we wish to evaluate\n+   * @param distName the name of the theoretical distribution\n+   * @param params Variable length parameter for distribution's parameters\n+   * @return KSTestResult summarizing the test results (pval, statistic, and null hypothesis)\n+   */\n+  @varargs\n+  def testOneSample(data: RDD[Double], distName: String, params: Double*): KSTestResult = {\n+    val distanceCalc =\n+      distName match {\n+        case \"norm\" => () => {\n+          if (params.nonEmpty) {\n+            // parameters are passed, then can only be 2\n+            require(params.length == 2, \"Normal distribution requires mean and standard \" +\n+              \"deviation as parameters\")\n+            new NormalDistribution(params(0), params(1))\n+          } else {\n+            // if no parameters passed in initializes to standard normal\n+            logInfo(\"No parameters specified for Normal distribution,\" +\n+              \"initialized to standard normal (i.e. N(0, 1))\")\n+            new NormalDistribution(0, 1)\n+          }\n+        }\n+        case  _ => throw new UnsupportedOperationException(s\"$distName not yet supported through\" +\n+          s\" convenience method. Current options are:[stdnorm].\")"
  }],
  "prId": 6994
}]