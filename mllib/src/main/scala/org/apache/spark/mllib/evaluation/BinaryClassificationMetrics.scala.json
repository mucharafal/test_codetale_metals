[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "I'm not sure if this class has ever been used with Java, but doesn't this break binary compatibility with Java (b/c of the default parameter)?  Should you add a separate 1-argument constructor just taking scoreAndLabels?\n",
    "commit": "1d34d05c499ea7be6327da68c9bd2457ffe5aa59",
    "createdAt": "2014-12-22T19:05:45Z",
    "diffHunk": "@@ -28,9 +28,23 @@ import org.apache.spark.rdd.{RDD, UnionRDD}\n  * Evaluator for binary classification.\n  *\n  * @param scoreAndLabels an RDD of (score, label) pairs.\n+ * @param numBins if greater than 0, then the curves (ROC curve, PR curve) computed internally\n+ *  will be down-sampled to this many \"bins\". This is useful because the curve contains a\n+ *  point for each distinct score in the input, and this could be as large as the input itself --\n+ *  millions of points or more, when thousands may be entirely sufficient to summarize the curve.\n+ *  After down-sampling, the curves will instead be made of approximately `numBins` points instead.\n+ *  Points are made from bins of equal numbers of consecutive points. The size of each bin\n+ *  is `floor(scoreAndLabels.count() / numBins)`, which means the resulting number of bins\n+ *  may not exactly equal numBins. The last bin in each partition may be smaller as a result,\n+ *  meaning there may be an extra sample at partition boundaries.\n+ *  If `numBins` is 0, no down-sampling will occur.\n  */\n @Experimental\n-class BinaryClassificationMetrics(scoreAndLabels: RDD[(Double, Double)]) extends Logging {\n+class BinaryClassificationMetrics(\n+    val scoreAndLabels: RDD[(Double, Double)],\n+    val numBins: Int = 0) extends Logging {"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Ah probably. What about just adding a setter? \n",
    "commit": "1d34d05c499ea7be6327da68c9bd2457ffe5aa59",
    "createdAt": "2014-12-22T22:01:03Z",
    "diffHunk": "@@ -28,9 +28,23 @@ import org.apache.spark.rdd.{RDD, UnionRDD}\n  * Evaluator for binary classification.\n  *\n  * @param scoreAndLabels an RDD of (score, label) pairs.\n+ * @param numBins if greater than 0, then the curves (ROC curve, PR curve) computed internally\n+ *  will be down-sampled to this many \"bins\". This is useful because the curve contains a\n+ *  point for each distinct score in the input, and this could be as large as the input itself --\n+ *  millions of points or more, when thousands may be entirely sufficient to summarize the curve.\n+ *  After down-sampling, the curves will instead be made of approximately `numBins` points instead.\n+ *  Points are made from bins of equal numbers of consecutive points. The size of each bin\n+ *  is `floor(scoreAndLabels.count() / numBins)`, which means the resulting number of bins\n+ *  may not exactly equal numBins. The last bin in each partition may be smaller as a result,\n+ *  meaning there may be an extra sample at partition boundaries.\n+ *  If `numBins` is 0, no down-sampling will occur.\n  */\n @Experimental\n-class BinaryClassificationMetrics(scoreAndLabels: RDD[(Double, Double)]) extends Logging {\n+class BinaryClassificationMetrics(\n+    val scoreAndLabels: RDD[(Double, Double)],\n+    val numBins: Int = 0) extends Logging {"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "That sounds good to me too.\n",
    "commit": "1d34d05c499ea7be6327da68c9bd2457ffe5aa59",
    "createdAt": "2014-12-22T22:38:48Z",
    "diffHunk": "@@ -28,9 +28,23 @@ import org.apache.spark.rdd.{RDD, UnionRDD}\n  * Evaluator for binary classification.\n  *\n  * @param scoreAndLabels an RDD of (score, label) pairs.\n+ * @param numBins if greater than 0, then the curves (ROC curve, PR curve) computed internally\n+ *  will be down-sampled to this many \"bins\". This is useful because the curve contains a\n+ *  point for each distinct score in the input, and this could be as large as the input itself --\n+ *  millions of points or more, when thousands may be entirely sufficient to summarize the curve.\n+ *  After down-sampling, the curves will instead be made of approximately `numBins` points instead.\n+ *  Points are made from bins of equal numbers of consecutive points. The size of each bin\n+ *  is `floor(scoreAndLabels.count() / numBins)`, which means the resulting number of bins\n+ *  may not exactly equal numBins. The last bin in each partition may be smaller as a result,\n+ *  meaning there may be an extra sample at partition boundaries.\n+ *  If `numBins` is 0, no down-sampling will occur.\n  */\n @Experimental\n-class BinaryClassificationMetrics(scoreAndLabels: RDD[(Double, Double)]) extends Logging {\n+class BinaryClassificationMetrics(\n+    val scoreAndLabels: RDD[(Double, Double)],\n+    val numBins: Int = 0) extends Logging {"
  }],
  "prId": 3702
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "I think this should set grouping to Int.MaxValue  (and print a warning) since it is these really big datasets which cause problems.  The default behavior should avoid failure.\n",
    "commit": "1d34d05c499ea7be6327da68c9bd2457ffe5aa59",
    "createdAt": "2014-12-22T19:05:47Z",
    "diffHunk": "@@ -103,7 +117,37 @@ class BinaryClassificationMetrics(scoreAndLabels: RDD[(Double, Double)]) extends\n       mergeValue = (c: BinaryLabelCounter, label: Double) => c += label,\n       mergeCombiners = (c1: BinaryLabelCounter, c2: BinaryLabelCounter) => c1 += c2\n     ).sortByKey(ascending = false)\n-    val agg = counts.values.mapPartitions { iter =>\n+\n+    val binnedCounts =\n+      // Only down-sample if bins is > 0\n+      if (numBins == 0) {\n+        // Use original directly\n+        counts\n+      } else {\n+        val countsSize = counts.count()\n+        // Group the iterator into chunks of about countsSize / numBins points,\n+        // so that the resulting number of bins is about numBins\n+        val grouping = countsSize / numBins\n+        if (grouping < 2) {\n+          // numBins was more than half of the size; no real point in down-sampling to bins\n+          logInfo(s\"Curve is too small ($countsSize) for $numBins bins to be useful\")\n+          counts\n+        } else if (grouping >= Int.MaxValue) {\n+          logWarning(s\"Curve is too large ($countsSize) for $numBins bins; ignoring\")"
  }],
  "prId": 3702
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Indent comment?  Also, looking at this, I like having the explanation, but it might be nice to format as:\nQuick description.\nDefault value.\nCaveat about approximation\n",
    "commit": "1d34d05c499ea7be6327da68c9bd2457ffe5aa59",
    "createdAt": "2014-12-23T20:12:10Z",
    "diffHunk": "@@ -28,9 +28,28 @@ import org.apache.spark.rdd.{RDD, UnionRDD}\n  * Evaluator for binary classification.\n  *\n  * @param scoreAndLabels an RDD of (score, label) pairs.\n+ * @param numBins if greater than 0, then the curves (ROC curve, PR curve) computed internally\n+ *  will be down-sampled to this many \"bins\". This is useful because the curve contains a"
  }],
  "prId": 3702
}]