[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Should be private.\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-12T23:43:51Z",
    "diffHunk": "@@ -83,5 +95,87 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  object SaveLoadV1_0 {"
  }, {
    "author": {
      "login": "MechCoder"
    },
    "body": "Agreed it should be private, but then it should be private in all other files as well.\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-13T19:35:00Z",
    "diffHunk": "@@ -83,5 +95,87 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  object SaveLoadV1_0 {"
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Shall we flatten them? So if one inspect the Parquet file, they can see each center as a record. Btw, I'm not sure whether we should implement `MatrixUDT` first or use dense arrays here. \n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-12T23:43:52Z",
    "diffHunk": "@@ -83,5 +95,87 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  object SaveLoadV1_0 {\n+\n+    case class Data(weights: Array[Double], mus: Array[Vector], sigmas: Array[Array[Double]])"
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove space before `: Unit`\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-12T23:43:53Z",
    "diffHunk": "@@ -83,5 +95,87 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  object SaveLoadV1_0 {\n+\n+    case class Data(weights: Array[Double], mus: Array[Vector], sigmas: Array[Array[Double]])\n+\n+    def formatVersionV1_0 = \"1.0\"\n+\n+    def classNameV1_0 = \"org.apache.spark.mllib.clustering.GaussianMixtureModel\"\n+\n+    def save(\n+        sc: SparkContext,\n+        path: String,\n+        weights: Array[Double],\n+        gaussians: Array[MultivariateGaussian]) : Unit = {"
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "As I mentioned before, let's flatten the data into rows, where each row corresponds to a Gaussian distribution.\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-23T20:32:57Z",
    "diffHunk": "@@ -83,5 +95,82 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  private object SaveLoadV1_0 {\n+\n+    case class Data(weights: Array[Double], mus: Array[Vector], sigmas: Array[Matrix])"
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`weight`, `mu`, and `sigma`.\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-24T18:58:18Z",
    "diffHunk": "@@ -83,5 +95,81 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  private object SaveLoadV1_0 {\n+\n+    case class Data(weights: Double, mus: Vector, sigmas: Matrix)"
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "This is not efficient because it may trigger multiple passes to the parquet file. Let's call `collect()` directly.\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-24T18:59:41Z",
    "diffHunk": "@@ -83,5 +95,81 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  private object SaveLoadV1_0 {\n+\n+    case class Data(weights: Double, mus: Vector, sigmas: Matrix)\n+\n+    def formatVersionV1_0 = \"1.0\"\n+\n+    def classNameV1_0 = \"org.apache.spark.mllib.clustering.GaussianMixtureModel\"\n+\n+    def save(\n+        sc: SparkContext,\n+        path: String,\n+        weights: Array[Double],\n+        gaussians: Array[MultivariateGaussian]): Unit = {\n+\n+      val sqlContext = new SQLContext(sc)\n+      import sqlContext.implicits._\n+\n+      // Create JSON metadata.\n+      val metadata = compact(render\n+        ((\"class\" -> classNameV1_0) ~ (\"version\" -> formatVersionV1_0) ~ (\"k\" -> weights.length)))\n+      sc.parallelize(Seq(metadata), 1).saveAsTextFile(Loader.metadataPath(path))\n+\n+      // Create Parquet data.\n+      val dataArray = Array.tabulate(weights.length){ i =>\n+        Data(weights(i), gaussians(i).mu, gaussians(i).sigma)\n+      }\n+      val dataRDD: DataFrame = sc.parallelize(dataArray, 1).toDF()\n+      dataRDD.saveAsParquetFile(Loader.dataPath(path))\n+    }\n+\n+    def load(sc: SparkContext, path: String) : GaussianMixtureModel = {\n+      val datapath = Loader.dataPath(path)\n+      val sqlContext = new SQLContext(sc)\n+      val dataRDD = sqlContext.parquetFile(datapath)\n+      val numGaussians = dataRDD.count().toInt\n+      val dataArray = dataRDD.select(\"weights\", \"mus\", \"sigmas\").take(numGaussians)"
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`${model.gaussians.length}`\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-24T19:00:23Z",
    "diffHunk": "@@ -83,5 +95,81 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  private object SaveLoadV1_0 {\n+\n+    case class Data(weights: Double, mus: Vector, sigmas: Matrix)\n+\n+    def formatVersionV1_0 = \"1.0\"\n+\n+    def classNameV1_0 = \"org.apache.spark.mllib.clustering.GaussianMixtureModel\"\n+\n+    def save(\n+        sc: SparkContext,\n+        path: String,\n+        weights: Array[Double],\n+        gaussians: Array[MultivariateGaussian]): Unit = {\n+\n+      val sqlContext = new SQLContext(sc)\n+      import sqlContext.implicits._\n+\n+      // Create JSON metadata.\n+      val metadata = compact(render\n+        ((\"class\" -> classNameV1_0) ~ (\"version\" -> formatVersionV1_0) ~ (\"k\" -> weights.length)))\n+      sc.parallelize(Seq(metadata), 1).saveAsTextFile(Loader.metadataPath(path))\n+\n+      // Create Parquet data.\n+      val dataArray = Array.tabulate(weights.length){ i =>\n+        Data(weights(i), gaussians(i).mu, gaussians(i).sigma)\n+      }\n+      val dataRDD: DataFrame = sc.parallelize(dataArray, 1).toDF()\n+      dataRDD.saveAsParquetFile(Loader.dataPath(path))\n+    }\n+\n+    def load(sc: SparkContext, path: String) : GaussianMixtureModel = {\n+      val datapath = Loader.dataPath(path)\n+      val sqlContext = new SQLContext(sc)\n+      val dataRDD = sqlContext.parquetFile(datapath)\n+      val numGaussians = dataRDD.count().toInt\n+      val dataArray = dataRDD.select(\"weights\", \"mus\", \"sigmas\").take(numGaussians)\n+      // Check schema explicitly since erasure makes it hard to use match-case for checking.\n+      Loader.checkSchema[Data](dataRDD.schema)\n+\n+      val (weights, gaussians) = Array.tabulate(numGaussians) { i =>\n+        val currentMu = dataArray(i).getAs[Vector](1)\n+        val currentSigma = dataArray(i).getAs[Matrix](2)\n+        (dataArray(i).getAs[Double](0), new MultivariateGaussian(currentMu, currentSigma))\n+      }.unzip\n+\n+      return new GaussianMixtureModel(weights.toArray, gaussians.toArray)\n+    }\n+  }\n+\n+  override def load(sc: SparkContext, path: String) : GaussianMixtureModel = {\n+    val (loadedClassName, version, metadata) = Loader.loadMetadata(sc, path)\n+    implicit val formats = DefaultFormats\n+    val k = (metadata \\ \"k\").extract[Int]\n+    val classNameV1_0 = SaveLoadV1_0.classNameV1_0\n+    (loadedClassName, version) match {\n+      case (classNameV1_0, \"1.0\") => {\n+        val model = SaveLoadV1_0.load(sc, path)\n+        require(model.weights.length == k,\n+          s\"GaussianMixtureModel requires weights of length $k \" +\n+          s\"got weights of length $model.weights.length\")\n+        require(model.gaussians.length == k,\n+          s\"GaussianMixtureModel requires gaussians of length $k\" +\n+          s\"got gaussians of length $model.gaussians.length\")"
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove the space before `:`\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-24T20:15:32Z",
    "diffHunk": "@@ -41,10 +47,16 @@ import org.apache.spark.rdd.RDD\n @Experimental\n class GaussianMixtureModel(\n   val weights: Array[Double], \n-  val gaussians: Array[MultivariateGaussian]) extends Serializable {\n+  val gaussians: Array[MultivariateGaussian]) extends Serializable with Saveable{\n   \n   require(weights.length == gaussians.length, \"Length of weight and Gaussian arrays must match\")\n-  \n+\n+  override protected def formatVersion = \"1.0\"\n+\n+  override def save(sc: SparkContext, path: String) : Unit = {"
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`def` -> `val`\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-24T20:15:34Z",
    "diffHunk": "@@ -83,5 +95,81 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  private object SaveLoadV1_0 {\n+\n+    case class Data(weight: Double, mu: Vector, sigma: Matrix)\n+\n+    def formatVersionV1_0 = \"1.0\""
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`def` -> `val`\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-24T20:15:35Z",
    "diffHunk": "@@ -83,5 +95,81 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  private object SaveLoadV1_0 {\n+\n+    case class Data(weight: Double, mu: Vector, sigma: Matrix)\n+\n+    def formatVersionV1_0 = \"1.0\"\n+\n+    def classNameV1_0 = \"org.apache.spark.mllib.clustering.GaussianMixtureModel\""
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "space bofore `{`\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-24T20:15:37Z",
    "diffHunk": "@@ -83,5 +95,81 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  private object SaveLoadV1_0 {\n+\n+    case class Data(weight: Double, mu: Vector, sigma: Matrix)\n+\n+    def formatVersionV1_0 = \"1.0\"\n+\n+    def classNameV1_0 = \"org.apache.spark.mllib.clustering.GaussianMixtureModel\"\n+\n+    def save(\n+        sc: SparkContext,\n+        path: String,\n+        weights: Array[Double],\n+        gaussians: Array[MultivariateGaussian]): Unit = {\n+\n+      val sqlContext = new SQLContext(sc)\n+      import sqlContext.implicits._\n+\n+      // Create JSON metadata.\n+      val metadata = compact(render\n+        ((\"class\" -> classNameV1_0) ~ (\"version\" -> formatVersionV1_0) ~ (\"k\" -> weights.length)))\n+      sc.parallelize(Seq(metadata), 1).saveAsTextFile(Loader.metadataPath(path))\n+\n+      // Create Parquet data.\n+      val dataArray = Array.tabulate(weights.length){ i =>"
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Chain operations.\n\n``` scala\n sc.parallelize(dataArray, 1).toDF.saveAsParquetFile(Loader.dataPath(path))\n```\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-24T20:15:38Z",
    "diffHunk": "@@ -83,5 +95,81 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  private object SaveLoadV1_0 {\n+\n+    case class Data(weight: Double, mu: Vector, sigma: Matrix)\n+\n+    def formatVersionV1_0 = \"1.0\"\n+\n+    def classNameV1_0 = \"org.apache.spark.mllib.clustering.GaussianMixtureModel\"\n+\n+    def save(\n+        sc: SparkContext,\n+        path: String,\n+        weights: Array[Double],\n+        gaussians: Array[MultivariateGaussian]): Unit = {\n+\n+      val sqlContext = new SQLContext(sc)\n+      import sqlContext.implicits._\n+\n+      // Create JSON metadata.\n+      val metadata = compact(render\n+        ((\"class\" -> classNameV1_0) ~ (\"version\" -> formatVersionV1_0) ~ (\"k\" -> weights.length)))\n+      sc.parallelize(Seq(metadata), 1).saveAsTextFile(Loader.metadataPath(path))\n+\n+      // Create Parquet data.\n+      val dataArray = Array.tabulate(weights.length){ i =>\n+        Data(weights(i), gaussians(i).mu, gaussians(i).sigma)\n+      }\n+      val dataRDD: DataFrame = sc.parallelize(dataArray, 1).toDF()\n+      dataRDD.saveAsParquetFile(Loader.dataPath(path))"
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove the space before `:`\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-24T20:15:41Z",
    "diffHunk": "@@ -83,5 +95,81 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  private object SaveLoadV1_0 {\n+\n+    case class Data(weight: Double, mu: Vector, sigma: Matrix)\n+\n+    def formatVersionV1_0 = \"1.0\"\n+\n+    def classNameV1_0 = \"org.apache.spark.mllib.clustering.GaussianMixtureModel\"\n+\n+    def save(\n+        sc: SparkContext,\n+        path: String,\n+        weights: Array[Double],\n+        gaussians: Array[MultivariateGaussian]): Unit = {\n+\n+      val sqlContext = new SQLContext(sc)\n+      import sqlContext.implicits._\n+\n+      // Create JSON metadata.\n+      val metadata = compact(render\n+        ((\"class\" -> classNameV1_0) ~ (\"version\" -> formatVersionV1_0) ~ (\"k\" -> weights.length)))\n+      sc.parallelize(Seq(metadata), 1).saveAsTextFile(Loader.metadataPath(path))\n+\n+      // Create Parquet data.\n+      val dataArray = Array.tabulate(weights.length){ i =>\n+        Data(weights(i), gaussians(i).mu, gaussians(i).sigma)\n+      }\n+      val dataRDD: DataFrame = sc.parallelize(dataArray, 1).toDF()\n+      dataRDD.saveAsParquetFile(Loader.dataPath(path))\n+    }\n+\n+    def load(sc: SparkContext, path: String) : GaussianMixtureModel = {"
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`datapath` -> `dataPath` (camelCase)\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-24T20:15:43Z",
    "diffHunk": "@@ -83,5 +95,81 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  private object SaveLoadV1_0 {\n+\n+    case class Data(weight: Double, mu: Vector, sigma: Matrix)\n+\n+    def formatVersionV1_0 = \"1.0\"\n+\n+    def classNameV1_0 = \"org.apache.spark.mllib.clustering.GaussianMixtureModel\"\n+\n+    def save(\n+        sc: SparkContext,\n+        path: String,\n+        weights: Array[Double],\n+        gaussians: Array[MultivariateGaussian]): Unit = {\n+\n+      val sqlContext = new SQLContext(sc)\n+      import sqlContext.implicits._\n+\n+      // Create JSON metadata.\n+      val metadata = compact(render\n+        ((\"class\" -> classNameV1_0) ~ (\"version\" -> formatVersionV1_0) ~ (\"k\" -> weights.length)))\n+      sc.parallelize(Seq(metadata), 1).saveAsTextFile(Loader.metadataPath(path))\n+\n+      // Create Parquet data.\n+      val dataArray = Array.tabulate(weights.length){ i =>\n+        Data(weights(i), gaussians(i).mu, gaussians(i).sigma)\n+      }\n+      val dataRDD: DataFrame = sc.parallelize(dataArray, 1).toDF()\n+      dataRDD.saveAsParquetFile(Loader.dataPath(path))\n+    }\n+\n+    def load(sc: SparkContext, path: String) : GaussianMixtureModel = {\n+      val datapath = Loader.dataPath(path)"
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`DataFrame` is no longer an RDD.\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-24T20:15:45Z",
    "diffHunk": "@@ -83,5 +95,81 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  private object SaveLoadV1_0 {\n+\n+    case class Data(weight: Double, mu: Vector, sigma: Matrix)\n+\n+    def formatVersionV1_0 = \"1.0\"\n+\n+    def classNameV1_0 = \"org.apache.spark.mllib.clustering.GaussianMixtureModel\"\n+\n+    def save(\n+        sc: SparkContext,\n+        path: String,\n+        weights: Array[Double],\n+        gaussians: Array[MultivariateGaussian]): Unit = {\n+\n+      val sqlContext = new SQLContext(sc)\n+      import sqlContext.implicits._\n+\n+      // Create JSON metadata.\n+      val metadata = compact(render\n+        ((\"class\" -> classNameV1_0) ~ (\"version\" -> formatVersionV1_0) ~ (\"k\" -> weights.length)))\n+      sc.parallelize(Seq(metadata), 1).saveAsTextFile(Loader.metadataPath(path))\n+\n+      // Create Parquet data.\n+      val dataArray = Array.tabulate(weights.length){ i =>\n+        Data(weights(i), gaussians(i).mu, gaussians(i).sigma)\n+      }\n+      val dataRDD: DataFrame = sc.parallelize(dataArray, 1).toDF()\n+      dataRDD.saveAsParquetFile(Loader.dataPath(path))\n+    }\n+\n+    def load(sc: SparkContext, path: String) : GaussianMixtureModel = {\n+      val datapath = Loader.dataPath(path)\n+      val sqlContext = new SQLContext(sc)\n+      val dataRDD = sqlContext.parquetFile(datapath)"
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "It is not necessary to call `count()`. You can call `val numGaussians = dataArray.length` after `collect()`.\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-24T20:15:46Z",
    "diffHunk": "@@ -83,5 +95,81 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  private object SaveLoadV1_0 {\n+\n+    case class Data(weight: Double, mu: Vector, sigma: Matrix)\n+\n+    def formatVersionV1_0 = \"1.0\"\n+\n+    def classNameV1_0 = \"org.apache.spark.mllib.clustering.GaussianMixtureModel\"\n+\n+    def save(\n+        sc: SparkContext,\n+        path: String,\n+        weights: Array[Double],\n+        gaussians: Array[MultivariateGaussian]): Unit = {\n+\n+      val sqlContext = new SQLContext(sc)\n+      import sqlContext.implicits._\n+\n+      // Create JSON metadata.\n+      val metadata = compact(render\n+        ((\"class\" -> classNameV1_0) ~ (\"version\" -> formatVersionV1_0) ~ (\"k\" -> weights.length)))\n+      sc.parallelize(Seq(metadata), 1).saveAsTextFile(Loader.metadataPath(path))\n+\n+      // Create Parquet data.\n+      val dataArray = Array.tabulate(weights.length){ i =>\n+        Data(weights(i), gaussians(i).mu, gaussians(i).sigma)\n+      }\n+      val dataRDD: DataFrame = sc.parallelize(dataArray, 1).toDF()\n+      dataRDD.saveAsParquetFile(Loader.dataPath(path))\n+    }\n+\n+    def load(sc: SparkContext, path: String) : GaussianMixtureModel = {\n+      val datapath = Loader.dataPath(path)\n+      val sqlContext = new SQLContext(sc)\n+      val dataRDD = sqlContext.parquetFile(datapath)\n+      val numGaussians = dataRDD.count().toInt"
  }],
  "prId": 4986
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`getAs[Double]` -> `getDouble`. Actually, it is better to write\n\n``` scala\nval (weights, gaussians) = dataArray.map { case Row(weight: Double, mu: Vector, sigma: Matrix) =>\n  (weight, new MultivariateGaussian(mu, sigma))   \n}.unzip\n```\n",
    "commit": "7d2cd561bcb0b3071c21b622ab1bbd69bdb64545",
    "createdAt": "2015-03-24T20:15:48Z",
    "diffHunk": "@@ -83,5 +95,81 @@ class GaussianMixtureModel(\n       p(i) /= pSum\n     }\n     p\n-  }  \n+  }\n+}\n+\n+@Experimental\n+object GaussianMixtureModel extends Loader[GaussianMixtureModel] {\n+\n+  private object SaveLoadV1_0 {\n+\n+    case class Data(weight: Double, mu: Vector, sigma: Matrix)\n+\n+    def formatVersionV1_0 = \"1.0\"\n+\n+    def classNameV1_0 = \"org.apache.spark.mllib.clustering.GaussianMixtureModel\"\n+\n+    def save(\n+        sc: SparkContext,\n+        path: String,\n+        weights: Array[Double],\n+        gaussians: Array[MultivariateGaussian]): Unit = {\n+\n+      val sqlContext = new SQLContext(sc)\n+      import sqlContext.implicits._\n+\n+      // Create JSON metadata.\n+      val metadata = compact(render\n+        ((\"class\" -> classNameV1_0) ~ (\"version\" -> formatVersionV1_0) ~ (\"k\" -> weights.length)))\n+      sc.parallelize(Seq(metadata), 1).saveAsTextFile(Loader.metadataPath(path))\n+\n+      // Create Parquet data.\n+      val dataArray = Array.tabulate(weights.length){ i =>\n+        Data(weights(i), gaussians(i).mu, gaussians(i).sigma)\n+      }\n+      val dataRDD: DataFrame = sc.parallelize(dataArray, 1).toDF()\n+      dataRDD.saveAsParquetFile(Loader.dataPath(path))\n+    }\n+\n+    def load(sc: SparkContext, path: String) : GaussianMixtureModel = {\n+      val datapath = Loader.dataPath(path)\n+      val sqlContext = new SQLContext(sc)\n+      val dataRDD = sqlContext.parquetFile(datapath)\n+      val numGaussians = dataRDD.count().toInt\n+      val dataArray = dataRDD.select(\"weight\", \"mu\", \"sigma\").collect()\n+      // Check schema explicitly since erasure makes it hard to use match-case for checking.\n+      Loader.checkSchema[Data](dataRDD.schema)\n+\n+      val (weights, gaussians) = Array.tabulate(numGaussians) { i =>\n+        val currentMu = dataArray(i).getAs[Vector](1)\n+        val currentSigma = dataArray(i).getAs[Matrix](2)\n+        (dataArray(i).getAs[Double](0), new MultivariateGaussian(currentMu, currentSigma))"
  }],
  "prId": 4986
}]