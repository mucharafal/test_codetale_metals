[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "old comments\n",
    "commit": "2ea711c96990c24ed29a4b9476a3809ef26403d6",
    "createdAt": "2014-09-19T22:05:49Z",
    "diffHunk": "@@ -111,18 +112,22 @@ class L1Updater extends Updater {\n       regParam: Double): (Vector, Double) = {\n     val thisIterStepSize = stepSize / math.sqrt(iter)\n     // Take gradient step\n-    val brzWeights: BV[Double] = weightsOld.toBreeze.toDenseVector\n-    brzAxpy(-thisIterStepSize, gradient.toBreeze, brzWeights)\n+    //println(s\"\\n$iter:\")",
    "line": 51
  }],
  "prId": 2451
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "update doc (matrix size)\n",
    "commit": "2ea711c96990c24ed29a4b9476a3809ef26403d6",
    "createdAt": "2014-09-19T22:12:11Z",
    "diffHunk": "@@ -145,12 +150,151 @@ class SquaredL2Updater extends Updater {\n     // w' = w - thisIterStepSize * (gradient + regParam * w)\n     // w' = (1 - thisIterStepSize * regParam) * w - thisIterStepSize * gradient\n     val thisIterStepSize = stepSize / math.sqrt(iter)\n-    val brzWeights: BV[Double] = weightsOld.toBreeze.toDenseVector\n-    brzWeights :*= (1.0 - thisIterStepSize * regParam)\n-    brzAxpy(-thisIterStepSize, gradient.toBreeze, brzWeights)\n-    val norm = brzNorm(brzWeights, 2.0)\n+    scal(1.0 - thisIterStepSize * regParam, weightsOld)\n+    axpy(-thisIterStepSize, gradient, weightsOld)\n+    val norm = brzNorm(weightsOld.toBreeze, 2.0)\n \n-    (Vectors.fromBreeze(brzWeights), 0.5 * regParam * norm * norm)\n+    (weightsOld, 0.5 * regParam * norm * norm)\n   }\n }\n \n+/**\n+ * :: DeveloperApi ::\n+ * Class used to perform steps (weight update) using Gradient Descent methods.\n+ *\n+ * For general minimization problems, or for regularized problems of the form\n+ *         min  L(w) + regParam * R(w),\n+ * the compute function performs the actual update step, when given some\n+ * (e.g. stochastic) gradient direction for the loss L(w),\n+ * and a desired step-size (learning rate).\n+ *\n+ * The updater is responsible to also perform the update coming from the\n+ * regularization term R(w) (if any regularization is used).\n+ */\n+@DeveloperApi\n+abstract class MultiModelUpdater extends Serializable {\n+  /**\n+   * Compute an updated value for weights given the gradient, stepSize, iteration number and\n+   * regularization parameter. Also returns the regularization value regParam * R(w)\n+   * computed using the *updated* weights.\n+   *\n+   * @param weightsOld - Column matrix of size dx1 where d is the number of features.",
    "line": 121
  }],
  "prId": 2451
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "spacing\n",
    "commit": "2ea711c96990c24ed29a4b9476a3809ef26403d6",
    "createdAt": "2014-09-19T22:13:49Z",
    "diffHunk": "@@ -145,12 +150,151 @@ class SquaredL2Updater extends Updater {\n     // w' = w - thisIterStepSize * (gradient + regParam * w)\n     // w' = (1 - thisIterStepSize * regParam) * w - thisIterStepSize * gradient\n     val thisIterStepSize = stepSize / math.sqrt(iter)\n-    val brzWeights: BV[Double] = weightsOld.toBreeze.toDenseVector\n-    brzWeights :*= (1.0 - thisIterStepSize * regParam)\n-    brzAxpy(-thisIterStepSize, gradient.toBreeze, brzWeights)\n-    val norm = brzNorm(brzWeights, 2.0)\n+    scal(1.0 - thisIterStepSize * regParam, weightsOld)\n+    axpy(-thisIterStepSize, gradient, weightsOld)\n+    val norm = brzNorm(weightsOld.toBreeze, 2.0)\n \n-    (Vectors.fromBreeze(brzWeights), 0.5 * regParam * norm * norm)\n+    (weightsOld, 0.5 * regParam * norm * norm)\n   }\n }\n \n+/**\n+ * :: DeveloperApi ::\n+ * Class used to perform steps (weight update) using Gradient Descent methods.\n+ *\n+ * For general minimization problems, or for regularized problems of the form\n+ *         min  L(w) + regParam * R(w),\n+ * the compute function performs the actual update step, when given some\n+ * (e.g. stochastic) gradient direction for the loss L(w),\n+ * and a desired step-size (learning rate).\n+ *\n+ * The updater is responsible to also perform the update coming from the\n+ * regularization term R(w) (if any regularization is used).\n+ */\n+@DeveloperApi\n+abstract class MultiModelUpdater extends Serializable {\n+  /**\n+   * Compute an updated value for weights given the gradient, stepSize, iteration number and\n+   * regularization parameter. Also returns the regularization value regParam * R(w)\n+   * computed using the *updated* weights.\n+   *\n+   * @param weightsOld - Column matrix of size dx1 where d is the number of features.\n+   * @param gradient - Column matrix of size dx1 where d is the number of features.\n+   * @param stepSize - step size across iterations\n+   * @param iter - Iteration number\n+   * @param regParam - Regularization parameter\n+   *\n+   * @return A tuple of 2 elements. The first element is a column matrix containing updated weights,\n+   *         and the second element is the regularization value computed using updated weights.\n+   */\n+  def compute(\n+      weightsOld: DenseMatrix,\n+      gradient: DenseMatrix,\n+      stepSize: DenseMatrix,\n+      iter: Int,\n+      regParam: Matrix): (DenseMatrix, Matrix)\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * A simple updater for gradient descent *without* any regularization.\n+ * Uses a step-size decreasing with the square root of the number of iterations.\n+ */\n+@DeveloperApi\n+class MultiModelSimpleUpdater extends MultiModelUpdater {\n+  def compute(\n+     weightsOld: DenseMatrix,\n+     gradient: DenseMatrix,\n+     stepSize: DenseMatrix,\n+     iter: Int,\n+     regParam: Matrix): (DenseMatrix, Matrix) = {\n+    val thisIterStepSize =\n+      SparseMatrix.diag(Vectors.dense(stepSize.map(-_ / sqrt(iter)).toArray))\n+\n+    gemm(1.0, gradient,thisIterStepSize, 1.0, weightsOld)",
    "line": 154
  }],
  "prId": 2451
}]