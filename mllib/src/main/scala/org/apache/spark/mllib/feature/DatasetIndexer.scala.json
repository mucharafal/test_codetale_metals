[{
  "comments": [{
    "author": {
      "login": "manishamde"
    },
    "body": "maxCategories as a threshold is a good default. In the future, we may want to add different criteria for some features. Thoughts? Moreover, should we have a reasonable default value?\n",
    "commit": "5956d9197de833bfee870dadd51bbed7ec136ea1",
    "createdAt": "2014-10-30T20:22:13Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vectors, DenseVector, SparseVector, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.collection.OpenHashSet\n+\n+/**\n+ * :: Experimental ::\n+ * Class for indexing columns in a dataset.\n+ *\n+ * This helps process a dataset of unknown vectors into a dataset with some continuous features\n+ * and some categorical features. The choice between continuous and categorical is based upon\n+ * a maxCategories parameter."
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "I agree about adding more criteria and options later on.\nFor a default value, does 16 seem reasonable?\n",
    "commit": "5956d9197de833bfee870dadd51bbed7ec136ea1",
    "createdAt": "2014-10-30T20:41:33Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vectors, DenseVector, SparseVector, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.collection.OpenHashSet\n+\n+/**\n+ * :: Experimental ::\n+ * Class for indexing columns in a dataset.\n+ *\n+ * This helps process a dataset of unknown vectors into a dataset with some continuous features\n+ * and some categorical features. The choice between continuous and categorical is based upon\n+ * a maxCategories parameter."
  }, {
    "author": {
      "login": "manishamde"
    },
    "body": "Sure. I was going to suggest 32 but 16 is reasonable as well. I think R only supports up to 32 for categorical features so that was my motivation.\n",
    "commit": "5956d9197de833bfee870dadd51bbed7ec136ea1",
    "createdAt": "2014-10-30T20:44:38Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vectors, DenseVector, SparseVector, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.collection.OpenHashSet\n+\n+/**\n+ * :: Experimental ::\n+ * Class for indexing columns in a dataset.\n+ *\n+ * This helps process a dataset of unknown vectors into a dataset with some continuous features\n+ * and some categorical features. The choice between continuous and categorical is based upon\n+ * a maxCategories parameter."
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "OK, 32 sounds good\n",
    "commit": "5956d9197de833bfee870dadd51bbed7ec136ea1",
    "createdAt": "2014-10-30T20:55:21Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vectors, DenseVector, SparseVector, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.collection.OpenHashSet\n+\n+/**\n+ * :: Experimental ::\n+ * Class for indexing columns in a dataset.\n+ *\n+ * This helps process a dataset of unknown vectors into a dataset with some continuous features\n+ * and some categorical features. The choice between continuous and categorical is based upon\n+ * a maxCategories parameter."
  }],
  "prId": 3000
}, {
  "comments": [{
    "author": {
      "login": "manishamde"
    },
    "body": "Are we ignoring some key-value pairs of fs2 when fs1.size + fs2.size > maxCategories?\n",
    "commit": "5956d9197de833bfee870dadd51bbed7ec136ea1",
    "createdAt": "2014-10-30T20:30:08Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vectors, DenseVector, SparseVector, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.collection.OpenHashSet\n+\n+/**\n+ * :: Experimental ::\n+ * Class for indexing columns in a dataset.\n+ *\n+ * This helps process a dataset of unknown vectors into a dataset with some continuous features\n+ * and some categorical features. The choice between continuous and categorical is based upon\n+ * a maxCategories parameter.\n+ *\n+ * This can also map categorical feature values to 0-based indices.\n+ *\n+ * Usage:\n+ *   val myData1: RDD[Vector] = ...\n+ *   val myData2: RDD[Vector] = ...\n+ *   val datasetIndexer = new DatasetIndexer(maxCategories)\n+ *   datasetIndexer.fit(myData1)\n+ *   val indexedData1: RDD[Vector] = datasetIndexer.transform(myData1)\n+ *   datasetIndexer.fit(myData2)\n+ *   val indexedData2: RDD[Vector] = datasetIndexer.transform(myData2)\n+ *   val categoricalFeaturesInfo: Map[Int, Int] = datasetIndexer.getCategoricalFeaturesInfo()\n+ *\n+ * TODO: Add option for transform: defaultForUnknownValue (default index for unknown category).\n+ *\n+ * TODO: Add warning if a categorical feature has only 1 category.\n+ */\n+@Experimental\n+class DatasetIndexer(\n+    val maxCategories: Int,\n+    val ignoreUnrecognizedCategories: Boolean = true)\n+  extends Logging with Serializable {\n+\n+  require(maxCategories > 1,\n+    s\"DatasetIndexer given maxCategories = $maxCategories, but requires maxCategories > 1.\")\n+\n+  private class FeatureValueStats(val numFeatures: Int, val maxCategories: Int)\n+    extends Serializable {\n+\n+    val featureValueSets = Array.fill[OpenHashSet[Double]](numFeatures)(new OpenHashSet[Double]())\n+\n+    /**\n+     * Merge other [[FeatureValueStats]] into this instance, modifying this instance.\n+     * @param other  Other instance.  Not modified.\n+     * @return This instance\n+     */\n+    def merge(other: FeatureValueStats): FeatureValueStats = {\n+      featureValueSets.zip(other.featureValueSets).foreach { case (fvs1, fvs2) =>\n+        fvs2.iterator.foreach { val2 =>\n+          if (fvs1.size <= maxCategories) fvs1.add(val2)"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "We will, but all we need to know is that the sum of the sizes is > maxCategories.  Once we know that, then the feature will definitely be considered continuous, so we don't need to collect more statistics on it.\n",
    "commit": "5956d9197de833bfee870dadd51bbed7ec136ea1",
    "createdAt": "2014-10-30T20:42:19Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vectors, DenseVector, SparseVector, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.collection.OpenHashSet\n+\n+/**\n+ * :: Experimental ::\n+ * Class for indexing columns in a dataset.\n+ *\n+ * This helps process a dataset of unknown vectors into a dataset with some continuous features\n+ * and some categorical features. The choice between continuous and categorical is based upon\n+ * a maxCategories parameter.\n+ *\n+ * This can also map categorical feature values to 0-based indices.\n+ *\n+ * Usage:\n+ *   val myData1: RDD[Vector] = ...\n+ *   val myData2: RDD[Vector] = ...\n+ *   val datasetIndexer = new DatasetIndexer(maxCategories)\n+ *   datasetIndexer.fit(myData1)\n+ *   val indexedData1: RDD[Vector] = datasetIndexer.transform(myData1)\n+ *   datasetIndexer.fit(myData2)\n+ *   val indexedData2: RDD[Vector] = datasetIndexer.transform(myData2)\n+ *   val categoricalFeaturesInfo: Map[Int, Int] = datasetIndexer.getCategoricalFeaturesInfo()\n+ *\n+ * TODO: Add option for transform: defaultForUnknownValue (default index for unknown category).\n+ *\n+ * TODO: Add warning if a categorical feature has only 1 category.\n+ */\n+@Experimental\n+class DatasetIndexer(\n+    val maxCategories: Int,\n+    val ignoreUnrecognizedCategories: Boolean = true)\n+  extends Logging with Serializable {\n+\n+  require(maxCategories > 1,\n+    s\"DatasetIndexer given maxCategories = $maxCategories, but requires maxCategories > 1.\")\n+\n+  private class FeatureValueStats(val numFeatures: Int, val maxCategories: Int)\n+    extends Serializable {\n+\n+    val featureValueSets = Array.fill[OpenHashSet[Double]](numFeatures)(new OpenHashSet[Double]())\n+\n+    /**\n+     * Merge other [[FeatureValueStats]] into this instance, modifying this instance.\n+     * @param other  Other instance.  Not modified.\n+     * @return This instance\n+     */\n+    def merge(other: FeatureValueStats): FeatureValueStats = {\n+      featureValueSets.zip(other.featureValueSets).foreach { case (fvs1, fvs2) =>\n+        fvs2.iterator.foreach { val2 =>\n+          if (fvs1.size <= maxCategories) fvs1.add(val2)"
  }, {
    "author": {
      "login": "manishamde"
    },
    "body": "Ignore my statement above. I noticed below that you are using size == (maxCategories + 1) as a signal for continuous feature. May be a small comment might be helpful here.\n",
    "commit": "5956d9197de833bfee870dadd51bbed7ec136ea1",
    "createdAt": "2014-10-30T20:43:28Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vectors, DenseVector, SparseVector, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.collection.OpenHashSet\n+\n+/**\n+ * :: Experimental ::\n+ * Class for indexing columns in a dataset.\n+ *\n+ * This helps process a dataset of unknown vectors into a dataset with some continuous features\n+ * and some categorical features. The choice between continuous and categorical is based upon\n+ * a maxCategories parameter.\n+ *\n+ * This can also map categorical feature values to 0-based indices.\n+ *\n+ * Usage:\n+ *   val myData1: RDD[Vector] = ...\n+ *   val myData2: RDD[Vector] = ...\n+ *   val datasetIndexer = new DatasetIndexer(maxCategories)\n+ *   datasetIndexer.fit(myData1)\n+ *   val indexedData1: RDD[Vector] = datasetIndexer.transform(myData1)\n+ *   datasetIndexer.fit(myData2)\n+ *   val indexedData2: RDD[Vector] = datasetIndexer.transform(myData2)\n+ *   val categoricalFeaturesInfo: Map[Int, Int] = datasetIndexer.getCategoricalFeaturesInfo()\n+ *\n+ * TODO: Add option for transform: defaultForUnknownValue (default index for unknown category).\n+ *\n+ * TODO: Add warning if a categorical feature has only 1 category.\n+ */\n+@Experimental\n+class DatasetIndexer(\n+    val maxCategories: Int,\n+    val ignoreUnrecognizedCategories: Boolean = true)\n+  extends Logging with Serializable {\n+\n+  require(maxCategories > 1,\n+    s\"DatasetIndexer given maxCategories = $maxCategories, but requires maxCategories > 1.\")\n+\n+  private class FeatureValueStats(val numFeatures: Int, val maxCategories: Int)\n+    extends Serializable {\n+\n+    val featureValueSets = Array.fill[OpenHashSet[Double]](numFeatures)(new OpenHashSet[Double]())\n+\n+    /**\n+     * Merge other [[FeatureValueStats]] into this instance, modifying this instance.\n+     * @param other  Other instance.  Not modified.\n+     * @return This instance\n+     */\n+    def merge(other: FeatureValueStats): FeatureValueStats = {\n+      featureValueSets.zip(other.featureValueSets).foreach { case (fvs1, fvs2) =>\n+        fvs2.iterator.foreach { val2 =>\n+          if (fvs1.size <= maxCategories) fvs1.add(val2)"
  }],
  "prId": 3000
}, {
  "comments": [{
    "author": {
      "login": "manishamde"
    },
    "body": "Minor: extra space.\n",
    "commit": "5956d9197de833bfee870dadd51bbed7ec136ea1",
    "createdAt": "2014-10-30T21:05:32Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vectors, DenseVector, SparseVector, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.collection.OpenHashSet\n+\n+/**\n+ * :: Experimental ::\n+ * Class for indexing columns in a dataset.\n+ *\n+ * This helps process a dataset of unknown vectors into a dataset with some continuous features\n+ * and some categorical features. The choice between continuous and categorical is based upon\n+ * a maxCategories parameter.\n+ *\n+ * This can also map categorical feature values to 0-based indices.\n+ *\n+ * Usage:\n+ *   val myData1: RDD[Vector] = ...\n+ *   val myData2: RDD[Vector] = ...\n+ *   val datasetIndexer = new DatasetIndexer(maxCategories)\n+ *   datasetIndexer.fit(myData1)\n+ *   val indexedData1: RDD[Vector] = datasetIndexer.transform(myData1)\n+ *   datasetIndexer.fit(myData2)\n+ *   val indexedData2: RDD[Vector] = datasetIndexer.transform(myData2)\n+ *   val categoricalFeaturesInfo: Map[Int, Int] = datasetIndexer.getCategoricalFeaturesInfo()\n+ *\n+ * TODO: Add option for transform: defaultForUnknownValue (default index for unknown category).\n+ *\n+ * TODO: Add warning if a categorical feature has only 1 category.\n+ */\n+@Experimental\n+class DatasetIndexer(\n+    val maxCategories: Int,\n+    val ignoreUnrecognizedCategories: Boolean = true)\n+  extends Logging with Serializable {\n+\n+  require(maxCategories > 1,\n+    s\"DatasetIndexer given maxCategories = $maxCategories, but requires maxCategories > 1.\")\n+\n+  private class FeatureValueStats(val numFeatures: Int, val maxCategories: Int)\n+    extends Serializable {\n+\n+    val featureValueSets = Array.fill[OpenHashSet[Double]](numFeatures)(new OpenHashSet[Double]())\n+\n+    /**\n+     * Merge other [[FeatureValueStats]] into this instance, modifying this instance.\n+     * @param other  Other instance.  Not modified.\n+     * @return This instance\n+     */\n+    def merge(other: FeatureValueStats): FeatureValueStats = {\n+      featureValueSets.zip(other.featureValueSets).foreach { case (fvs1, fvs2) =>\n+        fvs2.iterator.foreach { val2 =>\n+          if (fvs1.size <= maxCategories) fvs1.add(val2)\n+        }\n+      }\n+      this\n+    }\n+\n+    def addDenseVector(dv: DenseVector): Unit = {\n+      var i = 0\n+      while (i < dv.size) {\n+        if (featureValueSets(i).size <= maxCategories) {\n+          featureValueSets(i).add(dv(i))\n+        }\n+        i += 1\n+      }\n+    }\n+\n+    def addSparseVector(sv: SparseVector): Unit = {\n+      // TODO: This could be made more efficient.\n+      var vecIndex = 0 // index into vector\n+      var nzIndex = 0 // index into non-zero elements\n+      while (vecIndex < sv.size) {\n+        val featureValue = if (nzIndex < sv.indices.size && vecIndex == sv.indices(nzIndex)) {\n+          nzIndex += 1\n+          sv.values(nzIndex - 1)\n+        } else {\n+          0.0\n+        }\n+        if (featureValueSets(vecIndex).size <= maxCategories) {\n+          featureValueSets(vecIndex).add(featureValue)\n+        }\n+        vecIndex += 1\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * Array (over features) of sets of distinct feature values (up to maxCategories values).\n+   * Null values in array indicate feature has been determined to be continuous.\n+   *\n+   * Once the number of elements in a feature's set reaches maxCategories + 1,\n+   * then it is declared continuous, and we stop adding elements.\n+   */\n+  private var featureValueStats: Option[FeatureValueStats] = None\n+\n+  /**\n+   * Scans a dataset once and updates statistics about each column.\n+   * The statistics are used to choose categorical features and re-index them.\n+   *\n+   * Warning: Calling this on a new dataset changes the feature statistics and thus\n+   *          can change the behavior of [[transform]] and [[getCategoricalFeatureIndexes]].\n+   *          It is best to [[fit]] on all datasets before calling [[transform]] on any.\n+   *\n+   * @param data  Dataset with equal-length vectors.\n+   *              NOTE: A single instance of [[DatasetIndexer]] must always be given vectors of\n+   *              the same length.  If given non-matching vectors, this method will throw an error."
  }],
  "prId": 3000
}, {
  "comments": [{
    "author": {
      "login": "manishamde"
    },
    "body": "-1 is used in several places to signify uninitialized value. A private val might be better even it's value is -1.\n",
    "commit": "5956d9197de833bfee870dadd51bbed7ec136ea1",
    "createdAt": "2014-10-31T00:30:09Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vectors, DenseVector, SparseVector, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.collection.OpenHashSet\n+\n+/**\n+ * :: Experimental ::\n+ * Class for indexing columns in a dataset.\n+ *\n+ * This helps process a dataset of unknown vectors into a dataset with some continuous features\n+ * and some categorical features. The choice between continuous and categorical is based upon\n+ * a maxCategories parameter.\n+ *\n+ * This can also map categorical feature values to 0-based indices.\n+ *\n+ * Usage:\n+ *   val myData1: RDD[Vector] = ...\n+ *   val myData2: RDD[Vector] = ...\n+ *   val datasetIndexer = new DatasetIndexer(maxCategories)\n+ *   datasetIndexer.fit(myData1)\n+ *   val indexedData1: RDD[Vector] = datasetIndexer.transform(myData1)\n+ *   datasetIndexer.fit(myData2)\n+ *   val indexedData2: RDD[Vector] = datasetIndexer.transform(myData2)\n+ *   val categoricalFeaturesInfo: Map[Int, Int] = datasetIndexer.getCategoricalFeaturesInfo()\n+ *\n+ * TODO: Add option for transform: defaultForUnknownValue (default index for unknown category).\n+ *\n+ * TODO: Add warning if a categorical feature has only 1 category.\n+ */\n+@Experimental\n+class DatasetIndexer(\n+    val maxCategories: Int,\n+    val ignoreUnrecognizedCategories: Boolean = true)\n+  extends Logging with Serializable {\n+\n+  require(maxCategories > 1,\n+    s\"DatasetIndexer given maxCategories = $maxCategories, but requires maxCategories > 1.\")\n+\n+  private class FeatureValueStats(val numFeatures: Int, val maxCategories: Int)\n+    extends Serializable {\n+\n+    val featureValueSets = Array.fill[OpenHashSet[Double]](numFeatures)(new OpenHashSet[Double]())\n+\n+    /**\n+     * Merge other [[FeatureValueStats]] into this instance, modifying this instance.\n+     * @param other  Other instance.  Not modified.\n+     * @return This instance\n+     */\n+    def merge(other: FeatureValueStats): FeatureValueStats = {\n+      featureValueSets.zip(other.featureValueSets).foreach { case (fvs1, fvs2) =>\n+        fvs2.iterator.foreach { val2 =>\n+          if (fvs1.size <= maxCategories) fvs1.add(val2)\n+        }\n+      }\n+      this\n+    }\n+\n+    def addDenseVector(dv: DenseVector): Unit = {\n+      var i = 0\n+      while (i < dv.size) {\n+        if (featureValueSets(i).size <= maxCategories) {\n+          featureValueSets(i).add(dv(i))\n+        }\n+        i += 1\n+      }\n+    }\n+\n+    def addSparseVector(sv: SparseVector): Unit = {\n+      // TODO: This could be made more efficient.\n+      var vecIndex = 0 // index into vector\n+      var nzIndex = 0 // index into non-zero elements\n+      while (vecIndex < sv.size) {\n+        val featureValue = if (nzIndex < sv.indices.size && vecIndex == sv.indices(nzIndex)) {\n+          nzIndex += 1\n+          sv.values(nzIndex - 1)\n+        } else {\n+          0.0\n+        }\n+        if (featureValueSets(vecIndex).size <= maxCategories) {\n+          featureValueSets(vecIndex).add(featureValue)\n+        }\n+        vecIndex += 1\n+      }\n+    }\n+\n+  }\n+\n+  /**\n+   * Array (over features) of sets of distinct feature values (up to maxCategories values).\n+   * Null values in array indicate feature has been determined to be continuous.\n+   *\n+   * Once the number of elements in a feature's set reaches maxCategories + 1,\n+   * then it is declared continuous, and we stop adding elements.\n+   */\n+  private var featureValueStats: Option[FeatureValueStats] = None\n+\n+  /**\n+   * Scans a dataset once and updates statistics about each column.\n+   * The statistics are used to choose categorical features and re-index them.\n+   *\n+   * Warning: Calling this on a new dataset changes the feature statistics and thus\n+   *          can change the behavior of [[transform]] and [[getCategoricalFeatureIndexes]].\n+   *          It is best to [[fit]] on all datasets before calling [[transform]] on any.\n+   *\n+   * @param data  Dataset with equal-length vectors.\n+   *              NOTE: A single instance of [[DatasetIndexer]] must always be given vectors of\n+   *              the same length.  If given non-matching vectors, this method will throw an error.\n+   */\n+  def fit(data: RDD[Vector]): Unit = {\n+    // For each partition, get (featureValueStats, newNumFeatures).\n+    //  If all vectors have the same length, then newNumFeatures = -1.\n+    //  If a vector with a new length is found, then newNumFeatures is set to that length.\n+    val partitionFeatureValueSets: RDD[(Option[FeatureValueStats], Int)] =\n+      data.mapPartitions { iter =>\n+        // Make local copy of featureValueStats.\n+        //  This will be None initially if this is the first dataset to be fitted.\n+        var localFeatureValueStats: Option[FeatureValueStats] = featureValueStats\n+        var localNumFeatures: Int = -1"
  }],
  "prId": 3000
}]