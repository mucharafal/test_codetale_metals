[{
  "comments": [{
    "author": {
      "login": "witgo"
    },
    "body": "This is only a temporary solution.\nThe related PR #2631\n",
    "commit": "68360c48445572218cf601903bb77fc8002c22ab",
    "createdAt": "2014-10-11T10:14:48Z",
    "diffHunk": "@@ -0,0 +1,674 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import java.util.Random\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, sum => brzSum}\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.graphx._\n+import org.apache.spark.mllib.linalg.distributed.{MatrixEntry, RowMatrix}\n+import org.apache.spark.mllib.linalg.{DenseVector => SDV, SparseVector => SSV, Vector => SV}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.serializer.KryoRegistrator\n+import org.apache.spark.storage.StorageLevel\n+\n+import TopicModeling._\n+\n+class TopicModeling private[mllib](\n+  @transient var corpus: Graph[VD, ED],\n+  val numTopics: Int,\n+  val numTerms: Int,\n+  val alpha: Double,\n+  val beta: Double,\n+  @transient val storageLevel: StorageLevel)\n+  extends Serializable with Logging {\n+\n+  def this(docs: RDD[(TopicModeling.DocId, SSV)],\n+    numTopics: Int,\n+    alpha: Double,\n+    beta: Double,\n+    storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK,\n+    computedModel: Broadcast[TopicModel] = null) {\n+    this(initializeCorpus(docs, numTopics, storageLevel, computedModel),\n+      numTopics, docs.first()._2.size, alpha, beta, storageLevel)\n+  }\n+\n+\n+  /**\n+   * The number of documents in the corpus\n+   */\n+  val numDocs = docVertices.count()\n+\n+  /**\n+   * The number of terms in the corpus\n+   */\n+  private val sumTerms = corpus.edges.map(e => e.attr.size.toDouble).sum().toLong\n+\n+  /**\n+   * The total counts for each topic\n+   */\n+  @transient private var globalTopicCounter: BV[Count] = collectGlobalCounter(corpus, numTopics)\n+  assert(brzSum(globalTopicCounter) == sumTerms)\n+  @transient private val sc = corpus.vertices.context\n+  @transient private val seed = new Random().nextInt()\n+  @transient private var innerIter = 1\n+  @transient private var cachedEdges: EdgeRDD[ED, VD] = null\n+  @transient private var cachedVertices: VertexRDD[VD] = null\n+\n+  private def termVertices = corpus.vertices.filter(t => t._1 >= 0)\n+\n+  private def docVertices = corpus.vertices.filter(t => t._1 < 0)\n+\n+  private def gibbsSampling(cachedEdges: EdgeRDD[ED, VD],\n+    cachedVertices: VertexRDD[VD]): (EdgeRDD[ED, VD], VertexRDD[VD]) = {\n+\n+    val corpusTopicDist = collectTermTopicDist(corpus, globalTopicCounter,\n+      sumTerms, numTerms, numTopics, alpha, beta)\n+\n+    val corpusSampleTopics = sampleTopics(corpusTopicDist, globalTopicCounter,\n+      sumTerms, innerIter + seed, numTerms, numTopics, alpha, beta)\n+    corpusSampleTopics.edges.setName(s\"edges-$innerIter\").cache().count()\n+    Option(cachedEdges).foreach(_.unpersist())\n+    val edges = corpusSampleTopics.edges\n+\n+    corpus = updateCounter(corpusSampleTopics, numTopics)\n+    corpus.vertices.setName(s\"vertices-$innerIter\").cache()\n+    globalTopicCounter = collectGlobalCounter(corpus, numTopics)\n+    assert(brzSum(globalTopicCounter) == sumTerms)\n+    Option(cachedVertices).foreach(_.unpersist())\n+    val vertices = corpus.vertices\n+\n+    if (innerIter % 10 == 0 && sc.getCheckpointDir.isDefined) {"
  }],
  "prId": 2388
}, {
  "comments": [{
    "author": {
      "login": "witgo"
    },
    "body": "There is a small bug.\n",
    "commit": "68360c48445572218cf601903bb77fc8002c22ab",
    "createdAt": "2014-10-13T13:14:11Z",
    "diffHunk": "@@ -0,0 +1,682 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.feature\n+\n+import java.util.Random\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, sum => brzSum}\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.graphx._\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.distributed.{MatrixEntry, RowMatrix}\n+import org.apache.spark.mllib.linalg.{DenseVector => SDV, SparseVector => SSV, Vector => SV}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.serializer.KryoRegistrator\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.SparkContext._\n+\n+import TopicModeling._\n+\n+class TopicModeling private[mllib](\n+  @transient var corpus: Graph[VD, ED],\n+  val numTopics: Int,\n+  val numTerms: Int,\n+  val alpha: Double,\n+  val beta: Double,\n+  @transient val storageLevel: StorageLevel)\n+  extends Serializable with Logging {\n+\n+  def this(docs: RDD[(TopicModeling.DocId, SSV)],\n+    numTopics: Int,\n+    alpha: Double,\n+    beta: Double,\n+    storageLevel: StorageLevel = StorageLevel.MEMORY_AND_DISK,\n+    computedModel: Broadcast[TopicModel] = null) {\n+    this(initializeCorpus(docs, numTopics, storageLevel, computedModel),\n+      numTopics, docs.first()._2.size, alpha, beta, storageLevel)\n+  }\n+\n+\n+  /**\n+   * The number of documents in the corpus\n+   */\n+  val numDocs = docVertices.count()\n+\n+  /**\n+   * The number of terms in the corpus\n+   */\n+  private val sumTerms = corpus.edges.map(e => e.attr.size.toDouble).sum().toLong\n+\n+  /**\n+   * The total counts for each topic\n+   */\n+  @transient private var globalTopicCounter: BDV[Count] = collectGlobalCounter(corpus, numTopics)\n+  assert(brzSum(globalTopicCounter) == sumTerms)\n+\n+  @transient private val sc = corpus.vertices.context\n+  @transient private val seed = new Random().nextInt()\n+  @transient private var innerIter = 1\n+  @transient private var cachedEdges: EdgeRDD[ED, VD] = corpus.edges\n+  @transient private var cachedVertices: VertexRDD[VD] = corpus.vertices\n+\n+  private def termVertices = corpus.vertices.filter(t => t._1 >= 0)\n+\n+  private def docVertices = corpus.vertices.filter(t => t._1 < 0)\n+\n+  private def checkpoint(): Unit = {\n+    if (innerIter % 10 == 0 && sc.getCheckpointDir.isDefined) {\n+      val edges = corpus.edges.map(t => t)\n+      edges.checkpoint()\n+      val newCorpus: Graph[VD, ED] = Graph.fromEdges(edges, null,\n+        storageLevel, storageLevel)\n+      corpus = updateCounter(newCorpus, numTopics).cache()\n+    }\n+  }\n+\n+  private def gibbsSampling(): Unit = {\n+    val corpusTopicDist = collectTermTopicDist(corpus, globalTopicCounter,\n+      sumTerms, numTerms, numTopics, alpha, beta)\n+\n+    val corpusSampleTopics = sampleTopics(corpusTopicDist, globalTopicCounter,\n+      sumTerms, innerIter + seed, numTerms, numTopics, alpha, beta)\n+    corpusSampleTopics.edges.setName(s\"edges-$innerIter\").cache().count()\n+    Option(cachedEdges).foreach(_.unpersist())\n+    cachedEdges = corpusSampleTopics.edges\n+\n+    corpus = updateCounter(corpusSampleTopics, numTopics)\n+    corpus.vertices.setName(s\"vertices-$innerIter\").cache()\n+    globalTopicCounter = collectGlobalCounter(corpus, numTopics)\n+    assert(brzSum(globalTopicCounter) == sumTerms)\n+    Option(cachedVertices).foreach(_.unpersist())\n+    cachedVertices = corpus.vertices\n+\n+    checkpoint()\n+    innerIter += 1\n+  }\n+\n+  def saveTopicModel(burnInIter: Int): TopicModel = {\n+    val topicModel = TopicModel(numTopics, numTerms, alpha, beta)\n+    for (iter <- 1 to burnInIter) {\n+      logInfo(\"Save TopicModel (Iteration %d/%d)\".format(iter, burnInIter))\n+      gibbsSampling()\n+      updateTopicModel(termVertices, topicModel)\n+    }\n+    topicModel.gtc :/= burnInIter.toDouble\n+    topicModel.ttc.foreach(_ :/= burnInIter.toDouble)\n+    topicModel\n+  }\n+\n+  def runGibbsSampling(iterations: Int): Unit = {\n+    for (iter <- 1 to iterations) {\n+      logInfo(\"Start Gibbs sampling (Iteration %d/%d)\".format(iter, iterations))\n+      gibbsSampling()\n+    }\n+  }\n+\n+  @Experimental\n+  def mergeDuplicateTopic(threshold: Double = 0.95D): Map[Int, Int] = {\n+    val rows = termVertices.map(t => t._2.counter).map { bsv =>\n+      val length = bsv.length\n+      val used = bsv.used\n+      val index = bsv.index.slice(0, used)\n+      val data = bsv.data.slice(0, used).map(_.toDouble)\n+      new SSV(length, index, data).asInstanceOf[SV]\n+    }\n+    val simMatrix = new RowMatrix(rows).columnSimilarities()\n+    val minMap = simMatrix.entries.filter { case MatrixEntry(row, column, sim) =>\n+      sim > threshold && row != column\n+    }.map { case MatrixEntry(row, column, sim) =>\n+      (column.toInt, row.toInt)\n+    }.groupByKey().map { case (topic, simTopics) =>\n+      (topic, simTopics.min)\n+    }.collect().toMap\n+    if (minMap.size > 0) {\n+      corpus = corpus.mapEdges(edges => {\n+        edges.attr.map { topic =>\n+          minMap.get(topic).getOrElse(topic)\n+        }\n+      })\n+      corpus = updateCounter(corpus, numTopics)\n+    }\n+    minMap\n+  }\n+\n+  def perplexity(): Double = {\n+    val totalTopicCounter = this.globalTopicCounter\n+    val numTopics = this.numTopics\n+    val numTerms = this.numTerms\n+    val alpha = this.alpha\n+    val beta = this.beta\n+\n+    val newCounts = corpus.mapReduceTriplets[Int](triplet => {\n+      val size = triplet.attr.size\n+      val docId = triplet.dstId\n+      val wordId = triplet.srcId\n+      Iterator((docId, size), (wordId, size))\n+    }, (a, b) => a + b)\n+    val (termProb, totalNum) = corpus.outerJoinVertices(newCounts) {\n+      (_, f, n) =>\n+        (f.counter, n.get)\n+    }.mapTriplets {\n+      triplet =>\n+        val (termCounter, _) = triplet.srcAttr\n+        val (docTopicCounter, docTopicCount) = triplet.dstAttr\n+        var probWord = 0D\n+        val size = triplet.attr.size\n+        (0 until numTopics).foreach {\n+          topic =>\n+            val phi = (termCounter(topic) + beta) / (totalTopicCounter(topic) + numTerms * beta)\n+            val theta = (docTopicCounter(topic) + alpha) / (docTopicCount + alpha * numTopics)\n+            probWord += phi * theta\n+        }\n+        (Math.log(probWord * size) * size, size)\n+    }.edges.map(t => t.attr).reduce {\n+      (lhs, rhs) =>\n+        (lhs._1 + rhs._1, lhs._2 + rhs._2)\n+    }\n+    math.exp(-1 * termProb / totalNum)\n+  }\n+}\n+\n+\n+object TopicModeling {\n+\n+  private[mllib] type DocId = VertexId\n+  private[mllib] type WordId = VertexId\n+  private[mllib] type Count = Int\n+  private[mllib] type ED = Array[Count]\n+\n+  private[mllib] case class VD(counter: BSV[Count], dist: BSV[Double], dist1: BSV[Double])\n+\n+  def train(docs: RDD[(DocId, SSV)],\n+    numTopics: Int = 2048,\n+    totalIter: Int = 150,\n+    burnIn: Int = 5,\n+    alpha: Double = 0.1,\n+    beta: Double = 0.01): TopicModel = {\n+    require(totalIter > burnIn, \"totalIter is less than burnIn\")\n+    require(totalIter > 0, \"totalIter is less than 0\")\n+    require(burnIn > 0, \"burnIn is less than 0\")\n+    val topicModeling = new TopicModeling(docs, numTopics, alpha, beta)\n+    topicModeling.runGibbsSampling(totalIter - burnIn)\n+    topicModeling.saveTopicModel(burnIn)\n+  }\n+\n+  def incrementalTrain(docs: RDD[(DocId, SSV)],\n+    computedModel: TopicModel,\n+    totalIter: Int = 150,\n+    burnIn: Int = 5): TopicModel = {\n+    require(totalIter > burnIn, \"totalIter is less than burnIn\")\n+    require(totalIter > 0, \"totalIter is less than 0\")\n+    require(burnIn > 0, \"burnIn is less than 0\")\n+    val numTopics = computedModel.ttc.size\n+    val alpha = computedModel.alpha\n+    val beta = computedModel.beta\n+\n+    val broadcastModel = docs.context.broadcast(computedModel)\n+    val topicModeling = new TopicModeling(docs, numTopics, alpha, beta,\n+      computedModel = broadcastModel)\n+    broadcastModel.unpersist()\n+    topicModeling.runGibbsSampling(totalIter - burnIn)\n+    topicModeling.saveTopicModel(burnIn)\n+  }\n+\n+  private[mllib] def collectTermTopicDist(graph: Graph[VD, ED],\n+    totalTopicCounter: BDV[Count],\n+    sumTerms: Long,\n+    numTerms: Int,\n+    numTopics: Int,\n+    alpha: Double,\n+    beta: Double): Graph[VD, ED] = {\n+    val newVD = graph.vertices.filter(_._1 >= 0).map { v =>\n+      val vertexId = v._1\n+      val termTopicCounter = v._2.counter\n+      termTopicCounter.compact()\n+      val length = termTopicCounter.length\n+      val used = termTopicCounter.used\n+      val index = termTopicCounter.index\n+      val data = termTopicCounter.data\n+      val w = new Array[Double](used)\n+      val w1 = new Array[Double](used)\n+\n+      var wi = 0D\n+      var i = 0\n+\n+      while (i < used) {\n+        val topic = index(i)\n+        val count = data(i)\n+        var adjustment = 0D\n+        val alphaAS = alpha\n+\n+        w(i) = count * ((totalTopicCounter(topic) * (alpha * numTopics)) +\n+          (alpha * numTopics) * (adjustment + alphaAS) +\n+          adjustment * (sumTerms - 1 + (alphaAS * numTopics))) /\n+          (totalTopicCounter(topic) + (numTerms * beta)) /"
  }],
  "prId": 2388
}]