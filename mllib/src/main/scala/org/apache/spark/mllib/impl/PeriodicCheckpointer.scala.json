[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Do we need `currentData` at construction time? It might be cleaner to let user call `update` to add the initial dataset.\n",
    "commit": "d41902c085504ca30714d7665ee924c2c2a7fd91",
    "createdAt": "2015-07-30T00:41:00Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.storage.StorageLevel\n+\n+\n+/**\n+ * This abstraction helps with persisting and checkpointing RDDs and types derived from RDDs\n+ * (such as Graphs and DataFrames).  In documentation, we use the phrase \"Dataset\" to refer to\n+ * the distributed data type (RDD, Graph, etc.).\n+ *\n+ * Specifically, this abstraction automatically handles persisting and (optionally) checkpointing,\n+ * as well as unpersisting and removing checkpoint files.\n+ *\n+ * Users should call update() when a new Dataset has been created,\n+ * before the Dataset has been materialized.  After updating [[PeriodicCheckpointer]], users are\n+ * responsible for materializing the Dataset to ensure that persisting and checkpointing actually\n+ * occur.\n+ *\n+ * When update() is called, this does the following:\n+ *  - Persist new Dataset (if not yet persisted), and put in queue of persisted Datasets.\n+ *  - Unpersist Datasets from queue until there are at most 3 persisted Datasets.\n+ *  - If using checkpointing and the checkpoint interval has been reached,\n+ *     - Checkpoint the new Dataset, and put in a queue of checkpointed Datasets.\n+ *     - Remove older checkpoints.\n+ *\n+ * WARNINGS:\n+ *  - This class should NOT be copied (since copies may conflict on which Datasets should be\n+ *    checkpointed).\n+ *  - This class removes checkpoint files once later Datasets have been checkpointed.\n+ *    However, references to the older Datasets will still return isCheckpointed = true.\n+ *\n+ * Example usage:\n+ * {{{\n+ *  val (data1, data2, data3, ...) = ...\n+ *  val cp = new PeriodicCheckpointer(data1, dir, 2)\n+ *  data1.count();\n+ *  // persisted: data1\n+ *  cp.update(data2)\n+ *  data2.count();\n+ *  // persisted: data1, data2\n+ *  // checkpointed: data2\n+ *  cp.update(data3)\n+ *  data3.count();\n+ *  // persisted: data1, data2, data3\n+ *  // checkpointed: data2\n+ *  cp.update(data4)\n+ *  data4.count();\n+ *  // persisted: data2, data3, data4\n+ *  // checkpointed: data4\n+ *  cp.update(data5)\n+ *  data5.count();\n+ *  // persisted: data3, data4, data5\n+ *  // checkpointed: data4\n+ * }}}\n+ *\n+ * @param currentData Initial Dataset\n+ * @param checkpointInterval  Datasets will be checkpointed at this interval\n+ * @param sc  SparkContext for the Datasets given to this checkpointer\n+ * @tparam T  Dataset type, such as RDD[Double]\n+ */\n+private[mllib] abstract class PeriodicCheckpointer[T](\n+    var currentData: T,"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "The only problem with not doing that is that the type parameter have to be given explicitly to the constructor, but that's fine with me.  I'll make the change.\n",
    "commit": "d41902c085504ca30714d7665ee924c2c2a7fd91",
    "createdAt": "2015-07-30T01:58:46Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.storage.StorageLevel\n+\n+\n+/**\n+ * This abstraction helps with persisting and checkpointing RDDs and types derived from RDDs\n+ * (such as Graphs and DataFrames).  In documentation, we use the phrase \"Dataset\" to refer to\n+ * the distributed data type (RDD, Graph, etc.).\n+ *\n+ * Specifically, this abstraction automatically handles persisting and (optionally) checkpointing,\n+ * as well as unpersisting and removing checkpoint files.\n+ *\n+ * Users should call update() when a new Dataset has been created,\n+ * before the Dataset has been materialized.  After updating [[PeriodicCheckpointer]], users are\n+ * responsible for materializing the Dataset to ensure that persisting and checkpointing actually\n+ * occur.\n+ *\n+ * When update() is called, this does the following:\n+ *  - Persist new Dataset (if not yet persisted), and put in queue of persisted Datasets.\n+ *  - Unpersist Datasets from queue until there are at most 3 persisted Datasets.\n+ *  - If using checkpointing and the checkpoint interval has been reached,\n+ *     - Checkpoint the new Dataset, and put in a queue of checkpointed Datasets.\n+ *     - Remove older checkpoints.\n+ *\n+ * WARNINGS:\n+ *  - This class should NOT be copied (since copies may conflict on which Datasets should be\n+ *    checkpointed).\n+ *  - This class removes checkpoint files once later Datasets have been checkpointed.\n+ *    However, references to the older Datasets will still return isCheckpointed = true.\n+ *\n+ * Example usage:\n+ * {{{\n+ *  val (data1, data2, data3, ...) = ...\n+ *  val cp = new PeriodicCheckpointer(data1, dir, 2)\n+ *  data1.count();\n+ *  // persisted: data1\n+ *  cp.update(data2)\n+ *  data2.count();\n+ *  // persisted: data1, data2\n+ *  // checkpointed: data2\n+ *  cp.update(data3)\n+ *  data3.count();\n+ *  // persisted: data1, data2, data3\n+ *  // checkpointed: data2\n+ *  cp.update(data4)\n+ *  data4.count();\n+ *  // persisted: data2, data3, data4\n+ *  // checkpointed: data4\n+ *  cp.update(data5)\n+ *  data5.count();\n+ *  // persisted: data3, data4, data5\n+ *  // checkpointed: data4\n+ * }}}\n+ *\n+ * @param currentData Initial Dataset\n+ * @param checkpointInterval  Datasets will be checkpointed at this interval\n+ * @param sc  SparkContext for the Datasets given to this checkpointer\n+ * @tparam T  Dataset type, such as RDD[Double]\n+ */\n+private[mllib] abstract class PeriodicCheckpointer[T](\n+    var currentData: T,"
  }],
  "prId": 7728
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`.get(1).get` -> `(0)` or `head`\n",
    "commit": "d41902c085504ca30714d7665ee924c2c2a7fd91",
    "createdAt": "2015-07-30T00:41:02Z",
    "diffHunk": "@@ -0,0 +1,184 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+import org.apache.spark.{SparkContext, Logging}\n+import org.apache.spark.storage.StorageLevel\n+\n+\n+/**\n+ * This abstraction helps with persisting and checkpointing RDDs and types derived from RDDs\n+ * (such as Graphs and DataFrames).  In documentation, we use the phrase \"Dataset\" to refer to\n+ * the distributed data type (RDD, Graph, etc.).\n+ *\n+ * Specifically, this abstraction automatically handles persisting and (optionally) checkpointing,\n+ * as well as unpersisting and removing checkpoint files.\n+ *\n+ * Users should call update() when a new Dataset has been created,\n+ * before the Dataset has been materialized.  After updating [[PeriodicCheckpointer]], users are\n+ * responsible for materializing the Dataset to ensure that persisting and checkpointing actually\n+ * occur.\n+ *\n+ * When update() is called, this does the following:\n+ *  - Persist new Dataset (if not yet persisted), and put in queue of persisted Datasets.\n+ *  - Unpersist Datasets from queue until there are at most 3 persisted Datasets.\n+ *  - If using checkpointing and the checkpoint interval has been reached,\n+ *     - Checkpoint the new Dataset, and put in a queue of checkpointed Datasets.\n+ *     - Remove older checkpoints.\n+ *\n+ * WARNINGS:\n+ *  - This class should NOT be copied (since copies may conflict on which Datasets should be\n+ *    checkpointed).\n+ *  - This class removes checkpoint files once later Datasets have been checkpointed.\n+ *    However, references to the older Datasets will still return isCheckpointed = true.\n+ *\n+ * Example usage:\n+ * {{{\n+ *  val (data1, data2, data3, ...) = ...\n+ *  val cp = new PeriodicCheckpointer(data1, dir, 2)\n+ *  data1.count();\n+ *  // persisted: data1\n+ *  cp.update(data2)\n+ *  data2.count();\n+ *  // persisted: data1, data2\n+ *  // checkpointed: data2\n+ *  cp.update(data3)\n+ *  data3.count();\n+ *  // persisted: data1, data2, data3\n+ *  // checkpointed: data2\n+ *  cp.update(data4)\n+ *  data4.count();\n+ *  // persisted: data2, data3, data4\n+ *  // checkpointed: data4\n+ *  cp.update(data5)\n+ *  data5.count();\n+ *  // persisted: data3, data4, data5\n+ *  // checkpointed: data4\n+ * }}}\n+ *\n+ * @param currentData Initial Dataset\n+ * @param checkpointInterval  Datasets will be checkpointed at this interval\n+ * @param sc  SparkContext for the Datasets given to this checkpointer\n+ * @tparam T  Dataset type, such as RDD[Double]\n+ */\n+private[mllib] abstract class PeriodicCheckpointer[T](\n+    var currentData: T,\n+    val checkpointInterval: Int,\n+    val sc: SparkContext) extends Logging {\n+\n+  /** FIFO queue of past checkpointed Datasets */\n+  private val checkpointQueue = mutable.Queue[T]()\n+\n+  /** FIFO queue of past persisted Datasets */\n+  private val persistedQueue = mutable.Queue[T]()\n+\n+  /** Number of times [[update()]] has been called */\n+  private var updateCount = 0\n+\n+  update(currentData)\n+\n+  /**\n+   * Update [[currentData]] with a new Dataset. Handle persistence and checkpointing as needed.\n+   * Since this handles persistence and checkpointing, this should be called before the Dataset\n+   * has been materialized.\n+   *\n+   * @param newData  New Dataset created from previous Datasets in the lineage.\n+   */\n+  def update(newData: T): Unit = {\n+    persist(newData)\n+    persistedQueue.enqueue(newData)\n+    // We try to maintain 2 Datasets in persistedQueue to support the semantics of this class:\n+    // Users should call [[update()]] when a new Dataset has been created,\n+    // before the Dataset has been materialized.\n+    while (persistedQueue.size > 3) {\n+      val dataToUnpersist = persistedQueue.dequeue()\n+      unpersist(dataToUnpersist)\n+    }\n+    updateCount += 1\n+\n+    // Handle checkpointing (after persisting)\n+    if ((updateCount % checkpointInterval) == 0 && sc.getCheckpointDir.nonEmpty) {\n+      // Add new checkpoint before removing old checkpoints.\n+      checkpoint(newData)\n+      checkpointQueue.enqueue(newData)\n+      // Remove checkpoints before the latest one.\n+      var canDelete = true\n+      while (checkpointQueue.size > 1 && canDelete) {\n+        // Delete the oldest checkpoint only if the next checkpoint exists.\n+        if (isCheckpointed(checkpointQueue.get(1).get)) {"
  }],
  "prId": 7728
}]