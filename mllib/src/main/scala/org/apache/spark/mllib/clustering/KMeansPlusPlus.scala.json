[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "4-space indentation\n",
    "commit": "35da8e9e188e669460000d5799d061ecc3ca150f",
    "createdAt": "2014-10-06T21:49:43Z",
    "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import org.apache.spark.mllib.base.{PointOps, FP, Infinity, One, Zero}\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+/**\n+ *\n+ * The KMeans++ initialization algorithm\n+ *\n+ * @param pointOps distance function\n+ * @tparam P point type\n+ * @tparam C center type\n+ */\n+private[mllib] class KMeansPlusPlus[P <: FP : ClassTag, C <: FP : ClassTag](\n+  pointOps: PointOps[P, C]) extends Serializable with Logging {\n+\n+  /**\n+   * We will maintain for each point the distance to its closest cluster center.\n+   * Since only one center is added on each iteration, recomputing the closest cluster center\n+   * only requires computing the distance to the new cluster center if\n+   * that distance is less than the closest cluster center.  \n+   */\n+  case class FatPoint(location: P, index: Int, weight: Double, distance: Double)\n+\n+  /**\n+   * K-means++ on the weighted point set `points`. This first does the K-means++\n+   * initialization procedure and then rounds of Lloyd's algorithm.\n+   */\n+\n+  def cluster(\n+               sc: SparkContext,"
  }],
  "prId": 2634
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "chop parameters and 4-space indentation\n",
    "commit": "35da8e9e188e669460000d5799d061ecc3ca150f",
    "createdAt": "2014-10-06T21:49:44Z",
    "diffHunk": "@@ -0,0 +1,200 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import org.apache.spark.mllib.base.{PointOps, FP, Infinity, One, Zero}\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkContext}\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+/**\n+ *\n+ * The KMeans++ initialization algorithm\n+ *\n+ * @param pointOps distance function\n+ * @tparam P point type\n+ * @tparam C center type\n+ */\n+private[mllib] class KMeansPlusPlus[P <: FP : ClassTag, C <: FP : ClassTag](\n+  pointOps: PointOps[P, C]) extends Serializable with Logging {\n+\n+  /**\n+   * We will maintain for each point the distance to its closest cluster center.\n+   * Since only one center is added on each iteration, recomputing the closest cluster center\n+   * only requires computing the distance to the new cluster center if\n+   * that distance is less than the closest cluster center.  \n+   */\n+  case class FatPoint(location: P, index: Int, weight: Double, distance: Double)\n+\n+  /**\n+   * K-means++ on the weighted point set `points`. This first does the K-means++\n+   * initialization procedure and then rounds of Lloyd's algorithm.\n+   */\n+\n+  def cluster(\n+               sc: SparkContext,\n+               seed: Int,\n+               points: Array[C],\n+               weights: Array[Double],\n+               k: Int,\n+               maxIterations: Int,\n+               numPartitions: Int): Array[C] = {\n+    val centers: Array[C] = getCenters(sc, seed, points, weights, k, numPartitions, 1)\n+    val pts =  sc.parallelize(points.map(pointOps.centerToPoint))\n+    new MultiKMeans(pointOps, maxIterations).cluster(pts, Array(centers))._2.centers\n+  }\n+\n+  /**\n+   * Select centers in rounds.  On each round, select 'perRound' centers, with probability of\n+   * selection equal to the product of the given weights and distance to the closest cluster center\n+   * of the previous round.\n+   *\n+   * @param sc the Spark context\n+   * @param seed a random number seed\n+   * @param points  the candidate centers\n+   * @param weights  the weights on the candidate centers\n+   * @param k  the total number of centers to select\n+   * @param numPartitions the number of data partitions to use\n+   * @param perRound the number of centers to add per round\n+   * @return   an array of at most k cluster centers\n+   */\n+  def getCenters(sc: SparkContext, seed: Int, points: Array[C], weights: Array[Double], k: Int,"
  }],
  "prId": 2634
}]