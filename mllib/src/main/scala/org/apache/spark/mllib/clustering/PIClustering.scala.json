[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Let's not handle I/O here. We can have an example code under `examples/` and load files there.\n",
    "commit": "45508507ddb0f097224176f6688ecf740100caa2",
    "createdAt": "2015-01-28T21:59:36Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\n+import org.apache.log4j.Logger\n+import org.apache.spark.SparkContext\n+import org.apache.spark.graphx._\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+import scala.language.existentials\n+\n+/**\n+ * Implements the scalable graph clustering algorithm Power Iteration Clustering (see\n+ * www.icml2010.org/papers/387.pdf).  From the abstract:\n+ *\n+ * The input data is first transformed to a normalized Affinity Matrix via Gaussian pairwise\n+ * distance calculations. Power iteration is then used to find a dimensionality-reduced\n+ * representation.  The resulting pseudo-eigenvector provides effective clustering - as\n+ * performed by Parallel KMeans.\n+ */\n+object PIClustering {\n+\n+  private val logger = Logger.getLogger(getClass.getName())\n+\n+  type LabeledPoint = (VertexId, BDV[Double])\n+  type Points = Seq[LabeledPoint]\n+  type DGraph = Graph[Double, Double]\n+  type IndexedVector[Double] = (Long, BDV[Double])\n+\n+  // Terminate iteration when norm changes by less than this value\n+  private[mllib] val DefaultMinNormChange: Double = 1e-11\n+\n+  // Default σ for Gaussian Distance calculations\n+  private[mllib] val DefaultSigma = 1.0\n+\n+  // Default number of iterations for PIC loop\n+  private[mllib] val DefaultIterations: Int = 20\n+\n+  // Default minimum affinity between points - lower than this it is considered\n+  // zero and no edge will be created\n+  private[mllib] val DefaultMinAffinity = 1e-11\n+\n+  // Do not allow divide by zero: change to this value instead\n+  val DefaultDivideByZeroVal: Double = 1e-15\n+\n+  // Default number of runs by the KMeans.run() method\n+  val DefaultKMeansRuns = 10\n+\n+  /**\n+   *\n+   * Run a Power Iteration Clustering\n+   *\n+   * @param sc  Spark Context\n+   * @param points  Input Points in format of [(VertexId,(x,y)]\n+   *                where VertexId is a Long\n+   * @param nClusters  Number of clusters to create\n+   * @param nIterations Number of iterations of the PIC algorithm\n+   *                    that calculates primary PseudoEigenvector and Eigenvalue\n+   * @param sigma   Sigma for Gaussian distribution calculation according to\n+   *                [1/2 *sqrt(pi*sigma)] exp (- (x-y)**2 / 2sigma**2\n+   * @param minAffinity  Minimum Affinity between two Points in the input dataset: below\n+   *                     this threshold the affinity will be considered \"close to\" zero and\n+   *                     no Edge will be created between those Points in the sparse matrix\n+   * @param nRuns  Number of runs for the KMeans clustering\n+   * @return Tuple of (Seq[(Cluster Id,Cluster Center)],\n+   *         Seq[(VertexId, ClusterID Membership)]\n+   */\n+  def run(sc: SparkContext,\n+          points: Points,\n+          nClusters: Int,\n+          nIterations: Int = DefaultIterations,\n+          sigma: Double = DefaultSigma,\n+          minAffinity: Double = DefaultMinAffinity,\n+          nRuns: Int = DefaultKMeansRuns)\n+  : (Seq[(Int, Vector)], Seq[((VertexId, Vector), Int)]) = {\n+    val vidsRdd = sc.parallelize(points.map(_._1).sorted)\n+    val nVertices = points.length\n+\n+    val (wRdd, rowSums) = createNormalizedAffinityMatrix(sc, points, sigma)\n+    val initialVt = createInitialVector(sc, points.map(_._1), rowSums)\n+    if (logger.isDebugEnabled) {\n+      logger.debug(s\"Vt(0)=${\n+        printVector(new BDV(initialVt.map {\n+          _._2\n+        }.toArray))\n+      }\")\n+    }\n+    val edgesRdd = createSparseEdgesRdd(sc, wRdd, minAffinity)\n+    val G = createGraphFromEdges(sc, edgesRdd, points.size, Some(initialVt))\n+    if (logger.isDebugEnabled) {\n+      logger.debug(printMatrixFromEdges(G.edges))\n+    }\n+    val (gUpdated, lambda, vt) = getPrincipalEigen(sc, G, nIterations)\n+    // TODO: avoid local collect and then sc.parallelize.\n+    val localVt = vt.collect.sortBy(_._1)\n+    val vectRdd = sc.parallelize(localVt.map(v => (v._1, Vectors.dense(v._2))))\n+    vectRdd.cache()\n+    val model = KMeans.train(vectRdd.map {\n+      _._2\n+    }, nClusters, nRuns)\n+    vectRdd.unpersist()\n+    if (logger.isDebugEnabled) {\n+      logger.debug(s\"Eigenvalue = $lambda EigenVector: ${localVt.mkString(\",\")}\")\n+    }\n+    val estimates = vectRdd.zip(model.predict(vectRdd.map(_._2)))\n+    if (logger.isDebugEnabled) {\n+      logger.debug(s\"lambda=$lambda  eigen=${localVt.mkString(\",\")}\")\n+    }\n+    val ccs = (0 until model.clusterCenters.length).zip(model.clusterCenters)\n+    if (logger.isDebugEnabled) {\n+      logger.debug(s\"Kmeans model cluster centers: ${ccs.mkString(\",\")}\")\n+    }\n+    val estCollected = estimates.collect.sortBy(_._1._1)\n+    if (logger.isDebugEnabled) {\n+      val clusters = estCollected.map(_._2)\n+      val counts = estCollected.groupBy(_._2).mapValues {\n+        _.length\n+      }\n+      logger.debug(s\"Cluster counts: Counts: ${counts.mkString(\",\")}\"\n+        + s\"\\nCluster Estimates: ${estCollected.mkString(\",\")}\")\n+    }\n+    (ccs, estCollected)\n+  }\n+\n+  /**\n+   * Read Points from an input file in the following format:\n+   * Vertex1Id Coord11 Coord12 CoordX13 .. Coord1D\n+   * Vertex2Id Coord21 Coord22 CoordX23 .. Coord2D\n+   * ..\n+   * VertexNId CoordN1 CoordN2 CoordN23 .. CoordND\n+   *\n+   * Where N is the number of observations, each a D-dimension point\n+   *\n+   * E.g.\n+   *\n+   * 19\t1.8035177495\t0.7460582552\t0.2361611395\t-0.8645567427\t-0.8613062\n+   * 10\t0.5534111111\t1.0456386879\t1.7045663273\t0.7281759816\t1.0807487792\n+   * 911\t1.200749626\t1.8962364439\t2.5117192131\t-0.4034737281\t-0.9069696484\n+   *\n+   * Which represents three 5-dimensional input Points with VertexIds 19,10, and 911\n+   * @param verticesFile Local filesystem path to the Points input file\n+   * @return Set of Vertices in format appropriate for consumption by the PIC algorithm\n+   */\n+  def readVerticesfromFile(verticesFile: String): Points = {"
  }, {
    "author": {
      "login": "javadba"
    },
    "body": "OK\n",
    "commit": "45508507ddb0f097224176f6688ecf740100caa2",
    "createdAt": "2015-01-28T22:13:49Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\n+import org.apache.log4j.Logger\n+import org.apache.spark.SparkContext\n+import org.apache.spark.graphx._\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+import scala.language.existentials\n+\n+/**\n+ * Implements the scalable graph clustering algorithm Power Iteration Clustering (see\n+ * www.icml2010.org/papers/387.pdf).  From the abstract:\n+ *\n+ * The input data is first transformed to a normalized Affinity Matrix via Gaussian pairwise\n+ * distance calculations. Power iteration is then used to find a dimensionality-reduced\n+ * representation.  The resulting pseudo-eigenvector provides effective clustering - as\n+ * performed by Parallel KMeans.\n+ */\n+object PIClustering {\n+\n+  private val logger = Logger.getLogger(getClass.getName())\n+\n+  type LabeledPoint = (VertexId, BDV[Double])\n+  type Points = Seq[LabeledPoint]\n+  type DGraph = Graph[Double, Double]\n+  type IndexedVector[Double] = (Long, BDV[Double])\n+\n+  // Terminate iteration when norm changes by less than this value\n+  private[mllib] val DefaultMinNormChange: Double = 1e-11\n+\n+  // Default σ for Gaussian Distance calculations\n+  private[mllib] val DefaultSigma = 1.0\n+\n+  // Default number of iterations for PIC loop\n+  private[mllib] val DefaultIterations: Int = 20\n+\n+  // Default minimum affinity between points - lower than this it is considered\n+  // zero and no edge will be created\n+  private[mllib] val DefaultMinAffinity = 1e-11\n+\n+  // Do not allow divide by zero: change to this value instead\n+  val DefaultDivideByZeroVal: Double = 1e-15\n+\n+  // Default number of runs by the KMeans.run() method\n+  val DefaultKMeansRuns = 10\n+\n+  /**\n+   *\n+   * Run a Power Iteration Clustering\n+   *\n+   * @param sc  Spark Context\n+   * @param points  Input Points in format of [(VertexId,(x,y)]\n+   *                where VertexId is a Long\n+   * @param nClusters  Number of clusters to create\n+   * @param nIterations Number of iterations of the PIC algorithm\n+   *                    that calculates primary PseudoEigenvector and Eigenvalue\n+   * @param sigma   Sigma for Gaussian distribution calculation according to\n+   *                [1/2 *sqrt(pi*sigma)] exp (- (x-y)**2 / 2sigma**2\n+   * @param minAffinity  Minimum Affinity between two Points in the input dataset: below\n+   *                     this threshold the affinity will be considered \"close to\" zero and\n+   *                     no Edge will be created between those Points in the sparse matrix\n+   * @param nRuns  Number of runs for the KMeans clustering\n+   * @return Tuple of (Seq[(Cluster Id,Cluster Center)],\n+   *         Seq[(VertexId, ClusterID Membership)]\n+   */\n+  def run(sc: SparkContext,\n+          points: Points,\n+          nClusters: Int,\n+          nIterations: Int = DefaultIterations,\n+          sigma: Double = DefaultSigma,\n+          minAffinity: Double = DefaultMinAffinity,\n+          nRuns: Int = DefaultKMeansRuns)\n+  : (Seq[(Int, Vector)], Seq[((VertexId, Vector), Int)]) = {\n+    val vidsRdd = sc.parallelize(points.map(_._1).sorted)\n+    val nVertices = points.length\n+\n+    val (wRdd, rowSums) = createNormalizedAffinityMatrix(sc, points, sigma)\n+    val initialVt = createInitialVector(sc, points.map(_._1), rowSums)\n+    if (logger.isDebugEnabled) {\n+      logger.debug(s\"Vt(0)=${\n+        printVector(new BDV(initialVt.map {\n+          _._2\n+        }.toArray))\n+      }\")\n+    }\n+    val edgesRdd = createSparseEdgesRdd(sc, wRdd, minAffinity)\n+    val G = createGraphFromEdges(sc, edgesRdd, points.size, Some(initialVt))\n+    if (logger.isDebugEnabled) {\n+      logger.debug(printMatrixFromEdges(G.edges))\n+    }\n+    val (gUpdated, lambda, vt) = getPrincipalEigen(sc, G, nIterations)\n+    // TODO: avoid local collect and then sc.parallelize.\n+    val localVt = vt.collect.sortBy(_._1)\n+    val vectRdd = sc.parallelize(localVt.map(v => (v._1, Vectors.dense(v._2))))\n+    vectRdd.cache()\n+    val model = KMeans.train(vectRdd.map {\n+      _._2\n+    }, nClusters, nRuns)\n+    vectRdd.unpersist()\n+    if (logger.isDebugEnabled) {\n+      logger.debug(s\"Eigenvalue = $lambda EigenVector: ${localVt.mkString(\",\")}\")\n+    }\n+    val estimates = vectRdd.zip(model.predict(vectRdd.map(_._2)))\n+    if (logger.isDebugEnabled) {\n+      logger.debug(s\"lambda=$lambda  eigen=${localVt.mkString(\",\")}\")\n+    }\n+    val ccs = (0 until model.clusterCenters.length).zip(model.clusterCenters)\n+    if (logger.isDebugEnabled) {\n+      logger.debug(s\"Kmeans model cluster centers: ${ccs.mkString(\",\")}\")\n+    }\n+    val estCollected = estimates.collect.sortBy(_._1._1)\n+    if (logger.isDebugEnabled) {\n+      val clusters = estCollected.map(_._2)\n+      val counts = estCollected.groupBy(_._2).mapValues {\n+        _.length\n+      }\n+      logger.debug(s\"Cluster counts: Counts: ${counts.mkString(\",\")}\"\n+        + s\"\\nCluster Estimates: ${estCollected.mkString(\",\")}\")\n+    }\n+    (ccs, estCollected)\n+  }\n+\n+  /**\n+   * Read Points from an input file in the following format:\n+   * Vertex1Id Coord11 Coord12 CoordX13 .. Coord1D\n+   * Vertex2Id Coord21 Coord22 CoordX23 .. Coord2D\n+   * ..\n+   * VertexNId CoordN1 CoordN2 CoordN23 .. CoordND\n+   *\n+   * Where N is the number of observations, each a D-dimension point\n+   *\n+   * E.g.\n+   *\n+   * 19\t1.8035177495\t0.7460582552\t0.2361611395\t-0.8645567427\t-0.8613062\n+   * 10\t0.5534111111\t1.0456386879\t1.7045663273\t0.7281759816\t1.0807487792\n+   * 911\t1.200749626\t1.8962364439\t2.5117192131\t-0.4034737281\t-0.9069696484\n+   *\n+   * Which represents three 5-dimensional input Points with VertexIds 19,10, and 911\n+   * @param verticesFile Local filesystem path to the Points input file\n+   * @return Set of Vertices in format appropriate for consumption by the PIC algorithm\n+   */\n+  def readVerticesfromFile(verticesFile: String): Points = {"
  }],
  "prId": 4254
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`PI` is not a well-recognized acronym. We can use its full name: `PowerIterationClustering`. Add `PowerIterationClustering` class and put setters there, similar to `KMeans`.\n",
    "commit": "45508507ddb0f097224176f6688ecf740100caa2",
    "createdAt": "2015-01-28T22:01:17Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\n+import org.apache.log4j.Logger\n+import org.apache.spark.SparkContext\n+import org.apache.spark.graphx._\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+import scala.language.existentials\n+\n+/**\n+ * Implements the scalable graph clustering algorithm Power Iteration Clustering (see\n+ * www.icml2010.org/papers/387.pdf).  From the abstract:\n+ *\n+ * The input data is first transformed to a normalized Affinity Matrix via Gaussian pairwise\n+ * distance calculations. Power iteration is then used to find a dimensionality-reduced\n+ * representation.  The resulting pseudo-eigenvector provides effective clustering - as\n+ * performed by Parallel KMeans.\n+ */\n+object PIClustering {"
  }, {
    "author": {
      "login": "javadba"
    },
    "body": "OK\n",
    "commit": "45508507ddb0f097224176f6688ecf740100caa2",
    "createdAt": "2015-01-28T22:13:56Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\n+import org.apache.log4j.Logger\n+import org.apache.spark.SparkContext\n+import org.apache.spark.graphx._\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+import scala.language.existentials\n+\n+/**\n+ * Implements the scalable graph clustering algorithm Power Iteration Clustering (see\n+ * www.icml2010.org/papers/387.pdf).  From the abstract:\n+ *\n+ * The input data is first transformed to a normalized Affinity Matrix via Gaussian pairwise\n+ * distance calculations. Power iteration is then used to find a dimensionality-reduced\n+ * representation.  The resulting pseudo-eigenvector provides effective clustering - as\n+ * performed by Parallel KMeans.\n+ */\n+object PIClustering {"
  }],
  "prId": 4254
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`defaultMinNormChange`\n",
    "commit": "45508507ddb0f097224176f6688ecf740100caa2",
    "createdAt": "2015-01-28T22:01:39Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\n+import org.apache.log4j.Logger\n+import org.apache.spark.SparkContext\n+import org.apache.spark.graphx._\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+import scala.language.existentials\n+\n+/**\n+ * Implements the scalable graph clustering algorithm Power Iteration Clustering (see\n+ * www.icml2010.org/papers/387.pdf).  From the abstract:\n+ *\n+ * The input data is first transformed to a normalized Affinity Matrix via Gaussian pairwise\n+ * distance calculations. Power iteration is then used to find a dimensionality-reduced\n+ * representation.  The resulting pseudo-eigenvector provides effective clustering - as\n+ * performed by Parallel KMeans.\n+ */\n+object PIClustering {\n+\n+  private val logger = Logger.getLogger(getClass.getName())\n+\n+  type LabeledPoint = (VertexId, BDV[Double])\n+  type Points = Seq[LabeledPoint]\n+  type DGraph = Graph[Double, Double]\n+  type IndexedVector[Double] = (Long, BDV[Double])\n+\n+  // Terminate iteration when norm changes by less than this value\n+  private[mllib] val DefaultMinNormChange: Double = 1e-11"
  }, {
    "author": {
      "login": "javadba"
    },
    "body": "Typical scala style is to use CamelCase with Capitalized first letter for constants. Would you pls explain briefly?\n",
    "commit": "45508507ddb0f097224176f6688ecf740100caa2",
    "createdAt": "2015-01-28T22:14:22Z",
    "diffHunk": "@@ -0,0 +1,433 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV}\n+import org.apache.log4j.Logger\n+import org.apache.spark.SparkContext\n+import org.apache.spark.graphx._\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+import scala.language.existentials\n+\n+/**\n+ * Implements the scalable graph clustering algorithm Power Iteration Clustering (see\n+ * www.icml2010.org/papers/387.pdf).  From the abstract:\n+ *\n+ * The input data is first transformed to a normalized Affinity Matrix via Gaussian pairwise\n+ * distance calculations. Power iteration is then used to find a dimensionality-reduced\n+ * representation.  The resulting pseudo-eigenvector provides effective clustering - as\n+ * performed by Parallel KMeans.\n+ */\n+object PIClustering {\n+\n+  private val logger = Logger.getLogger(getClass.getName())\n+\n+  type LabeledPoint = (VertexId, BDV[Double])\n+  type Points = Seq[LabeledPoint]\n+  type DGraph = Graph[Double, Double]\n+  type IndexedVector[Double] = (Long, BDV[Double])\n+\n+  // Terminate iteration when norm changes by less than this value\n+  private[mllib] val DefaultMinNormChange: Double = 1e-11"
  }],
  "prId": 4254
}]