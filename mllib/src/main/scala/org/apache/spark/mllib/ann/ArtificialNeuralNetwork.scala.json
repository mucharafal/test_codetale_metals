[{
  "comments": [{
    "author": {
      "login": "manishamde"
    },
    "body": "Fix indentatation.\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-03T01:40:04Z",
    "diffHunk": "@@ -0,0 +1,528 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.ann\n+\n+import breeze.linalg.{DenseVector, Vector => BV, axpy => brzAxpy}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.optimization._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+/*\n+ * Implements a Artificial Neural Network (ANN)\n+ *\n+ * The data consists of an input vector and an output vector, combined into a single vector\n+ * as follows:\n+ *\n+ * [ ---input--- ---output--- ]\n+ *\n+ * NOTE: output values should be in the range [0,1]\n+ *\n+ * For a network of H hidden layers:\n+ *\n+ * hiddenLayersTopology(h) indicates the number of nodes in hidden layer h, excluding the bias\n+ * node. h counts from 0 (first hidden layer, taking inputs from input layer) to H - 1 (last\n+ * hidden layer, sending outputs to the output layer).\n+ *\n+ * hiddenLayersTopology is converted internally to topology, which adds the number of nodes\n+ * in the input and output layers.\n+ *\n+ * noInput = topology(0), the number of input nodes\n+ * noOutput = topology(L-1), the number of output nodes\n+ *\n+ * input = data( 0 to noInput-1 )\n+ * output = data( noInput to noInput + noOutput - 1 )\n+ *\n+ * W_ijl is the weight from node i in layer l-1 to node j in layer l\n+ * W_ijl goes to position ofsWeight(l) + j*(topology(l-1)+1) + i in the weights vector\n+ *\n+ * B_jl is the bias input of node j in layer l\n+ * B_jl goes to position ofsWeight(l) + j*(topology(l-1)+1) + topology(l-1) in the weights vector\n+ *\n+ * error function: E( O, Y ) = sum( O_j - Y_j )\n+ * (with O = (O_0, ..., O_(noOutput-1)) the output of the ANN,\n+ * and (Y_0, ..., Y_(noOutput-1)) the input)\n+ *\n+ * node_jl is node j in layer l\n+ * node_jl goes to position ofsNode(l) + j\n+ *\n+ * The weights gradient is defined as dE/dW_ijl and dE/dB_jl\n+ * It has same mapping as W_ijl and B_jl\n+ *\n+ * For back propagation:\n+ * delta_jl = dE/dS_jl, where S_jl the output of node_jl, but before applying the sigmoid\n+ * delta_jl has the same mapping as node_jl\n+ *\n+ * Where E = ((estOutput-output),(estOutput-output)),\n+ * the inner product of the difference between estimation and target output with itself.\n+ *\n+ */\n+\n+/**\n+ * Artificial neural network (ANN) model\n+ *\n+ * @param weights the weights between the neurons in the ANN.\n+ * @param topology array containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ */\n+class ArtificialNeuralNetworkModel private[mllib](val weights: Vector, val topology: Array[Int])\n+  extends Serializable with ANNHelper {\n+\n+  /**\n+   * Predicts values for a single data point using the trained model.\n+   *\n+   * @param testData represents a single data point.\n+   * @return prediction using the trained model.\n+   */\n+  def predict(testData: Vector): Vector = {\n+    Vectors.dense(computeValues(testData.toArray, weights.toArray))\n+  }\n+\n+  /**\n+   * Predict values for an RDD of data points using the trained model.\n+   *\n+   * @param testDataRDD RDD representing the input vectors.\n+   * @return RDD with predictions using the trained model as (input, output) pairs.\n+   */\n+  def predict(testDataRDD: RDD[Vector]): RDD[(Vector,Vector)] = {\n+    testDataRDD.map(T => (T, predict(T)) )\n+  }\n+\n+  private def computeValues(arrData: Array[Double], arrWeights: Array[Double]): Array[Double] = {\n+    val arrNodes = forwardRun(arrData, arrWeights)\n+    arrNodes.slice(arrNodes.size - topology(L), arrNodes.size)\n+  }\n+}\n+\n+/**\n+ * Performs the training of an Artificial Neural Network (ANN)\n+ *\n+ * @param topology A vector containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ * @param maxNumIterations The maximum number of iterations for the training phase.\n+ * @param convergenceTol Convergence tolerance for LBFGS. Smaller value for closer convergence.\n+ */\n+class ArtificialNeuralNetwork private[mllib](\n+    topology: Array[Int],\n+    maxNumIterations: Int,\n+    convergenceTol: Double)\n+  extends Serializable {\n+\n+  private val gradient = new ANNLeastSquaresGradient(topology)\n+  private val updater = new ANNUpdater()\n+  private val optimizer = new LBFGS(gradient, updater).\n+    setConvergenceTol(convergenceTol).\n+    setMaxNumIterations(maxNumIterations)\n+\n+ /**\n+   * Trains the ANN model.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param initialWeights the initial weights of the ANN\n+   * @return ANN model.\n+   */\n+  private def run(trainingRDD: RDD[(Vector, Vector)], initialWeights: Vector):\n+      ArtificialNeuralNetworkModel = {\n+    val data = trainingRDD.map(v =>\n+      (0.0,\n+        Vectors.fromBreeze(DenseVector.vertcat(\n+          v._1.toBreeze.toDenseVector,\n+          v._2.toBreeze.toDenseVector))\n+        ))\n+    val weights = optimizer.optimize(data, initialWeights)\n+    new ArtificialNeuralNetworkModel(weights, topology)\n+  }\n+}\n+\n+/**\n+ * Top level methods for training the artificial neural network (ANN)\n+ */\n+object ArtificialNeuralNetwork {\n+\n+  private val defaultTolerance: Double = 1e-4\n+\n+  /**\n+   * Trains an ANN.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param maxNumIterations specifies maximum number of training iterations.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector, Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      maxNumIterations: Int): ArtificialNeuralNetworkModel = {\n+    train(trainingRDD, hiddenLayersTopology, maxNumIterations, defaultTolerance)\n+  }\n+\n+  /**\n+   * Continues training of an ANN.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param model model of an already partly trained ANN.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      model: ArtificialNeuralNetworkModel,\n+      maxNumIterations: Int): ArtificialNeuralNetworkModel = {\n+    train(trainingRDD, model, maxNumIterations, defaultTolerance)\n+  }\n+\n+  /**\n+   * Trains an ANN with given initial weights.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param initialWeights initial weights vector.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      initialWeights: Vector,\n+      maxNumIterations: Int): ArtificialNeuralNetworkModel = {\n+    train(trainingRDD, hiddenLayersTopology, initialWeights, maxNumIterations, defaultTolerance)\n+  }\n+\n+  /**\n+   * Trains an ANN using customized convergence tolerance.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param model model of an already partly trained ANN.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @param convergenceTol convergence tolerance for LBFGS. Smaller value for closer convergence.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      model: ArtificialNeuralNetworkModel,\n+      maxNumIterations: Int,\n+      convergenceTol: Double): ArtificialNeuralNetworkModel = {\n+    new ArtificialNeuralNetwork(model.topology, maxNumIterations, convergenceTol).\n+      run(trainingRDD, model.weights)\n+  }\n+\n+  /**\n+   * Continues training of an ANN using customized convergence tolerance.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @param convergenceTol convergence tolerance for LBFGS. Smaller value for closer convergence.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector, Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      maxNumIterations: Int,\n+      convergenceTol: Double): ArtificialNeuralNetworkModel = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    new ArtificialNeuralNetwork(topology, maxNumIterations, convergenceTol).\n+      run(trainingRDD, randomWeights(topology, false))\n+  }\n+\n+  /**\n+   * Trains an ANN with given initial weights.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param initialWeights initial weights vector.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @param convergenceTol convergence tolerance for LBFGS. Smaller value for closer convergence.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      initialWeights: Vector,\n+      maxNumIterations: Int,\n+      convergenceTol: Double): ArtificialNeuralNetworkModel = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    new ArtificialNeuralNetwork(topology, maxNumIterations, convergenceTol).\n+      run(trainingRDD, initialWeights)\n+  }\n+\n+  /**\n+   * Provides a random weights vector.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @return random weights vector.\n+   */\n+  def randomWeights(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int]): Vector = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    return randomWeights(topology, false)\n+  }\n+\n+  /**\n+   * Provides a random weights vector, using given random seed.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for later training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param seed random generator seed.\n+   * @return random weights vector.\n+   */\n+  def randomWeights(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      seed: Int): Vector = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    return randomWeights(topology, true, seed)\n+  }\n+\n+  /**\n+   * Provides a random weights vector, using given random seed.\n+   *\n+   * @param inputLayerSize size of input layer.\n+   * @param outputLayerSize size of output layer.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param seed random generator seed.\n+   * @return random weights vector.\n+   */\n+  def randomWeights(\n+                     inputLayerSize: Int,"
  }, {
    "author": {
      "login": "bgreeven"
    },
    "body": "Fixed\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-03T13:57:47Z",
    "diffHunk": "@@ -0,0 +1,528 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.ann\n+\n+import breeze.linalg.{DenseVector, Vector => BV, axpy => brzAxpy}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.optimization._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+/*\n+ * Implements a Artificial Neural Network (ANN)\n+ *\n+ * The data consists of an input vector and an output vector, combined into a single vector\n+ * as follows:\n+ *\n+ * [ ---input--- ---output--- ]\n+ *\n+ * NOTE: output values should be in the range [0,1]\n+ *\n+ * For a network of H hidden layers:\n+ *\n+ * hiddenLayersTopology(h) indicates the number of nodes in hidden layer h, excluding the bias\n+ * node. h counts from 0 (first hidden layer, taking inputs from input layer) to H - 1 (last\n+ * hidden layer, sending outputs to the output layer).\n+ *\n+ * hiddenLayersTopology is converted internally to topology, which adds the number of nodes\n+ * in the input and output layers.\n+ *\n+ * noInput = topology(0), the number of input nodes\n+ * noOutput = topology(L-1), the number of output nodes\n+ *\n+ * input = data( 0 to noInput-1 )\n+ * output = data( noInput to noInput + noOutput - 1 )\n+ *\n+ * W_ijl is the weight from node i in layer l-1 to node j in layer l\n+ * W_ijl goes to position ofsWeight(l) + j*(topology(l-1)+1) + i in the weights vector\n+ *\n+ * B_jl is the bias input of node j in layer l\n+ * B_jl goes to position ofsWeight(l) + j*(topology(l-1)+1) + topology(l-1) in the weights vector\n+ *\n+ * error function: E( O, Y ) = sum( O_j - Y_j )\n+ * (with O = (O_0, ..., O_(noOutput-1)) the output of the ANN,\n+ * and (Y_0, ..., Y_(noOutput-1)) the input)\n+ *\n+ * node_jl is node j in layer l\n+ * node_jl goes to position ofsNode(l) + j\n+ *\n+ * The weights gradient is defined as dE/dW_ijl and dE/dB_jl\n+ * It has same mapping as W_ijl and B_jl\n+ *\n+ * For back propagation:\n+ * delta_jl = dE/dS_jl, where S_jl the output of node_jl, but before applying the sigmoid\n+ * delta_jl has the same mapping as node_jl\n+ *\n+ * Where E = ((estOutput-output),(estOutput-output)),\n+ * the inner product of the difference between estimation and target output with itself.\n+ *\n+ */\n+\n+/**\n+ * Artificial neural network (ANN) model\n+ *\n+ * @param weights the weights between the neurons in the ANN.\n+ * @param topology array containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ */\n+class ArtificialNeuralNetworkModel private[mllib](val weights: Vector, val topology: Array[Int])\n+  extends Serializable with ANNHelper {\n+\n+  /**\n+   * Predicts values for a single data point using the trained model.\n+   *\n+   * @param testData represents a single data point.\n+   * @return prediction using the trained model.\n+   */\n+  def predict(testData: Vector): Vector = {\n+    Vectors.dense(computeValues(testData.toArray, weights.toArray))\n+  }\n+\n+  /**\n+   * Predict values for an RDD of data points using the trained model.\n+   *\n+   * @param testDataRDD RDD representing the input vectors.\n+   * @return RDD with predictions using the trained model as (input, output) pairs.\n+   */\n+  def predict(testDataRDD: RDD[Vector]): RDD[(Vector,Vector)] = {\n+    testDataRDD.map(T => (T, predict(T)) )\n+  }\n+\n+  private def computeValues(arrData: Array[Double], arrWeights: Array[Double]): Array[Double] = {\n+    val arrNodes = forwardRun(arrData, arrWeights)\n+    arrNodes.slice(arrNodes.size - topology(L), arrNodes.size)\n+  }\n+}\n+\n+/**\n+ * Performs the training of an Artificial Neural Network (ANN)\n+ *\n+ * @param topology A vector containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ * @param maxNumIterations The maximum number of iterations for the training phase.\n+ * @param convergenceTol Convergence tolerance for LBFGS. Smaller value for closer convergence.\n+ */\n+class ArtificialNeuralNetwork private[mllib](\n+    topology: Array[Int],\n+    maxNumIterations: Int,\n+    convergenceTol: Double)\n+  extends Serializable {\n+\n+  private val gradient = new ANNLeastSquaresGradient(topology)\n+  private val updater = new ANNUpdater()\n+  private val optimizer = new LBFGS(gradient, updater).\n+    setConvergenceTol(convergenceTol).\n+    setMaxNumIterations(maxNumIterations)\n+\n+ /**\n+   * Trains the ANN model.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param initialWeights the initial weights of the ANN\n+   * @return ANN model.\n+   */\n+  private def run(trainingRDD: RDD[(Vector, Vector)], initialWeights: Vector):\n+      ArtificialNeuralNetworkModel = {\n+    val data = trainingRDD.map(v =>\n+      (0.0,\n+        Vectors.fromBreeze(DenseVector.vertcat(\n+          v._1.toBreeze.toDenseVector,\n+          v._2.toBreeze.toDenseVector))\n+        ))\n+    val weights = optimizer.optimize(data, initialWeights)\n+    new ArtificialNeuralNetworkModel(weights, topology)\n+  }\n+}\n+\n+/**\n+ * Top level methods for training the artificial neural network (ANN)\n+ */\n+object ArtificialNeuralNetwork {\n+\n+  private val defaultTolerance: Double = 1e-4\n+\n+  /**\n+   * Trains an ANN.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param maxNumIterations specifies maximum number of training iterations.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector, Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      maxNumIterations: Int): ArtificialNeuralNetworkModel = {\n+    train(trainingRDD, hiddenLayersTopology, maxNumIterations, defaultTolerance)\n+  }\n+\n+  /**\n+   * Continues training of an ANN.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param model model of an already partly trained ANN.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      model: ArtificialNeuralNetworkModel,\n+      maxNumIterations: Int): ArtificialNeuralNetworkModel = {\n+    train(trainingRDD, model, maxNumIterations, defaultTolerance)\n+  }\n+\n+  /**\n+   * Trains an ANN with given initial weights.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param initialWeights initial weights vector.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      initialWeights: Vector,\n+      maxNumIterations: Int): ArtificialNeuralNetworkModel = {\n+    train(trainingRDD, hiddenLayersTopology, initialWeights, maxNumIterations, defaultTolerance)\n+  }\n+\n+  /**\n+   * Trains an ANN using customized convergence tolerance.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param model model of an already partly trained ANN.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @param convergenceTol convergence tolerance for LBFGS. Smaller value for closer convergence.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      model: ArtificialNeuralNetworkModel,\n+      maxNumIterations: Int,\n+      convergenceTol: Double): ArtificialNeuralNetworkModel = {\n+    new ArtificialNeuralNetwork(model.topology, maxNumIterations, convergenceTol).\n+      run(trainingRDD, model.weights)\n+  }\n+\n+  /**\n+   * Continues training of an ANN using customized convergence tolerance.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @param convergenceTol convergence tolerance for LBFGS. Smaller value for closer convergence.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector, Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      maxNumIterations: Int,\n+      convergenceTol: Double): ArtificialNeuralNetworkModel = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    new ArtificialNeuralNetwork(topology, maxNumIterations, convergenceTol).\n+      run(trainingRDD, randomWeights(topology, false))\n+  }\n+\n+  /**\n+   * Trains an ANN with given initial weights.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param initialWeights initial weights vector.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @param convergenceTol convergence tolerance for LBFGS. Smaller value for closer convergence.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      initialWeights: Vector,\n+      maxNumIterations: Int,\n+      convergenceTol: Double): ArtificialNeuralNetworkModel = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    new ArtificialNeuralNetwork(topology, maxNumIterations, convergenceTol).\n+      run(trainingRDD, initialWeights)\n+  }\n+\n+  /**\n+   * Provides a random weights vector.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @return random weights vector.\n+   */\n+  def randomWeights(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int]): Vector = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    return randomWeights(topology, false)\n+  }\n+\n+  /**\n+   * Provides a random weights vector, using given random seed.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for later training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param seed random generator seed.\n+   * @return random weights vector.\n+   */\n+  def randomWeights(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      seed: Int): Vector = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    return randomWeights(topology, true, seed)\n+  }\n+\n+  /**\n+   * Provides a random weights vector, using given random seed.\n+   *\n+   * @param inputLayerSize size of input layer.\n+   * @param outputLayerSize size of output layer.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param seed random generator seed.\n+   * @return random weights vector.\n+   */\n+  def randomWeights(\n+                     inputLayerSize: Int,"
  }],
  "prId": 1290
}, {
  "comments": [{
    "author": {
      "login": "manishamde"
    },
    "body": "Should be a separate file.\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-03T02:39:20Z",
    "diffHunk": "@@ -0,0 +1,528 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.ann\n+\n+import breeze.linalg.{DenseVector, Vector => BV, axpy => brzAxpy}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.optimization._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+/*\n+ * Implements a Artificial Neural Network (ANN)\n+ *\n+ * The data consists of an input vector and an output vector, combined into a single vector\n+ * as follows:\n+ *\n+ * [ ---input--- ---output--- ]\n+ *\n+ * NOTE: output values should be in the range [0,1]\n+ *\n+ * For a network of H hidden layers:\n+ *\n+ * hiddenLayersTopology(h) indicates the number of nodes in hidden layer h, excluding the bias\n+ * node. h counts from 0 (first hidden layer, taking inputs from input layer) to H - 1 (last\n+ * hidden layer, sending outputs to the output layer).\n+ *\n+ * hiddenLayersTopology is converted internally to topology, which adds the number of nodes\n+ * in the input and output layers.\n+ *\n+ * noInput = topology(0), the number of input nodes\n+ * noOutput = topology(L-1), the number of output nodes\n+ *\n+ * input = data( 0 to noInput-1 )\n+ * output = data( noInput to noInput + noOutput - 1 )\n+ *\n+ * W_ijl is the weight from node i in layer l-1 to node j in layer l\n+ * W_ijl goes to position ofsWeight(l) + j*(topology(l-1)+1) + i in the weights vector\n+ *\n+ * B_jl is the bias input of node j in layer l\n+ * B_jl goes to position ofsWeight(l) + j*(topology(l-1)+1) + topology(l-1) in the weights vector\n+ *\n+ * error function: E( O, Y ) = sum( O_j - Y_j )\n+ * (with O = (O_0, ..., O_(noOutput-1)) the output of the ANN,\n+ * and (Y_0, ..., Y_(noOutput-1)) the input)\n+ *\n+ * node_jl is node j in layer l\n+ * node_jl goes to position ofsNode(l) + j\n+ *\n+ * The weights gradient is defined as dE/dW_ijl and dE/dB_jl\n+ * It has same mapping as W_ijl and B_jl\n+ *\n+ * For back propagation:\n+ * delta_jl = dE/dS_jl, where S_jl the output of node_jl, but before applying the sigmoid\n+ * delta_jl has the same mapping as node_jl\n+ *\n+ * Where E = ((estOutput-output),(estOutput-output)),\n+ * the inner product of the difference between estimation and target output with itself.\n+ *\n+ */\n+\n+/**\n+ * Artificial neural network (ANN) model\n+ *\n+ * @param weights the weights between the neurons in the ANN.\n+ * @param topology array containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ */\n+class ArtificialNeuralNetworkModel private[mllib](val weights: Vector, val topology: Array[Int])\n+  extends Serializable with ANNHelper {\n+\n+  /**\n+   * Predicts values for a single data point using the trained model.\n+   *\n+   * @param testData represents a single data point.\n+   * @return prediction using the trained model.\n+   */\n+  def predict(testData: Vector): Vector = {\n+    Vectors.dense(computeValues(testData.toArray, weights.toArray))\n+  }\n+\n+  /**\n+   * Predict values for an RDD of data points using the trained model.\n+   *\n+   * @param testDataRDD RDD representing the input vectors.\n+   * @return RDD with predictions using the trained model as (input, output) pairs.\n+   */\n+  def predict(testDataRDD: RDD[Vector]): RDD[(Vector,Vector)] = {\n+    testDataRDD.map(T => (T, predict(T)) )\n+  }\n+\n+  private def computeValues(arrData: Array[Double], arrWeights: Array[Double]): Array[Double] = {\n+    val arrNodes = forwardRun(arrData, arrWeights)\n+    arrNodes.slice(arrNodes.size - topology(L), arrNodes.size)\n+  }\n+}\n+\n+/**\n+ * Performs the training of an Artificial Neural Network (ANN)\n+ *\n+ * @param topology A vector containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ * @param maxNumIterations The maximum number of iterations for the training phase.\n+ * @param convergenceTol Convergence tolerance for LBFGS. Smaller value for closer convergence.\n+ */\n+class ArtificialNeuralNetwork private[mllib](\n+    topology: Array[Int],\n+    maxNumIterations: Int,\n+    convergenceTol: Double)\n+  extends Serializable {\n+\n+  private val gradient = new ANNLeastSquaresGradient(topology)\n+  private val updater = new ANNUpdater()\n+  private val optimizer = new LBFGS(gradient, updater).\n+    setConvergenceTol(convergenceTol).\n+    setMaxNumIterations(maxNumIterations)\n+\n+ /**\n+   * Trains the ANN model.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param initialWeights the initial weights of the ANN\n+   * @return ANN model.\n+   */\n+  private def run(trainingRDD: RDD[(Vector, Vector)], initialWeights: Vector):\n+      ArtificialNeuralNetworkModel = {\n+    val data = trainingRDD.map(v =>\n+      (0.0,\n+        Vectors.fromBreeze(DenseVector.vertcat(\n+          v._1.toBreeze.toDenseVector,\n+          v._2.toBreeze.toDenseVector))\n+        ))\n+    val weights = optimizer.optimize(data, initialWeights)\n+    new ArtificialNeuralNetworkModel(weights, topology)\n+  }\n+}\n+\n+/**\n+ * Top level methods for training the artificial neural network (ANN)\n+ */\n+object ArtificialNeuralNetwork {\n+\n+  private val defaultTolerance: Double = 1e-4\n+\n+  /**\n+   * Trains an ANN.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param maxNumIterations specifies maximum number of training iterations.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector, Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      maxNumIterations: Int): ArtificialNeuralNetworkModel = {\n+    train(trainingRDD, hiddenLayersTopology, maxNumIterations, defaultTolerance)\n+  }\n+\n+  /**\n+   * Continues training of an ANN.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param model model of an already partly trained ANN.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      model: ArtificialNeuralNetworkModel,\n+      maxNumIterations: Int): ArtificialNeuralNetworkModel = {\n+    train(trainingRDD, model, maxNumIterations, defaultTolerance)\n+  }\n+\n+  /**\n+   * Trains an ANN with given initial weights.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param initialWeights initial weights vector.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      initialWeights: Vector,\n+      maxNumIterations: Int): ArtificialNeuralNetworkModel = {\n+    train(trainingRDD, hiddenLayersTopology, initialWeights, maxNumIterations, defaultTolerance)\n+  }\n+\n+  /**\n+   * Trains an ANN using customized convergence tolerance.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param model model of an already partly trained ANN.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @param convergenceTol convergence tolerance for LBFGS. Smaller value for closer convergence.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      model: ArtificialNeuralNetworkModel,\n+      maxNumIterations: Int,\n+      convergenceTol: Double): ArtificialNeuralNetworkModel = {\n+    new ArtificialNeuralNetwork(model.topology, maxNumIterations, convergenceTol).\n+      run(trainingRDD, model.weights)\n+  }\n+\n+  /**\n+   * Continues training of an ANN using customized convergence tolerance.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @param convergenceTol convergence tolerance for LBFGS. Smaller value for closer convergence.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector, Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      maxNumIterations: Int,\n+      convergenceTol: Double): ArtificialNeuralNetworkModel = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    new ArtificialNeuralNetwork(topology, maxNumIterations, convergenceTol).\n+      run(trainingRDD, randomWeights(topology, false))\n+  }\n+\n+  /**\n+   * Trains an ANN with given initial weights.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param initialWeights initial weights vector.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @param convergenceTol convergence tolerance for LBFGS. Smaller value for closer convergence.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      initialWeights: Vector,\n+      maxNumIterations: Int,\n+      convergenceTol: Double): ArtificialNeuralNetworkModel = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    new ArtificialNeuralNetwork(topology, maxNumIterations, convergenceTol).\n+      run(trainingRDD, initialWeights)\n+  }\n+\n+  /**\n+   * Provides a random weights vector.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @return random weights vector.\n+   */\n+  def randomWeights(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int]): Vector = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    return randomWeights(topology, false)\n+  }\n+\n+  /**\n+   * Provides a random weights vector, using given random seed.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for later training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param seed random generator seed.\n+   * @return random weights vector.\n+   */\n+  def randomWeights(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      seed: Int): Vector = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    return randomWeights(topology, true, seed)\n+  }\n+\n+  /**\n+   * Provides a random weights vector, using given random seed.\n+   *\n+   * @param inputLayerSize size of input layer.\n+   * @param outputLayerSize size of output layer.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param seed random generator seed.\n+   * @return random weights vector.\n+   */\n+  def randomWeights(\n+                     inputLayerSize: Int,\n+                     outputLayerSize: Int,\n+                     hiddenLayersTopology: Array[Int],\n+                     seed: Int): Vector = {\n+    val topology = inputLayerSize +: hiddenLayersTopology :+ outputLayerSize\n+    return randomWeights(topology, true, seed)\n+  }\n+\n+  private def convertTopology(\n+      input: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int] ): Array[Int] = {\n+    val firstElt = input.first\n+    firstElt._1.size +: hiddenLayersTopology :+ firstElt._2.size\n+  }\n+\n+  private def randomWeights(topology: Array[Int], useSeed: Boolean, seed: Int = 0): Vector = {\n+    val rand: XORShiftRandom =\n+      if( useSeed == false ) new XORShiftRandom() else new XORShiftRandom(seed)\n+    var i: Int = 0\n+    var l: Int = 0\n+    val noWeights = {\n+      var tmp = 0\n+      var i = 1\n+      while (i < topology.size) {\n+        tmp = tmp + topology(i) * (topology(i - 1) + 1)\n+        i += 1\n+      }\n+      tmp\n+    }\n+    val initialWeightsArr = new Array[Double](noWeights)\n+    var pos = 0\n+    l = 1\n+    while (l < topology.length) {\n+      i = 0\n+      while (i < (topology(l) * (topology(l - 1) + 1))) {\n+        initialWeightsArr(pos) = (rand.nextDouble * 4.8 - 2.4) / (topology(l - 1) + 1)\n+        pos += 1\n+        i += 1\n+      }\n+      l += 1\n+    }\n+    Vectors.dense(initialWeightsArr)\n+  }\n+}\n+\n+/**\n+ * Helper methods for ANN\n+ */\n+private[ann] trait ANNHelper {"
  }],
  "prId": 1290
}, {
  "comments": [{
    "author": {
      "login": "manishamde"
    },
    "body": "Best to move to a separate file.\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-03T02:40:02Z",
    "diffHunk": "@@ -0,0 +1,528 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.ann\n+\n+import breeze.linalg.{DenseVector, Vector => BV, axpy => brzAxpy}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.optimization._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+/*\n+ * Implements a Artificial Neural Network (ANN)\n+ *\n+ * The data consists of an input vector and an output vector, combined into a single vector\n+ * as follows:\n+ *\n+ * [ ---input--- ---output--- ]\n+ *\n+ * NOTE: output values should be in the range [0,1]\n+ *\n+ * For a network of H hidden layers:\n+ *\n+ * hiddenLayersTopology(h) indicates the number of nodes in hidden layer h, excluding the bias\n+ * node. h counts from 0 (first hidden layer, taking inputs from input layer) to H - 1 (last\n+ * hidden layer, sending outputs to the output layer).\n+ *\n+ * hiddenLayersTopology is converted internally to topology, which adds the number of nodes\n+ * in the input and output layers.\n+ *\n+ * noInput = topology(0), the number of input nodes\n+ * noOutput = topology(L-1), the number of output nodes\n+ *\n+ * input = data( 0 to noInput-1 )\n+ * output = data( noInput to noInput + noOutput - 1 )\n+ *\n+ * W_ijl is the weight from node i in layer l-1 to node j in layer l\n+ * W_ijl goes to position ofsWeight(l) + j*(topology(l-1)+1) + i in the weights vector\n+ *\n+ * B_jl is the bias input of node j in layer l\n+ * B_jl goes to position ofsWeight(l) + j*(topology(l-1)+1) + topology(l-1) in the weights vector\n+ *\n+ * error function: E( O, Y ) = sum( O_j - Y_j )\n+ * (with O = (O_0, ..., O_(noOutput-1)) the output of the ANN,\n+ * and (Y_0, ..., Y_(noOutput-1)) the input)\n+ *\n+ * node_jl is node j in layer l\n+ * node_jl goes to position ofsNode(l) + j\n+ *\n+ * The weights gradient is defined as dE/dW_ijl and dE/dB_jl\n+ * It has same mapping as W_ijl and B_jl\n+ *\n+ * For back propagation:\n+ * delta_jl = dE/dS_jl, where S_jl the output of node_jl, but before applying the sigmoid\n+ * delta_jl has the same mapping as node_jl\n+ *\n+ * Where E = ((estOutput-output),(estOutput-output)),\n+ * the inner product of the difference between estimation and target output with itself.\n+ *\n+ */\n+\n+/**\n+ * Artificial neural network (ANN) model\n+ *\n+ * @param weights the weights between the neurons in the ANN.\n+ * @param topology array containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ */\n+class ArtificialNeuralNetworkModel private[mllib](val weights: Vector, val topology: Array[Int])\n+  extends Serializable with ANNHelper {\n+\n+  /**\n+   * Predicts values for a single data point using the trained model.\n+   *\n+   * @param testData represents a single data point.\n+   * @return prediction using the trained model.\n+   */\n+  def predict(testData: Vector): Vector = {\n+    Vectors.dense(computeValues(testData.toArray, weights.toArray))\n+  }\n+\n+  /**\n+   * Predict values for an RDD of data points using the trained model.\n+   *\n+   * @param testDataRDD RDD representing the input vectors.\n+   * @return RDD with predictions using the trained model as (input, output) pairs.\n+   */\n+  def predict(testDataRDD: RDD[Vector]): RDD[(Vector,Vector)] = {\n+    testDataRDD.map(T => (T, predict(T)) )\n+  }\n+\n+  private def computeValues(arrData: Array[Double], arrWeights: Array[Double]): Array[Double] = {\n+    val arrNodes = forwardRun(arrData, arrWeights)\n+    arrNodes.slice(arrNodes.size - topology(L), arrNodes.size)\n+  }\n+}\n+\n+/**\n+ * Performs the training of an Artificial Neural Network (ANN)\n+ *\n+ * @param topology A vector containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ * @param maxNumIterations The maximum number of iterations for the training phase.\n+ * @param convergenceTol Convergence tolerance for LBFGS. Smaller value for closer convergence.\n+ */\n+class ArtificialNeuralNetwork private[mllib](\n+    topology: Array[Int],\n+    maxNumIterations: Int,\n+    convergenceTol: Double)\n+  extends Serializable {\n+\n+  private val gradient = new ANNLeastSquaresGradient(topology)\n+  private val updater = new ANNUpdater()\n+  private val optimizer = new LBFGS(gradient, updater).\n+    setConvergenceTol(convergenceTol).\n+    setMaxNumIterations(maxNumIterations)\n+\n+ /**\n+   * Trains the ANN model.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param initialWeights the initial weights of the ANN\n+   * @return ANN model.\n+   */\n+  private def run(trainingRDD: RDD[(Vector, Vector)], initialWeights: Vector):\n+      ArtificialNeuralNetworkModel = {\n+    val data = trainingRDD.map(v =>\n+      (0.0,\n+        Vectors.fromBreeze(DenseVector.vertcat(\n+          v._1.toBreeze.toDenseVector,\n+          v._2.toBreeze.toDenseVector))\n+        ))\n+    val weights = optimizer.optimize(data, initialWeights)\n+    new ArtificialNeuralNetworkModel(weights, topology)\n+  }\n+}\n+\n+/**\n+ * Top level methods for training the artificial neural network (ANN)\n+ */\n+object ArtificialNeuralNetwork {\n+\n+  private val defaultTolerance: Double = 1e-4\n+\n+  /**\n+   * Trains an ANN.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param maxNumIterations specifies maximum number of training iterations.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector, Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      maxNumIterations: Int): ArtificialNeuralNetworkModel = {\n+    train(trainingRDD, hiddenLayersTopology, maxNumIterations, defaultTolerance)\n+  }\n+\n+  /**\n+   * Continues training of an ANN.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param model model of an already partly trained ANN.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      model: ArtificialNeuralNetworkModel,\n+      maxNumIterations: Int): ArtificialNeuralNetworkModel = {\n+    train(trainingRDD, model, maxNumIterations, defaultTolerance)\n+  }\n+\n+  /**\n+   * Trains an ANN with given initial weights.\n+   * Uses default convergence tolerance 1e-4 for LBFGS.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param initialWeights initial weights vector.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      initialWeights: Vector,\n+      maxNumIterations: Int): ArtificialNeuralNetworkModel = {\n+    train(trainingRDD, hiddenLayersTopology, initialWeights, maxNumIterations, defaultTolerance)\n+  }\n+\n+  /**\n+   * Trains an ANN using customized convergence tolerance.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param model model of an already partly trained ANN.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @param convergenceTol convergence tolerance for LBFGS. Smaller value for closer convergence.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      model: ArtificialNeuralNetworkModel,\n+      maxNumIterations: Int,\n+      convergenceTol: Double): ArtificialNeuralNetworkModel = {\n+    new ArtificialNeuralNetwork(model.topology, maxNumIterations, convergenceTol).\n+      run(trainingRDD, model.weights)\n+  }\n+\n+  /**\n+   * Continues training of an ANN using customized convergence tolerance.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @param convergenceTol convergence tolerance for LBFGS. Smaller value for closer convergence.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector, Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      maxNumIterations: Int,\n+      convergenceTol: Double): ArtificialNeuralNetworkModel = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    new ArtificialNeuralNetwork(topology, maxNumIterations, convergenceTol).\n+      run(trainingRDD, randomWeights(topology, false))\n+  }\n+\n+  /**\n+   * Trains an ANN with given initial weights.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param initialWeights initial weights vector.\n+   * @param maxNumIterations maximum number of training iterations.\n+   * @param convergenceTol convergence tolerance for LBFGS. Smaller value for closer convergence.\n+   * @return ANN model.\n+   */\n+  def train(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      initialWeights: Vector,\n+      maxNumIterations: Int,\n+      convergenceTol: Double): ArtificialNeuralNetworkModel = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    new ArtificialNeuralNetwork(topology, maxNumIterations, convergenceTol).\n+      run(trainingRDD, initialWeights)\n+  }\n+\n+  /**\n+   * Provides a random weights vector.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @return random weights vector.\n+   */\n+  def randomWeights(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int]): Vector = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    return randomWeights(topology, false)\n+  }\n+\n+  /**\n+   * Provides a random weights vector, using given random seed.\n+   *\n+   * @param trainingRDD RDD containing (input, output) pairs for later training.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param seed random generator seed.\n+   * @return random weights vector.\n+   */\n+  def randomWeights(\n+      trainingRDD: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int],\n+      seed: Int): Vector = {\n+    val topology = convertTopology(trainingRDD, hiddenLayersTopology)\n+    return randomWeights(topology, true, seed)\n+  }\n+\n+  /**\n+   * Provides a random weights vector, using given random seed.\n+   *\n+   * @param inputLayerSize size of input layer.\n+   * @param outputLayerSize size of output layer.\n+   * @param hiddenLayersTopology number of nodes per hidden layer, excluding the bias nodes.\n+   * @param seed random generator seed.\n+   * @return random weights vector.\n+   */\n+  def randomWeights(\n+                     inputLayerSize: Int,\n+                     outputLayerSize: Int,\n+                     hiddenLayersTopology: Array[Int],\n+                     seed: Int): Vector = {\n+    val topology = inputLayerSize +: hiddenLayersTopology :+ outputLayerSize\n+    return randomWeights(topology, true, seed)\n+  }\n+\n+  private def convertTopology(\n+      input: RDD[(Vector,Vector)],\n+      hiddenLayersTopology: Array[Int] ): Array[Int] = {\n+    val firstElt = input.first\n+    firstElt._1.size +: hiddenLayersTopology :+ firstElt._2.size\n+  }\n+\n+  private def randomWeights(topology: Array[Int], useSeed: Boolean, seed: Int = 0): Vector = {\n+    val rand: XORShiftRandom =\n+      if( useSeed == false ) new XORShiftRandom() else new XORShiftRandom(seed)\n+    var i: Int = 0\n+    var l: Int = 0\n+    val noWeights = {\n+      var tmp = 0\n+      var i = 1\n+      while (i < topology.size) {\n+        tmp = tmp + topology(i) * (topology(i - 1) + 1)\n+        i += 1\n+      }\n+      tmp\n+    }\n+    val initialWeightsArr = new Array[Double](noWeights)\n+    var pos = 0\n+    l = 1\n+    while (l < topology.length) {\n+      i = 0\n+      while (i < (topology(l) * (topology(l - 1) + 1))) {\n+        initialWeightsArr(pos) = (rand.nextDouble * 4.8 - 2.4) / (topology(l - 1) + 1)\n+        pos += 1\n+        i += 1\n+      }\n+      l += 1\n+    }\n+    Vectors.dense(initialWeightsArr)\n+  }\n+}\n+\n+/**\n+ * Helper methods for ANN\n+ */\n+private[ann] trait ANNHelper {\n+  protected val topology: Array[Int]\n+  protected def g(x: Double) = 1.0 / (1.0 + math.exp(-x))\n+  protected val L = topology.length - 1\n+  protected val noWeights = {\n+    var tmp = 0\n+    var l = 1\n+    while (l <= L) {\n+      tmp = tmp + topology(l) * (topology(l - 1) + 1)\n+      l += 1\n+    }\n+    tmp\n+  }\n+  protected val ofsWeight: Array[Int] = {\n+    val tmp = new Array[Int](L + 1)\n+    var curPos = 0\n+    tmp(0) = 0\n+    var l = 1\n+    while (l <= L) {\n+      tmp(l) = curPos\n+      curPos = curPos + (topology(l - 1) + 1) * topology(l)\n+      l += 1\n+    }\n+    tmp\n+  }\n+  protected val noNodes: Int = {\n+    var tmp: Integer = 0\n+    var l = 0\n+    while (l < topology.size) {\n+      tmp = tmp + topology(l)\n+      l += 1\n+    }\n+    tmp\n+  }\n+  protected val ofsNode: Array[Int] = {\n+    val tmp = new Array[Int](L + 1)\n+    tmp(0) = 0\n+    var l = 1\n+    while (l <= L) {\n+      tmp(l) = tmp(l - 1) + topology(l - 1)\n+      l += 1\n+    }\n+    tmp\n+  }\n+\n+  protected def forwardRun(arrData: Array[Double], arrWeights: Array[Double]): Array[Double] = {\n+    val arrNodes = new Array[Double](noNodes)\n+    var i: Int = 0\n+    var j: Int = 0\n+    var l: Int = 0\n+    i = 0\n+    while (i < topology(0)) {\n+      arrNodes(i) = arrData(i)\n+      i += 1\n+    }\n+    l = 1\n+    while (l <= L) {\n+      j = 0\n+      while (j < topology(l)) {\n+        var cum: Double = 0.0\n+        i = 0\n+        while (i < topology(l - 1)) {\n+          cum = cum +\n+            arrWeights(ofsWeight(l) + (topology(l - 1) + 1) * j + i) *\n+              arrNodes(ofsNode(l - 1) + i)\n+          i += 1\n+        }\n+        cum = cum + arrWeights(ofsWeight(l) + (topology(l - 1) + 1) * j + topology(l - 1))\n+        arrNodes(ofsNode(l) + j) = g(cum)\n+        j += 1\n+      }\n+      l += 1\n+    }\n+    arrNodes\n+  }\n+}\n+\n+private class ANNLeastSquaresGradient(val topology: Array[Int]) extends Gradient with ANNHelper {"
  }],
  "prId": 1290
}, {
  "comments": [{
    "author": {
      "login": "manishamde"
    },
    "body": "I think the doc is misaligned by one space.\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-03T03:06:24Z",
    "diffHunk": "@@ -0,0 +1,528 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.ann\n+\n+import breeze.linalg.{DenseVector, Vector => BV, axpy => brzAxpy}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.optimization._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+/*\n+ * Implements a Artificial Neural Network (ANN)\n+ *\n+ * The data consists of an input vector and an output vector, combined into a single vector\n+ * as follows:\n+ *\n+ * [ ---input--- ---output--- ]\n+ *\n+ * NOTE: output values should be in the range [0,1]\n+ *\n+ * For a network of H hidden layers:\n+ *\n+ * hiddenLayersTopology(h) indicates the number of nodes in hidden layer h, excluding the bias\n+ * node. h counts from 0 (first hidden layer, taking inputs from input layer) to H - 1 (last\n+ * hidden layer, sending outputs to the output layer).\n+ *\n+ * hiddenLayersTopology is converted internally to topology, which adds the number of nodes\n+ * in the input and output layers.\n+ *\n+ * noInput = topology(0), the number of input nodes\n+ * noOutput = topology(L-1), the number of output nodes\n+ *\n+ * input = data( 0 to noInput-1 )\n+ * output = data( noInput to noInput + noOutput - 1 )\n+ *\n+ * W_ijl is the weight from node i in layer l-1 to node j in layer l\n+ * W_ijl goes to position ofsWeight(l) + j*(topology(l-1)+1) + i in the weights vector\n+ *\n+ * B_jl is the bias input of node j in layer l\n+ * B_jl goes to position ofsWeight(l) + j*(topology(l-1)+1) + topology(l-1) in the weights vector\n+ *\n+ * error function: E( O, Y ) = sum( O_j - Y_j )\n+ * (with O = (O_0, ..., O_(noOutput-1)) the output of the ANN,\n+ * and (Y_0, ..., Y_(noOutput-1)) the input)\n+ *\n+ * node_jl is node j in layer l\n+ * node_jl goes to position ofsNode(l) + j\n+ *\n+ * The weights gradient is defined as dE/dW_ijl and dE/dB_jl\n+ * It has same mapping as W_ijl and B_jl\n+ *\n+ * For back propagation:\n+ * delta_jl = dE/dS_jl, where S_jl the output of node_jl, but before applying the sigmoid\n+ * delta_jl has the same mapping as node_jl\n+ *\n+ * Where E = ((estOutput-output),(estOutput-output)),\n+ * the inner product of the difference between estimation and target output with itself.\n+ *\n+ */\n+\n+/**\n+ * Artificial neural network (ANN) model\n+ *\n+ * @param weights the weights between the neurons in the ANN.\n+ * @param topology array containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ */\n+class ArtificialNeuralNetworkModel private[mllib](val weights: Vector, val topology: Array[Int])\n+  extends Serializable with ANNHelper {\n+\n+  /**\n+   * Predicts values for a single data point using the trained model.\n+   *\n+   * @param testData represents a single data point.\n+   * @return prediction using the trained model.\n+   */\n+  def predict(testData: Vector): Vector = {\n+    Vectors.dense(computeValues(testData.toArray, weights.toArray))\n+  }\n+\n+  /**\n+   * Predict values for an RDD of data points using the trained model.\n+   *\n+   * @param testDataRDD RDD representing the input vectors.\n+   * @return RDD with predictions using the trained model as (input, output) pairs.\n+   */\n+  def predict(testDataRDD: RDD[Vector]): RDD[(Vector,Vector)] = {\n+    testDataRDD.map(T => (T, predict(T)) )\n+  }\n+\n+  private def computeValues(arrData: Array[Double], arrWeights: Array[Double]): Array[Double] = {\n+    val arrNodes = forwardRun(arrData, arrWeights)\n+    arrNodes.slice(arrNodes.size - topology(L), arrNodes.size)\n+  }\n+}\n+\n+/**\n+ * Performs the training of an Artificial Neural Network (ANN)\n+ *\n+ * @param topology A vector containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ * @param maxNumIterations The maximum number of iterations for the training phase.\n+ * @param convergenceTol Convergence tolerance for LBFGS. Smaller value for closer convergence.\n+ */\n+class ArtificialNeuralNetwork private[mllib](\n+    topology: Array[Int],\n+    maxNumIterations: Int,\n+    convergenceTol: Double)\n+  extends Serializable {\n+\n+  private val gradient = new ANNLeastSquaresGradient(topology)\n+  private val updater = new ANNUpdater()\n+  private val optimizer = new LBFGS(gradient, updater).\n+    setConvergenceTol(convergenceTol).\n+    setMaxNumIterations(maxNumIterations)\n+\n+ /**\n+   * Trains the ANN model."
  }, {
    "author": {
      "login": "bgreeven"
    },
    "body": "Fixed\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-03T13:56:21Z",
    "diffHunk": "@@ -0,0 +1,528 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.ann\n+\n+import breeze.linalg.{DenseVector, Vector => BV, axpy => brzAxpy}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.optimization._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+/*\n+ * Implements a Artificial Neural Network (ANN)\n+ *\n+ * The data consists of an input vector and an output vector, combined into a single vector\n+ * as follows:\n+ *\n+ * [ ---input--- ---output--- ]\n+ *\n+ * NOTE: output values should be in the range [0,1]\n+ *\n+ * For a network of H hidden layers:\n+ *\n+ * hiddenLayersTopology(h) indicates the number of nodes in hidden layer h, excluding the bias\n+ * node. h counts from 0 (first hidden layer, taking inputs from input layer) to H - 1 (last\n+ * hidden layer, sending outputs to the output layer).\n+ *\n+ * hiddenLayersTopology is converted internally to topology, which adds the number of nodes\n+ * in the input and output layers.\n+ *\n+ * noInput = topology(0), the number of input nodes\n+ * noOutput = topology(L-1), the number of output nodes\n+ *\n+ * input = data( 0 to noInput-1 )\n+ * output = data( noInput to noInput + noOutput - 1 )\n+ *\n+ * W_ijl is the weight from node i in layer l-1 to node j in layer l\n+ * W_ijl goes to position ofsWeight(l) + j*(topology(l-1)+1) + i in the weights vector\n+ *\n+ * B_jl is the bias input of node j in layer l\n+ * B_jl goes to position ofsWeight(l) + j*(topology(l-1)+1) + topology(l-1) in the weights vector\n+ *\n+ * error function: E( O, Y ) = sum( O_j - Y_j )\n+ * (with O = (O_0, ..., O_(noOutput-1)) the output of the ANN,\n+ * and (Y_0, ..., Y_(noOutput-1)) the input)\n+ *\n+ * node_jl is node j in layer l\n+ * node_jl goes to position ofsNode(l) + j\n+ *\n+ * The weights gradient is defined as dE/dW_ijl and dE/dB_jl\n+ * It has same mapping as W_ijl and B_jl\n+ *\n+ * For back propagation:\n+ * delta_jl = dE/dS_jl, where S_jl the output of node_jl, but before applying the sigmoid\n+ * delta_jl has the same mapping as node_jl\n+ *\n+ * Where E = ((estOutput-output),(estOutput-output)),\n+ * the inner product of the difference between estimation and target output with itself.\n+ *\n+ */\n+\n+/**\n+ * Artificial neural network (ANN) model\n+ *\n+ * @param weights the weights between the neurons in the ANN.\n+ * @param topology array containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ */\n+class ArtificialNeuralNetworkModel private[mllib](val weights: Vector, val topology: Array[Int])\n+  extends Serializable with ANNHelper {\n+\n+  /**\n+   * Predicts values for a single data point using the trained model.\n+   *\n+   * @param testData represents a single data point.\n+   * @return prediction using the trained model.\n+   */\n+  def predict(testData: Vector): Vector = {\n+    Vectors.dense(computeValues(testData.toArray, weights.toArray))\n+  }\n+\n+  /**\n+   * Predict values for an RDD of data points using the trained model.\n+   *\n+   * @param testDataRDD RDD representing the input vectors.\n+   * @return RDD with predictions using the trained model as (input, output) pairs.\n+   */\n+  def predict(testDataRDD: RDD[Vector]): RDD[(Vector,Vector)] = {\n+    testDataRDD.map(T => (T, predict(T)) )\n+  }\n+\n+  private def computeValues(arrData: Array[Double], arrWeights: Array[Double]): Array[Double] = {\n+    val arrNodes = forwardRun(arrData, arrWeights)\n+    arrNodes.slice(arrNodes.size - topology(L), arrNodes.size)\n+  }\n+}\n+\n+/**\n+ * Performs the training of an Artificial Neural Network (ANN)\n+ *\n+ * @param topology A vector containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ * @param maxNumIterations The maximum number of iterations for the training phase.\n+ * @param convergenceTol Convergence tolerance for LBFGS. Smaller value for closer convergence.\n+ */\n+class ArtificialNeuralNetwork private[mllib](\n+    topology: Array[Int],\n+    maxNumIterations: Int,\n+    convergenceTol: Double)\n+  extends Serializable {\n+\n+  private val gradient = new ANNLeastSquaresGradient(topology)\n+  private val updater = new ANNUpdater()\n+  private val optimizer = new LBFGS(gradient, updater).\n+    setConvergenceTol(convergenceTol).\n+    setMaxNumIterations(maxNumIterations)\n+\n+ /**\n+   * Trains the ANN model."
  }],
  "prId": 1290
}, {
  "comments": [{
    "author": {
      "login": "manishamde"
    },
    "body": "I think the convergence is parameter in the constructor and you are setting it for the optimizer `setConvergenceTol(convergenceTol)`. Is this comment still valid?\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-03T03:10:11Z",
    "diffHunk": "@@ -0,0 +1,528 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.ann\n+\n+import breeze.linalg.{DenseVector, Vector => BV, axpy => brzAxpy}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.optimization._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+/*\n+ * Implements a Artificial Neural Network (ANN)\n+ *\n+ * The data consists of an input vector and an output vector, combined into a single vector\n+ * as follows:\n+ *\n+ * [ ---input--- ---output--- ]\n+ *\n+ * NOTE: output values should be in the range [0,1]\n+ *\n+ * For a network of H hidden layers:\n+ *\n+ * hiddenLayersTopology(h) indicates the number of nodes in hidden layer h, excluding the bias\n+ * node. h counts from 0 (first hidden layer, taking inputs from input layer) to H - 1 (last\n+ * hidden layer, sending outputs to the output layer).\n+ *\n+ * hiddenLayersTopology is converted internally to topology, which adds the number of nodes\n+ * in the input and output layers.\n+ *\n+ * noInput = topology(0), the number of input nodes\n+ * noOutput = topology(L-1), the number of output nodes\n+ *\n+ * input = data( 0 to noInput-1 )\n+ * output = data( noInput to noInput + noOutput - 1 )\n+ *\n+ * W_ijl is the weight from node i in layer l-1 to node j in layer l\n+ * W_ijl goes to position ofsWeight(l) + j*(topology(l-1)+1) + i in the weights vector\n+ *\n+ * B_jl is the bias input of node j in layer l\n+ * B_jl goes to position ofsWeight(l) + j*(topology(l-1)+1) + topology(l-1) in the weights vector\n+ *\n+ * error function: E( O, Y ) = sum( O_j - Y_j )\n+ * (with O = (O_0, ..., O_(noOutput-1)) the output of the ANN,\n+ * and (Y_0, ..., Y_(noOutput-1)) the input)\n+ *\n+ * node_jl is node j in layer l\n+ * node_jl goes to position ofsNode(l) + j\n+ *\n+ * The weights gradient is defined as dE/dW_ijl and dE/dB_jl\n+ * It has same mapping as W_ijl and B_jl\n+ *\n+ * For back propagation:\n+ * delta_jl = dE/dS_jl, where S_jl the output of node_jl, but before applying the sigmoid\n+ * delta_jl has the same mapping as node_jl\n+ *\n+ * Where E = ((estOutput-output),(estOutput-output)),\n+ * the inner product of the difference between estimation and target output with itself.\n+ *\n+ */\n+\n+/**\n+ * Artificial neural network (ANN) model\n+ *\n+ * @param weights the weights between the neurons in the ANN.\n+ * @param topology array containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ */\n+class ArtificialNeuralNetworkModel private[mllib](val weights: Vector, val topology: Array[Int])\n+  extends Serializable with ANNHelper {\n+\n+  /**\n+   * Predicts values for a single data point using the trained model.\n+   *\n+   * @param testData represents a single data point.\n+   * @return prediction using the trained model.\n+   */\n+  def predict(testData: Vector): Vector = {\n+    Vectors.dense(computeValues(testData.toArray, weights.toArray))\n+  }\n+\n+  /**\n+   * Predict values for an RDD of data points using the trained model.\n+   *\n+   * @param testDataRDD RDD representing the input vectors.\n+   * @return RDD with predictions using the trained model as (input, output) pairs.\n+   */\n+  def predict(testDataRDD: RDD[Vector]): RDD[(Vector,Vector)] = {\n+    testDataRDD.map(T => (T, predict(T)) )\n+  }\n+\n+  private def computeValues(arrData: Array[Double], arrWeights: Array[Double]): Array[Double] = {\n+    val arrNodes = forwardRun(arrData, arrWeights)\n+    arrNodes.slice(arrNodes.size - topology(L), arrNodes.size)\n+  }\n+}\n+\n+/**\n+ * Performs the training of an Artificial Neural Network (ANN)\n+ *\n+ * @param topology A vector containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ * @param maxNumIterations The maximum number of iterations for the training phase.\n+ * @param convergenceTol Convergence tolerance for LBFGS. Smaller value for closer convergence.\n+ */\n+class ArtificialNeuralNetwork private[mllib](\n+    topology: Array[Int],\n+    maxNumIterations: Int,\n+    convergenceTol: Double)\n+  extends Serializable {\n+\n+  private val gradient = new ANNLeastSquaresGradient(topology)\n+  private val updater = new ANNUpdater()\n+  private val optimizer = new LBFGS(gradient, updater).\n+    setConvergenceTol(convergenceTol).\n+    setMaxNumIterations(maxNumIterations)\n+\n+ /**\n+   * Trains the ANN model.\n+   * Uses default convergence tolerance 1e-4 for LBFGS."
  }, {
    "author": {
      "login": "bgreeven"
    },
    "body": "The convergenceTol is used by the LBFGS algorithm. So the convergenceTol is taken from the ArtificialNeuralNetwork header and serves as input to the LBFGS algorithm. (setConvergenceTol accesses the LBFGS class, not the ArtificialNeuralNetwork class.)\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-03T08:54:12Z",
    "diffHunk": "@@ -0,0 +1,528 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.ann\n+\n+import breeze.linalg.{DenseVector, Vector => BV, axpy => brzAxpy}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.optimization._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+/*\n+ * Implements a Artificial Neural Network (ANN)\n+ *\n+ * The data consists of an input vector and an output vector, combined into a single vector\n+ * as follows:\n+ *\n+ * [ ---input--- ---output--- ]\n+ *\n+ * NOTE: output values should be in the range [0,1]\n+ *\n+ * For a network of H hidden layers:\n+ *\n+ * hiddenLayersTopology(h) indicates the number of nodes in hidden layer h, excluding the bias\n+ * node. h counts from 0 (first hidden layer, taking inputs from input layer) to H - 1 (last\n+ * hidden layer, sending outputs to the output layer).\n+ *\n+ * hiddenLayersTopology is converted internally to topology, which adds the number of nodes\n+ * in the input and output layers.\n+ *\n+ * noInput = topology(0), the number of input nodes\n+ * noOutput = topology(L-1), the number of output nodes\n+ *\n+ * input = data( 0 to noInput-1 )\n+ * output = data( noInput to noInput + noOutput - 1 )\n+ *\n+ * W_ijl is the weight from node i in layer l-1 to node j in layer l\n+ * W_ijl goes to position ofsWeight(l) + j*(topology(l-1)+1) + i in the weights vector\n+ *\n+ * B_jl is the bias input of node j in layer l\n+ * B_jl goes to position ofsWeight(l) + j*(topology(l-1)+1) + topology(l-1) in the weights vector\n+ *\n+ * error function: E( O, Y ) = sum( O_j - Y_j )\n+ * (with O = (O_0, ..., O_(noOutput-1)) the output of the ANN,\n+ * and (Y_0, ..., Y_(noOutput-1)) the input)\n+ *\n+ * node_jl is node j in layer l\n+ * node_jl goes to position ofsNode(l) + j\n+ *\n+ * The weights gradient is defined as dE/dW_ijl and dE/dB_jl\n+ * It has same mapping as W_ijl and B_jl\n+ *\n+ * For back propagation:\n+ * delta_jl = dE/dS_jl, where S_jl the output of node_jl, but before applying the sigmoid\n+ * delta_jl has the same mapping as node_jl\n+ *\n+ * Where E = ((estOutput-output),(estOutput-output)),\n+ * the inner product of the difference between estimation and target output with itself.\n+ *\n+ */\n+\n+/**\n+ * Artificial neural network (ANN) model\n+ *\n+ * @param weights the weights between the neurons in the ANN.\n+ * @param topology array containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ */\n+class ArtificialNeuralNetworkModel private[mllib](val weights: Vector, val topology: Array[Int])\n+  extends Serializable with ANNHelper {\n+\n+  /**\n+   * Predicts values for a single data point using the trained model.\n+   *\n+   * @param testData represents a single data point.\n+   * @return prediction using the trained model.\n+   */\n+  def predict(testData: Vector): Vector = {\n+    Vectors.dense(computeValues(testData.toArray, weights.toArray))\n+  }\n+\n+  /**\n+   * Predict values for an RDD of data points using the trained model.\n+   *\n+   * @param testDataRDD RDD representing the input vectors.\n+   * @return RDD with predictions using the trained model as (input, output) pairs.\n+   */\n+  def predict(testDataRDD: RDD[Vector]): RDD[(Vector,Vector)] = {\n+    testDataRDD.map(T => (T, predict(T)) )\n+  }\n+\n+  private def computeValues(arrData: Array[Double], arrWeights: Array[Double]): Array[Double] = {\n+    val arrNodes = forwardRun(arrData, arrWeights)\n+    arrNodes.slice(arrNodes.size - topology(L), arrNodes.size)\n+  }\n+}\n+\n+/**\n+ * Performs the training of an Artificial Neural Network (ANN)\n+ *\n+ * @param topology A vector containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ * @param maxNumIterations The maximum number of iterations for the training phase.\n+ * @param convergenceTol Convergence tolerance for LBFGS. Smaller value for closer convergence.\n+ */\n+class ArtificialNeuralNetwork private[mllib](\n+    topology: Array[Int],\n+    maxNumIterations: Int,\n+    convergenceTol: Double)\n+  extends Serializable {\n+\n+  private val gradient = new ANNLeastSquaresGradient(topology)\n+  private val updater = new ANNUpdater()\n+  private val optimizer = new LBFGS(gradient, updater).\n+    setConvergenceTol(convergenceTol).\n+    setMaxNumIterations(maxNumIterations)\n+\n+ /**\n+   * Trains the ANN model.\n+   * Uses default convergence tolerance 1e-4 for LBFGS."
  }],
  "prId": 1290
}, {
  "comments": [{
    "author": {
      "login": "witgo"
    },
    "body": "We should use `GradientDescent`? \n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-19T17:04:07Z",
    "diffHunk": "@@ -0,0 +1,528 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.ann\n+\n+import breeze.linalg.{DenseVector, Vector => BV, axpy => brzAxpy}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.optimization._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+/*\n+ * Implements a Artificial Neural Network (ANN)\n+ *\n+ * The data consists of an input vector and an output vector, combined into a single vector\n+ * as follows:\n+ *\n+ * [ ---input--- ---output--- ]\n+ *\n+ * NOTE: output values should be in the range [0,1]\n+ *\n+ * For a network of H hidden layers:\n+ *\n+ * hiddenLayersTopology(h) indicates the number of nodes in hidden layer h, excluding the bias\n+ * node. h counts from 0 (first hidden layer, taking inputs from input layer) to H - 1 (last\n+ * hidden layer, sending outputs to the output layer).\n+ *\n+ * hiddenLayersTopology is converted internally to topology, which adds the number of nodes\n+ * in the input and output layers.\n+ *\n+ * noInput = topology(0), the number of input nodes\n+ * noOutput = topology(L-1), the number of output nodes\n+ *\n+ * input = data( 0 to noInput-1 )\n+ * output = data( noInput to noInput + noOutput - 1 )\n+ *\n+ * W_ijl is the weight from node i in layer l-1 to node j in layer l\n+ * W_ijl goes to position ofsWeight(l) + j*(topology(l-1)+1) + i in the weights vector\n+ *\n+ * B_jl is the bias input of node j in layer l\n+ * B_jl goes to position ofsWeight(l) + j*(topology(l-1)+1) + topology(l-1) in the weights vector\n+ *\n+ * error function: E( O, Y ) = sum( O_j - Y_j )\n+ * (with O = (O_0, ..., O_(noOutput-1)) the output of the ANN,\n+ * and (Y_0, ..., Y_(noOutput-1)) the input)\n+ *\n+ * node_jl is node j in layer l\n+ * node_jl goes to position ofsNode(l) + j\n+ *\n+ * The weights gradient is defined as dE/dW_ijl and dE/dB_jl\n+ * It has same mapping as W_ijl and B_jl\n+ *\n+ * For back propagation:\n+ * delta_jl = dE/dS_jl, where S_jl the output of node_jl, but before applying the sigmoid\n+ * delta_jl has the same mapping as node_jl\n+ *\n+ * Where E = ((estOutput-output),(estOutput-output)),\n+ * the inner product of the difference between estimation and target output with itself.\n+ *\n+ */\n+\n+/**\n+ * Artificial neural network (ANN) model\n+ *\n+ * @param weights the weights between the neurons in the ANN.\n+ * @param topology array containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ */\n+class ArtificialNeuralNetworkModel private[mllib](val weights: Vector, val topology: Array[Int])\n+  extends Serializable with ANNHelper {\n+\n+  /**\n+   * Predicts values for a single data point using the trained model.\n+   *\n+   * @param testData represents a single data point.\n+   * @return prediction using the trained model.\n+   */\n+  def predict(testData: Vector): Vector = {\n+    Vectors.dense(computeValues(testData.toArray, weights.toArray))\n+  }\n+\n+  /**\n+   * Predict values for an RDD of data points using the trained model.\n+   *\n+   * @param testDataRDD RDD representing the input vectors.\n+   * @return RDD with predictions using the trained model as (input, output) pairs.\n+   */\n+  def predict(testDataRDD: RDD[Vector]): RDD[(Vector,Vector)] = {\n+    testDataRDD.map(T => (T, predict(T)) )\n+  }\n+\n+  private def computeValues(arrData: Array[Double], arrWeights: Array[Double]): Array[Double] = {\n+    val arrNodes = forwardRun(arrData, arrWeights)\n+    arrNodes.slice(arrNodes.size - topology(L), arrNodes.size)\n+  }\n+}\n+\n+/**\n+ * Performs the training of an Artificial Neural Network (ANN)\n+ *\n+ * @param topology A vector containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ * @param maxNumIterations The maximum number of iterations for the training phase.\n+ * @param convergenceTol Convergence tolerance for LBFGS. Smaller value for closer convergence.\n+ */\n+class ArtificialNeuralNetwork private[mllib](\n+    topology: Array[Int],\n+    maxNumIterations: Int,\n+    convergenceTol: Double)\n+  extends Serializable {\n+\n+  private val gradient = new ANNLeastSquaresGradient(topology)\n+  private val updater = new ANNUpdater()\n+  private val optimizer = new LBFGS(gradient, updater)."
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "I suspect LBFGS will converge faster.\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-19T17:19:27Z",
    "diffHunk": "@@ -0,0 +1,528 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.ann\n+\n+import breeze.linalg.{DenseVector, Vector => BV, axpy => brzAxpy}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.optimization._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+/*\n+ * Implements a Artificial Neural Network (ANN)\n+ *\n+ * The data consists of an input vector and an output vector, combined into a single vector\n+ * as follows:\n+ *\n+ * [ ---input--- ---output--- ]\n+ *\n+ * NOTE: output values should be in the range [0,1]\n+ *\n+ * For a network of H hidden layers:\n+ *\n+ * hiddenLayersTopology(h) indicates the number of nodes in hidden layer h, excluding the bias\n+ * node. h counts from 0 (first hidden layer, taking inputs from input layer) to H - 1 (last\n+ * hidden layer, sending outputs to the output layer).\n+ *\n+ * hiddenLayersTopology is converted internally to topology, which adds the number of nodes\n+ * in the input and output layers.\n+ *\n+ * noInput = topology(0), the number of input nodes\n+ * noOutput = topology(L-1), the number of output nodes\n+ *\n+ * input = data( 0 to noInput-1 )\n+ * output = data( noInput to noInput + noOutput - 1 )\n+ *\n+ * W_ijl is the weight from node i in layer l-1 to node j in layer l\n+ * W_ijl goes to position ofsWeight(l) + j*(topology(l-1)+1) + i in the weights vector\n+ *\n+ * B_jl is the bias input of node j in layer l\n+ * B_jl goes to position ofsWeight(l) + j*(topology(l-1)+1) + topology(l-1) in the weights vector\n+ *\n+ * error function: E( O, Y ) = sum( O_j - Y_j )\n+ * (with O = (O_0, ..., O_(noOutput-1)) the output of the ANN,\n+ * and (Y_0, ..., Y_(noOutput-1)) the input)\n+ *\n+ * node_jl is node j in layer l\n+ * node_jl goes to position ofsNode(l) + j\n+ *\n+ * The weights gradient is defined as dE/dW_ijl and dE/dB_jl\n+ * It has same mapping as W_ijl and B_jl\n+ *\n+ * For back propagation:\n+ * delta_jl = dE/dS_jl, where S_jl the output of node_jl, but before applying the sigmoid\n+ * delta_jl has the same mapping as node_jl\n+ *\n+ * Where E = ((estOutput-output),(estOutput-output)),\n+ * the inner product of the difference between estimation and target output with itself.\n+ *\n+ */\n+\n+/**\n+ * Artificial neural network (ANN) model\n+ *\n+ * @param weights the weights between the neurons in the ANN.\n+ * @param topology array containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ */\n+class ArtificialNeuralNetworkModel private[mllib](val weights: Vector, val topology: Array[Int])\n+  extends Serializable with ANNHelper {\n+\n+  /**\n+   * Predicts values for a single data point using the trained model.\n+   *\n+   * @param testData represents a single data point.\n+   * @return prediction using the trained model.\n+   */\n+  def predict(testData: Vector): Vector = {\n+    Vectors.dense(computeValues(testData.toArray, weights.toArray))\n+  }\n+\n+  /**\n+   * Predict values for an RDD of data points using the trained model.\n+   *\n+   * @param testDataRDD RDD representing the input vectors.\n+   * @return RDD with predictions using the trained model as (input, output) pairs.\n+   */\n+  def predict(testDataRDD: RDD[Vector]): RDD[(Vector,Vector)] = {\n+    testDataRDD.map(T => (T, predict(T)) )\n+  }\n+\n+  private def computeValues(arrData: Array[Double], arrWeights: Array[Double]): Array[Double] = {\n+    val arrNodes = forwardRun(arrData, arrWeights)\n+    arrNodes.slice(arrNodes.size - topology(L), arrNodes.size)\n+  }\n+}\n+\n+/**\n+ * Performs the training of an Artificial Neural Network (ANN)\n+ *\n+ * @param topology A vector containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ * @param maxNumIterations The maximum number of iterations for the training phase.\n+ * @param convergenceTol Convergence tolerance for LBFGS. Smaller value for closer convergence.\n+ */\n+class ArtificialNeuralNetwork private[mllib](\n+    topology: Array[Int],\n+    maxNumIterations: Int,\n+    convergenceTol: Double)\n+  extends Serializable {\n+\n+  private val gradient = new ANNLeastSquaresGradient(topology)\n+  private val updater = new ANNUpdater()\n+  private val optimizer = new LBFGS(gradient, updater)."
  }, {
    "author": {
      "login": "bgreeven"
    },
    "body": "@witgo @jkbradley: We started out using GradientDescent, and replaced it with LBFGS later. Indeed LBFGS is much faster, however, as a result it also shows earlier overfitting.\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-20T00:41:28Z",
    "diffHunk": "@@ -0,0 +1,528 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.ann\n+\n+import breeze.linalg.{DenseVector, Vector => BV, axpy => brzAxpy}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.optimization._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+/*\n+ * Implements a Artificial Neural Network (ANN)\n+ *\n+ * The data consists of an input vector and an output vector, combined into a single vector\n+ * as follows:\n+ *\n+ * [ ---input--- ---output--- ]\n+ *\n+ * NOTE: output values should be in the range [0,1]\n+ *\n+ * For a network of H hidden layers:\n+ *\n+ * hiddenLayersTopology(h) indicates the number of nodes in hidden layer h, excluding the bias\n+ * node. h counts from 0 (first hidden layer, taking inputs from input layer) to H - 1 (last\n+ * hidden layer, sending outputs to the output layer).\n+ *\n+ * hiddenLayersTopology is converted internally to topology, which adds the number of nodes\n+ * in the input and output layers.\n+ *\n+ * noInput = topology(0), the number of input nodes\n+ * noOutput = topology(L-1), the number of output nodes\n+ *\n+ * input = data( 0 to noInput-1 )\n+ * output = data( noInput to noInput + noOutput - 1 )\n+ *\n+ * W_ijl is the weight from node i in layer l-1 to node j in layer l\n+ * W_ijl goes to position ofsWeight(l) + j*(topology(l-1)+1) + i in the weights vector\n+ *\n+ * B_jl is the bias input of node j in layer l\n+ * B_jl goes to position ofsWeight(l) + j*(topology(l-1)+1) + topology(l-1) in the weights vector\n+ *\n+ * error function: E( O, Y ) = sum( O_j - Y_j )\n+ * (with O = (O_0, ..., O_(noOutput-1)) the output of the ANN,\n+ * and (Y_0, ..., Y_(noOutput-1)) the input)\n+ *\n+ * node_jl is node j in layer l\n+ * node_jl goes to position ofsNode(l) + j\n+ *\n+ * The weights gradient is defined as dE/dW_ijl and dE/dB_jl\n+ * It has same mapping as W_ijl and B_jl\n+ *\n+ * For back propagation:\n+ * delta_jl = dE/dS_jl, where S_jl the output of node_jl, but before applying the sigmoid\n+ * delta_jl has the same mapping as node_jl\n+ *\n+ * Where E = ((estOutput-output),(estOutput-output)),\n+ * the inner product of the difference between estimation and target output with itself.\n+ *\n+ */\n+\n+/**\n+ * Artificial neural network (ANN) model\n+ *\n+ * @param weights the weights between the neurons in the ANN.\n+ * @param topology array containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ */\n+class ArtificialNeuralNetworkModel private[mllib](val weights: Vector, val topology: Array[Int])\n+  extends Serializable with ANNHelper {\n+\n+  /**\n+   * Predicts values for a single data point using the trained model.\n+   *\n+   * @param testData represents a single data point.\n+   * @return prediction using the trained model.\n+   */\n+  def predict(testData: Vector): Vector = {\n+    Vectors.dense(computeValues(testData.toArray, weights.toArray))\n+  }\n+\n+  /**\n+   * Predict values for an RDD of data points using the trained model.\n+   *\n+   * @param testDataRDD RDD representing the input vectors.\n+   * @return RDD with predictions using the trained model as (input, output) pairs.\n+   */\n+  def predict(testDataRDD: RDD[Vector]): RDD[(Vector,Vector)] = {\n+    testDataRDD.map(T => (T, predict(T)) )\n+  }\n+\n+  private def computeValues(arrData: Array[Double], arrWeights: Array[Double]): Array[Double] = {\n+    val arrNodes = forwardRun(arrData, arrWeights)\n+    arrNodes.slice(arrNodes.size - topology(L), arrNodes.size)\n+  }\n+}\n+\n+/**\n+ * Performs the training of an Artificial Neural Network (ANN)\n+ *\n+ * @param topology A vector containing the number of nodes per layer in the network, including\n+ * the nodes in the input and output layer, but excluding the bias nodes.\n+ * @param maxNumIterations The maximum number of iterations for the training phase.\n+ * @param convergenceTol Convergence tolerance for LBFGS. Smaller value for closer convergence.\n+ */\n+class ArtificialNeuralNetwork private[mllib](\n+    topology: Array[Int],\n+    maxNumIterations: Int,\n+    convergenceTol: Double)\n+  extends Serializable {\n+\n+  private val gradient = new ANNLeastSquaresGradient(topology)\n+  private val updater = new ANNUpdater()\n+  private val optimizer = new LBFGS(gradient, updater)."
  }],
  "prId": 1290
}]