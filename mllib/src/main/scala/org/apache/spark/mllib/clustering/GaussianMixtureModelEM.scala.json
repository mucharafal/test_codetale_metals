[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "no semicolon\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-15T21:49:37Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map( u => u.toBreeze.toDenseVector ).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // gaussians will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var gaussians = (0 until k).map{ i => (1.0 / k, \n+                                  vectorMean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  initCovariance(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                                  }.toArray\n+    \n+    val accW     = new Array[Accumulator[Double]](k)\n+    val accMu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val accSigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // reset accumulators\n+      for (i <- 0 until k) {\n+        accW(i)     = ctx.accumulator(0.0)\n+        accMu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        accSigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val logLikelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map{ i => \n+                                  new MultivariateGaussian(gaussians(i)._2, gaussians(i)._3)\n+                                }.toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => gaussians(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  \n+        val p = (0 until k).map{ i => \n+                  eps + weights.value(i) * dists.value(i).pdf(x)\n+                }.toArray\n+        \n+        val pSum = p.sum \n+        \n+        logLikelihood += math.log(pSum)  \n+          \n+        // accumulate weighted sums  \n+        val xxt = x * new Transpose(x)\n+        for (i <- 0 until k) {\n+          p(i) /= pSum\n+          accW(i) += p(i)\n+          accMu(i) += x * p(i)\n+          accSigma(i) += xxt * p(i)\n+        }\n+      })\n+      \n+      // Collect the computed sums\n+      val W = (0 until k).map(i => accW(i).value).toArray\n+      val MU = (0 until k).map(i => accMu(i).value).toArray\n+      val SIGMA = (0 until k).map(i => accSigma(i).value).toArray\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      gaussians = (0 until k).map{ i => {\n+            val weight = W(i) / W.sum\n+            val mu = MU(i) / W(i)\n+            val sigma = SIGMA(i) / W(i) - mu * new Transpose(mu)\n+            (weight, mu, sigma)\n+          }\n+        }.toArray\n+      \n+      llhp = llh; // current becomes previous"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "I would format as:\n\n```\nvar gaussians = (0 until k).map{ i =>\n  (1.0 / k, \n    vectorMean(samples.slice(i * nSamples, (i + 1) * nSamples)),\n    initCovariance(samples.slice(i * nSamples, (i + 1) * nSamples)))\n}.toArray\n```\n\n(indentation + ending the first line with \"=>\")\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-15T22:01:04Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map( u => u.toBreeze.toDenseVector ).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // gaussians will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var gaussians = (0 until k).map{ i => (1.0 / k, "
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "indentation (as above)\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-15T22:01:06Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map( u => u.toBreeze.toDenseVector ).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // gaussians will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var gaussians = (0 until k).map{ i => (1.0 / k, \n+                                  vectorMean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  initCovariance(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                                  }.toArray\n+    \n+    val accW     = new Array[Accumulator[Double]](k)\n+    val accMu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val accSigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // reset accumulators\n+      for (i <- 0 until k) {\n+        accW(i)     = ctx.accumulator(0.0)\n+        accMu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        accSigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val logLikelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map{ i => \n+                                  new MultivariateGaussian(gaussians(i)._2, gaussians(i)._3)"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Use \"{\" instead of \"(\" for multi-line foreach/map/etc. calls:\n\n```\nbreezeData.foreach { x =>\n```\n\nAlso, no need for \"{\" to wrap block on right-hand side of \"=>\"\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-15T22:01:09Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map( u => u.toBreeze.toDenseVector ).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // gaussians will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var gaussians = (0 until k).map{ i => (1.0 / k, \n+                                  vectorMean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  initCovariance(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                                  }.toArray\n+    \n+    val accW     = new Array[Accumulator[Double]](k)\n+    val accMu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val accSigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // reset accumulators\n+      for (i <- 0 until k) {\n+        accW(i)     = ctx.accumulator(0.0)\n+        accMu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        accSigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val logLikelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map{ i => \n+                                  new MultivariateGaussian(gaussians(i)._2, gaussians(i)._3)\n+                                }.toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => gaussians(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  "
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "No need for second \"{\" in this line\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-15T22:01:11Z",
    "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map( u => u.toBreeze.toDenseVector ).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // gaussians will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var gaussians = (0 until k).map{ i => (1.0 / k, \n+                                  vectorMean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  initCovariance(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                                  }.toArray\n+    \n+    val accW     = new Array[Accumulator[Double]](k)\n+    val accMu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val accSigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // reset accumulators\n+      for (i <- 0 until k) {\n+        accW(i)     = ctx.accumulator(0.0)\n+        accMu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        accSigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val logLikelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map{ i => \n+                                  new MultivariateGaussian(gaussians(i)._2, gaussians(i)._3)\n+                                }.toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => gaussians(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  \n+        val p = (0 until k).map{ i => \n+                  eps + weights.value(i) * dists.value(i).pdf(x)\n+                }.toArray\n+        \n+        val pSum = p.sum \n+        \n+        logLikelihood += math.log(pSum)  \n+          \n+        // accumulate weighted sums  \n+        val xxt = x * new Transpose(x)\n+        for (i <- 0 until k) {\n+          p(i) /= pSum\n+          accW(i) += p(i)\n+          accMu(i) += x * p(i)\n+          accSigma(i) += xxt * p(i)\n+        }\n+      })\n+      \n+      // Collect the computed sums\n+      val W = (0 until k).map(i => accW(i).value).toArray\n+      val MU = (0 until k).map(i => accMu(i).value).toArray\n+      val SIGMA = (0 until k).map(i => accSigma(i).value).toArray\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      gaussians = (0 until k).map{ i => {"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "It is more common to write:\n\n```\nval p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n```\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-18T00:22:36Z",
    "diffHunk": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Matrix, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(weights: Array[Double], dists: Array[MultivariateGaussian])\n+      (model: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = model._2.length\n+    val p = (0 until k).map(i => eps + weights(i) * dists(i).pdf(x)).toArray"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "The documentation should tell users to set K _before_ setting initialGmm.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-18T00:22:37Z",
    "diffHunk": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Matrix, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(weights: Array[Double], dists: Array[MultivariateGaussian])\n+      (model: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = model._2.length\n+    val p = (0 until k).map(i => eps + weights(i) * dists(i).pdf(x)).toArray\n+    val pSum = p.sum\n+    model._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    for (i <- 0 until k) {\n+      p(i) /= pSum\n+      model._2(i) += p(i)\n+      model._3(i) += x * p(i)\n+      model._4(i) += xxt * p(i)\n+    }\n+    model\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization */"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "It's more common to use `sc` than `ctx`\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-18T00:22:39Z",
    "diffHunk": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Matrix, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(weights: Array[Double], dists: Array[MultivariateGaussian])\n+      (model: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = model._2.length\n+    val p = (0 until k).map(i => eps + weights(i) * dists(i).pdf(x)).toArray\n+    val pSum = p.sum\n+    model._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    for (i <- 0 until k) {\n+      p(i) /= pSum\n+      model._2(i) += p(i)\n+      model._3(i) += x * p(i)\n+      model._4(i) += xxt * p(i)\n+    }\n+    model\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialiGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "This code could be made easier to read by using an array of weights + an array of MultivariateGaussian instances, rather than a tuple.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-18T00:22:40Z",
    "diffHunk": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Matrix, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(weights: Array[Double], dists: Array[MultivariateGaussian])\n+      (model: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = model._2.length\n+    val p = (0 until k).map(i => eps + weights(i) * dists(i).pdf(x)).toArray\n+    val pSum = p.sum\n+    model._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    for (i <- 0 until k) {\n+      p(i) /= pSum\n+      model._2(i) += p(i)\n+      model._3(i) += x * p(i)\n+      model._4(i) += xxt * p(i)\n+    }\n+    model\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialiGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // gaussians will be array of (weight, mean, covariance) tuples.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var gaussians = initialGmm match {"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Use a temp value to store the slice so it is not computed twice.  Also, I believe using `samples.view.slice` will be more efficient since it avoids creating an explicit copy of the data.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-18T00:22:41Z",
    "diffHunk": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Matrix, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(weights: Array[Double], dists: Array[MultivariateGaussian])\n+      (model: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = model._2.length\n+    val p = (0 until k).map(i => eps + weights(i) * dists(i).pdf(x)).toArray\n+    val pSum = p.sum\n+    model._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    for (i <- 0 until k) {\n+      p(i) /= pSum\n+      model._2(i) += p(i)\n+      model._3(i) += x * p(i)\n+      model._4(i) += xxt * p(i)\n+    }\n+    model\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialiGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // gaussians will be array of (weight, mean, covariance) tuples.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var gaussians = initialGmm match {\n+      case Some(gmm) => (0 until k).map{ i =>\n+        (gmm.weight(i), gmm.mu(i).toBreeze.toDenseVector, gmm.sigma(i).toBreeze.toDenseMatrix)\n+      }.toArray\n+      \n+      case None => {\n+        // For each Gaussian, we will initialize the mean as the average\n+        // of some random samples from the data\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+          \n+        (0 until k).map{ i => \n+          (1.0 / k, \n+            vectorMean(samples.slice(i * nSamples, (i + 1) * nSamples)), "
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "For the record, you can simplify this:\n\n```\nval weights = gaussians.map(_._1)\n```\n\n(but my other comments may obviate the need for this change)\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-18T00:22:43Z",
    "diffHunk": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Matrix, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(weights: Array[Double], dists: Array[MultivariateGaussian])\n+      (model: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = model._2.length\n+    val p = (0 until k).map(i => eps + weights(i) * dists(i).pdf(x)).toArray\n+    val pSum = p.sum\n+    model._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    for (i <- 0 until k) {\n+      p(i) /= pSum\n+      model._2(i) += p(i)\n+      model._3(i) += x * p(i)\n+      model._4(i) += xxt * p(i)\n+    }\n+    model\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialiGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // gaussians will be array of (weight, mean, covariance) tuples.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var gaussians = initialGmm match {\n+      case Some(gmm) => (0 until k).map{ i =>\n+        (gmm.weight(i), gmm.mu(i).toBreeze.toDenseVector, gmm.sigma(i).toBreeze.toDenseMatrix)\n+      }.toArray\n+      \n+      case None => {\n+        // For each Gaussian, we will initialize the mean as the average\n+        // of some random samples from the data\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+          \n+        (0 until k).map{ i => \n+          (1.0 / k, \n+            vectorMean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+            initCovariance(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+        }.toArray\n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // pivot gaussians into weight and distribution arrays \n+      val weights = (0 until k).map(i => gaussians(i)._1).toArray"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "This is brittle since it fails when the covariance matrix is not full rank.  I'd say any of these is acceptable for now:\n- Temporary fix: Check for an exception and printing a warning about the data not being full rank.\n- OK fix: Adding a little epsilon smoothing.\n- Actual fix: Do a matrix decomposition (like Cholesky) instead of a direct inversion to handle non-full rank matrices.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-18T00:22:44Z",
    "diffHunk": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Matrix, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(weights: Array[Double], dists: Array[MultivariateGaussian])\n+      (model: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = model._2.length\n+    val p = (0 until k).map(i => eps + weights(i) * dists(i).pdf(x)).toArray\n+    val pSum = p.sum\n+    model._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    for (i <- 0 until k) {\n+      p(i) /= pSum\n+      model._2(i) += p(i)\n+      model._3(i) += x * p(i)\n+      model._4(i) += xxt * p(i)\n+    }\n+    model\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialiGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // gaussians will be array of (weight, mean, covariance) tuples.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var gaussians = initialGmm match {\n+      case Some(gmm) => (0 until k).map{ i =>\n+        (gmm.weight(i), gmm.mu(i).toBreeze.toDenseVector, gmm.sigma(i).toBreeze.toDenseMatrix)\n+      }.toArray\n+      \n+      case None => {\n+        // For each Gaussian, we will initialize the mean as the average\n+        // of some random samples from the data\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+          \n+        (0 until k).map{ i => \n+          (1.0 / k, \n+            vectorMean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+            initCovariance(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+        }.toArray\n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // pivot gaussians into weight and distribution arrays \n+      val weights = (0 until k).map(i => gaussians(i)._1).toArray\n+      val dists = (0 until k).map{ i => \n+        new MultivariateGaussian(gaussians(i)._2, gaussians(i)._3)"
  }, {
    "author": {
      "login": "tgaloppo"
    },
    "body": "I had considered using pseudo inverse here for that reason (I ultimately decided this was unlikely to actually cause a problem in practice); what do you think of using pinv instead?\n\n> On Dec 17, 2014, at 7:23 PM, jkbradley notifications@github.com wrote:\n> \n> In mllib/src/main/scala/org/apache/spark/mllib/clustering/GaussianMixtureModelEM.scala:\n> \n> > -          (1.0 / k, \n> > -            vectorMean(samples.slice(i \\* nSamples, (i + 1) \\* nSamples)), \n> > -            initCovariance(samples.slice(i \\* nSamples, (i + 1) \\* nSamples)))\n> > -        }.toArray\n> > -      }\n> > -    }\n> > -   \n> > -    var llh = Double.MinValue // current log-likelihood \n> > -    var llhp = 0.0            // previous log-likelihood\n> > -   \n> > -    var iter = 0\n> > -    do {\n> > -      // pivot gaussians into weight and distribution arrays \n> > -      val weights = (0 until k).map(i => gaussians(i)._1).toArray\n> > -      val dists = (0 until k).map{ i => \n> > -        new MultivariateGaussian(gaussians(i)._2, gaussians(i)._3)\n> >   This is brittle since it fails when the covariance matrix is not full rank. I'd say any of these is acceptable for now:\n> \n> Temporary fix: Check for an exception and printing a warning about the data not being full rank.\n> OK fix: Adding a little epsilon smoothing.\n> Actual fix: Do a matrix decomposition (like Cholesky) instead of a direct inversion to handle non-full rank matrices.\n> \n> Reply to this email directly or view it on GitHub.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-18T00:54:06Z",
    "diffHunk": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Matrix, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(weights: Array[Double], dists: Array[MultivariateGaussian])\n+      (model: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = model._2.length\n+    val p = (0 until k).map(i => eps + weights(i) * dists(i).pdf(x)).toArray\n+    val pSum = p.sum\n+    model._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    for (i <- 0 until k) {\n+      p(i) /= pSum\n+      model._2(i) += p(i)\n+      model._3(i) += x * p(i)\n+      model._4(i) += xxt * p(i)\n+    }\n+    model\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialiGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // gaussians will be array of (weight, mean, covariance) tuples.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var gaussians = initialGmm match {\n+      case Some(gmm) => (0 until k).map{ i =>\n+        (gmm.weight(i), gmm.mu(i).toBreeze.toDenseVector, gmm.sigma(i).toBreeze.toDenseMatrix)\n+      }.toArray\n+      \n+      case None => {\n+        // For each Gaussian, we will initialize the mean as the average\n+        // of some random samples from the data\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+          \n+        (0 until k).map{ i => \n+          (1.0 / k, \n+            vectorMean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+            initCovariance(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+        }.toArray\n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // pivot gaussians into weight and distribution arrays \n+      val weights = (0 until k).map(i => gaussians(i)._1).toArray\n+      val dists = (0 until k).map{ i => \n+        new MultivariateGaussian(gaussians(i)._2, gaussians(i)._3)"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "I don't think the pseudoinverse is quite what we need.  Thinking more about it, I feel like some smoothing might be best.  It will not bias the estimate too much, it will make the algorithm more robust, and it should be simple to add.  What do you think?\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-18T17:53:59Z",
    "diffHunk": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Matrix, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(weights: Array[Double], dists: Array[MultivariateGaussian])\n+      (model: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = model._2.length\n+    val p = (0 until k).map(i => eps + weights(i) * dists(i).pdf(x)).toArray\n+    val pSum = p.sum\n+    model._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    for (i <- 0 until k) {\n+      p(i) /= pSum\n+      model._2(i) += p(i)\n+      model._3(i) += x * p(i)\n+      model._4(i) += xxt * p(i)\n+    }\n+    model\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialiGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // gaussians will be array of (weight, mean, covariance) tuples.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var gaussians = initialGmm match {\n+      case Some(gmm) => (0 until k).map{ i =>\n+        (gmm.weight(i), gmm.mu(i).toBreeze.toDenseVector, gmm.sigma(i).toBreeze.toDenseMatrix)\n+      }.toArray\n+      \n+      case None => {\n+        // For each Gaussian, we will initialize the mean as the average\n+        // of some random samples from the data\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+          \n+        (0 until k).map{ i => \n+          (1.0 / k, \n+            vectorMean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+            initCovariance(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+        }.toArray\n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // pivot gaussians into weight and distribution arrays \n+      val weights = (0 until k).map(i => gaussians(i)._1).toArray\n+      val dists = (0 until k).map{ i => \n+        new MultivariateGaussian(gaussians(i)._2, gaussians(i)._3)"
  }, {
    "author": {
      "login": "tgaloppo"
    },
    "body": "I can add some small conditioning along the matrix diagonal prior to the inversion... before I do, however, please test the version I committed using pinv(), as I am rather confident it solves the issue without changing the behavior when the matrix would be invertible anyway.  If you find a failure, then I'll take the smoothing route.  Sound fair?\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-18T18:12:05Z",
    "diffHunk": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Matrix, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(weights: Array[Double], dists: Array[MultivariateGaussian])\n+      (model: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = model._2.length\n+    val p = (0 until k).map(i => eps + weights(i) * dists(i).pdf(x)).toArray\n+    val pSum = p.sum\n+    model._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    for (i <- 0 until k) {\n+      p(i) /= pSum\n+      model._2(i) += p(i)\n+      model._3(i) += x * p(i)\n+      model._4(i) += xxt * p(i)\n+    }\n+    model\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialiGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // gaussians will be array of (weight, mean, covariance) tuples.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var gaussians = initialGmm match {\n+      case Some(gmm) => (0 until k).map{ i =>\n+        (gmm.weight(i), gmm.mu(i).toBreeze.toDenseVector, gmm.sigma(i).toBreeze.toDenseMatrix)\n+      }.toArray\n+      \n+      case None => {\n+        // For each Gaussian, we will initialize the mean as the average\n+        // of some random samples from the data\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+          \n+        (0 until k).map{ i => \n+          (1.0 / k, \n+            vectorMean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+            initCovariance(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+        }.toArray\n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // pivot gaussians into weight and distribution arrays \n+      val weights = (0 until k).map(i => gaussians(i)._1).toArray\n+      val dists = (0 until k).map{ i => \n+        new MultivariateGaussian(gaussians(i)._2, gaussians(i)._3)"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Should only compute W.sum once, not every iteration.\nAfter that change, it may work well to avoid defining W, MU, SIGMA, and instead calling `sums.map ...`.  With that change, you will not need the `(i)` indices within this loop.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-18T00:22:45Z",
    "diffHunk": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Matrix, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(weights: Array[Double], dists: Array[MultivariateGaussian])\n+      (model: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = model._2.length\n+    val p = (0 until k).map(i => eps + weights(i) * dists(i).pdf(x)).toArray\n+    val pSum = p.sum\n+    model._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    for (i <- 0 until k) {\n+      p(i) /= pSum\n+      model._2(i) += p(i)\n+      model._3(i) += x * p(i)\n+      model._4(i) += xxt * p(i)\n+    }\n+    model\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialiGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // gaussians will be array of (weight, mean, covariance) tuples.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var gaussians = initialGmm match {\n+      case Some(gmm) => (0 until k).map{ i =>\n+        (gmm.weight(i), gmm.mu(i).toBreeze.toDenseVector, gmm.sigma(i).toBreeze.toDenseMatrix)\n+      }.toArray\n+      \n+      case None => {\n+        // For each Gaussian, we will initialize the mean as the average\n+        // of some random samples from the data\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+          \n+        (0 until k).map{ i => \n+          (1.0 / k, \n+            vectorMean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+            initCovariance(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+        }.toArray\n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // pivot gaussians into weight and distribution arrays \n+      val weights = (0 until k).map(i => gaussians(i)._1).toArray\n+      val dists = (0 until k).map{ i => \n+        new MultivariateGaussian(gaussians(i)._2, gaussians(i)._3)\n+      }.toArray\n+      \n+      // create and broadcast curried cluster contribution function\n+      val compute = ctx.broadcast(computeExpectation(weights, dists)_)\n+      \n+      // aggregate the cluster contribution for all sample points\n+      val sums = breezeData.aggregate(zeroExpectationSum(k, d))(compute.value, addExpectationSums)\n+      \n+      // Assignments to make the code more readable\n+      val logLikelihood = sums._1(0)\n+      val W = sums._2\n+      val MU = sums._3\n+      val SIGMA = sums._4\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      gaussians = (0 until k).map{ i => \n+        val weight = W(i) / W.sum"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Scala style (1 argument per line for a method declaration which won't fit on 1 line)\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-18T00:22:47Z",
    "diffHunk": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Matrix, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(weights: Array[Double], dists: Array[MultivariateGaussian])\n+      (model: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = model._2.length\n+    val p = (0 until k).map(i => eps + weights(i) * dists(i).pdf(x)).toArray\n+    val pSum = p.sum\n+    model._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    for (i <- 0 until k) {\n+      p(i) /= pSum\n+      model._2(i) += p(i)\n+      model._3(i) += x * p(i)\n+      model._4(i) += xxt * p(i)\n+    }\n+    model\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialiGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // gaussians will be array of (weight, mean, covariance) tuples.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var gaussians = initialGmm match {\n+      case Some(gmm) => (0 until k).map{ i =>\n+        (gmm.weight(i), gmm.mu(i).toBreeze.toDenseVector, gmm.sigma(i).toBreeze.toDenseMatrix)\n+      }.toArray\n+      \n+      case None => {\n+        // For each Gaussian, we will initialize the mean as the average\n+        // of some random samples from the data\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+          \n+        (0 until k).map{ i => \n+          (1.0 / k, \n+            vectorMean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+            initCovariance(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+        }.toArray\n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // pivot gaussians into weight and distribution arrays \n+      val weights = (0 until k).map(i => gaussians(i)._1).toArray\n+      val dists = (0 until k).map{ i => \n+        new MultivariateGaussian(gaussians(i)._2, gaussians(i)._3)\n+      }.toArray\n+      \n+      // create and broadcast curried cluster contribution function\n+      val compute = ctx.broadcast(computeExpectation(weights, dists)_)\n+      \n+      // aggregate the cluster contribution for all sample points\n+      val sums = breezeData.aggregate(zeroExpectationSum(k, d))(compute.value, addExpectationSums)\n+      \n+      // Assignments to make the code more readable\n+      val logLikelihood = sums._1(0)\n+      val W = sums._2\n+      val MU = sums._3\n+      val SIGMA = sums._4\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      gaussians = (0 until k).map{ i => \n+        val weight = W(i) / W.sum\n+        val mu = MU(i) / W(i)\n+        val sigma = SIGMA(i) / W(i) - mu * new Transpose(mu)\n+        (weight, mu, sigma)\n+      }.toArray\n+      \n+      llhp = llh // current becomes previous\n+      llh = logLikelihood // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > convergenceTol)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val weights = (0 until k).map(i => gaussians(i)._1).toArray\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(gaussians(i)._2)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(gaussians(i)._3)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+    \n+  /** Average of dense breeze vectors */\n+  private def vectorMean(x: Array[DenseDoubleVector]): DenseDoubleVector = {\n+    val v = BreezeVector.zeros[Double](x(0).length)\n+    x.foreach(xi => v += xi)\n+    v / x.length.asInstanceOf[Double] \n+  }\n+  \n+  /**\n+   * Construct matrix where diagonal entries are element-wise\n+   * variance of input vectors (computes biased variance)\n+   */\n+  private def initCovariance(x: Array[DenseDoubleVector]): DenseDoubleMatrix = {\n+    val mu = vectorMean(x)\n+    val ss = BreezeVector.zeros[Double](x(0).length)\n+    val cov = BreezeMatrix.eye[Double](ss.length)\n+    x.map(xi => (xi - mu) :^ 2.0).foreach(u => ss += u)\n+    (0 until ss.length).foreach(i => cov(i,i) = ss(i) / x.length)\n+    cov\n+  }\n+  \n+  /**\n+   * Given the input vectors, return the membership value of each vector\n+   * to all mixture components. \n+   */\n+  def predictClusters(points: RDD[Vector], mu: Array[Vector], sigma: Array[Matrix],"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Scala style again.\nAlso, can this be private?\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-18T00:22:49Z",
    "diffHunk": "@@ -0,0 +1,284 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Matrix, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(weights: Array[Double], dists: Array[MultivariateGaussian])\n+      (model: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = model._2.length\n+    val p = (0 until k).map(i => eps + weights(i) * dists(i).pdf(x)).toArray\n+    val pSum = p.sum\n+    model._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    for (i <- 0 until k) {\n+      p(i) /= pSum\n+      model._2(i) += p(i)\n+      model._3(i) += x * p(i)\n+      model._4(i) += xxt * p(i)\n+    }\n+    model\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialiGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // gaussians will be array of (weight, mean, covariance) tuples.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var gaussians = initialGmm match {\n+      case Some(gmm) => (0 until k).map{ i =>\n+        (gmm.weight(i), gmm.mu(i).toBreeze.toDenseVector, gmm.sigma(i).toBreeze.toDenseMatrix)\n+      }.toArray\n+      \n+      case None => {\n+        // For each Gaussian, we will initialize the mean as the average\n+        // of some random samples from the data\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+          \n+        (0 until k).map{ i => \n+          (1.0 / k, \n+            vectorMean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+            initCovariance(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+        }.toArray\n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // pivot gaussians into weight and distribution arrays \n+      val weights = (0 until k).map(i => gaussians(i)._1).toArray\n+      val dists = (0 until k).map{ i => \n+        new MultivariateGaussian(gaussians(i)._2, gaussians(i)._3)\n+      }.toArray\n+      \n+      // create and broadcast curried cluster contribution function\n+      val compute = ctx.broadcast(computeExpectation(weights, dists)_)\n+      \n+      // aggregate the cluster contribution for all sample points\n+      val sums = breezeData.aggregate(zeroExpectationSum(k, d))(compute.value, addExpectationSums)\n+      \n+      // Assignments to make the code more readable\n+      val logLikelihood = sums._1(0)\n+      val W = sums._2\n+      val MU = sums._3\n+      val SIGMA = sums._4\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      gaussians = (0 until k).map{ i => \n+        val weight = W(i) / W.sum\n+        val mu = MU(i) / W(i)\n+        val sigma = SIGMA(i) / W(i) - mu * new Transpose(mu)\n+        (weight, mu, sigma)\n+      }.toArray\n+      \n+      llhp = llh // current becomes previous\n+      llh = logLikelihood // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > convergenceTol)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val weights = (0 until k).map(i => gaussians(i)._1).toArray\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(gaussians(i)._2)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(gaussians(i)._3)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+    \n+  /** Average of dense breeze vectors */\n+  private def vectorMean(x: Array[DenseDoubleVector]): DenseDoubleVector = {\n+    val v = BreezeVector.zeros[Double](x(0).length)\n+    x.foreach(xi => v += xi)\n+    v / x.length.asInstanceOf[Double] \n+  }\n+  \n+  /**\n+   * Construct matrix where diagonal entries are element-wise\n+   * variance of input vectors (computes biased variance)\n+   */\n+  private def initCovariance(x: Array[DenseDoubleVector]): DenseDoubleMatrix = {\n+    val mu = vectorMean(x)\n+    val ss = BreezeVector.zeros[Double](x(0).length)\n+    val cov = BreezeMatrix.eye[Double](ss.length)\n+    x.map(xi => (xi - mu) :^ 2.0).foreach(u => ss += u)\n+    (0 until ss.length).foreach(i => cov(i,i) = ss(i) / x.length)\n+    cov\n+  }\n+  \n+  /**\n+   * Given the input vectors, return the membership value of each vector\n+   * to all mixture components. \n+   */\n+  def predictClusters(points: RDD[Vector], mu: Array[Vector], sigma: Array[Matrix],\n+      weight: Array[Double], k: Int): RDD[Array[Double]] = {\n+    val ctx = points.sparkContext\n+    val dists = ctx.broadcast{\n+      (0 until k).map{ i => \n+        new MultivariateGaussian(mu(i).toBreeze.toDenseVector, sigma(i).toBreeze.toDenseMatrix)\n+      }.toArray\n+    }\n+    val weights = ctx.broadcast((0 until k).map(i => weight(i)).toArray)\n+    points.map{ x => \n+      computeSoftAssignments(x.toBreeze.toDenseVector, dists.value, weights.value, k)\n+    }\n+  }\n+  \n+  /**\n+   * Compute the partial assignments for each vector\n+   */\n+  def computeSoftAssignments(pt: DenseDoubleVector, dists: Array[MultivariateGaussian],"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "This could be a Double instead of an Array.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T00:50:44Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), "
  }, {
    "author": {
      "login": "tgaloppo"
    },
    "body": "Lol. Actually, this is a hack because just putting a Double here does not allow an in-place update of the tuple; I would have to create a new tuple out of the sum arrays and the combined double value. I know it is kind of ugly, but I suspect it is more performant than creating a new tuple each time.  The alternative would be to make ExpectationSum an actual class...\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T01:00:47Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), "
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "In Scala, for loops are slower than while loops.  Since this is an inner loop, it may be worthwhile to change it to a while loop.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T00:50:46Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Ditto here (while instead of for loop)\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T00:50:49Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    for (i <- 0 until k) {"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "typo: \"getInitialiGmm\" --> \"getInitialGmm\"\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T00:50:51Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    for (i <- 0 until k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)\n+    }\n+    sums\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization.\n+   *  You must call setK() prior to calling this method, and the condition\n+   *  (gmm.k == this.k) must be met; failure will result in an IllegalArgumentException\n+   */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialiGmm: Option[GaussianMixtureModel] = initialGmm"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "This could be cleaner: Instead of\n\n```\n(0 until k).map(_ => 1.0 / k).toArray\n```\n\nyou can write\n\n```\nArray.fill[Double](k)(1.0 / k)\n```\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T00:50:54Z",
    "diffHunk": "@@ -0,0 +1,244 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    for (i <- 0 until m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    for (i <- 0 until k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)\n+    }\n+    sums\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization.\n+   *  You must call setK() prior to calling this method, and the condition\n+   *  (gmm.k == this.k) must be met; failure will result in an IllegalArgumentException\n+   */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialiGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val sc = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // Determine initial weights and corresponding Gaussians.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples    \n+    val (weights, gaussians) = initialGmm match {\n+      case Some(gmm) => (gmm.weight, gmm.mu.zip(gmm.sigma).map{ case(mu, sigma) => \n+        new MultivariateGaussian(mu.toBreeze.toDenseVector, sigma.toBreeze.toDenseMatrix) \n+      }.toArray)\n+      \n+      case None => {\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+        ((0 until k).map(_ => 1.0 / k).toArray, (0 until k).map{ i => "
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "organize imports into groups: \n\nhttps://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide#SparkCodeStyleGuide-Imports\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:37:10Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "minor: GMM's name contains `Model`, which is a little confusing: `GaussianMixtureModelEM` produces `GaussianMixtureModel`. I don't have good suggestions. Maybe we could rename `GaussianMixtureModelEM` to `GaussianMixtureEM`.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:37:12Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private ("
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "+1 for renaming.  No great solution here, but GaussianMixtureEM sounds OK to me.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-22T19:32:18Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private ("
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Does this alias simplify any code?\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:37:14Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`IndexedSeq` should be sufficient.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:37:24Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "The code could be more readable if we define `ExpectionSum` as a private class with `var loglik`, `weights`, `mean`, and `cov`, then implement `add` method. As a result, we don't use `._1`, `._2`, .... in the code.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:37:25Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = ("
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`Array.fill(k)(BDV.zeros[Double](d))`\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:37:29Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "See my previous comments about `ExpectationSum`.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:37:31Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "minor: The implementation in this block allocates unnecessary temp memory. For example, this is a rank-1 update. Computing `xxt` allocates unnecessary memory. We can use `BLAS.dsyr` instead. Another optimization we can do is packing the covariance matrix into upper-triangular form. It is not necessary to do those optimizations in this PR. Could you leave a TODO?\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:37:33Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    var i = 0\n+    while (i < m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+      i = i + 1\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    var i = 0\n+    while (i < k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Please move the constructor to the beginning.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:37:35Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    var i = 0\n+    while (i < m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+      i = i + 1\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    var i = 0\n+    while (i < k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)\n+      i = i + 1\n+    }\n+    sums\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "minor: `Gmm` -> `Model`? There are other algorithms where we use `setInitialModel` (if I remember correctly).\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:37:37Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    var i = 0\n+    while (i < m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+      i = i + 1\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    var i = 0\n+    while (i < k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)\n+      i = i + 1\n+    }\n+    sums\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization.\n+   *  You must call setK() prior to calling this method, and the condition\n+   *  (gmm.k == this.k) must be met; failure will result in an IllegalArgumentException\n+   */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove `[Double]`\n`(0 until k).map { i =>` -> `Array.tabulate(k) { i =>`\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:37:40Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    var i = 0\n+    while (i < m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+      i = i + 1\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    var i = 0\n+    while (i < k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)\n+      i = i + 1\n+    }\n+    sums\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization.\n+   *  You must call setK() prior to calling this method, and the condition\n+   *  (gmm.k == this.k) must be met; failure will result in an IllegalArgumentException\n+   */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val sc = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // Determine initial weights and corresponding Gaussians.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples    \n+    val (weights, gaussians) = initialGmm match {\n+      case Some(gmm) => (gmm.weight, gmm.mu.zip(gmm.sigma).map{ case(mu, sigma) => \n+        new MultivariateGaussian(mu.toBreeze.toDenseVector, sigma.toBreeze.toDenseMatrix) \n+      }.toArray)\n+      \n+      case None => {\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+        (Array.fill[Double](k)(1.0 / k), (0 until k).map{ i => "
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Shall we use `while () { ...` instead? We may just want to test the initialization.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:37:59Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    var i = 0\n+    while (i < m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+      i = i + 1\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    var i = 0\n+    while (i < k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)\n+      i = i + 1\n+    }\n+    sums\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization.\n+   *  You must call setK() prior to calling this method, and the condition\n+   *  (gmm.k == this.k) must be met; failure will result in an IllegalArgumentException\n+   */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val sc = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // Determine initial weights and corresponding Gaussians.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples    \n+    val (weights, gaussians) = initialGmm match {\n+      case Some(gmm) => (gmm.weight, gmm.mu.zip(gmm.sigma).map{ case(mu, sigma) => \n+        new MultivariateGaussian(mu.toBreeze.toDenseVector, sigma.toBreeze.toDenseMatrix) \n+      }.toArray)\n+      \n+      case None => {\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+        (Array.fill[Double](k)(1.0 / k), (0 until k).map{ i => \n+          val slice = samples.view(i * nSamples, (i + 1) * nSamples)\n+          new MultivariateGaussian(vectorMean(slice), initCovariance(slice)) \n+        }.toArray)  \n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`treeAggreate` may be better than `aggregate` here.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:38:01Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    var i = 0\n+    while (i < m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+      i = i + 1\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    var i = 0\n+    while (i < k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)\n+      i = i + 1\n+    }\n+    sums\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization.\n+   *  You must call setK() prior to calling this method, and the condition\n+   *  (gmm.k == this.k) must be met; failure will result in an IllegalArgumentException\n+   */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val sc = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // Determine initial weights and corresponding Gaussians.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples    \n+    val (weights, gaussians) = initialGmm match {\n+      case Some(gmm) => (gmm.weight, gmm.mu.zip(gmm.sigma).map{ case(mu, sigma) => \n+        new MultivariateGaussian(mu.toBreeze.toDenseVector, sigma.toBreeze.toDenseMatrix) \n+      }.toArray)\n+      \n+      case None => {\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+        (Array.fill[Double](k)(1.0 / k), (0 until k).map{ i => \n+          val slice = samples.view(i * nSamples, (i + 1) * nSamples)\n+          new MultivariateGaussian(vectorMean(slice), initCovariance(slice)) \n+        }.toArray)  \n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // create and broadcast curried cluster contribution function\n+      val compute = sc.broadcast(computeExpectation(weights, gaussians)_)\n+      \n+      // aggregate the cluster contribution for all sample points\n+      val (logLikelihood, wSums, muSums, sigmaSums) = \n+        breezeData.aggregate(zeroExpectationSum(k, d))(compute.value, addExpectationSums)"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Use `while` instead of `for`.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:38:02Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    var i = 0\n+    while (i < m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+      i = i + 1\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    var i = 0\n+    while (i < k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)\n+      i = i + 1\n+    }\n+    sums\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization.\n+   *  You must call setK() prior to calling this method, and the condition\n+   *  (gmm.k == this.k) must be met; failure will result in an IllegalArgumentException\n+   */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val sc = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // Determine initial weights and corresponding Gaussians.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples    \n+    val (weights, gaussians) = initialGmm match {\n+      case Some(gmm) => (gmm.weight, gmm.mu.zip(gmm.sigma).map{ case(mu, sigma) => \n+        new MultivariateGaussian(mu.toBreeze.toDenseVector, sigma.toBreeze.toDenseMatrix) \n+      }.toArray)\n+      \n+      case None => {\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+        (Array.fill[Double](k)(1.0 / k), (0 until k).map{ i => \n+          val slice = samples.view(i * nSamples, (i + 1) * nSamples)\n+          new MultivariateGaussian(vectorMean(slice), initCovariance(slice)) \n+        }.toArray)  \n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // create and broadcast curried cluster contribution function\n+      val compute = sc.broadcast(computeExpectation(weights, gaussians)_)\n+      \n+      // aggregate the cluster contribution for all sample points\n+      val (logLikelihood, wSums, muSums, sigmaSums) = \n+        breezeData.aggregate(zeroExpectationSum(k, d))(compute.value, addExpectationSums)\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      val sumWeights = wSums.sum\n+      for (i <- 0 until k) {"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Please use `BLAS.dsyr` or leave a TODO note.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:38:04Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    var i = 0\n+    while (i < m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+      i = i + 1\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    var i = 0\n+    while (i < k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)\n+      i = i + 1\n+    }\n+    sums\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization.\n+   *  You must call setK() prior to calling this method, and the condition\n+   *  (gmm.k == this.k) must be met; failure will result in an IllegalArgumentException\n+   */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val sc = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // Determine initial weights and corresponding Gaussians.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples    \n+    val (weights, gaussians) = initialGmm match {\n+      case Some(gmm) => (gmm.weight, gmm.mu.zip(gmm.sigma).map{ case(mu, sigma) => \n+        new MultivariateGaussian(mu.toBreeze.toDenseVector, sigma.toBreeze.toDenseMatrix) \n+      }.toArray)\n+      \n+      case None => {\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+        (Array.fill[Double](k)(1.0 / k), (0 until k).map{ i => \n+          val slice = samples.view(i * nSamples, (i + 1) * nSamples)\n+          new MultivariateGaussian(vectorMean(slice), initCovariance(slice)) \n+        }.toArray)  \n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // create and broadcast curried cluster contribution function\n+      val compute = sc.broadcast(computeExpectation(weights, gaussians)_)\n+      \n+      // aggregate the cluster contribution for all sample points\n+      val (logLikelihood, wSums, muSums, sigmaSums) = \n+        breezeData.aggregate(zeroExpectationSum(k, d))(compute.value, addExpectationSums)\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      val sumWeights = wSums.sum\n+      for (i <- 0 until k) {\n+        val mu = muSums(i) / wSums(i)\n+        val sigma = sigmaSums(i) / wSums(i) - mu * new Transpose(mu)"
  }, {
    "author": {
      "login": "tgaloppo"
    },
    "body": "I don't see dsyr in BLAS... perhaps I am out of date (or blind)?\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-20T00:09:16Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    var i = 0\n+    while (i < m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+      i = i + 1\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    var i = 0\n+    while (i < k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)\n+      i = i + 1\n+    }\n+    sums\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization.\n+   *  You must call setK() prior to calling this method, and the condition\n+   *  (gmm.k == this.k) must be met; failure will result in an IllegalArgumentException\n+   */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val sc = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // Determine initial weights and corresponding Gaussians.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples    \n+    val (weights, gaussians) = initialGmm match {\n+      case Some(gmm) => (gmm.weight, gmm.mu.zip(gmm.sigma).map{ case(mu, sigma) => \n+        new MultivariateGaussian(mu.toBreeze.toDenseVector, sigma.toBreeze.toDenseMatrix) \n+      }.toArray)\n+      \n+      case None => {\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+        (Array.fill[Double](k)(1.0 / k), (0 until k).map{ i => \n+          val slice = samples.view(i * nSamples, (i + 1) * nSamples)\n+          new MultivariateGaussian(vectorMean(slice), initCovariance(slice)) \n+        }.toArray)  \n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // create and broadcast curried cluster contribution function\n+      val compute = sc.broadcast(computeExpectation(weights, gaussians)_)\n+      \n+      // aggregate the cluster contribution for all sample points\n+      val (logLikelihood, wSums, muSums, sigmaSums) = \n+        breezeData.aggregate(zeroExpectationSum(k, d))(compute.value, addExpectationSums)\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      val sumWeights = wSums.sum\n+      for (i <- 0 until k) {\n+        val mu = muSums(i) / wSums(i)\n+        val sigma = sigmaSums(i) / wSums(i) - mu * new Transpose(mu)"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`Array.tabulate`\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:38:06Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    var i = 0\n+    while (i < m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+      i = i + 1\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    var i = 0\n+    while (i < k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)\n+      i = i + 1\n+    }\n+    sums\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization.\n+   *  You must call setK() prior to calling this method, and the condition\n+   *  (gmm.k == this.k) must be met; failure will result in an IllegalArgumentException\n+   */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val sc = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // Determine initial weights and corresponding Gaussians.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples    \n+    val (weights, gaussians) = initialGmm match {\n+      case Some(gmm) => (gmm.weight, gmm.mu.zip(gmm.sigma).map{ case(mu, sigma) => \n+        new MultivariateGaussian(mu.toBreeze.toDenseVector, sigma.toBreeze.toDenseMatrix) \n+      }.toArray)\n+      \n+      case None => {\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+        (Array.fill[Double](k)(1.0 / k), (0 until k).map{ i => \n+          val slice = samples.view(i * nSamples, (i + 1) * nSamples)\n+          new MultivariateGaussian(vectorMean(slice), initCovariance(slice)) \n+        }.toArray)  \n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // create and broadcast curried cluster contribution function\n+      val compute = sc.broadcast(computeExpectation(weights, gaussians)_)\n+      \n+      // aggregate the cluster contribution for all sample points\n+      val (logLikelihood, wSums, muSums, sigmaSums) = \n+        breezeData.aggregate(zeroExpectationSum(k, d))(compute.value, addExpectationSums)\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      val sumWeights = wSums.sum\n+      for (i <- 0 until k) {\n+        val mu = muSums(i) / wSums(i)\n+        val sigma = sigmaSums(i) / wSums(i) - mu * new Transpose(mu)\n+        weights(i) = wSums(i) / sumWeights\n+        gaussians(i) = new MultivariateGaussian(mu, sigma)\n+      }\n+   \n+      llhp = llh // current becomes previous\n+      llh = logLikelihood(0) // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > convergenceTol)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(gaussians(i).mu)).toArray"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "toDouble\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:38:08Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    var i = 0\n+    while (i < m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+      i = i + 1\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    var i = 0\n+    while (i < k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)\n+      i = i + 1\n+    }\n+    sums\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization.\n+   *  You must call setK() prior to calling this method, and the condition\n+   *  (gmm.k == this.k) must be met; failure will result in an IllegalArgumentException\n+   */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val sc = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // Determine initial weights and corresponding Gaussians.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples    \n+    val (weights, gaussians) = initialGmm match {\n+      case Some(gmm) => (gmm.weight, gmm.mu.zip(gmm.sigma).map{ case(mu, sigma) => \n+        new MultivariateGaussian(mu.toBreeze.toDenseVector, sigma.toBreeze.toDenseMatrix) \n+      }.toArray)\n+      \n+      case None => {\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+        (Array.fill[Double](k)(1.0 / k), (0 until k).map{ i => \n+          val slice = samples.view(i * nSamples, (i + 1) * nSamples)\n+          new MultivariateGaussian(vectorMean(slice), initCovariance(slice)) \n+        }.toArray)  \n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // create and broadcast curried cluster contribution function\n+      val compute = sc.broadcast(computeExpectation(weights, gaussians)_)\n+      \n+      // aggregate the cluster contribution for all sample points\n+      val (logLikelihood, wSums, muSums, sigmaSums) = \n+        breezeData.aggregate(zeroExpectationSum(k, d))(compute.value, addExpectationSums)\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      val sumWeights = wSums.sum\n+      for (i <- 0 until k) {\n+        val mu = muSums(i) / wSums(i)\n+        val sigma = sigmaSums(i) / wSums(i) - mu * new Transpose(mu)\n+        weights(i) = wSums(i) / sumWeights\n+        gaussians(i) = new MultivariateGaussian(mu, sigma)\n+      }\n+   \n+      llhp = llh // current becomes previous\n+      llh = logLikelihood(0) // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > convergenceTol)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(gaussians(i).mu)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(gaussians(i).sigma)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+    \n+  /** Average of dense breeze vectors */\n+  private def vectorMean(x: VectorArrayView): DenseDoubleVector = {\n+    val v = BreezeVector.zeros[Double](x(0).length)\n+    x.foreach(xi => v += xi)\n+    v / x.length.asInstanceOf[Double] "
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "breeze has `squaredDistance`.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:38:11Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    var i = 0\n+    while (i < m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+      i = i + 1\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    var i = 0\n+    while (i < k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)\n+      i = i + 1\n+    }\n+    sums\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization.\n+   *  You must call setK() prior to calling this method, and the condition\n+   *  (gmm.k == this.k) must be met; failure will result in an IllegalArgumentException\n+   */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val sc = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // Determine initial weights and corresponding Gaussians.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples    \n+    val (weights, gaussians) = initialGmm match {\n+      case Some(gmm) => (gmm.weight, gmm.mu.zip(gmm.sigma).map{ case(mu, sigma) => \n+        new MultivariateGaussian(mu.toBreeze.toDenseVector, sigma.toBreeze.toDenseMatrix) \n+      }.toArray)\n+      \n+      case None => {\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+        (Array.fill[Double](k)(1.0 / k), (0 until k).map{ i => \n+          val slice = samples.view(i * nSamples, (i + 1) * nSamples)\n+          new MultivariateGaussian(vectorMean(slice), initCovariance(slice)) \n+        }.toArray)  \n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // create and broadcast curried cluster contribution function\n+      val compute = sc.broadcast(computeExpectation(weights, gaussians)_)\n+      \n+      // aggregate the cluster contribution for all sample points\n+      val (logLikelihood, wSums, muSums, sigmaSums) = \n+        breezeData.aggregate(zeroExpectationSum(k, d))(compute.value, addExpectationSums)\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      val sumWeights = wSums.sum\n+      for (i <- 0 until k) {\n+        val mu = muSums(i) / wSums(i)\n+        val sigma = sigmaSums(i) / wSums(i) - mu * new Transpose(mu)\n+        weights(i) = wSums(i) / sumWeights\n+        gaussians(i) = new MultivariateGaussian(mu, sigma)\n+      }\n+   \n+      llhp = llh // current becomes previous\n+      llh = logLikelihood(0) // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > convergenceTol)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(gaussians(i).mu)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(gaussians(i).sigma)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+    \n+  /** Average of dense breeze vectors */\n+  private def vectorMean(x: VectorArrayView): DenseDoubleVector = {\n+    val v = BreezeVector.zeros[Double](x(0).length)\n+    x.foreach(xi => v += xi)\n+    v / x.length.asInstanceOf[Double] \n+  }\n+  \n+  /**\n+   * Construct matrix where diagonal entries are element-wise\n+   * variance of input vectors (computes biased variance)\n+   */\n+  private def initCovariance(x: VectorArrayView): DenseDoubleMatrix = {\n+    val mu = vectorMean(x)\n+    val ss = BreezeVector.zeros[Double](x(0).length)\n+    val cov = BreezeMatrix.eye[Double](ss.length)\n+    x.map(xi => (xi - mu) :^ 2.0).foreach(u => ss += u)"
  }, {
    "author": {
      "login": "tgaloppo"
    },
    "body": "squaredDistance returns a scalar... I want the squared entry values.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T23:52:30Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    var i = 0\n+    while (i < m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+      i = i + 1\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    var i = 0\n+    while (i < k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)\n+      i = i + 1\n+    }\n+    sums\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization.\n+   *  You must call setK() prior to calling this method, and the condition\n+   *  (gmm.k == this.k) must be met; failure will result in an IllegalArgumentException\n+   */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val sc = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // Determine initial weights and corresponding Gaussians.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples    \n+    val (weights, gaussians) = initialGmm match {\n+      case Some(gmm) => (gmm.weight, gmm.mu.zip(gmm.sigma).map{ case(mu, sigma) => \n+        new MultivariateGaussian(mu.toBreeze.toDenseVector, sigma.toBreeze.toDenseMatrix) \n+      }.toArray)\n+      \n+      case None => {\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+        (Array.fill[Double](k)(1.0 / k), (0 until k).map{ i => \n+          val slice = samples.view(i * nSamples, (i + 1) * nSamples)\n+          new MultivariateGaussian(vectorMean(slice), initCovariance(slice)) \n+        }.toArray)  \n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // create and broadcast curried cluster contribution function\n+      val compute = sc.broadcast(computeExpectation(weights, gaussians)_)\n+      \n+      // aggregate the cluster contribution for all sample points\n+      val (logLikelihood, wSums, muSums, sigmaSums) = \n+        breezeData.aggregate(zeroExpectationSum(k, d))(compute.value, addExpectationSums)\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      val sumWeights = wSums.sum\n+      for (i <- 0 until k) {\n+        val mu = muSums(i) / wSums(i)\n+        val sigma = sigmaSums(i) / wSums(i) - mu * new Transpose(mu)\n+        weights(i) = wSums(i) / sumWeights\n+        gaussians(i) = new MultivariateGaussian(mu, sigma)\n+      }\n+   \n+      llhp = llh // current becomes previous\n+      llh = logLikelihood(0) // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > convergenceTol)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(gaussians(i).mu)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(gaussians(i).sigma)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+    \n+  /** Average of dense breeze vectors */\n+  private def vectorMean(x: VectorArrayView): DenseDoubleVector = {\n+    val v = BreezeVector.zeros[Double](x(0).length)\n+    x.foreach(xi => v += xi)\n+    v / x.length.asInstanceOf[Double] \n+  }\n+  \n+  /**\n+   * Construct matrix where diagonal entries are element-wise\n+   * variance of input vectors (computes biased variance)\n+   */\n+  private def initCovariance(x: VectorArrayView): DenseDoubleMatrix = {\n+    val mu = vectorMean(x)\n+    val ss = BreezeVector.zeros[Double](x(0).length)\n+    val cov = BreezeMatrix.eye[Double](ss.length)\n+    x.map(xi => (xi - mu) :^ 2.0).foreach(u => ss += u)"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "breeze has `diag`\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-19T07:38:14Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.Transpose\n+\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+import scala.collection.mutable.IndexedSeqView\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  private type VectorArrayView = IndexedSeqView[DenseDoubleVector, Array[DenseDoubleVector]]\n+  \n+  private type ExpectationSum = (\n+    Array[Double], // log-likelihood in index 0\n+    Array[Double], // array of weights\n+    Array[DenseDoubleVector], // array of means\n+    Array[DenseDoubleMatrix]) // array of cov matrices\n+  \n+  // create a zero'd ExpectationSum instance\n+  private def zeroExpectationSum(k: Int, d: Int): ExpectationSum = {\n+    (Array(0.0), \n+      new Array[Double](k),\n+      (0 until k).map(_ => BreezeVector.zeros[Double](d)).toArray,\n+      (0 until k).map(_ => BreezeMatrix.zeros[Double](d,d)).toArray)\n+  }\n+  \n+  // add two ExpectationSum objects (allowed to use modify m1)\n+  // (U, U) => U for aggregation\n+  private def addExpectationSums(m1: ExpectationSum, m2: ExpectationSum): ExpectationSum = {\n+    m1._1(0) += m2._1(0)\n+    var i = 0\n+    while (i < m1._2.length) {\n+      m1._2(i) += m2._2(i)\n+      m1._3(i) += m2._3(i)\n+      m1._4(i) += m2._4(i)\n+      i = i + 1\n+    }\n+    m1\n+  }\n+  \n+  // compute cluster contributions for each input point\n+  // (U, T) => U for aggregation\n+  private def computeExpectation(\n+      weights: Array[Double], \n+      dists: Array[MultivariateGaussian])\n+      (sums: ExpectationSum, x: DenseDoubleVector): ExpectationSum = {\n+    val k = sums._2.length\n+    val p = weights.zip(dists).map { case (weight, dist) => eps + weight * dist.pdf(x) }\n+    val pSum = p.sum\n+    sums._1(0) += math.log(pSum)\n+    val xxt = x * new Transpose(x)\n+    var i = 0\n+    while (i < k) {\n+      p(i) /= pSum\n+      sums._2(i) += p(i)\n+      sums._3(i) += x * p(i)\n+      sums._4(i) += xxt * p(i)\n+      i = i + 1\n+    }\n+    sums\n+  }\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5\n+  \n+  // an initializing GMM can be provided rather than using the \n+  // default random starting point\n+  private var initialGmm: Option[GaussianMixtureModel] = None\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the initial GMM starting point, bypassing the random initialization.\n+   *  You must call setK() prior to calling this method, and the condition\n+   *  (gmm.k == this.k) must be met; failure will result in an IllegalArgumentException\n+   */\n+  def setInitialGmm(gmm: GaussianMixtureModel): this.type = {\n+    if (gmm.k == k) {\n+      initialGmm = Some(gmm)\n+    } else {\n+      throw new IllegalArgumentException(\"initialing GMM has mismatched cluster count (gmm.k != k)\")\n+    }\n+    this\n+  }\n+  \n+  /** Return the user supplied initial GMM, if supplied */\n+  def getInitialGmm: Option[GaussianMixtureModel] = initialGmm\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Return the number of Gaussians in the mixture model */\n+  def getK: Int = k\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /** Return the maximum number of iterations to run */\n+  def getMaxIterations: Int = maxIterations\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+  \n+  /** Return the largest change in log-likelihood at which convergence is\n+   *  considered to have occurred.\n+   */\n+  def getConvergenceTol: Double = convergenceTol\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val sc = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map(u => u.toBreeze.toDenseVector).cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // Determine initial weights and corresponding Gaussians.\n+    // If the user supplied an initial GMM, we use those values, otherwise\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples    \n+    val (weights, gaussians) = initialGmm match {\n+      case Some(gmm) => (gmm.weight, gmm.mu.zip(gmm.sigma).map{ case(mu, sigma) => \n+        new MultivariateGaussian(mu.toBreeze.toDenseVector, sigma.toBreeze.toDenseMatrix) \n+      }.toArray)\n+      \n+      case None => {\n+        val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+        (Array.fill[Double](k)(1.0 / k), (0 until k).map{ i => \n+          val slice = samples.view(i * nSamples, (i + 1) * nSamples)\n+          new MultivariateGaussian(vectorMean(slice), initCovariance(slice)) \n+        }.toArray)  \n+      }\n+    }\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var iter = 0\n+    do {\n+      // create and broadcast curried cluster contribution function\n+      val compute = sc.broadcast(computeExpectation(weights, gaussians)_)\n+      \n+      // aggregate the cluster contribution for all sample points\n+      val (logLikelihood, wSums, muSums, sigmaSums) = \n+        breezeData.aggregate(zeroExpectationSum(k, d))(compute.value, addExpectationSums)\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      val sumWeights = wSums.sum\n+      for (i <- 0 until k) {\n+        val mu = muSums(i) / wSums(i)\n+        val sigma = sigmaSums(i) / wSums(i) - mu * new Transpose(mu)\n+        weights(i) = wSums(i) / sumWeights\n+        gaussians(i) = new MultivariateGaussian(mu, sigma)\n+      }\n+   \n+      llhp = llh // current becomes previous\n+      llh = logLikelihood(0) // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > convergenceTol)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(gaussians(i).mu)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(gaussians(i).sigma)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+    \n+  /** Average of dense breeze vectors */\n+  private def vectorMean(x: VectorArrayView): DenseDoubleVector = {\n+    val v = BreezeVector.zeros[Double](x(0).length)\n+    x.foreach(xi => v += xi)\n+    v / x.length.asInstanceOf[Double] \n+  }\n+  \n+  /**\n+   * Construct matrix where diagonal entries are element-wise\n+   * variance of input vectors (computes biased variance)\n+   */\n+  private def initCovariance(x: VectorArrayView): DenseDoubleMatrix = {\n+    val mu = vectorMean(x)\n+    val ss = BreezeVector.zeros[Double](x(0).length)\n+    val cov = BreezeMatrix.eye[Double](ss.length)\n+    x.map(xi => (xi - mu) :^ 2.0).foreach(u => ss += u)\n+    (0 until ss.length).foreach(i => cov(i,i) = ss(i) / x.length)"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Remove extra newlines\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-22T19:43:03Z",
    "diffHunk": "@@ -0,0 +1,242 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.IndexedSeq\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix, diag, Transpose}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.mllib.stat.impl.MultivariateGaussian\n+\n+/**\n+ * This class performs expectation maximization for multivariate Gaussian\n+ * Mixture Models (GMMs).  A GMM represents a composite distribution of\n+ * independent Gaussian distributions with associated \"mixing\" weights\n+ * specifying each's contribution to the composite.\n+ *\n+ * Given a set of sample points, this class will maximize the log-likelihood \n+ * for a mixture of k Gaussians, iterating until the log-likelihood changes by \n+ * less than convergenceTol, or until it has reached the max number of iterations.\n+ * While this process is generally guaranteed to converge, it is not guaranteed\n+ * to find a global optimum.  \n+ * \n+ * @param k The number of independent Gaussians in the mixture model\n+ * @param convergenceTol The maximum change in log-likelihood at which convergence\n+ * is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform\n+ */\n+class GaussianMixtureModelEM private (\n+    private var k: Int, \n+    private var convergenceTol: Double, \n+    private var maxIterations: Int) extends Serializable {\n+  \n+  /** A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold */\n+  def this() = this(2, 0.01, 100)\n+  "
  }],
  "prId": 3022
}]