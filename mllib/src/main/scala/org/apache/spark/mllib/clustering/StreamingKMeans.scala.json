[{
  "comments": [{
    "author": {
      "login": "coderxiang"
    },
    "body": "indent of 4 spaces?\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T00:10:56Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm.\n+ *\n+ * The update algorithm uses the \"mini-batch\" KMeans rule,\n+ * generalized to incorporate forgetfullness (i.e. decay).\n+ * The basic update rule (for each cluster) is:\n+ *\n+ * c_t+1 = [(c_t * n_t) + (x_t * m_t)] / [n_t + m_t]\n+ * n_t+t = n_t + m_t\n+ *\n+ * Where c_t is the previously estimated centroid for that cluster,\n+ * n_t is the number of points assigned to it thus far, x_t is the centroid\n+ * estimated on the current batch, and m_t is the number of points assigned\n+ * to that centroid in the current batch.\n+ *\n+ * This update rule is modified with a decay factor 'a' that scales\n+ * the contribution of the clusters as estimated thus far.\n+ * If a=1, all batches are weighted equally. If a=0, new centroids\n+ * are determined entirely by recent data. Lower values correspond to\n+ * more forgetting.\n+ *\n+ * Decay can optionally be specified as a decay fraction 'q',\n+ * which corresponds to the fraction of batches (or points)\n+ * after which the past will be reduced to a contribution of 0.5.\n+ * This decay fraction can be specified in units of 'points' or 'batches'.\n+ * if 'batches', behavior will be independent of the number of points per batch;\n+ * if 'points', the expected number of points per batch must be specified.\n+ *\n+ * Use a builder pattern to construct a streaming KMeans analysis\n+ * in an application, like:\n+ *\n+ *  val model = new StreamingKMeans()\n+ *    .setDecayFactor(0.5)\n+ *    .setK(3)\n+ *    .setRandomCenters(5)\n+ *    .trainOn(DStream)\n+ *\n+ */\n+@DeveloperApi\n+class StreamingKMeansModel(\n+    override val clusterCenters: Array[Vector],\n+    val clusterCounts: Array[Long]) extends KMeansModel(clusterCenters) with Logging {\n+\n+  // do a sequential KMeans update on a batch of data\n+  def update(data: RDD[Vector], a: Double, units: String): StreamingKMeansModel = {\n+\n+    val centers = clusterCenters\n+    val counts = clusterCounts\n+\n+    // find nearest cluster to each point\n+    val closest = data.map(point => (this.predict(point), (point.toBreeze, 1.toLong)))\n+\n+    // get sums and counts for updating each cluster\n+    type WeightedPoint = (BV[Double], Long)\n+    def mergeContribs(p1: WeightedPoint, p2: WeightedPoint): WeightedPoint = {\n+      (p1._1 += p2._1, p1._2 + p2._2)\n+    }\n+    val pointStats: Array[(Int, (BV[Double], Long))] =\n+      closest.reduceByKey{mergeContribs}.collectAsMap().toArray\n+\n+    // implement update rule\n+    for (newP <- pointStats) {\n+      // store old count and centroid\n+      val oldCount = counts(newP._1)\n+      val oldCentroid = centers(newP._1).toBreeze\n+      // get new count and centroid\n+      val newCount = newP._2._2\n+      val newCentroid = newP._2._1 / newCount.toDouble\n+      // compute the normalized scale factor that controls forgetting\n+      val decayFactor = units match {\n+        case \"batches\" =>  newCount / (a * oldCount + newCount)\n+        case \"points\" => newCount / (math.pow(a, newCount) * oldCount + newCount)\n+      }\n+      // perform the update\n+      val updatedCentroid = oldCentroid + (newCentroid - oldCentroid) * decayFactor\n+      // store the new counts and centers\n+      counts(newP._1) = oldCount + newCount\n+      centers(newP._1) = Vectors.fromBreeze(updatedCentroid)\n+\n+      // display the updated cluster centers\n+      val display = centers(newP._1).size match {\n+        case x if x > 100 => centers(newP._1).toArray.take(100).mkString(\"[\", \",\", \"...\")\n+        case _ => centers(newP._1).toArray.mkString(\"[\", \",\", \"]\")\n+      }\n+      logInfo(\"Cluster %d updated: %s \".format (newP._1, display))\n+    }\n+    new StreamingKMeansModel(centers, counts)\n+  }\n+\n+}\n+\n+@DeveloperApi\n+class StreamingKMeans(\n+     var k: Int,"
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "coderxiang"
    },
    "body": "how about `Array.fill[Long](this.k)(0)`?\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T00:11:50Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm.\n+ *\n+ * The update algorithm uses the \"mini-batch\" KMeans rule,\n+ * generalized to incorporate forgetfullness (i.e. decay).\n+ * The basic update rule (for each cluster) is:\n+ *\n+ * c_t+1 = [(c_t * n_t) + (x_t * m_t)] / [n_t + m_t]\n+ * n_t+t = n_t + m_t\n+ *\n+ * Where c_t is the previously estimated centroid for that cluster,\n+ * n_t is the number of points assigned to it thus far, x_t is the centroid\n+ * estimated on the current batch, and m_t is the number of points assigned\n+ * to that centroid in the current batch.\n+ *\n+ * This update rule is modified with a decay factor 'a' that scales\n+ * the contribution of the clusters as estimated thus far.\n+ * If a=1, all batches are weighted equally. If a=0, new centroids\n+ * are determined entirely by recent data. Lower values correspond to\n+ * more forgetting.\n+ *\n+ * Decay can optionally be specified as a decay fraction 'q',\n+ * which corresponds to the fraction of batches (or points)\n+ * after which the past will be reduced to a contribution of 0.5.\n+ * This decay fraction can be specified in units of 'points' or 'batches'.\n+ * if 'batches', behavior will be independent of the number of points per batch;\n+ * if 'points', the expected number of points per batch must be specified.\n+ *\n+ * Use a builder pattern to construct a streaming KMeans analysis\n+ * in an application, like:\n+ *\n+ *  val model = new StreamingKMeans()\n+ *    .setDecayFactor(0.5)\n+ *    .setK(3)\n+ *    .setRandomCenters(5)\n+ *    .trainOn(DStream)\n+ *\n+ */\n+@DeveloperApi\n+class StreamingKMeansModel(\n+    override val clusterCenters: Array[Vector],\n+    val clusterCounts: Array[Long]) extends KMeansModel(clusterCenters) with Logging {\n+\n+  // do a sequential KMeans update on a batch of data\n+  def update(data: RDD[Vector], a: Double, units: String): StreamingKMeansModel = {\n+\n+    val centers = clusterCenters\n+    val counts = clusterCounts\n+\n+    // find nearest cluster to each point\n+    val closest = data.map(point => (this.predict(point), (point.toBreeze, 1.toLong)))\n+\n+    // get sums and counts for updating each cluster\n+    type WeightedPoint = (BV[Double], Long)\n+    def mergeContribs(p1: WeightedPoint, p2: WeightedPoint): WeightedPoint = {\n+      (p1._1 += p2._1, p1._2 + p2._2)\n+    }\n+    val pointStats: Array[(Int, (BV[Double], Long))] =\n+      closest.reduceByKey{mergeContribs}.collectAsMap().toArray\n+\n+    // implement update rule\n+    for (newP <- pointStats) {\n+      // store old count and centroid\n+      val oldCount = counts(newP._1)\n+      val oldCentroid = centers(newP._1).toBreeze\n+      // get new count and centroid\n+      val newCount = newP._2._2\n+      val newCentroid = newP._2._1 / newCount.toDouble\n+      // compute the normalized scale factor that controls forgetting\n+      val decayFactor = units match {\n+        case \"batches\" =>  newCount / (a * oldCount + newCount)\n+        case \"points\" => newCount / (math.pow(a, newCount) * oldCount + newCount)\n+      }\n+      // perform the update\n+      val updatedCentroid = oldCentroid + (newCentroid - oldCentroid) * decayFactor\n+      // store the new counts and centers\n+      counts(newP._1) = oldCount + newCount\n+      centers(newP._1) = Vectors.fromBreeze(updatedCentroid)\n+\n+      // display the updated cluster centers\n+      val display = centers(newP._1).size match {\n+        case x if x > 100 => centers(newP._1).toArray.take(100).mkString(\"[\", \",\", \"...\")\n+        case _ => centers(newP._1).toArray.mkString(\"[\", \",\", \"]\")\n+      }\n+      logInfo(\"Cluster %d updated: %s \".format (newP._1, display))\n+    }\n+    new StreamingKMeansModel(centers, counts)\n+  }\n+\n+}\n+\n+@DeveloperApi\n+class StreamingKMeans(\n+     var k: Int,\n+     var a: Double,\n+     var units: String) extends Logging {\n+\n+  protected var model: StreamingKMeansModel = new StreamingKMeansModel(null, null)\n+\n+  def this() = this(2, 1.0, \"batches\")\n+\n+  /** Set the number of clusters. */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+\n+  /** Set the decay factor directly (for forgetful algorithms). */\n+  def setDecayFactor(a: Double): this.type = {\n+    this.a = a\n+    this\n+  }\n+\n+  /** Set the decay units for forgetful algorithms (\"batches\" or \"points\"). */\n+  def setUnits(units: String): this.type = {\n+    if (units != \"batches\" && units != \"points\") {\n+      throw new IllegalArgumentException(\"Invalid units for decay: \" + units)\n+    }\n+    this.units = units\n+    this\n+  }\n+\n+  /** Set decay fraction in units of batches. */\n+  def setDecayFractionBatches(q: Double): this.type = {\n+    this.a = math.log(1 - q) / math.log(0.5)\n+    this.units = \"batches\"\n+    this\n+  }\n+\n+  /** Set decay fraction in units of points. Must specify expected number of points per batch. */\n+  def setDecayFractionPoints(q: Double, m: Double): this.type = {\n+    this.a = math.pow(math.log(1 - q) / math.log(0.5), 1/m)\n+    this.units = \"points\"\n+    this\n+  }\n+\n+  /** Specify initial explicitly directly. */\n+  def setInitialCenters(initialCenters: Array[Vector]): this.type = {\n+    val clusterCounts = Array.fill(this.k)(0).map(_.toLong)"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "Actually since the initial value is 0L, you can just do\n\n``` scala\nnew Array[Long](this.k)\n```\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T06:00:40Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm.\n+ *\n+ * The update algorithm uses the \"mini-batch\" KMeans rule,\n+ * generalized to incorporate forgetfullness (i.e. decay).\n+ * The basic update rule (for each cluster) is:\n+ *\n+ * c_t+1 = [(c_t * n_t) + (x_t * m_t)] / [n_t + m_t]\n+ * n_t+t = n_t + m_t\n+ *\n+ * Where c_t is the previously estimated centroid for that cluster,\n+ * n_t is the number of points assigned to it thus far, x_t is the centroid\n+ * estimated on the current batch, and m_t is the number of points assigned\n+ * to that centroid in the current batch.\n+ *\n+ * This update rule is modified with a decay factor 'a' that scales\n+ * the contribution of the clusters as estimated thus far.\n+ * If a=1, all batches are weighted equally. If a=0, new centroids\n+ * are determined entirely by recent data. Lower values correspond to\n+ * more forgetting.\n+ *\n+ * Decay can optionally be specified as a decay fraction 'q',\n+ * which corresponds to the fraction of batches (or points)\n+ * after which the past will be reduced to a contribution of 0.5.\n+ * This decay fraction can be specified in units of 'points' or 'batches'.\n+ * if 'batches', behavior will be independent of the number of points per batch;\n+ * if 'points', the expected number of points per batch must be specified.\n+ *\n+ * Use a builder pattern to construct a streaming KMeans analysis\n+ * in an application, like:\n+ *\n+ *  val model = new StreamingKMeans()\n+ *    .setDecayFactor(0.5)\n+ *    .setK(3)\n+ *    .setRandomCenters(5)\n+ *    .trainOn(DStream)\n+ *\n+ */\n+@DeveloperApi\n+class StreamingKMeansModel(\n+    override val clusterCenters: Array[Vector],\n+    val clusterCounts: Array[Long]) extends KMeansModel(clusterCenters) with Logging {\n+\n+  // do a sequential KMeans update on a batch of data\n+  def update(data: RDD[Vector], a: Double, units: String): StreamingKMeansModel = {\n+\n+    val centers = clusterCenters\n+    val counts = clusterCounts\n+\n+    // find nearest cluster to each point\n+    val closest = data.map(point => (this.predict(point), (point.toBreeze, 1.toLong)))\n+\n+    // get sums and counts for updating each cluster\n+    type WeightedPoint = (BV[Double], Long)\n+    def mergeContribs(p1: WeightedPoint, p2: WeightedPoint): WeightedPoint = {\n+      (p1._1 += p2._1, p1._2 + p2._2)\n+    }\n+    val pointStats: Array[(Int, (BV[Double], Long))] =\n+      closest.reduceByKey{mergeContribs}.collectAsMap().toArray\n+\n+    // implement update rule\n+    for (newP <- pointStats) {\n+      // store old count and centroid\n+      val oldCount = counts(newP._1)\n+      val oldCentroid = centers(newP._1).toBreeze\n+      // get new count and centroid\n+      val newCount = newP._2._2\n+      val newCentroid = newP._2._1 / newCount.toDouble\n+      // compute the normalized scale factor that controls forgetting\n+      val decayFactor = units match {\n+        case \"batches\" =>  newCount / (a * oldCount + newCount)\n+        case \"points\" => newCount / (math.pow(a, newCount) * oldCount + newCount)\n+      }\n+      // perform the update\n+      val updatedCentroid = oldCentroid + (newCentroid - oldCentroid) * decayFactor\n+      // store the new counts and centers\n+      counts(newP._1) = oldCount + newCount\n+      centers(newP._1) = Vectors.fromBreeze(updatedCentroid)\n+\n+      // display the updated cluster centers\n+      val display = centers(newP._1).size match {\n+        case x if x > 100 => centers(newP._1).toArray.take(100).mkString(\"[\", \",\", \"...\")\n+        case _ => centers(newP._1).toArray.mkString(\"[\", \",\", \"]\")\n+      }\n+      logInfo(\"Cluster %d updated: %s \".format (newP._1, display))\n+    }\n+    new StreamingKMeansModel(centers, counts)\n+  }\n+\n+}\n+\n+@DeveloperApi\n+class StreamingKMeans(\n+     var k: Int,\n+     var a: Double,\n+     var units: String) extends Logging {\n+\n+  protected var model: StreamingKMeansModel = new StreamingKMeansModel(null, null)\n+\n+  def this() = this(2, 1.0, \"batches\")\n+\n+  /** Set the number of clusters. */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+\n+  /** Set the decay factor directly (for forgetful algorithms). */\n+  def setDecayFactor(a: Double): this.type = {\n+    this.a = a\n+    this\n+  }\n+\n+  /** Set the decay units for forgetful algorithms (\"batches\" or \"points\"). */\n+  def setUnits(units: String): this.type = {\n+    if (units != \"batches\" && units != \"points\") {\n+      throw new IllegalArgumentException(\"Invalid units for decay: \" + units)\n+    }\n+    this.units = units\n+    this\n+  }\n+\n+  /** Set decay fraction in units of batches. */\n+  def setDecayFractionBatches(q: Double): this.type = {\n+    this.a = math.log(1 - q) / math.log(0.5)\n+    this.units = \"batches\"\n+    this\n+  }\n+\n+  /** Set decay fraction in units of points. Must specify expected number of points per batch. */\n+  def setDecayFractionPoints(q: Double, m: Double): this.type = {\n+    this.a = math.pow(math.log(1 - q) / math.log(0.5), 1/m)\n+    this.units = \"points\"\n+    this\n+  }\n+\n+  /** Specify initial explicitly directly. */\n+  def setInitialCenters(initialCenters: Array[Vector]): this.type = {\n+    val clusterCounts = Array.fill(this.k)(0).map(_.toLong)"
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "might be better to remove the logError, and just put the error message as part of IllegalArgumentException.\n\nActually this is not really IllegalArgumentException. It is more like IllegalStateException\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T06:01:49Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm.\n+ *\n+ * The update algorithm uses the \"mini-batch\" KMeans rule,\n+ * generalized to incorporate forgetfullness (i.e. decay).\n+ * The basic update rule (for each cluster) is:\n+ *\n+ * c_t+1 = [(c_t * n_t) + (x_t * m_t)] / [n_t + m_t]\n+ * n_t+t = n_t + m_t\n+ *\n+ * Where c_t is the previously estimated centroid for that cluster,\n+ * n_t is the number of points assigned to it thus far, x_t is the centroid\n+ * estimated on the current batch, and m_t is the number of points assigned\n+ * to that centroid in the current batch.\n+ *\n+ * This update rule is modified with a decay factor 'a' that scales\n+ * the contribution of the clusters as estimated thus far.\n+ * If a=1, all batches are weighted equally. If a=0, new centroids\n+ * are determined entirely by recent data. Lower values correspond to\n+ * more forgetting.\n+ *\n+ * Decay can optionally be specified as a decay fraction 'q',\n+ * which corresponds to the fraction of batches (or points)\n+ * after which the past will be reduced to a contribution of 0.5.\n+ * This decay fraction can be specified in units of 'points' or 'batches'.\n+ * if 'batches', behavior will be independent of the number of points per batch;\n+ * if 'points', the expected number of points per batch must be specified.\n+ *\n+ * Use a builder pattern to construct a streaming KMeans analysis\n+ * in an application, like:\n+ *\n+ *  val model = new StreamingKMeans()\n+ *    .setDecayFactor(0.5)\n+ *    .setK(3)\n+ *    .setRandomCenters(5)\n+ *    .trainOn(DStream)\n+ *\n+ */\n+@DeveloperApi\n+class StreamingKMeansModel(\n+    override val clusterCenters: Array[Vector],\n+    val clusterCounts: Array[Long]) extends KMeansModel(clusterCenters) with Logging {\n+\n+  // do a sequential KMeans update on a batch of data\n+  def update(data: RDD[Vector], a: Double, units: String): StreamingKMeansModel = {\n+\n+    val centers = clusterCenters\n+    val counts = clusterCounts\n+\n+    // find nearest cluster to each point\n+    val closest = data.map(point => (this.predict(point), (point.toBreeze, 1.toLong)))\n+\n+    // get sums and counts for updating each cluster\n+    type WeightedPoint = (BV[Double], Long)\n+    def mergeContribs(p1: WeightedPoint, p2: WeightedPoint): WeightedPoint = {\n+      (p1._1 += p2._1, p1._2 + p2._2)\n+    }\n+    val pointStats: Array[(Int, (BV[Double], Long))] =\n+      closest.reduceByKey{mergeContribs}.collectAsMap().toArray\n+\n+    // implement update rule\n+    for (newP <- pointStats) {\n+      // store old count and centroid\n+      val oldCount = counts(newP._1)\n+      val oldCentroid = centers(newP._1).toBreeze\n+      // get new count and centroid\n+      val newCount = newP._2._2\n+      val newCentroid = newP._2._1 / newCount.toDouble\n+      // compute the normalized scale factor that controls forgetting\n+      val decayFactor = units match {\n+        case \"batches\" =>  newCount / (a * oldCount + newCount)\n+        case \"points\" => newCount / (math.pow(a, newCount) * oldCount + newCount)\n+      }\n+      // perform the update\n+      val updatedCentroid = oldCentroid + (newCentroid - oldCentroid) * decayFactor\n+      // store the new counts and centers\n+      counts(newP._1) = oldCount + newCount\n+      centers(newP._1) = Vectors.fromBreeze(updatedCentroid)\n+\n+      // display the updated cluster centers\n+      val display = centers(newP._1).size match {\n+        case x if x > 100 => centers(newP._1).toArray.take(100).mkString(\"[\", \",\", \"...\")\n+        case _ => centers(newP._1).toArray.mkString(\"[\", \",\", \"]\")\n+      }\n+      logInfo(\"Cluster %d updated: %s \".format (newP._1, display))\n+    }\n+    new StreamingKMeansModel(centers, counts)\n+  }\n+\n+}\n+\n+@DeveloperApi\n+class StreamingKMeans(\n+     var k: Int,\n+     var a: Double,\n+     var units: String) extends Logging {\n+\n+  protected var model: StreamingKMeansModel = new StreamingKMeansModel(null, null)\n+\n+  def this() = this(2, 1.0, \"batches\")\n+\n+  /** Set the number of clusters. */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+\n+  /** Set the decay factor directly (for forgetful algorithms). */\n+  def setDecayFactor(a: Double): this.type = {\n+    this.a = a\n+    this\n+  }\n+\n+  /** Set the decay units for forgetful algorithms (\"batches\" or \"points\"). */\n+  def setUnits(units: String): this.type = {\n+    if (units != \"batches\" && units != \"points\") {\n+      throw new IllegalArgumentException(\"Invalid units for decay: \" + units)\n+    }\n+    this.units = units\n+    this\n+  }\n+\n+  /** Set decay fraction in units of batches. */\n+  def setDecayFractionBatches(q: Double): this.type = {\n+    this.a = math.log(1 - q) / math.log(0.5)\n+    this.units = \"batches\"\n+    this\n+  }\n+\n+  /** Set decay fraction in units of points. Must specify expected number of points per batch. */\n+  def setDecayFractionPoints(q: Double, m: Double): this.type = {\n+    this.a = math.pow(math.log(1 - q) / math.log(0.5), 1/m)\n+    this.units = \"points\"\n+    this\n+  }\n+\n+  /** Specify initial explicitly directly. */\n+  def setInitialCenters(initialCenters: Array[Vector]): this.type = {\n+    val clusterCounts = Array.fill(this.k)(0).map(_.toLong)\n+    this.model = new StreamingKMeansModel(initialCenters, clusterCounts)\n+    this\n+  }\n+\n+  /** Initialize random centers, requiring only the number of dimensions. */\n+  def setRandomCenters(d: Int): this.type = {\n+    val initialCenters = (0 until k).map(_ => Vectors.dense(Array.fill(d)(nextGaussian()))).toArray\n+    val clusterCounts = Array.fill(this.k)(0).map(_.toLong)\n+    this.model = new StreamingKMeansModel(initialCenters, clusterCounts)\n+    this\n+  }\n+\n+  /** Return the latest model. */\n+  def latestModel(): StreamingKMeansModel = {\n+    model\n+  }\n+\n+  /**\n+   * Update the clustering model by training on batches of data from a DStream.\n+   * This operation registers a DStream for training the model,\n+   * checks whether the cluster centers have been initialized,\n+   * and updates the model using each batch of data from the stream.\n+   *\n+   * @param data DStream containing vector data\n+   */\n+  def trainOn(data: DStream[Vector]) {\n+    this.isInitialized\n+    data.foreachRDD { (rdd, time) =>\n+      model = model.update(rdd, this.a, this.units)\n+    }\n+  }\n+\n+  /**\n+   * Use the clustering model to make predictions on batches of data from a DStream.\n+   *\n+   * @param data DStream containing vector data\n+   * @return DStream containing predictions\n+   */\n+  def predictOn(data: DStream[Vector]): DStream[Int] = {\n+    this.isInitialized\n+    data.map(model.predict)\n+  }\n+\n+  /**\n+   * Use the model to make predictions on the values of a DStream and carry over its keys.\n+   *\n+   * @param data DStream containing (key, feature vector) pairs\n+   * @tparam K key type\n+   * @return DStream containing the input keys and the predictions as values\n+   */\n+  def predictOnValues[K: ClassTag](data: DStream[(K, Vector)]): DStream[(K, Int)] = {\n+    this.isInitialized\n+    data.mapValues(model.predict)\n+  }\n+\n+  /**\n+   * Check whether cluster centers have been initialized.\n+   *\n+   * @return Boolean, True if cluster centrs have been initialized\n+   */\n+  def isInitialized: Boolean = {\n+    if (Option(model.clusterCenters) == None) {\n+      logError(\"Initial cluster centers must be set before starting predictions\")\n+      throw new IllegalArgumentException"
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "move this import group after scala's imports\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T17:57:31Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}"
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove this empty line to make the real doc show up in the generated doc\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T17:57:33Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *"
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`KMeans` -> `k-means`\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T17:57:45Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm."
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "use JavaDoc for methods\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T17:58:58Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm.\n+ *\n+ * The update algorithm uses the \"mini-batch\" KMeans rule,\n+ * generalized to incorporate forgetfullness (i.e. decay).\n+ * The basic update rule (for each cluster) is:\n+ *\n+ * c_t+1 = [(c_t * n_t) + (x_t * m_t)] / [n_t + m_t]\n+ * n_t+t = n_t + m_t\n+ *\n+ * Where c_t is the previously estimated centroid for that cluster,\n+ * n_t is the number of points assigned to it thus far, x_t is the centroid\n+ * estimated on the current batch, and m_t is the number of points assigned\n+ * to that centroid in the current batch.\n+ *\n+ * This update rule is modified with a decay factor 'a' that scales\n+ * the contribution of the clusters as estimated thus far.\n+ * If a=1, all batches are weighted equally. If a=0, new centroids\n+ * are determined entirely by recent data. Lower values correspond to\n+ * more forgetting.\n+ *\n+ * Decay can optionally be specified as a decay fraction 'q',\n+ * which corresponds to the fraction of batches (or points)\n+ * after which the past will be reduced to a contribution of 0.5.\n+ * This decay fraction can be specified in units of 'points' or 'batches'.\n+ * if 'batches', behavior will be independent of the number of points per batch;\n+ * if 'points', the expected number of points per batch must be specified.\n+ *\n+ * Use a builder pattern to construct a streaming KMeans analysis\n+ * in an application, like:\n+ *\n+ *  val model = new StreamingKMeans()\n+ *    .setDecayFactor(0.5)\n+ *    .setK(3)\n+ *    .setRandomCenters(5)\n+ *    .trainOn(DStream)\n+ *\n+ */\n+@DeveloperApi\n+class StreamingKMeansModel(\n+    override val clusterCenters: Array[Vector],\n+    val clusterCounts: Array[Long]) extends KMeansModel(clusterCenters) with Logging {\n+\n+  // do a sequential KMeans update on a batch of data"
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`{...}` -> `(...)`\n\n`.collectAsMap().toArray` -> `.collect()`\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T17:59:00Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm.\n+ *\n+ * The update algorithm uses the \"mini-batch\" KMeans rule,\n+ * generalized to incorporate forgetfullness (i.e. decay).\n+ * The basic update rule (for each cluster) is:\n+ *\n+ * c_t+1 = [(c_t * n_t) + (x_t * m_t)] / [n_t + m_t]\n+ * n_t+t = n_t + m_t\n+ *\n+ * Where c_t is the previously estimated centroid for that cluster,\n+ * n_t is the number of points assigned to it thus far, x_t is the centroid\n+ * estimated on the current batch, and m_t is the number of points assigned\n+ * to that centroid in the current batch.\n+ *\n+ * This update rule is modified with a decay factor 'a' that scales\n+ * the contribution of the clusters as estimated thus far.\n+ * If a=1, all batches are weighted equally. If a=0, new centroids\n+ * are determined entirely by recent data. Lower values correspond to\n+ * more forgetting.\n+ *\n+ * Decay can optionally be specified as a decay fraction 'q',\n+ * which corresponds to the fraction of batches (or points)\n+ * after which the past will be reduced to a contribution of 0.5.\n+ * This decay fraction can be specified in units of 'points' or 'batches'.\n+ * if 'batches', behavior will be independent of the number of points per batch;\n+ * if 'points', the expected number of points per batch must be specified.\n+ *\n+ * Use a builder pattern to construct a streaming KMeans analysis\n+ * in an application, like:\n+ *\n+ *  val model = new StreamingKMeans()\n+ *    .setDecayFactor(0.5)\n+ *    .setK(3)\n+ *    .setRandomCenters(5)\n+ *    .trainOn(DStream)\n+ *\n+ */\n+@DeveloperApi\n+class StreamingKMeansModel(\n+    override val clusterCenters: Array[Vector],\n+    val clusterCounts: Array[Long]) extends KMeansModel(clusterCenters) with Logging {\n+\n+  // do a sequential KMeans update on a batch of data\n+  def update(data: RDD[Vector], a: Double, units: String): StreamingKMeansModel = {\n+\n+    val centers = clusterCenters\n+    val counts = clusterCounts\n+\n+    // find nearest cluster to each point\n+    val closest = data.map(point => (this.predict(point), (point.toBreeze, 1.toLong)))\n+\n+    // get sums and counts for updating each cluster\n+    type WeightedPoint = (BV[Double], Long)\n+    def mergeContribs(p1: WeightedPoint, p2: WeightedPoint): WeightedPoint = {\n+      (p1._1 += p2._1, p1._2 + p2._2)\n+    }\n+    val pointStats: Array[(Int, (BV[Double], Long))] =\n+      closest.reduceByKey{mergeContribs}.collectAsMap().toArray"
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "More readable with pattern matching:\n\n```\npointStats.foreach { case (pred, (mean, count)) =>\n...\n}\n```\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T17:59:08Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm.\n+ *\n+ * The update algorithm uses the \"mini-batch\" KMeans rule,\n+ * generalized to incorporate forgetfullness (i.e. decay).\n+ * The basic update rule (for each cluster) is:\n+ *\n+ * c_t+1 = [(c_t * n_t) + (x_t * m_t)] / [n_t + m_t]\n+ * n_t+t = n_t + m_t\n+ *\n+ * Where c_t is the previously estimated centroid for that cluster,\n+ * n_t is the number of points assigned to it thus far, x_t is the centroid\n+ * estimated on the current batch, and m_t is the number of points assigned\n+ * to that centroid in the current batch.\n+ *\n+ * This update rule is modified with a decay factor 'a' that scales\n+ * the contribution of the clusters as estimated thus far.\n+ * If a=1, all batches are weighted equally. If a=0, new centroids\n+ * are determined entirely by recent data. Lower values correspond to\n+ * more forgetting.\n+ *\n+ * Decay can optionally be specified as a decay fraction 'q',\n+ * which corresponds to the fraction of batches (or points)\n+ * after which the past will be reduced to a contribution of 0.5.\n+ * This decay fraction can be specified in units of 'points' or 'batches'.\n+ * if 'batches', behavior will be independent of the number of points per batch;\n+ * if 'points', the expected number of points per batch must be specified.\n+ *\n+ * Use a builder pattern to construct a streaming KMeans analysis\n+ * in an application, like:\n+ *\n+ *  val model = new StreamingKMeans()\n+ *    .setDecayFactor(0.5)\n+ *    .setK(3)\n+ *    .setRandomCenters(5)\n+ *    .trainOn(DStream)\n+ *\n+ */\n+@DeveloperApi\n+class StreamingKMeansModel(\n+    override val clusterCenters: Array[Vector],\n+    val clusterCounts: Array[Long]) extends KMeansModel(clusterCenters) with Logging {\n+\n+  // do a sequential KMeans update on a batch of data\n+  def update(data: RDD[Vector], a: Double, units: String): StreamingKMeansModel = {\n+\n+    val centers = clusterCenters\n+    val counts = clusterCounts\n+\n+    // find nearest cluster to each point\n+    val closest = data.map(point => (this.predict(point), (point.toBreeze, 1.toLong)))\n+\n+    // get sums and counts for updating each cluster\n+    type WeightedPoint = (BV[Double], Long)\n+    def mergeContribs(p1: WeightedPoint, p2: WeightedPoint): WeightedPoint = {\n+      (p1._1 += p2._1, p1._2 + p2._2)\n+    }\n+    val pointStats: Array[(Int, (BV[Double], Long))] =\n+      closest.reduceByKey{mergeContribs}.collectAsMap().toArray\n+\n+    // implement update rule\n+    for (newP <- pointStats) {"
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "need doc\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T17:59:13Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm.\n+ *\n+ * The update algorithm uses the \"mini-batch\" KMeans rule,\n+ * generalized to incorporate forgetfullness (i.e. decay).\n+ * The basic update rule (for each cluster) is:\n+ *\n+ * c_t+1 = [(c_t * n_t) + (x_t * m_t)] / [n_t + m_t]\n+ * n_t+t = n_t + m_t\n+ *\n+ * Where c_t is the previously estimated centroid for that cluster,\n+ * n_t is the number of points assigned to it thus far, x_t is the centroid\n+ * estimated on the current batch, and m_t is the number of points assigned\n+ * to that centroid in the current batch.\n+ *\n+ * This update rule is modified with a decay factor 'a' that scales\n+ * the contribution of the clusters as estimated thus far.\n+ * If a=1, all batches are weighted equally. If a=0, new centroids\n+ * are determined entirely by recent data. Lower values correspond to\n+ * more forgetting.\n+ *\n+ * Decay can optionally be specified as a decay fraction 'q',\n+ * which corresponds to the fraction of batches (or points)\n+ * after which the past will be reduced to a contribution of 0.5.\n+ * This decay fraction can be specified in units of 'points' or 'batches'.\n+ * if 'batches', behavior will be independent of the number of points per batch;\n+ * if 'points', the expected number of points per batch must be specified.\n+ *\n+ * Use a builder pattern to construct a streaming KMeans analysis\n+ * in an application, like:\n+ *\n+ *  val model = new StreamingKMeans()\n+ *    .setDecayFactor(0.5)\n+ *    .setK(3)\n+ *    .setRandomCenters(5)\n+ *    .trainOn(DStream)\n+ *\n+ */\n+@DeveloperApi\n+class StreamingKMeansModel(\n+    override val clusterCenters: Array[Vector],\n+    val clusterCounts: Array[Long]) extends KMeansModel(clusterCenters) with Logging {\n+\n+  // do a sequential KMeans update on a batch of data\n+  def update(data: RDD[Vector], a: Double, units: String): StreamingKMeansModel = {\n+\n+    val centers = clusterCenters\n+    val counts = clusterCounts\n+\n+    // find nearest cluster to each point\n+    val closest = data.map(point => (this.predict(point), (point.toBreeze, 1.toLong)))\n+\n+    // get sums and counts for updating each cluster\n+    type WeightedPoint = (BV[Double], Long)\n+    def mergeContribs(p1: WeightedPoint, p2: WeightedPoint): WeightedPoint = {\n+      (p1._1 += p2._1, p1._2 + p2._2)\n+    }\n+    val pointStats: Array[(Int, (BV[Double], Long))] =\n+      closest.reduceByKey{mergeContribs}.collectAsMap().toArray\n+\n+    // implement update rule\n+    for (newP <- pointStats) {\n+      // store old count and centroid\n+      val oldCount = counts(newP._1)\n+      val oldCentroid = centers(newP._1).toBreeze\n+      // get new count and centroid\n+      val newCount = newP._2._2\n+      val newCentroid = newP._2._1 / newCount.toDouble\n+      // compute the normalized scale factor that controls forgetting\n+      val decayFactor = units match {\n+        case \"batches\" =>  newCount / (a * oldCount + newCount)\n+        case \"points\" => newCount / (math.pow(a, newCount) * oldCount + newCount)\n+      }\n+      // perform the update\n+      val updatedCentroid = oldCentroid + (newCentroid - oldCentroid) * decayFactor\n+      // store the new counts and centers\n+      counts(newP._1) = oldCount + newCount\n+      centers(newP._1) = Vectors.fromBreeze(updatedCentroid)\n+\n+      // display the updated cluster centers\n+      val display = centers(newP._1).size match {\n+        case x if x > 100 => centers(newP._1).toArray.take(100).mkString(\"[\", \",\", \"...\")\n+        case _ => centers(newP._1).toArray.mkString(\"[\", \",\", \"]\")\n+      }\n+      logInfo(\"Cluster %d updated: %s \".format (newP._1, display))\n+    }\n+    new StreamingKMeansModel(centers, counts)\n+  }\n+\n+}\n+\n+@DeveloperApi\n+class StreamingKMeans("
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "use a better name, e.g., `decayFactor`?\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T17:59:17Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm.\n+ *\n+ * The update algorithm uses the \"mini-batch\" KMeans rule,\n+ * generalized to incorporate forgetfullness (i.e. decay).\n+ * The basic update rule (for each cluster) is:\n+ *\n+ * c_t+1 = [(c_t * n_t) + (x_t * m_t)] / [n_t + m_t]\n+ * n_t+t = n_t + m_t\n+ *\n+ * Where c_t is the previously estimated centroid for that cluster,\n+ * n_t is the number of points assigned to it thus far, x_t is the centroid\n+ * estimated on the current batch, and m_t is the number of points assigned\n+ * to that centroid in the current batch.\n+ *\n+ * This update rule is modified with a decay factor 'a' that scales\n+ * the contribution of the clusters as estimated thus far.\n+ * If a=1, all batches are weighted equally. If a=0, new centroids\n+ * are determined entirely by recent data. Lower values correspond to\n+ * more forgetting.\n+ *\n+ * Decay can optionally be specified as a decay fraction 'q',\n+ * which corresponds to the fraction of batches (or points)\n+ * after which the past will be reduced to a contribution of 0.5.\n+ * This decay fraction can be specified in units of 'points' or 'batches'.\n+ * if 'batches', behavior will be independent of the number of points per batch;\n+ * if 'points', the expected number of points per batch must be specified.\n+ *\n+ * Use a builder pattern to construct a streaming KMeans analysis\n+ * in an application, like:\n+ *\n+ *  val model = new StreamingKMeans()\n+ *    .setDecayFactor(0.5)\n+ *    .setK(3)\n+ *    .setRandomCenters(5)\n+ *    .trainOn(DStream)\n+ *\n+ */\n+@DeveloperApi\n+class StreamingKMeansModel(\n+    override val clusterCenters: Array[Vector],\n+    val clusterCounts: Array[Long]) extends KMeansModel(clusterCenters) with Logging {\n+\n+  // do a sequential KMeans update on a batch of data\n+  def update(data: RDD[Vector], a: Double, units: String): StreamingKMeansModel = {\n+\n+    val centers = clusterCenters\n+    val counts = clusterCounts\n+\n+    // find nearest cluster to each point\n+    val closest = data.map(point => (this.predict(point), (point.toBreeze, 1.toLong)))\n+\n+    // get sums and counts for updating each cluster\n+    type WeightedPoint = (BV[Double], Long)\n+    def mergeContribs(p1: WeightedPoint, p2: WeightedPoint): WeightedPoint = {\n+      (p1._1 += p2._1, p1._2 + p2._2)\n+    }\n+    val pointStats: Array[(Int, (BV[Double], Long))] =\n+      closest.reduceByKey{mergeContribs}.collectAsMap().toArray\n+\n+    // implement update rule\n+    for (newP <- pointStats) {\n+      // store old count and centroid\n+      val oldCount = counts(newP._1)\n+      val oldCentroid = centers(newP._1).toBreeze\n+      // get new count and centroid\n+      val newCount = newP._2._2\n+      val newCentroid = newP._2._1 / newCount.toDouble\n+      // compute the normalized scale factor that controls forgetting\n+      val decayFactor = units match {\n+        case \"batches\" =>  newCount / (a * oldCount + newCount)\n+        case \"points\" => newCount / (math.pow(a, newCount) * oldCount + newCount)\n+      }\n+      // perform the update\n+      val updatedCentroid = oldCentroid + (newCentroid - oldCentroid) * decayFactor\n+      // store the new counts and centers\n+      counts(newP._1) = oldCount + newCount\n+      centers(newP._1) = Vectors.fromBreeze(updatedCentroid)\n+\n+      // display the updated cluster centers\n+      val display = centers(newP._1).size match {\n+        case x if x > 100 => centers(newP._1).toArray.take(100).mkString(\"[\", \",\", \"...\")\n+        case _ => centers(newP._1).toArray.mkString(\"[\", \",\", \"]\")\n+      }\n+      logInfo(\"Cluster %d updated: %s \".format (newP._1, display))\n+    }\n+    new StreamingKMeansModel(centers, counts)\n+  }\n+\n+}\n+\n+@DeveloperApi\n+class StreamingKMeans(\n+     var k: Int,\n+     var a: Double,"
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "It is not clear from the name that `units` is to control the decay. I don't have good suggestions, maybe `timeUnit`, which takes either `\"batch\"` or `\"point\"`.\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T17:59:41Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm.\n+ *\n+ * The update algorithm uses the \"mini-batch\" KMeans rule,\n+ * generalized to incorporate forgetfullness (i.e. decay).\n+ * The basic update rule (for each cluster) is:\n+ *\n+ * c_t+1 = [(c_t * n_t) + (x_t * m_t)] / [n_t + m_t]\n+ * n_t+t = n_t + m_t\n+ *\n+ * Where c_t is the previously estimated centroid for that cluster,\n+ * n_t is the number of points assigned to it thus far, x_t is the centroid\n+ * estimated on the current batch, and m_t is the number of points assigned\n+ * to that centroid in the current batch.\n+ *\n+ * This update rule is modified with a decay factor 'a' that scales\n+ * the contribution of the clusters as estimated thus far.\n+ * If a=1, all batches are weighted equally. If a=0, new centroids\n+ * are determined entirely by recent data. Lower values correspond to\n+ * more forgetting.\n+ *\n+ * Decay can optionally be specified as a decay fraction 'q',\n+ * which corresponds to the fraction of batches (or points)\n+ * after which the past will be reduced to a contribution of 0.5.\n+ * This decay fraction can be specified in units of 'points' or 'batches'.\n+ * if 'batches', behavior will be independent of the number of points per batch;\n+ * if 'points', the expected number of points per batch must be specified.\n+ *\n+ * Use a builder pattern to construct a streaming KMeans analysis\n+ * in an application, like:\n+ *\n+ *  val model = new StreamingKMeans()\n+ *    .setDecayFactor(0.5)\n+ *    .setK(3)\n+ *    .setRandomCenters(5)\n+ *    .trainOn(DStream)\n+ *\n+ */\n+@DeveloperApi\n+class StreamingKMeansModel(\n+    override val clusterCenters: Array[Vector],\n+    val clusterCounts: Array[Long]) extends KMeansModel(clusterCenters) with Logging {\n+\n+  // do a sequential KMeans update on a batch of data\n+  def update(data: RDD[Vector], a: Double, units: String): StreamingKMeansModel = {\n+\n+    val centers = clusterCenters\n+    val counts = clusterCounts\n+\n+    // find nearest cluster to each point\n+    val closest = data.map(point => (this.predict(point), (point.toBreeze, 1.toLong)))\n+\n+    // get sums and counts for updating each cluster\n+    type WeightedPoint = (BV[Double], Long)\n+    def mergeContribs(p1: WeightedPoint, p2: WeightedPoint): WeightedPoint = {\n+      (p1._1 += p2._1, p1._2 + p2._2)\n+    }\n+    val pointStats: Array[(Int, (BV[Double], Long))] =\n+      closest.reduceByKey{mergeContribs}.collectAsMap().toArray\n+\n+    // implement update rule\n+    for (newP <- pointStats) {\n+      // store old count and centroid\n+      val oldCount = counts(newP._1)\n+      val oldCentroid = centers(newP._1).toBreeze\n+      // get new count and centroid\n+      val newCount = newP._2._2\n+      val newCentroid = newP._2._1 / newCount.toDouble\n+      // compute the normalized scale factor that controls forgetting\n+      val decayFactor = units match {\n+        case \"batches\" =>  newCount / (a * oldCount + newCount)\n+        case \"points\" => newCount / (math.pow(a, newCount) * oldCount + newCount)\n+      }\n+      // perform the update\n+      val updatedCentroid = oldCentroid + (newCentroid - oldCentroid) * decayFactor\n+      // store the new counts and centers\n+      counts(newP._1) = oldCount + newCount\n+      centers(newP._1) = Vectors.fromBreeze(updatedCentroid)\n+\n+      // display the updated cluster centers\n+      val display = centers(newP._1).size match {\n+        case x if x > 100 => centers(newP._1).toArray.take(100).mkString(\"[\", \",\", \"...\")\n+        case _ => centers(newP._1).toArray.mkString(\"[\", \",\", \"]\")\n+      }\n+      logInfo(\"Cluster %d updated: %s \".format (newP._1, display))\n+    }\n+    new StreamingKMeansModel(centers, counts)\n+  }\n+\n+}\n+\n+@DeveloperApi\n+class StreamingKMeans(\n+     var k: Int,\n+     var a: Double,\n+     var units: String) extends Logging {"
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "It is worth noting that this overwrites `decayFactor` and `units`.\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T17:59:44Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm.\n+ *\n+ * The update algorithm uses the \"mini-batch\" KMeans rule,\n+ * generalized to incorporate forgetfullness (i.e. decay).\n+ * The basic update rule (for each cluster) is:\n+ *\n+ * c_t+1 = [(c_t * n_t) + (x_t * m_t)] / [n_t + m_t]\n+ * n_t+t = n_t + m_t\n+ *\n+ * Where c_t is the previously estimated centroid for that cluster,\n+ * n_t is the number of points assigned to it thus far, x_t is the centroid\n+ * estimated on the current batch, and m_t is the number of points assigned\n+ * to that centroid in the current batch.\n+ *\n+ * This update rule is modified with a decay factor 'a' that scales\n+ * the contribution of the clusters as estimated thus far.\n+ * If a=1, all batches are weighted equally. If a=0, new centroids\n+ * are determined entirely by recent data. Lower values correspond to\n+ * more forgetting.\n+ *\n+ * Decay can optionally be specified as a decay fraction 'q',\n+ * which corresponds to the fraction of batches (or points)\n+ * after which the past will be reduced to a contribution of 0.5.\n+ * This decay fraction can be specified in units of 'points' or 'batches'.\n+ * if 'batches', behavior will be independent of the number of points per batch;\n+ * if 'points', the expected number of points per batch must be specified.\n+ *\n+ * Use a builder pattern to construct a streaming KMeans analysis\n+ * in an application, like:\n+ *\n+ *  val model = new StreamingKMeans()\n+ *    .setDecayFactor(0.5)\n+ *    .setK(3)\n+ *    .setRandomCenters(5)\n+ *    .trainOn(DStream)\n+ *\n+ */\n+@DeveloperApi\n+class StreamingKMeansModel(\n+    override val clusterCenters: Array[Vector],\n+    val clusterCounts: Array[Long]) extends KMeansModel(clusterCenters) with Logging {\n+\n+  // do a sequential KMeans update on a batch of data\n+  def update(data: RDD[Vector], a: Double, units: String): StreamingKMeansModel = {\n+\n+    val centers = clusterCenters\n+    val counts = clusterCounts\n+\n+    // find nearest cluster to each point\n+    val closest = data.map(point => (this.predict(point), (point.toBreeze, 1.toLong)))\n+\n+    // get sums and counts for updating each cluster\n+    type WeightedPoint = (BV[Double], Long)\n+    def mergeContribs(p1: WeightedPoint, p2: WeightedPoint): WeightedPoint = {\n+      (p1._1 += p2._1, p1._2 + p2._2)\n+    }\n+    val pointStats: Array[(Int, (BV[Double], Long))] =\n+      closest.reduceByKey{mergeContribs}.collectAsMap().toArray\n+\n+    // implement update rule\n+    for (newP <- pointStats) {\n+      // store old count and centroid\n+      val oldCount = counts(newP._1)\n+      val oldCentroid = centers(newP._1).toBreeze\n+      // get new count and centroid\n+      val newCount = newP._2._2\n+      val newCentroid = newP._2._1 / newCount.toDouble\n+      // compute the normalized scale factor that controls forgetting\n+      val decayFactor = units match {\n+        case \"batches\" =>  newCount / (a * oldCount + newCount)\n+        case \"points\" => newCount / (math.pow(a, newCount) * oldCount + newCount)\n+      }\n+      // perform the update\n+      val updatedCentroid = oldCentroid + (newCentroid - oldCentroid) * decayFactor\n+      // store the new counts and centers\n+      counts(newP._1) = oldCount + newCount\n+      centers(newP._1) = Vectors.fromBreeze(updatedCentroid)\n+\n+      // display the updated cluster centers\n+      val display = centers(newP._1).size match {\n+        case x if x > 100 => centers(newP._1).toArray.take(100).mkString(\"[\", \",\", \"...\")\n+        case _ => centers(newP._1).toArray.mkString(\"[\", \",\", \"]\")\n+      }\n+      logInfo(\"Cluster %d updated: %s \".format (newP._1, display))\n+    }\n+    new StreamingKMeansModel(centers, counts)\n+  }\n+\n+}\n+\n+@DeveloperApi\n+class StreamingKMeans(\n+     var k: Int,\n+     var a: Double,\n+     var units: String) extends Logging {\n+\n+  protected var model: StreamingKMeansModel = new StreamingKMeansModel(null, null)\n+\n+  def this() = this(2, 1.0, \"batches\")\n+\n+  /** Set the number of clusters. */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+\n+  /** Set the decay factor directly (for forgetful algorithms). */\n+  def setDecayFactor(a: Double): this.type = {\n+    this.a = a\n+    this\n+  }\n+\n+  /** Set the decay units for forgetful algorithms (\"batches\" or \"points\"). */\n+  def setUnits(units: String): this.type = {\n+    if (units != \"batches\" && units != \"points\") {\n+      throw new IllegalArgumentException(\"Invalid units for decay: \" + units)\n+    }\n+    this.units = units\n+    this\n+  }\n+\n+  /** Set decay fraction in units of batches. */"
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "ditto: this overwrites `decayFactor` and `units`\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T18:00:26Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm.\n+ *\n+ * The update algorithm uses the \"mini-batch\" KMeans rule,\n+ * generalized to incorporate forgetfullness (i.e. decay).\n+ * The basic update rule (for each cluster) is:\n+ *\n+ * c_t+1 = [(c_t * n_t) + (x_t * m_t)] / [n_t + m_t]\n+ * n_t+t = n_t + m_t\n+ *\n+ * Where c_t is the previously estimated centroid for that cluster,\n+ * n_t is the number of points assigned to it thus far, x_t is the centroid\n+ * estimated on the current batch, and m_t is the number of points assigned\n+ * to that centroid in the current batch.\n+ *\n+ * This update rule is modified with a decay factor 'a' that scales\n+ * the contribution of the clusters as estimated thus far.\n+ * If a=1, all batches are weighted equally. If a=0, new centroids\n+ * are determined entirely by recent data. Lower values correspond to\n+ * more forgetting.\n+ *\n+ * Decay can optionally be specified as a decay fraction 'q',\n+ * which corresponds to the fraction of batches (or points)\n+ * after which the past will be reduced to a contribution of 0.5.\n+ * This decay fraction can be specified in units of 'points' or 'batches'.\n+ * if 'batches', behavior will be independent of the number of points per batch;\n+ * if 'points', the expected number of points per batch must be specified.\n+ *\n+ * Use a builder pattern to construct a streaming KMeans analysis\n+ * in an application, like:\n+ *\n+ *  val model = new StreamingKMeans()\n+ *    .setDecayFactor(0.5)\n+ *    .setK(3)\n+ *    .setRandomCenters(5)\n+ *    .trainOn(DStream)\n+ *\n+ */\n+@DeveloperApi\n+class StreamingKMeansModel(\n+    override val clusterCenters: Array[Vector],\n+    val clusterCounts: Array[Long]) extends KMeansModel(clusterCenters) with Logging {\n+\n+  // do a sequential KMeans update on a batch of data\n+  def update(data: RDD[Vector], a: Double, units: String): StreamingKMeansModel = {\n+\n+    val centers = clusterCenters\n+    val counts = clusterCounts\n+\n+    // find nearest cluster to each point\n+    val closest = data.map(point => (this.predict(point), (point.toBreeze, 1.toLong)))\n+\n+    // get sums and counts for updating each cluster\n+    type WeightedPoint = (BV[Double], Long)\n+    def mergeContribs(p1: WeightedPoint, p2: WeightedPoint): WeightedPoint = {\n+      (p1._1 += p2._1, p1._2 + p2._2)\n+    }\n+    val pointStats: Array[(Int, (BV[Double], Long))] =\n+      closest.reduceByKey{mergeContribs}.collectAsMap().toArray\n+\n+    // implement update rule\n+    for (newP <- pointStats) {\n+      // store old count and centroid\n+      val oldCount = counts(newP._1)\n+      val oldCentroid = centers(newP._1).toBreeze\n+      // get new count and centroid\n+      val newCount = newP._2._2\n+      val newCentroid = newP._2._1 / newCount.toDouble\n+      // compute the normalized scale factor that controls forgetting\n+      val decayFactor = units match {\n+        case \"batches\" =>  newCount / (a * oldCount + newCount)\n+        case \"points\" => newCount / (math.pow(a, newCount) * oldCount + newCount)\n+      }\n+      // perform the update\n+      val updatedCentroid = oldCentroid + (newCentroid - oldCentroid) * decayFactor\n+      // store the new counts and centers\n+      counts(newP._1) = oldCount + newCount\n+      centers(newP._1) = Vectors.fromBreeze(updatedCentroid)\n+\n+      // display the updated cluster centers\n+      val display = centers(newP._1).size match {\n+        case x if x > 100 => centers(newP._1).toArray.take(100).mkString(\"[\", \",\", \"...\")\n+        case _ => centers(newP._1).toArray.mkString(\"[\", \",\", \"]\")\n+      }\n+      logInfo(\"Cluster %d updated: %s \".format (newP._1, display))\n+    }\n+    new StreamingKMeansModel(centers, counts)\n+  }\n+\n+}\n+\n+@DeveloperApi\n+class StreamingKMeans(\n+     var k: Int,\n+     var a: Double,\n+     var units: String) extends Logging {\n+\n+  protected var model: StreamingKMeansModel = new StreamingKMeansModel(null, null)\n+\n+  def this() = this(2, 1.0, \"batches\")\n+\n+  /** Set the number of clusters. */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+\n+  /** Set the decay factor directly (for forgetful algorithms). */\n+  def setDecayFactor(a: Double): this.type = {\n+    this.a = a\n+    this\n+  }\n+\n+  /** Set the decay units for forgetful algorithms (\"batches\" or \"points\"). */\n+  def setUnits(units: String): this.type = {\n+    if (units != \"batches\" && units != \"points\") {\n+      throw new IllegalArgumentException(\"Invalid units for decay: \" + units)\n+    }\n+    this.units = units\n+    this\n+  }\n+\n+  /** Set decay fraction in units of batches. */\n+  def setDecayFractionBatches(q: Double): this.type = {\n+    this.a = math.log(1 - q) / math.log(0.5)\n+    this.units = \"batches\"\n+    this\n+  }\n+\n+  /** Set decay fraction in units of points. Must specify expected number of points per batch. */\n+  def setDecayFractionPoints(q: Double, m: Double): this.type = {"
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "We should add `seed` as an argument. This method needs `@param` in the doc or change `d` to `dim`.\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T18:20:32Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm.\n+ *\n+ * The update algorithm uses the \"mini-batch\" KMeans rule,\n+ * generalized to incorporate forgetfullness (i.e. decay).\n+ * The basic update rule (for each cluster) is:\n+ *\n+ * c_t+1 = [(c_t * n_t) + (x_t * m_t)] / [n_t + m_t]\n+ * n_t+t = n_t + m_t\n+ *\n+ * Where c_t is the previously estimated centroid for that cluster,\n+ * n_t is the number of points assigned to it thus far, x_t is the centroid\n+ * estimated on the current batch, and m_t is the number of points assigned\n+ * to that centroid in the current batch.\n+ *\n+ * This update rule is modified with a decay factor 'a' that scales\n+ * the contribution of the clusters as estimated thus far.\n+ * If a=1, all batches are weighted equally. If a=0, new centroids\n+ * are determined entirely by recent data. Lower values correspond to\n+ * more forgetting.\n+ *\n+ * Decay can optionally be specified as a decay fraction 'q',\n+ * which corresponds to the fraction of batches (or points)\n+ * after which the past will be reduced to a contribution of 0.5.\n+ * This decay fraction can be specified in units of 'points' or 'batches'.\n+ * if 'batches', behavior will be independent of the number of points per batch;\n+ * if 'points', the expected number of points per batch must be specified.\n+ *\n+ * Use a builder pattern to construct a streaming KMeans analysis\n+ * in an application, like:\n+ *\n+ *  val model = new StreamingKMeans()\n+ *    .setDecayFactor(0.5)\n+ *    .setK(3)\n+ *    .setRandomCenters(5)\n+ *    .trainOn(DStream)\n+ *\n+ */\n+@DeveloperApi\n+class StreamingKMeansModel(\n+    override val clusterCenters: Array[Vector],\n+    val clusterCounts: Array[Long]) extends KMeansModel(clusterCenters) with Logging {\n+\n+  // do a sequential KMeans update on a batch of data\n+  def update(data: RDD[Vector], a: Double, units: String): StreamingKMeansModel = {\n+\n+    val centers = clusterCenters\n+    val counts = clusterCounts\n+\n+    // find nearest cluster to each point\n+    val closest = data.map(point => (this.predict(point), (point.toBreeze, 1.toLong)))\n+\n+    // get sums and counts for updating each cluster\n+    type WeightedPoint = (BV[Double], Long)\n+    def mergeContribs(p1: WeightedPoint, p2: WeightedPoint): WeightedPoint = {\n+      (p1._1 += p2._1, p1._2 + p2._2)\n+    }\n+    val pointStats: Array[(Int, (BV[Double], Long))] =\n+      closest.reduceByKey{mergeContribs}.collectAsMap().toArray\n+\n+    // implement update rule\n+    for (newP <- pointStats) {\n+      // store old count and centroid\n+      val oldCount = counts(newP._1)\n+      val oldCentroid = centers(newP._1).toBreeze\n+      // get new count and centroid\n+      val newCount = newP._2._2\n+      val newCentroid = newP._2._1 / newCount.toDouble\n+      // compute the normalized scale factor that controls forgetting\n+      val decayFactor = units match {\n+        case \"batches\" =>  newCount / (a * oldCount + newCount)\n+        case \"points\" => newCount / (math.pow(a, newCount) * oldCount + newCount)\n+      }\n+      // perform the update\n+      val updatedCentroid = oldCentroid + (newCentroid - oldCentroid) * decayFactor\n+      // store the new counts and centers\n+      counts(newP._1) = oldCount + newCount\n+      centers(newP._1) = Vectors.fromBreeze(updatedCentroid)\n+\n+      // display the updated cluster centers\n+      val display = centers(newP._1).size match {\n+        case x if x > 100 => centers(newP._1).toArray.take(100).mkString(\"[\", \",\", \"...\")\n+        case _ => centers(newP._1).toArray.mkString(\"[\", \",\", \"]\")\n+      }\n+      logInfo(\"Cluster %d updated: %s \".format (newP._1, display))\n+    }\n+    new StreamingKMeansModel(centers, counts)\n+  }\n+\n+}\n+\n+@DeveloperApi\n+class StreamingKMeans(\n+     var k: Int,\n+     var a: Double,\n+     var units: String) extends Logging {\n+\n+  protected var model: StreamingKMeansModel = new StreamingKMeansModel(null, null)\n+\n+  def this() = this(2, 1.0, \"batches\")\n+\n+  /** Set the number of clusters. */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+\n+  /** Set the decay factor directly (for forgetful algorithms). */\n+  def setDecayFactor(a: Double): this.type = {\n+    this.a = a\n+    this\n+  }\n+\n+  /** Set the decay units for forgetful algorithms (\"batches\" or \"points\"). */\n+  def setUnits(units: String): this.type = {\n+    if (units != \"batches\" && units != \"points\") {\n+      throw new IllegalArgumentException(\"Invalid units for decay: \" + units)\n+    }\n+    this.units = units\n+    this\n+  }\n+\n+  /** Set decay fraction in units of batches. */\n+  def setDecayFractionBatches(q: Double): this.type = {\n+    this.a = math.log(1 - q) / math.log(0.5)\n+    this.units = \"batches\"\n+    this\n+  }\n+\n+  /** Set decay fraction in units of points. Must specify expected number of points per batch. */\n+  def setDecayFractionPoints(q: Double, m: Double): this.type = {\n+    this.a = math.pow(math.log(1 - q) / math.log(0.5), 1/m)\n+    this.units = \"points\"\n+    this\n+  }\n+\n+  /** Specify initial explicitly directly. */\n+  def setInitialCenters(initialCenters: Array[Vector]): this.type = {\n+    val clusterCounts = Array.fill(this.k)(0).map(_.toLong)\n+    this.model = new StreamingKMeansModel(initialCenters, clusterCounts)\n+    this\n+  }\n+\n+  /** Initialize random centers, requiring only the number of dimensions. */\n+  def setRandomCenters(d: Int): this.type = {"
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`new Array[Long](this.k)`\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T18:20:35Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm.\n+ *\n+ * The update algorithm uses the \"mini-batch\" KMeans rule,\n+ * generalized to incorporate forgetfullness (i.e. decay).\n+ * The basic update rule (for each cluster) is:\n+ *\n+ * c_t+1 = [(c_t * n_t) + (x_t * m_t)] / [n_t + m_t]\n+ * n_t+t = n_t + m_t\n+ *\n+ * Where c_t is the previously estimated centroid for that cluster,\n+ * n_t is the number of points assigned to it thus far, x_t is the centroid\n+ * estimated on the current batch, and m_t is the number of points assigned\n+ * to that centroid in the current batch.\n+ *\n+ * This update rule is modified with a decay factor 'a' that scales\n+ * the contribution of the clusters as estimated thus far.\n+ * If a=1, all batches are weighted equally. If a=0, new centroids\n+ * are determined entirely by recent data. Lower values correspond to\n+ * more forgetting.\n+ *\n+ * Decay can optionally be specified as a decay fraction 'q',\n+ * which corresponds to the fraction of batches (or points)\n+ * after which the past will be reduced to a contribution of 0.5.\n+ * This decay fraction can be specified in units of 'points' or 'batches'.\n+ * if 'batches', behavior will be independent of the number of points per batch;\n+ * if 'points', the expected number of points per batch must be specified.\n+ *\n+ * Use a builder pattern to construct a streaming KMeans analysis\n+ * in an application, like:\n+ *\n+ *  val model = new StreamingKMeans()\n+ *    .setDecayFactor(0.5)\n+ *    .setK(3)\n+ *    .setRandomCenters(5)\n+ *    .trainOn(DStream)\n+ *\n+ */\n+@DeveloperApi\n+class StreamingKMeansModel(\n+    override val clusterCenters: Array[Vector],\n+    val clusterCounts: Array[Long]) extends KMeansModel(clusterCenters) with Logging {\n+\n+  // do a sequential KMeans update on a batch of data\n+  def update(data: RDD[Vector], a: Double, units: String): StreamingKMeansModel = {\n+\n+    val centers = clusterCenters\n+    val counts = clusterCounts\n+\n+    // find nearest cluster to each point\n+    val closest = data.map(point => (this.predict(point), (point.toBreeze, 1.toLong)))\n+\n+    // get sums and counts for updating each cluster\n+    type WeightedPoint = (BV[Double], Long)\n+    def mergeContribs(p1: WeightedPoint, p2: WeightedPoint): WeightedPoint = {\n+      (p1._1 += p2._1, p1._2 + p2._2)\n+    }\n+    val pointStats: Array[(Int, (BV[Double], Long))] =\n+      closest.reduceByKey{mergeContribs}.collectAsMap().toArray\n+\n+    // implement update rule\n+    for (newP <- pointStats) {\n+      // store old count and centroid\n+      val oldCount = counts(newP._1)\n+      val oldCentroid = centers(newP._1).toBreeze\n+      // get new count and centroid\n+      val newCount = newP._2._2\n+      val newCentroid = newP._2._1 / newCount.toDouble\n+      // compute the normalized scale factor that controls forgetting\n+      val decayFactor = units match {\n+        case \"batches\" =>  newCount / (a * oldCount + newCount)\n+        case \"points\" => newCount / (math.pow(a, newCount) * oldCount + newCount)\n+      }\n+      // perform the update\n+      val updatedCentroid = oldCentroid + (newCentroid - oldCentroid) * decayFactor\n+      // store the new counts and centers\n+      counts(newP._1) = oldCount + newCount\n+      centers(newP._1) = Vectors.fromBreeze(updatedCentroid)\n+\n+      // display the updated cluster centers\n+      val display = centers(newP._1).size match {\n+        case x if x > 100 => centers(newP._1).toArray.take(100).mkString(\"[\", \",\", \"...\")\n+        case _ => centers(newP._1).toArray.mkString(\"[\", \",\", \"]\")\n+      }\n+      logInfo(\"Cluster %d updated: %s \".format (newP._1, display))\n+    }\n+    new StreamingKMeansModel(centers, counts)\n+  }\n+\n+}\n+\n+@DeveloperApi\n+class StreamingKMeans(\n+     var k: Int,\n+     var a: Double,\n+     var units: String) extends Logging {\n+\n+  protected var model: StreamingKMeansModel = new StreamingKMeansModel(null, null)\n+\n+  def this() = this(2, 1.0, \"batches\")\n+\n+  /** Set the number of clusters. */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+\n+  /** Set the decay factor directly (for forgetful algorithms). */\n+  def setDecayFactor(a: Double): this.type = {\n+    this.a = a\n+    this\n+  }\n+\n+  /** Set the decay units for forgetful algorithms (\"batches\" or \"points\"). */\n+  def setUnits(units: String): this.type = {\n+    if (units != \"batches\" && units != \"points\") {\n+      throw new IllegalArgumentException(\"Invalid units for decay: \" + units)\n+    }\n+    this.units = units\n+    this\n+  }\n+\n+  /** Set decay fraction in units of batches. */\n+  def setDecayFractionBatches(q: Double): this.type = {\n+    this.a = math.log(1 - q) / math.log(0.5)\n+    this.units = \"batches\"\n+    this\n+  }\n+\n+  /** Set decay fraction in units of points. Must specify expected number of points per batch. */\n+  def setDecayFractionPoints(q: Double, m: Double): this.type = {\n+    this.a = math.pow(math.log(1 - q) / math.log(0.5), 1/m)\n+    this.units = \"points\"\n+    this\n+  }\n+\n+  /** Specify initial explicitly directly. */\n+  def setInitialCenters(initialCenters: Array[Vector]): this.type = {\n+    val clusterCounts = Array.fill(this.k)(0).map(_.toLong)\n+    this.model = new StreamingKMeansModel(initialCenters, clusterCounts)\n+    this\n+  }\n+\n+  /** Initialize random centers, requiring only the number of dimensions. */\n+  def setRandomCenters(d: Int): this.type = {\n+    val initialCenters = (0 until k).map(_ => Vectors.dense(Array.fill(d)(nextGaussian()))).toArray\n+    val clusterCounts = Array.fill(this.k)(0).map(_.toLong)"
  }],
  "prId": 2942
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Usually, methods with name like `isAbc`do not throw an exception. Shall we rename it to `assertInitialized` and return `Unit`.\n",
    "commit": "b2e5b4a167e0e5835f3518d2b68e4063c3f9c955",
    "createdAt": "2014-10-28T18:21:19Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import scala.reflect.ClassTag\n+import scala.util.Random._\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.Logging\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.StreamingContext._\n+\n+/**\n+ * :: DeveloperApi ::\n+ *\n+ * StreamingKMeansModel extends MLlib's KMeansModel for streaming\n+ * algorithms, so it can keep track of the number of points assigned\n+ * to each cluster, and also update the model by doing a single iteration\n+ * of the standard KMeans algorithm.\n+ *\n+ * The update algorithm uses the \"mini-batch\" KMeans rule,\n+ * generalized to incorporate forgetfullness (i.e. decay).\n+ * The basic update rule (for each cluster) is:\n+ *\n+ * c_t+1 = [(c_t * n_t) + (x_t * m_t)] / [n_t + m_t]\n+ * n_t+t = n_t + m_t\n+ *\n+ * Where c_t is the previously estimated centroid for that cluster,\n+ * n_t is the number of points assigned to it thus far, x_t is the centroid\n+ * estimated on the current batch, and m_t is the number of points assigned\n+ * to that centroid in the current batch.\n+ *\n+ * This update rule is modified with a decay factor 'a' that scales\n+ * the contribution of the clusters as estimated thus far.\n+ * If a=1, all batches are weighted equally. If a=0, new centroids\n+ * are determined entirely by recent data. Lower values correspond to\n+ * more forgetting.\n+ *\n+ * Decay can optionally be specified as a decay fraction 'q',\n+ * which corresponds to the fraction of batches (or points)\n+ * after which the past will be reduced to a contribution of 0.5.\n+ * This decay fraction can be specified in units of 'points' or 'batches'.\n+ * if 'batches', behavior will be independent of the number of points per batch;\n+ * if 'points', the expected number of points per batch must be specified.\n+ *\n+ * Use a builder pattern to construct a streaming KMeans analysis\n+ * in an application, like:\n+ *\n+ *  val model = new StreamingKMeans()\n+ *    .setDecayFactor(0.5)\n+ *    .setK(3)\n+ *    .setRandomCenters(5)\n+ *    .trainOn(DStream)\n+ *\n+ */\n+@DeveloperApi\n+class StreamingKMeansModel(\n+    override val clusterCenters: Array[Vector],\n+    val clusterCounts: Array[Long]) extends KMeansModel(clusterCenters) with Logging {\n+\n+  // do a sequential KMeans update on a batch of data\n+  def update(data: RDD[Vector], a: Double, units: String): StreamingKMeansModel = {\n+\n+    val centers = clusterCenters\n+    val counts = clusterCounts\n+\n+    // find nearest cluster to each point\n+    val closest = data.map(point => (this.predict(point), (point.toBreeze, 1.toLong)))\n+\n+    // get sums and counts for updating each cluster\n+    type WeightedPoint = (BV[Double], Long)\n+    def mergeContribs(p1: WeightedPoint, p2: WeightedPoint): WeightedPoint = {\n+      (p1._1 += p2._1, p1._2 + p2._2)\n+    }\n+    val pointStats: Array[(Int, (BV[Double], Long))] =\n+      closest.reduceByKey{mergeContribs}.collectAsMap().toArray\n+\n+    // implement update rule\n+    for (newP <- pointStats) {\n+      // store old count and centroid\n+      val oldCount = counts(newP._1)\n+      val oldCentroid = centers(newP._1).toBreeze\n+      // get new count and centroid\n+      val newCount = newP._2._2\n+      val newCentroid = newP._2._1 / newCount.toDouble\n+      // compute the normalized scale factor that controls forgetting\n+      val decayFactor = units match {\n+        case \"batches\" =>  newCount / (a * oldCount + newCount)\n+        case \"points\" => newCount / (math.pow(a, newCount) * oldCount + newCount)\n+      }\n+      // perform the update\n+      val updatedCentroid = oldCentroid + (newCentroid - oldCentroid) * decayFactor\n+      // store the new counts and centers\n+      counts(newP._1) = oldCount + newCount\n+      centers(newP._1) = Vectors.fromBreeze(updatedCentroid)\n+\n+      // display the updated cluster centers\n+      val display = centers(newP._1).size match {\n+        case x if x > 100 => centers(newP._1).toArray.take(100).mkString(\"[\", \",\", \"...\")\n+        case _ => centers(newP._1).toArray.mkString(\"[\", \",\", \"]\")\n+      }\n+      logInfo(\"Cluster %d updated: %s \".format (newP._1, display))\n+    }\n+    new StreamingKMeansModel(centers, counts)\n+  }\n+\n+}\n+\n+@DeveloperApi\n+class StreamingKMeans(\n+     var k: Int,\n+     var a: Double,\n+     var units: String) extends Logging {\n+\n+  protected var model: StreamingKMeansModel = new StreamingKMeansModel(null, null)\n+\n+  def this() = this(2, 1.0, \"batches\")\n+\n+  /** Set the number of clusters. */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+\n+  /** Set the decay factor directly (for forgetful algorithms). */\n+  def setDecayFactor(a: Double): this.type = {\n+    this.a = a\n+    this\n+  }\n+\n+  /** Set the decay units for forgetful algorithms (\"batches\" or \"points\"). */\n+  def setUnits(units: String): this.type = {\n+    if (units != \"batches\" && units != \"points\") {\n+      throw new IllegalArgumentException(\"Invalid units for decay: \" + units)\n+    }\n+    this.units = units\n+    this\n+  }\n+\n+  /** Set decay fraction in units of batches. */\n+  def setDecayFractionBatches(q: Double): this.type = {\n+    this.a = math.log(1 - q) / math.log(0.5)\n+    this.units = \"batches\"\n+    this\n+  }\n+\n+  /** Set decay fraction in units of points. Must specify expected number of points per batch. */\n+  def setDecayFractionPoints(q: Double, m: Double): this.type = {\n+    this.a = math.pow(math.log(1 - q) / math.log(0.5), 1/m)\n+    this.units = \"points\"\n+    this\n+  }\n+\n+  /** Specify initial explicitly directly. */\n+  def setInitialCenters(initialCenters: Array[Vector]): this.type = {\n+    val clusterCounts = Array.fill(this.k)(0).map(_.toLong)\n+    this.model = new StreamingKMeansModel(initialCenters, clusterCounts)\n+    this\n+  }\n+\n+  /** Initialize random centers, requiring only the number of dimensions. */\n+  def setRandomCenters(d: Int): this.type = {\n+    val initialCenters = (0 until k).map(_ => Vectors.dense(Array.fill(d)(nextGaussian()))).toArray\n+    val clusterCounts = Array.fill(this.k)(0).map(_.toLong)\n+    this.model = new StreamingKMeansModel(initialCenters, clusterCounts)\n+    this\n+  }\n+\n+  /** Return the latest model. */\n+  def latestModel(): StreamingKMeansModel = {\n+    model\n+  }\n+\n+  /**\n+   * Update the clustering model by training on batches of data from a DStream.\n+   * This operation registers a DStream for training the model,\n+   * checks whether the cluster centers have been initialized,\n+   * and updates the model using each batch of data from the stream.\n+   *\n+   * @param data DStream containing vector data\n+   */\n+  def trainOn(data: DStream[Vector]) {\n+    this.isInitialized\n+    data.foreachRDD { (rdd, time) =>\n+      model = model.update(rdd, this.a, this.units)\n+    }\n+  }\n+\n+  /**\n+   * Use the clustering model to make predictions on batches of data from a DStream.\n+   *\n+   * @param data DStream containing vector data\n+   * @return DStream containing predictions\n+   */\n+  def predictOn(data: DStream[Vector]): DStream[Int] = {\n+    this.isInitialized\n+    data.map(model.predict)\n+  }\n+\n+  /**\n+   * Use the model to make predictions on the values of a DStream and carry over its keys.\n+   *\n+   * @param data DStream containing (key, feature vector) pairs\n+   * @tparam K key type\n+   * @return DStream containing the input keys and the predictions as values\n+   */\n+  def predictOnValues[K: ClassTag](data: DStream[(K, Vector)]): DStream[(K, Int)] = {\n+    this.isInitialized\n+    data.mapValues(model.predict)\n+  }\n+\n+  /**\n+   * Check whether cluster centers have been initialized.\n+   *\n+   * @return Boolean, True if cluster centrs have been initialized\n+   */\n+  def isInitialized: Boolean = {"
  }],
  "prId": 2942
}]