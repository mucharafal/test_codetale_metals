[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "You might not care about this at all, but calling `foreach` on an `Array` is actually notably slower than using a while loop over the indices.  `foreach` over a `Range` is actually pretty close to while loop (ie. `(0 until x.length).foreach{idx => s += x(idx)}`.  Or if you don't care about runtimes, then you can always just call `array.sum` (it actually comes from an implicit conversion to `WrappedArray`):\n\n```\nscala> ((0 to 100).map{_ / 100.0}.toArray).sum\nres2: Double = 50.5\n```\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-11-10T01:24:29Z",
    "diffHunk": "@@ -0,0 +1,246 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stores as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stores as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stores as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as some random\n+    // point from the data.  (This could be improved)\n+    val samples = breezeData.takeSample(true, k, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // identity matrices for covariance \n+    var C = (0 until k).map(i => (1.0/k, \n+                                  samples(i), \n+                                  BreezeMatrix.eye[Double](d))).toArray\n+    \n+    val acc_w     = new Array[Accumulator[Double]](k)\n+    val acc_mu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val acc_sigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var i, iter = 0\n+    do {\n+      // reset accumulators\n+      for(i <- 0 until k){\n+        acc_w(i)     = ctx.accumulator(0.0)\n+        acc_mu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        acc_sigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val log_likelihood = ctx.accumulator(0.0)\n+      \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map(i => \n+                                  new MultivariateGaussian(C(i)._2, C(i)._3)).toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => C(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  \n+        val p = (0 until k).map(i => \n+          eps + weights.value(i) * dists.value(i).pdf(x)).toArray\n+        val norm = sum(p)\n+        \n+        log_likelihood += math.log(norm)  \n+          \n+        // accumulate weighted sums  \n+        for(i <- 0 until k){\n+          p(i) /= norm\n+          acc_w(i) += p(i)\n+          acc_mu(i) += x * p(i)\n+          acc_sigma(i) += x * new Transpose(x) * p(i)\n+        }  \n+      })\n+      \n+      // Collect the computed sums\n+      val W = (0 until k).map(i => acc_w(i).value).toArray\n+      val MU = (0 until k).map(i => acc_mu(i).value).toArray\n+      val SIGMA = (0 until k).map(i => acc_sigma(i).value).toArray\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      C = (0 until k).map(i => {\n+            val weight = W(i) / sum(W)\n+            val mu = MU(i) / W(i)\n+            val sigma = SIGMA(i) / W(i) - mu * new Transpose(mu)\n+            (weight, mu, sigma)\n+          }).toArray\n+      \n+      llhp = llh; // current becomes previous\n+      llh = log_likelihood.value // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > delta)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val weights = (0 until k).map(i => C(i)._1).toArray\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(C(i)._2)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(C(i)._3)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+  \n+  /** Sum the values in array of doubles */\n+  private def sum(x : Array[Double]) : Double = {\n+    var s : Double = 0.0\n+    x.foreach(u => s += u)"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "space between groups of imports\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:09:50Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "It would be great if you could add a sentence or two explaining what GMMs are.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:09:53Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will "
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "It might be good to rename this class to \"GaussianMixtureModelEM\" so that its name is closer to the name of the model it produces.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:09:54Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private ("
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "I would recommend using \"convergenceTol\" since that is already used elsewhere (e.g., in LBFGS).\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:09:55Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, "
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "no semicolon\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:09:56Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Use \"/*\\* ... */\" for comment so it is part of the generated documentation.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:09:57Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "spaces around \"/\" operator\n\nAlso, it might be good to use a more descriptive variable name like \"centers\"\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:09:59Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var C = (0 until k).map(i => (1.0/k, "
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "i is not used (The for loops have implicit declarations of other \"i\" vars.)\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:10:00Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var C = (0 until k).map(i => (1.0/k, \n+                                  vec_mean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  init_cov(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                           ).toArray\n+    \n+    val acc_w     = new Array[Accumulator[Double]](k)\n+    val acc_mu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val acc_sigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var i, iter = 0"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "scala style:\n`for (i <- 0 until k) {`\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:10:01Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var C = (0 until k).map(i => (1.0/k, \n+                                  vec_mean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  init_cov(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                           ).toArray\n+    \n+    val acc_w     = new Array[Accumulator[Double]](k)\n+    val acc_mu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val acc_sigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var i, iter = 0\n+    do {\n+      // reset accumulators\n+      for(i <- 0 until k){"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "extra whitespace after }\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:10:03Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var C = (0 until k).map(i => (1.0/k, \n+                                  vec_mean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  init_cov(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                           ).toArray\n+    \n+    val acc_w     = new Array[Accumulator[Double]](k)\n+    val acc_mu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val acc_sigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var i, iter = 0\n+    do {\n+      // reset accumulators\n+      for(i <- 0 until k){\n+        acc_w(i)     = ctx.accumulator(0.0)\n+        acc_mu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        acc_sigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val log_likelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map(i => \n+                                  new MultivariateGaussian(C(i)._2, C(i)._3)).toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => C(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  \n+        val p = (0 until k).map(i => \n+          eps + weights.value(i) * dists.value(i).pdf(x)).toArray\n+        val norm = sum(p)\n+        \n+        log_likelihood += math.log(norm)  \n+          \n+        // accumulate weighted sums  \n+        val xxt = x * new Transpose(x)\n+        for(i <- 0 until k){\n+          p(i) /= norm\n+          acc_w(i) += p(i)\n+          acc_mu(i) += x * p(i)\n+          acc_sigma(i) += xxt * p(i)\n+        }  "
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "You should be able to write `x.sum`\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:10:04Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var C = (0 until k).map(i => (1.0/k, \n+                                  vec_mean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  init_cov(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                           ).toArray\n+    \n+    val acc_w     = new Array[Accumulator[Double]](k)\n+    val acc_mu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val acc_sigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var i, iter = 0\n+    do {\n+      // reset accumulators\n+      for(i <- 0 until k){\n+        acc_w(i)     = ctx.accumulator(0.0)\n+        acc_mu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        acc_sigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val log_likelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map(i => \n+                                  new MultivariateGaussian(C(i)._2, C(i)._3)).toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => C(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  \n+        val p = (0 until k).map(i => \n+          eps + weights.value(i) * dists.value(i).pdf(x)).toArray\n+        val norm = sum(p)\n+        \n+        log_likelihood += math.log(norm)  \n+          \n+        // accumulate weighted sums  \n+        val xxt = x * new Transpose(x)\n+        for(i <- 0 until k){\n+          p(i) /= norm\n+          acc_w(i) += p(i)\n+          acc_mu(i) += x * p(i)\n+          acc_sigma(i) += xxt * p(i)\n+        }  \n+      })\n+      \n+      // Collect the computed sums\n+      val W = (0 until k).map(i => acc_w(i).value).toArray\n+      val MU = (0 until k).map(i => acc_mu(i).value).toArray\n+      val SIGMA = (0 until k).map(i => acc_sigma(i).value).toArray\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      C = (0 until k).map(i => {\n+            val weight = W(i) / sum(W)\n+            val mu = MU(i) / W(i)\n+            val sigma = SIGMA(i) / W(i) - mu * new Transpose(mu)\n+            (weight, mu, sigma)\n+          }).toArray\n+      \n+      llhp = llh; // current becomes previous\n+      llh = log_likelihood.value // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > delta)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val weights = (0 until k).map(i => C(i)._1).toArray\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(C(i)._2)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(C(i)._3)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+  \n+  /** Sum the values in array of doubles */\n+  private def sum(x : Array[Double]) : Double = {"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "You can probably write `x.sum / x.length`\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:10:05Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var C = (0 until k).map(i => (1.0/k, \n+                                  vec_mean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  init_cov(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                           ).toArray\n+    \n+    val acc_w     = new Array[Accumulator[Double]](k)\n+    val acc_mu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val acc_sigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var i, iter = 0\n+    do {\n+      // reset accumulators\n+      for(i <- 0 until k){\n+        acc_w(i)     = ctx.accumulator(0.0)\n+        acc_mu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        acc_sigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val log_likelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map(i => \n+                                  new MultivariateGaussian(C(i)._2, C(i)._3)).toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => C(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  \n+        val p = (0 until k).map(i => \n+          eps + weights.value(i) * dists.value(i).pdf(x)).toArray\n+        val norm = sum(p)\n+        \n+        log_likelihood += math.log(norm)  \n+          \n+        // accumulate weighted sums  \n+        val xxt = x * new Transpose(x)\n+        for(i <- 0 until k){\n+          p(i) /= norm\n+          acc_w(i) += p(i)\n+          acc_mu(i) += x * p(i)\n+          acc_sigma(i) += xxt * p(i)\n+        }  \n+      })\n+      \n+      // Collect the computed sums\n+      val W = (0 until k).map(i => acc_w(i).value).toArray\n+      val MU = (0 until k).map(i => acc_mu(i).value).toArray\n+      val SIGMA = (0 until k).map(i => acc_sigma(i).value).toArray\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      C = (0 until k).map(i => {\n+            val weight = W(i) / sum(W)\n+            val mu = MU(i) / W(i)\n+            val sigma = SIGMA(i) / W(i) - mu * new Transpose(mu)\n+            (weight, mu, sigma)\n+          }).toArray\n+      \n+      llhp = llh; // current becomes previous\n+      llh = log_likelihood.value // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > delta)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val weights = (0 until k).map(i => C(i)._1).toArray\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(C(i)._2)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(C(i)._3)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+  \n+  /** Sum the values in array of doubles */\n+  private def sum(x : Array[Double]) : Double = {\n+    var s : Double = 0.0\n+    (0 until x.length).foreach(j => s += x(j))\n+    s\n+  }\n+  \n+  /** Average of dense breeze vectors */\n+  private def vec_mean(x : Array[DenseDoubleVector]) : DenseDoubleVector = {"
  }, {
    "author": {
      "login": "tgaloppo"
    },
    "body": "This does not work; the compiler can not find a suitable implicit conversion for the array of vectors when attempting x.sum\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T15:31:59Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var C = (0 until k).map(i => (1.0/k, \n+                                  vec_mean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  init_cov(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                           ).toArray\n+    \n+    val acc_w     = new Array[Accumulator[Double]](k)\n+    val acc_mu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val acc_sigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var i, iter = 0\n+    do {\n+      // reset accumulators\n+      for(i <- 0 until k){\n+        acc_w(i)     = ctx.accumulator(0.0)\n+        acc_mu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        acc_sigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val log_likelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map(i => \n+                                  new MultivariateGaussian(C(i)._2, C(i)._3)).toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => C(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  \n+        val p = (0 until k).map(i => \n+          eps + weights.value(i) * dists.value(i).pdf(x)).toArray\n+        val norm = sum(p)\n+        \n+        log_likelihood += math.log(norm)  \n+          \n+        // accumulate weighted sums  \n+        val xxt = x * new Transpose(x)\n+        for(i <- 0 until k){\n+          p(i) /= norm\n+          acc_w(i) += p(i)\n+          acc_mu(i) += x * p(i)\n+          acc_sigma(i) += xxt * p(i)\n+        }  \n+      })\n+      \n+      // Collect the computed sums\n+      val W = (0 until k).map(i => acc_w(i).value).toArray\n+      val MU = (0 until k).map(i => acc_mu(i).value).toArray\n+      val SIGMA = (0 until k).map(i => acc_sigma(i).value).toArray\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      C = (0 until k).map(i => {\n+            val weight = W(i) / sum(W)\n+            val mu = MU(i) / W(i)\n+            val sigma = SIGMA(i) / W(i) - mu * new Transpose(mu)\n+            (weight, mu, sigma)\n+          }).toArray\n+      \n+      llhp = llh; // current becomes previous\n+      llh = log_likelihood.value // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > delta)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val weights = (0 until k).map(i => C(i)._1).toArray\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(C(i)._2)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(C(i)._3)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+  \n+  /** Sum the values in array of doubles */\n+  private def sum(x : Array[Double]) : Double = {\n+    var s : Double = 0.0\n+    (0 until x.length).foreach(j => s += x(j))\n+    s\n+  }\n+  \n+  /** Average of dense breeze vectors */\n+  private def vec_mean(x : Array[DenseDoubleVector]) : DenseDoubleVector = {"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "Oh, OK, good to know!\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T18:12:03Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var C = (0 until k).map(i => (1.0/k, \n+                                  vec_mean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  init_cov(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                           ).toArray\n+    \n+    val acc_w     = new Array[Accumulator[Double]](k)\n+    val acc_mu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val acc_sigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var i, iter = 0\n+    do {\n+      // reset accumulators\n+      for(i <- 0 until k){\n+        acc_w(i)     = ctx.accumulator(0.0)\n+        acc_mu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        acc_sigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val log_likelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map(i => \n+                                  new MultivariateGaussian(C(i)._2, C(i)._3)).toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => C(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  \n+        val p = (0 until k).map(i => \n+          eps + weights.value(i) * dists.value(i).pdf(x)).toArray\n+        val norm = sum(p)\n+        \n+        log_likelihood += math.log(norm)  \n+          \n+        // accumulate weighted sums  \n+        val xxt = x * new Transpose(x)\n+        for(i <- 0 until k){\n+          p(i) /= norm\n+          acc_w(i) += p(i)\n+          acc_mu(i) += x * p(i)\n+          acc_sigma(i) += xxt * p(i)\n+        }  \n+      })\n+      \n+      // Collect the computed sums\n+      val W = (0 until k).map(i => acc_w(i).value).toArray\n+      val MU = (0 until k).map(i => acc_mu(i).value).toArray\n+      val SIGMA = (0 until k).map(i => acc_sigma(i).value).toArray\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      C = (0 until k).map(i => {\n+            val weight = W(i) / sum(W)\n+            val mu = MU(i) / W(i)\n+            val sigma = SIGMA(i) / W(i) - mu * new Transpose(mu)\n+            (weight, mu, sigma)\n+          }).toArray\n+      \n+      llhp = llh; // current becomes previous\n+      llh = log_likelihood.value // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > delta)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val weights = (0 until k).map(i => C(i)._1).toArray\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(C(i)._2)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(C(i)._3)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+  \n+  /** Sum the values in array of doubles */\n+  private def sum(x : Array[Double]) : Double = {\n+    var s : Double = 0.0\n+    (0 until x.length).foreach(j => s += x(j))\n+    s\n+  }\n+  \n+  /** Average of dense breeze vectors */\n+  private def vec_mean(x : Array[DenseDoubleVector]) : DenseDoubleVector = {"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Here and elsewhere, use camelCase naming convention\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:10:06Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var C = (0 until k).map(i => (1.0/k, \n+                                  vec_mean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  init_cov(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                           ).toArray\n+    \n+    val acc_w     = new Array[Accumulator[Double]](k)\n+    val acc_mu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val acc_sigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var i, iter = 0\n+    do {\n+      // reset accumulators\n+      for(i <- 0 until k){\n+        acc_w(i)     = ctx.accumulator(0.0)\n+        acc_mu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        acc_sigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val log_likelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map(i => \n+                                  new MultivariateGaussian(C(i)._2, C(i)._3)).toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => C(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  \n+        val p = (0 until k).map(i => \n+          eps + weights.value(i) * dists.value(i).pdf(x)).toArray\n+        val norm = sum(p)\n+        \n+        log_likelihood += math.log(norm)  \n+          \n+        // accumulate weighted sums  \n+        val xxt = x * new Transpose(x)\n+        for(i <- 0 until k){\n+          p(i) /= norm\n+          acc_w(i) += p(i)\n+          acc_mu(i) += x * p(i)\n+          acc_sigma(i) += xxt * p(i)\n+        }  \n+      })\n+      \n+      // Collect the computed sums\n+      val W = (0 until k).map(i => acc_w(i).value).toArray\n+      val MU = (0 until k).map(i => acc_mu(i).value).toArray\n+      val SIGMA = (0 until k).map(i => acc_sigma(i).value).toArray\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      C = (0 until k).map(i => {\n+            val weight = W(i) / sum(W)\n+            val mu = MU(i) / W(i)\n+            val sigma = SIGMA(i) / W(i) - mu * new Transpose(mu)\n+            (weight, mu, sigma)\n+          }).toArray\n+      \n+      llhp = llh; // current becomes previous\n+      llh = log_likelihood.value // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > delta)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val weights = (0 until k).map(i => C(i)._1).toArray\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(C(i)._2)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(C(i)._3)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+  \n+  /** Sum the values in array of doubles */\n+  private def sum(x : Array[Double]) : Double = {\n+    var s : Double = 0.0\n+    (0 until x.length).foreach(j => s += x(j))\n+    s\n+  }\n+  \n+  /** Average of dense breeze vectors */\n+  private def vec_mean(x : Array[DenseDoubleVector]) : DenseDoubleVector = {\n+    val v = BreezeVector.zeros[Double](x(0).length)\n+    (0 until x.length).foreach(j => v += x(j))\n+    v / x.length.asInstanceOf[Double] \n+  }\n+  \n+  /**\n+   * Construct matrix where diagonal entries are element-wise\n+   * variance of input vectors (computes biased variance)\n+   */\n+  private def init_cov(x : Array[DenseDoubleVector]) : DenseDoubleMatrix = {"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "\"result\" --> \"cov\" (or something more descriptive)\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:10:07Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var C = (0 until k).map(i => (1.0/k, \n+                                  vec_mean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  init_cov(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                           ).toArray\n+    \n+    val acc_w     = new Array[Accumulator[Double]](k)\n+    val acc_mu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val acc_sigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var i, iter = 0\n+    do {\n+      // reset accumulators\n+      for(i <- 0 until k){\n+        acc_w(i)     = ctx.accumulator(0.0)\n+        acc_mu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        acc_sigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val log_likelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map(i => \n+                                  new MultivariateGaussian(C(i)._2, C(i)._3)).toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => C(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  \n+        val p = (0 until k).map(i => \n+          eps + weights.value(i) * dists.value(i).pdf(x)).toArray\n+        val norm = sum(p)\n+        \n+        log_likelihood += math.log(norm)  \n+          \n+        // accumulate weighted sums  \n+        val xxt = x * new Transpose(x)\n+        for(i <- 0 until k){\n+          p(i) /= norm\n+          acc_w(i) += p(i)\n+          acc_mu(i) += x * p(i)\n+          acc_sigma(i) += xxt * p(i)\n+        }  \n+      })\n+      \n+      // Collect the computed sums\n+      val W = (0 until k).map(i => acc_w(i).value).toArray\n+      val MU = (0 until k).map(i => acc_mu(i).value).toArray\n+      val SIGMA = (0 until k).map(i => acc_sigma(i).value).toArray\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      C = (0 until k).map(i => {\n+            val weight = W(i) / sum(W)\n+            val mu = MU(i) / W(i)\n+            val sigma = SIGMA(i) / W(i) - mu * new Transpose(mu)\n+            (weight, mu, sigma)\n+          }).toArray\n+      \n+      llhp = llh; // current becomes previous\n+      llh = log_likelihood.value // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > delta)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val weights = (0 until k).map(i => C(i)._1).toArray\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(C(i)._2)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(C(i)._3)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+  \n+  /** Sum the values in array of doubles */\n+  private def sum(x : Array[Double]) : Double = {\n+    var s : Double = 0.0\n+    (0 until x.length).foreach(j => s += x(j))\n+    s\n+  }\n+  \n+  /** Average of dense breeze vectors */\n+  private def vec_mean(x : Array[DenseDoubleVector]) : DenseDoubleVector = {\n+    val v = BreezeVector.zeros[Double](x(0).length)\n+    (0 until x.length).foreach(j => v += x(j))\n+    v / x.length.asInstanceOf[Double] \n+  }\n+  \n+  /**\n+   * Construct matrix where diagonal entries are element-wise\n+   * variance of input vectors (computes biased variance)\n+   */\n+  private def init_cov(x : Array[DenseDoubleVector]) : DenseDoubleMatrix = {\n+    val mu = vec_mean(x)\n+    val ss = BreezeVector.zeros[Double](x(0).length)\n+    val result = BreezeMatrix.eye[Double](ss.length)"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Can you simplify this to be:\n\n```\nval ss = x.map(xi => (xi - mu) :^ 2.0).sum\n```\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:10:08Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var C = (0 until k).map(i => (1.0/k, \n+                                  vec_mean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  init_cov(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                           ).toArray\n+    \n+    val acc_w     = new Array[Accumulator[Double]](k)\n+    val acc_mu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val acc_sigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var i, iter = 0\n+    do {\n+      // reset accumulators\n+      for(i <- 0 until k){\n+        acc_w(i)     = ctx.accumulator(0.0)\n+        acc_mu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        acc_sigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val log_likelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map(i => \n+                                  new MultivariateGaussian(C(i)._2, C(i)._3)).toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => C(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  \n+        val p = (0 until k).map(i => \n+          eps + weights.value(i) * dists.value(i).pdf(x)).toArray\n+        val norm = sum(p)\n+        \n+        log_likelihood += math.log(norm)  \n+          \n+        // accumulate weighted sums  \n+        val xxt = x * new Transpose(x)\n+        for(i <- 0 until k){\n+          p(i) /= norm\n+          acc_w(i) += p(i)\n+          acc_mu(i) += x * p(i)\n+          acc_sigma(i) += xxt * p(i)\n+        }  \n+      })\n+      \n+      // Collect the computed sums\n+      val W = (0 until k).map(i => acc_w(i).value).toArray\n+      val MU = (0 until k).map(i => acc_mu(i).value).toArray\n+      val SIGMA = (0 until k).map(i => acc_sigma(i).value).toArray\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      C = (0 until k).map(i => {\n+            val weight = W(i) / sum(W)\n+            val mu = MU(i) / W(i)\n+            val sigma = SIGMA(i) / W(i) - mu * new Transpose(mu)\n+            (weight, mu, sigma)\n+          }).toArray\n+      \n+      llhp = llh; // current becomes previous\n+      llh = log_likelihood.value // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > delta)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val weights = (0 until k).map(i => C(i)._1).toArray\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(C(i)._2)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(C(i)._3)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+  \n+  /** Sum the values in array of doubles */\n+  private def sum(x : Array[Double]) : Double = {\n+    var s : Double = 0.0\n+    (0 until x.length).foreach(j => s += x(j))\n+    s\n+  }\n+  \n+  /** Average of dense breeze vectors */\n+  private def vec_mean(x : Array[DenseDoubleVector]) : DenseDoubleVector = {\n+    val v = BreezeVector.zeros[Double](x(0).length)\n+    (0 until x.length).foreach(j => v += x(j))\n+    v / x.length.asInstanceOf[Double] \n+  }\n+  \n+  /**\n+   * Construct matrix where diagonal entries are element-wise\n+   * variance of input vectors (computes biased variance)\n+   */\n+  private def init_cov(x : Array[DenseDoubleVector]) : DenseDoubleMatrix = {\n+    val mu = vec_mean(x)\n+    val ss = BreezeVector.zeros[Double](x(0).length)\n+    val result = BreezeMatrix.eye[Double](ss.length)\n+    (0 until x.length).map(i => (x(i) - mu) :^ 2.0).foreach(u => ss += u)"
  }, {
    "author": {
      "login": "tgaloppo"
    },
    "body": "Here again, sum method on array of vectors fails to compile.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T15:33:05Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var C = (0 until k).map(i => (1.0/k, \n+                                  vec_mean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  init_cov(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                           ).toArray\n+    \n+    val acc_w     = new Array[Accumulator[Double]](k)\n+    val acc_mu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val acc_sigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var i, iter = 0\n+    do {\n+      // reset accumulators\n+      for(i <- 0 until k){\n+        acc_w(i)     = ctx.accumulator(0.0)\n+        acc_mu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        acc_sigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val log_likelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map(i => \n+                                  new MultivariateGaussian(C(i)._2, C(i)._3)).toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => C(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  \n+        val p = (0 until k).map(i => \n+          eps + weights.value(i) * dists.value(i).pdf(x)).toArray\n+        val norm = sum(p)\n+        \n+        log_likelihood += math.log(norm)  \n+          \n+        // accumulate weighted sums  \n+        val xxt = x * new Transpose(x)\n+        for(i <- 0 until k){\n+          p(i) /= norm\n+          acc_w(i) += p(i)\n+          acc_mu(i) += x * p(i)\n+          acc_sigma(i) += xxt * p(i)\n+        }  \n+      })\n+      \n+      // Collect the computed sums\n+      val W = (0 until k).map(i => acc_w(i).value).toArray\n+      val MU = (0 until k).map(i => acc_mu(i).value).toArray\n+      val SIGMA = (0 until k).map(i => acc_sigma(i).value).toArray\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      C = (0 until k).map(i => {\n+            val weight = W(i) / sum(W)\n+            val mu = MU(i) / W(i)\n+            val sigma = SIGMA(i) / W(i) - mu * new Transpose(mu)\n+            (weight, mu, sigma)\n+          }).toArray\n+      \n+      llhp = llh; // current becomes previous\n+      llh = log_likelihood.value // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > delta)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val weights = (0 until k).map(i => C(i)._1).toArray\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(C(i)._2)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(C(i)._3)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+  \n+  /** Sum the values in array of doubles */\n+  private def sum(x : Array[Double]) : Double = {\n+    var s : Double = 0.0\n+    (0 until x.length).foreach(j => s += x(j))\n+    s\n+  }\n+  \n+  /** Average of dense breeze vectors */\n+  private def vec_mean(x : Array[DenseDoubleVector]) : DenseDoubleVector = {\n+    val v = BreezeVector.zeros[Double](x(0).length)\n+    (0 until x.length).foreach(j => v += x(j))\n+    v / x.length.asInstanceOf[Double] \n+  }\n+  \n+  /**\n+   * Construct matrix where diagonal entries are element-wise\n+   * variance of input vectors (computes biased variance)\n+   */\n+  private def init_cov(x : Array[DenseDoubleVector]) : DenseDoubleMatrix = {\n+    val mu = vec_mean(x)\n+    val ss = BreezeVector.zeros[Double](x(0).length)\n+    val result = BreezeMatrix.eye[Double](ss.length)\n+    (0 until x.length).map(i => (x(i) - mu) :^ 2.0).foreach(u => ss += u)"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "no space before \":\" in method declaration (here and elsewhere)\n\nAlso, \"initialVector\" --> \"initialMatrix\"\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:10:10Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var C = (0 until k).map(i => (1.0/k, \n+                                  vec_mean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  init_cov(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                           ).toArray\n+    \n+    val acc_w     = new Array[Accumulator[Double]](k)\n+    val acc_mu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val acc_sigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var i, iter = 0\n+    do {\n+      // reset accumulators\n+      for(i <- 0 until k){\n+        acc_w(i)     = ctx.accumulator(0.0)\n+        acc_mu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        acc_sigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val log_likelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map(i => \n+                                  new MultivariateGaussian(C(i)._2, C(i)._3)).toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => C(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  \n+        val p = (0 until k).map(i => \n+          eps + weights.value(i) * dists.value(i).pdf(x)).toArray\n+        val norm = sum(p)\n+        \n+        log_likelihood += math.log(norm)  \n+          \n+        // accumulate weighted sums  \n+        val xxt = x * new Transpose(x)\n+        for(i <- 0 until k){\n+          p(i) /= norm\n+          acc_w(i) += p(i)\n+          acc_mu(i) += x * p(i)\n+          acc_sigma(i) += xxt * p(i)\n+        }  \n+      })\n+      \n+      // Collect the computed sums\n+      val W = (0 until k).map(i => acc_w(i).value).toArray\n+      val MU = (0 until k).map(i => acc_mu(i).value).toArray\n+      val SIGMA = (0 until k).map(i => acc_sigma(i).value).toArray\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      C = (0 until k).map(i => {\n+            val weight = W(i) / sum(W)\n+            val mu = MU(i) / W(i)\n+            val sigma = SIGMA(i) / W(i) - mu * new Transpose(mu)\n+            (weight, mu, sigma)\n+          }).toArray\n+      \n+      llhp = llh; // current becomes previous\n+      llh = log_likelihood.value // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > delta)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val weights = (0 until k).map(i => C(i)._1).toArray\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(C(i)._2)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(C(i)._3)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+  \n+  /** Sum the values in array of doubles */\n+  private def sum(x : Array[Double]) : Double = {\n+    var s : Double = 0.0\n+    (0 until x.length).foreach(j => s += x(j))\n+    s\n+  }\n+  \n+  /** Average of dense breeze vectors */\n+  private def vec_mean(x : Array[DenseDoubleVector]) : DenseDoubleVector = {\n+    val v = BreezeVector.zeros[Double](x(0).length)\n+    (0 until x.length).foreach(j => v += x(j))\n+    v / x.length.asInstanceOf[Double] \n+  }\n+  \n+  /**\n+   * Construct matrix where diagonal entries are element-wise\n+   * variance of input vectors (computes biased variance)\n+   */\n+  private def init_cov(x : Array[DenseDoubleVector]) : DenseDoubleMatrix = {\n+    val mu = vec_mean(x)\n+    val ss = BreezeVector.zeros[Double](x(0).length)\n+    val result = BreezeMatrix.eye[Double](ss.length)\n+    (0 until x.length).map(i => (x(i) - mu) :^ 2.0).foreach(u => ss += u)\n+    (0 until ss.length).foreach(i => result(i,i) = ss(i) / x.length)\n+    result\n+  }\n+  \n+  /** AccumulatorParam for Dense Breeze Vectors */\n+  private object DenseDoubleVectorAccumulatorParam extends AccumulatorParam[DenseDoubleVector] {\n+    def zero(initialVector : DenseDoubleVector) : DenseDoubleVector = {\n+      BreezeVector.zeros[Double](initialVector.length)\n+    }\n+    \n+    def addInPlace(a : DenseDoubleVector, b : DenseDoubleVector) : DenseDoubleVector = {\n+      a += b\n+    }\n+  }\n+  \n+  /** AccumulatorParam for Dense Breeze Matrices */\n+  private object DenseDoubleMatrixAccumulatorParam extends AccumulatorParam[DenseDoubleMatrix] {\n+    def zero(initialVector : DenseDoubleMatrix) : DenseDoubleMatrix = {"
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Could you please move this class to a new folder mllib/stat/impl/ ?  There are a few PRs introducing standard distributions.  I think we should keep them private for now but collect them in stat/impl/.  Later on, we can standardize the APIs and make them public classes.\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:10:11Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var C = (0 until k).map(i => (1.0/k, \n+                                  vec_mean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  init_cov(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                           ).toArray\n+    \n+    val acc_w     = new Array[Accumulator[Double]](k)\n+    val acc_mu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val acc_sigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var i, iter = 0\n+    do {\n+      // reset accumulators\n+      for(i <- 0 until k){\n+        acc_w(i)     = ctx.accumulator(0.0)\n+        acc_mu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        acc_sigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val log_likelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map(i => \n+                                  new MultivariateGaussian(C(i)._2, C(i)._3)).toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => C(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  \n+        val p = (0 until k).map(i => \n+          eps + weights.value(i) * dists.value(i).pdf(x)).toArray\n+        val norm = sum(p)\n+        \n+        log_likelihood += math.log(norm)  \n+          \n+        // accumulate weighted sums  \n+        val xxt = x * new Transpose(x)\n+        for(i <- 0 until k){\n+          p(i) /= norm\n+          acc_w(i) += p(i)\n+          acc_mu(i) += x * p(i)\n+          acc_sigma(i) += xxt * p(i)\n+        }  \n+      })\n+      \n+      // Collect the computed sums\n+      val W = (0 until k).map(i => acc_w(i).value).toArray\n+      val MU = (0 until k).map(i => acc_mu(i).value).toArray\n+      val SIGMA = (0 until k).map(i => acc_sigma(i).value).toArray\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      C = (0 until k).map(i => {\n+            val weight = W(i) / sum(W)\n+            val mu = MU(i) / W(i)\n+            val sigma = SIGMA(i) / W(i) - mu * new Transpose(mu)\n+            (weight, mu, sigma)\n+          }).toArray\n+      \n+      llhp = llh; // current becomes previous\n+      llh = log_likelihood.value // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > delta)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val weights = (0 until k).map(i => C(i)._1).toArray\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(C(i)._2)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(C(i)._3)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+  \n+  /** Sum the values in array of doubles */\n+  private def sum(x : Array[Double]) : Double = {\n+    var s : Double = 0.0\n+    (0 until x.length).foreach(j => s += x(j))\n+    s\n+  }\n+  \n+  /** Average of dense breeze vectors */\n+  private def vec_mean(x : Array[DenseDoubleVector]) : DenseDoubleVector = {\n+    val v = BreezeVector.zeros[Double](x(0).length)\n+    (0 until x.length).foreach(j => v += x(j))\n+    v / x.length.asInstanceOf[Double] \n+  }\n+  \n+  /**\n+   * Construct matrix where diagonal entries are element-wise\n+   * variance of input vectors (computes biased variance)\n+   */\n+  private def init_cov(x : Array[DenseDoubleVector]) : DenseDoubleMatrix = {\n+    val mu = vec_mean(x)\n+    val ss = BreezeVector.zeros[Double](x(0).length)\n+    val result = BreezeMatrix.eye[Double](ss.length)\n+    (0 until x.length).map(i => (x(i) - mu) :^ 2.0).foreach(u => ss += u)\n+    (0 until ss.length).foreach(i => result(i,i) = ss(i) / x.length)\n+    result\n+  }\n+  \n+  /** AccumulatorParam for Dense Breeze Vectors */\n+  private object DenseDoubleVectorAccumulatorParam extends AccumulatorParam[DenseDoubleVector] {\n+    def zero(initialVector : DenseDoubleVector) : DenseDoubleVector = {\n+      BreezeVector.zeros[Double](initialVector.length)\n+    }\n+    \n+    def addInPlace(a : DenseDoubleVector, b : DenseDoubleVector) : DenseDoubleVector = {\n+      a += b\n+    }\n+  }\n+  \n+  /** AccumulatorParam for Dense Breeze Matrices */\n+  private object DenseDoubleMatrixAccumulatorParam extends AccumulatorParam[DenseDoubleMatrix] {\n+    def zero(initialVector : DenseDoubleMatrix) : DenseDoubleMatrix = {\n+      BreezeMatrix.zeros[Double](initialVector.rows, initialVector.cols)\n+    }\n+    \n+    def addInPlace(a : DenseDoubleMatrix, b : DenseDoubleMatrix) : DenseDoubleMatrix = {\n+      a += b\n+    }\n+  }  \n+  \n+  /** \n+   * Utility class to implement the density function for multivariate Gaussian distribution.\n+   * Breeze provides this functionality, but it requires the Apache Commons Math library,\n+   * so this class is here so-as to not introduce a new dependency in Spark.\n+   */\n+  private class MultivariateGaussian(val mu : DenseDoubleVector, val sigma : DenseDoubleMatrix) "
  }],
  "prId": 3022
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "put spaces around operators \"*\" and \"/\"\n",
    "commit": "aaa8f25a579d9c9aa191734377b503fb73299b78",
    "createdAt": "2014-12-11T03:10:12Z",
    "diffHunk": "@@ -0,0 +1,283 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BreezeVector, DenseMatrix => BreezeMatrix}\n+import breeze.linalg.{Transpose, det, inv}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.mllib.linalg.{Matrices, Vector, Vectors}\n+import org.apache.spark.{Accumulator, AccumulatorParam, SparkContext}\n+import org.apache.spark.SparkContext.DoubleAccumulatorParam\n+\n+/**\n+ * Expectation-Maximization for multivariate Gaussian Mixture Models.\n+ * \n+ */\n+object GMMExpectationMaximization {\n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k)\n+      .setMaxIterations(maxIterations)\n+      .setDelta(delta)\n+      .run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param maxIterations the maximum number of iterations to perform\n+   */\n+  def train(data: RDD[Vector], k: Int, maxIterations: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setMaxIterations(maxIterations).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   * @param delta change in log-likelihood at which convergence is considered achieved\n+   */\n+  def train(data: RDD[Vector], k: Int, delta: Double): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).setDelta(delta).run(data)\n+  }\n+  \n+  /**\n+   * Trains a GMM using the given parameters\n+   * \n+   * @param data training points stored as RDD[Vector]\n+   * @param k the number of Gaussians in the mixture\n+   */\n+  def train(data: RDD[Vector], k: Int): GaussianMixtureModel = {\n+    new GMMExpectationMaximization().setK(k).run(data)\n+  }\n+}\n+\n+/**\n+ * This class performs multivariate Gaussian expectation maximization.  It will \n+ * maximize the log-likelihood for a mixture of k Gaussians, iterating until\n+ * the log-likelihood changes by less than delta, or until it has reached\n+ * the max number of iterations.  \n+ */\n+class GMMExpectationMaximization private (\n+    private var k: Int, \n+    private var delta: Double, \n+    private var maxIterations: Int) extends Serializable {\n+      \n+  // Type aliases for convenience\n+  private type DenseDoubleVector = BreezeVector[Double]\n+  private type DenseDoubleMatrix = BreezeMatrix[Double]\n+  \n+  // number of samples per cluster to use when initializing Gaussians\n+  private val nSamples = 5;\n+  \n+  // A default instance, 2 Gaussians, 100 iterations, 0.01 log-likelihood threshold\n+  def this() = this(2, 0.01, 100)\n+  \n+  /** Set the number of Gaussians in the mixture model.  Default: 2 */\n+  def setK(k: Int): this.type = {\n+    this.k = k\n+    this\n+  }\n+  \n+  /** Set the maximum number of iterations to run. Default: 100 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+  \n+  /**\n+   * Set the largest change in log-likelihood at which convergence is \n+   * considered to have occurred.\n+   */\n+  def setDelta(delta: Double): this.type = {\n+    this.delta = delta\n+    this\n+  }\n+  \n+  /** Machine precision value used to ensure matrix conditioning */\n+  private val eps = math.pow(2.0, -52)\n+  \n+  /** Perform expectation maximization */\n+  def run(data: RDD[Vector]): GaussianMixtureModel = {\n+    val ctx = data.sparkContext\n+    \n+    // we will operate on the data as breeze data\n+    val breezeData = data.map{ u => u.toBreeze.toDenseVector }.cache()\n+    \n+    // Get length of the input vectors\n+    val d = breezeData.first.length \n+    \n+    // For each Gaussian, we will initialize the mean as the average\n+    // of some random samples from the data\n+    val samples = breezeData.takeSample(true, k * nSamples, scala.util.Random.nextInt)\n+    \n+    // C will be array of (weight, mean, covariance) tuples\n+    // we start with uniform weights, a random mean from the data, and\n+    // diagonal covariance matrices using component variances\n+    // derived from the samples \n+    var C = (0 until k).map(i => (1.0/k, \n+                                  vec_mean(samples.slice(i * nSamples, (i + 1) * nSamples)), \n+                                  init_cov(samples.slice(i * nSamples, (i + 1) * nSamples)))\n+                           ).toArray\n+    \n+    val acc_w     = new Array[Accumulator[Double]](k)\n+    val acc_mu    = new Array[Accumulator[DenseDoubleVector]](k)\n+    val acc_sigma = new Array[Accumulator[DenseDoubleMatrix]](k)\n+    \n+    var llh = Double.MinValue // current log-likelihood \n+    var llhp = 0.0            // previous log-likelihood\n+    \n+    var i, iter = 0\n+    do {\n+      // reset accumulators\n+      for(i <- 0 until k){\n+        acc_w(i)     = ctx.accumulator(0.0)\n+        acc_mu(i)    = ctx.accumulator(\n+                      BreezeVector.zeros[Double](d))(DenseDoubleVectorAccumulatorParam)\n+        acc_sigma(i) = ctx.accumulator(\n+                      BreezeMatrix.zeros[Double](d,d))(DenseDoubleMatrixAccumulatorParam)\n+      }\n+      \n+      val log_likelihood = ctx.accumulator(0.0)\n+            \n+      // broadcast the current weights and distributions to all nodes\n+      val dists = ctx.broadcast((0 until k).map(i => \n+                                  new MultivariateGaussian(C(i)._2, C(i)._3)).toArray)\n+      val weights = ctx.broadcast((0 until k).map(i => C(i)._1).toArray)\n+      \n+      // calculate partial assignments for each sample in the data\n+      // (often referred to as the \"E\" step in literature)\n+      breezeData.foreach(x => {  \n+        val p = (0 until k).map(i => \n+          eps + weights.value(i) * dists.value(i).pdf(x)).toArray\n+        val norm = sum(p)\n+        \n+        log_likelihood += math.log(norm)  \n+          \n+        // accumulate weighted sums  \n+        val xxt = x * new Transpose(x)\n+        for(i <- 0 until k){\n+          p(i) /= norm\n+          acc_w(i) += p(i)\n+          acc_mu(i) += x * p(i)\n+          acc_sigma(i) += xxt * p(i)\n+        }  \n+      })\n+      \n+      // Collect the computed sums\n+      val W = (0 until k).map(i => acc_w(i).value).toArray\n+      val MU = (0 until k).map(i => acc_mu(i).value).toArray\n+      val SIGMA = (0 until k).map(i => acc_sigma(i).value).toArray\n+      \n+      // Create new distributions based on the partial assignments\n+      // (often referred to as the \"M\" step in literature)\n+      C = (0 until k).map(i => {\n+            val weight = W(i) / sum(W)\n+            val mu = MU(i) / W(i)\n+            val sigma = SIGMA(i) / W(i) - mu * new Transpose(mu)\n+            (weight, mu, sigma)\n+          }).toArray\n+      \n+      llhp = llh; // current becomes previous\n+      llh = log_likelihood.value // this is the freshly computed log-likelihood\n+      iter += 1\n+    } while(iter < maxIterations && Math.abs(llh-llhp) > delta)\n+    \n+    // Need to convert the breeze matrices to MLlib matrices\n+    val weights = (0 until k).map(i => C(i)._1).toArray\n+    val means   = (0 until k).map(i => Vectors.fromBreeze(C(i)._2)).toArray\n+    val sigmas  = (0 until k).map(i => Matrices.fromBreeze(C(i)._3)).toArray\n+    new GaussianMixtureModel(weights, means, sigmas)\n+  }\n+  \n+  /** Sum the values in array of doubles */\n+  private def sum(x : Array[Double]) : Double = {\n+    var s : Double = 0.0\n+    (0 until x.length).foreach(j => s += x(j))\n+    s\n+  }\n+  \n+  /** Average of dense breeze vectors */\n+  private def vec_mean(x : Array[DenseDoubleVector]) : DenseDoubleVector = {\n+    val v = BreezeVector.zeros[Double](x(0).length)\n+    (0 until x.length).foreach(j => v += x(j))\n+    v / x.length.asInstanceOf[Double] \n+  }\n+  \n+  /**\n+   * Construct matrix where diagonal entries are element-wise\n+   * variance of input vectors (computes biased variance)\n+   */\n+  private def init_cov(x : Array[DenseDoubleVector]) : DenseDoubleMatrix = {\n+    val mu = vec_mean(x)\n+    val ss = BreezeVector.zeros[Double](x(0).length)\n+    val result = BreezeMatrix.eye[Double](ss.length)\n+    (0 until x.length).map(i => (x(i) - mu) :^ 2.0).foreach(u => ss += u)\n+    (0 until ss.length).foreach(i => result(i,i) = ss(i) / x.length)\n+    result\n+  }\n+  \n+  /** AccumulatorParam for Dense Breeze Vectors */\n+  private object DenseDoubleVectorAccumulatorParam extends AccumulatorParam[DenseDoubleVector] {\n+    def zero(initialVector : DenseDoubleVector) : DenseDoubleVector = {\n+      BreezeVector.zeros[Double](initialVector.length)\n+    }\n+    \n+    def addInPlace(a : DenseDoubleVector, b : DenseDoubleVector) : DenseDoubleVector = {\n+      a += b\n+    }\n+  }\n+  \n+  /** AccumulatorParam for Dense Breeze Matrices */\n+  private object DenseDoubleMatrixAccumulatorParam extends AccumulatorParam[DenseDoubleMatrix] {\n+    def zero(initialVector : DenseDoubleMatrix) : DenseDoubleMatrix = {\n+      BreezeMatrix.zeros[Double](initialVector.rows, initialVector.cols)\n+    }\n+    \n+    def addInPlace(a : DenseDoubleMatrix, b : DenseDoubleMatrix) : DenseDoubleMatrix = {\n+      a += b\n+    }\n+  }  \n+  \n+  /** \n+   * Utility class to implement the density function for multivariate Gaussian distribution.\n+   * Breeze provides this functionality, but it requires the Apache Commons Math library,\n+   * so this class is here so-as to not introduce a new dependency in Spark.\n+   */\n+  private class MultivariateGaussian(val mu : DenseDoubleVector, val sigma : DenseDoubleMatrix) \n+      extends Serializable {\n+    private val sigma_inv_2 = inv(sigma) * -0.5\n+    private val U = math.pow(2.0*math.Pi, -mu.length/2.0) * math.pow(det(sigma), -0.5)"
  }],
  "prId": 3022
}]