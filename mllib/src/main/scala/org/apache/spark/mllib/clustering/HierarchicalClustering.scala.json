[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "I don't think we need static train() methods since users can use the builder pattern from the HierarchicalClustering class.\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-04-01T21:23:42Z",
    "diffHunk": "@@ -0,0 +1,610 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+/**\n+ * Top-level methods for calling the hierarchical clustering algorithm\n+ */\n+object HierarchicalClustering extends Logging {"
  }, {
    "author": {
      "login": "yu-iskw"
    },
    "body": "Should we remove static train() methods of others? For example, KMeans has static train() method. I'd like to understand the difference. I think we should keep the design concept consistent for users. It is not good that some algorithms support static train() method and others don't support it.\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-04-02T04:34:47Z",
    "diffHunk": "@@ -0,0 +1,610 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+/**\n+ * Top-level methods for calling the hierarchical clustering algorithm\n+ */\n+object HierarchicalClustering extends Logging {"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "Those static train() methods used to be added everywhere, but at some point, we realized that they require a lot of duplicated code (since Java does not recognize default arguments).  We're trying to just add builder methods from now on, but we have to keep the old static train() methods for API stability.\n\nYou're right about consistency.  I've been thinking about whether we should start deprecating the old static train() methods.\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-04-02T18:12:22Z",
    "diffHunk": "@@ -0,0 +1,610 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+/**\n+ * Top-level methods for calling the hierarchical clustering algorithm\n+ */\n+object HierarchicalClustering extends Logging {"
  }, {
    "author": {
      "login": "yu-iskw"
    },
    "body": "I understand. I agree with that we should start deprecating the old ones. \nI am removing the train() static method from HierarchicalClustering object. \nThanks!\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-04-02T23:19:46Z",
    "diffHunk": "@@ -0,0 +1,610 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+/**\n+ * Top-level methods for calling the hierarchical clustering algorithm\n+ */\n+object HierarchicalClustering extends Logging {"
  }, {
    "author": {
      "login": "yu-iskw"
    },
    "body": "Sorry, one more question. How about consistency between Scala and Python? \nMLlib in PySpark doesn't support builder methods. How to call a train() method in PySpark should similar to that in Scala. I think that is a static train() method.\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-04-03T05:47:47Z",
    "diffHunk": "@@ -0,0 +1,610 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+/**\n+ * Top-level methods for calling the hierarchical clustering algorithm\n+ */\n+object HierarchicalClustering extends Logging {"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "Yes, that's an issue but one where there isn't a great solution. Let's discuss it on the JIRA: [https://issues.apache.org/jira/browse/SPARK-6682]\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-04-03T17:54:22Z",
    "diffHunk": "@@ -0,0 +1,610 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+/**\n+ * Top-level methods for calling the hierarchical clustering algorithm\n+ */\n+object HierarchicalClustering extends Logging {"
  }, {
    "author": {
      "login": "yu-iskw"
    },
    "body": "Alright. Thank you for letting me know.\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-04-03T17:57:01Z",
    "diffHunk": "@@ -0,0 +1,610 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+/**\n+ * Top-level methods for calling the hierarchical clustering algorithm\n+ */\n+object HierarchicalClustering extends Logging {"
  }],
  "prId": 5267
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "No parentheses for getters\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-04-01T21:23:45Z",
    "diffHunk": "@@ -0,0 +1,610 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+/**\n+ * Top-level methods for calling the hierarchical clustering algorithm\n+ */\n+object HierarchicalClustering extends Logging {\n+\n+  private[clustering] val ROOT_INDEX_KEY: Long = 1\n+\n+  /**\n+   * Trains a hierarchical clustering model with the given data\n+   *\n+   * @param data trained data\n+   * @param numClusters the maximum number of clusters you want\n+   * @return a hierarchical clustering model\n+   */\n+  def train(data: RDD[Vector], numClusters: Int): HierarchicalClusteringModel = {\n+    val algo = new HierarchicalClustering().setNumClusters(numClusters)\n+    algo.run(data)\n+  }\n+\n+  /**\n+   * Trains a hierarchical clustering model with the given data\n+   *\n+   * @param data training data\n+   * @param numClusters the maximum number of clusters you want\n+   * @param maxIterations the number of maximal iterations\n+   * @param maxRetries the number of maximum retries when the clustering can't be succeeded\n+   * @param seed the randomseed to generate the initial vectors for each bisecting\n+   * @return a hierarchical clustering model\n+   */\n+  def train(data: RDD[Vector],\n+    numClusters: Int,\n+    maxIterations: Int,\n+    maxRetries: Int,\n+    seed: Int): HierarchicalClusteringModel = {\n+\n+    val algo = new HierarchicalClustering().setNumClusters(numClusters)\n+        .setMaxIterations(maxIterations)\n+        .setMaxRetries(maxRetries)\n+        .setSeed(seed)\n+    algo.run(data)\n+  }\n+\n+  /**\n+   * Finds the closes cluster's center\n+   *\n+   * @param metric a distance metric\n+   * @param centers centers of the clusters\n+   * @param point a target point\n+   * @return an index of the array of clusters\n+   */\n+  private[mllib]\n+  def findClosestCenter(metric: Function2[BV[Double], BV[Double], Double])\n+        (centers: Seq[BV[Double]])(point: BV[Double]): Int = {\n+    val (closestCenter, closestIndex) =\n+      centers.zipWithIndex.map { case (center, idx) => (metric(center, point), idx)}.minBy(_._1)\n+    closestIndex\n+  }\n+}\n+\n+/**\n+ * This is a divisive hierarchical clustering algorithm based on bi-sect k-means algorithm.\n+ *\n+ * The main idea of this algorithm is based on \"A comparison of document clustering techniques\",\n+ * M. Steinbach, G. Karypis and V. Kumar. Workshop on Text Mining, KDD, 2000.\n+ * http://cs.fit.edu/~pkc/classes/ml-internet/papers/steinbach00tr.pdf\n+ *\n+ * @param numClusters tne number of clusters you want\n+ * @param clusterMap the pairs of cluster and its index as Map\n+ * @param maxIterations the number of maximal iterations\n+ * @param maxRetries the number of maximum retries\n+ * @param seed a random seed\n+ */\n+class HierarchicalClustering(\n+  private var numClusters: Int,\n+  private var clusterMap: Map[Long, ClusterTree],\n+  private var maxIterations: Int,\n+  private var maxRetries: Int,\n+  private var seed: Long) extends Logging {\n+\n+  /**\n+   * Constructs with the default configuration\n+   */\n+  def this() = this(20, mutable.ListMap.empty[Long, ClusterTree], 20, 10, 1)\n+\n+  /**\n+   * Sets the number of clusters you want\n+   */\n+  def setNumClusters(numClusters: Int): this.type = {\n+    this.numClusters = numClusters\n+    this\n+  }\n+\n+  /**\n+   * Sets the number of maximal iterations in each clustering step\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  def getSubIterations(): Int = this.maxIterations"
  }, {
    "author": {
      "login": "yu-iskw"
    },
    "body": "Got it. I am going to remove them. Thanks!\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-04-02T04:35:25Z",
    "diffHunk": "@@ -0,0 +1,610 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+/**\n+ * Top-level methods for calling the hierarchical clustering algorithm\n+ */\n+object HierarchicalClustering extends Logging {\n+\n+  private[clustering] val ROOT_INDEX_KEY: Long = 1\n+\n+  /**\n+   * Trains a hierarchical clustering model with the given data\n+   *\n+   * @param data trained data\n+   * @param numClusters the maximum number of clusters you want\n+   * @return a hierarchical clustering model\n+   */\n+  def train(data: RDD[Vector], numClusters: Int): HierarchicalClusteringModel = {\n+    val algo = new HierarchicalClustering().setNumClusters(numClusters)\n+    algo.run(data)\n+  }\n+\n+  /**\n+   * Trains a hierarchical clustering model with the given data\n+   *\n+   * @param data training data\n+   * @param numClusters the maximum number of clusters you want\n+   * @param maxIterations the number of maximal iterations\n+   * @param maxRetries the number of maximum retries when the clustering can't be succeeded\n+   * @param seed the randomseed to generate the initial vectors for each bisecting\n+   * @return a hierarchical clustering model\n+   */\n+  def train(data: RDD[Vector],\n+    numClusters: Int,\n+    maxIterations: Int,\n+    maxRetries: Int,\n+    seed: Int): HierarchicalClusteringModel = {\n+\n+    val algo = new HierarchicalClustering().setNumClusters(numClusters)\n+        .setMaxIterations(maxIterations)\n+        .setMaxRetries(maxRetries)\n+        .setSeed(seed)\n+    algo.run(data)\n+  }\n+\n+  /**\n+   * Finds the closes cluster's center\n+   *\n+   * @param metric a distance metric\n+   * @param centers centers of the clusters\n+   * @param point a target point\n+   * @return an index of the array of clusters\n+   */\n+  private[mllib]\n+  def findClosestCenter(metric: Function2[BV[Double], BV[Double], Double])\n+        (centers: Seq[BV[Double]])(point: BV[Double]): Int = {\n+    val (closestCenter, closestIndex) =\n+      centers.zipWithIndex.map { case (center, idx) => (metric(center, point), idx)}.minBy(_._1)\n+    closestIndex\n+  }\n+}\n+\n+/**\n+ * This is a divisive hierarchical clustering algorithm based on bi-sect k-means algorithm.\n+ *\n+ * The main idea of this algorithm is based on \"A comparison of document clustering techniques\",\n+ * M. Steinbach, G. Karypis and V. Kumar. Workshop on Text Mining, KDD, 2000.\n+ * http://cs.fit.edu/~pkc/classes/ml-internet/papers/steinbach00tr.pdf\n+ *\n+ * @param numClusters tne number of clusters you want\n+ * @param clusterMap the pairs of cluster and its index as Map\n+ * @param maxIterations the number of maximal iterations\n+ * @param maxRetries the number of maximum retries\n+ * @param seed a random seed\n+ */\n+class HierarchicalClustering(\n+  private var numClusters: Int,\n+  private var clusterMap: Map[Long, ClusterTree],\n+  private var maxIterations: Int,\n+  private var maxRetries: Int,\n+  private var seed: Long) extends Logging {\n+\n+  /**\n+   * Constructs with the default configuration\n+   */\n+  def this() = this(20, mutable.ListMap.empty[Long, ClusterTree], 20, 10, 1)\n+\n+  /**\n+   * Sets the number of clusters you want\n+   */\n+  def setNumClusters(numClusters: Int): this.type = {\n+    this.numClusters = numClusters\n+    this\n+  }\n+\n+  /**\n+   * Sets the number of maximal iterations in each clustering step\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  def getSubIterations(): Int = this.maxIterations"
  }],
  "prId": 5267
}, {
  "comments": [{
    "author": {
      "login": "freeman-lab"
    },
    "body": "Why the name swap? Shouldn't this be `getMaxIterations`?\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-04-27T05:05:27Z",
    "diffHunk": "@@ -0,0 +1,574 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+object HierarchicalClustering extends Logging {\n+\n+  private[clustering] val ROOT_INDEX_KEY: Long = 1\n+\n+  /**\n+   * Finds the closes cluster's center\n+   *\n+   * @param metric a distance metric\n+   * @param centers centers of the clusters\n+   * @param point a target point\n+   * @return an index of the array of clusters\n+   */\n+  private[mllib]\n+  def findClosestCenter(metric: Function2[BV[Double], BV[Double], Double])\n+        (centers: Seq[BV[Double]])(point: BV[Double]): Int = {\n+    val (closestCenter, closestIndex) =\n+      centers.zipWithIndex.map { case (center, idx) => (metric(center, point), idx)}.minBy(_._1)\n+    closestIndex\n+  }\n+}\n+\n+/**\n+ * This is a divisive hierarchical clustering algorithm based on bi-sect k-means algorithm.\n+ *\n+ * The main idea of this algorithm is based on \"A comparison of document clustering techniques\",\n+ * M. Steinbach, G. Karypis and V. Kumar. Workshop on Text Mining, KDD, 2000.\n+ * http://cs.fit.edu/~pkc/classes/ml-internet/papers/steinbach00tr.pdf\n+ *\n+ * @param numClusters tne number of clusters you want\n+ * @param clusterMap the pairs of cluster and its index as Map\n+ * @param maxIterations the number of maximal iterations\n+ * @param maxRetries the number of maximum retries\n+ * @param seed a random seed\n+ */\n+class HierarchicalClustering private (\n+  private var numClusters: Int,\n+  private var clusterMap: Map[Long, ClusterTree],\n+  private var maxIterations: Int,\n+  private var maxRetries: Int,\n+  private var seed: Long) extends Logging {\n+\n+  /**\n+   * Constructs with the default configuration\n+   */\n+  def this() = this(20, mutable.ListMap.empty[Long, ClusterTree], 20, 10, 1)\n+\n+  /**\n+   * Sets the number of clusters you want\n+   */\n+  def setNumClusters(numClusters: Int): this.type = {\n+    this.numClusters = numClusters\n+    this\n+  }\n+\n+  def getNumClusters: Int = this.numClusters\n+\n+  /**\n+   * Sets the number of maximal iterations in each clustering step\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  def getSubIterations: Int = this.maxIterations"
  }],
  "prId": 5267
}, {
  "comments": [{
    "author": {
      "login": "freeman-lab"
    },
    "body": "This use of `sc.broadcast` has no effect because the output isn't assigned or used. Instead, you want something like val `bcNewCenters = sc.broadcast(newCenters)` and then access within the map as `bcNewCenters.value`. This appears in a few other places as well.\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-04-27T05:19:04Z",
    "diffHunk": "@@ -0,0 +1,574 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+object HierarchicalClustering extends Logging {\n+\n+  private[clustering] val ROOT_INDEX_KEY: Long = 1\n+\n+  /**\n+   * Finds the closes cluster's center\n+   *\n+   * @param metric a distance metric\n+   * @param centers centers of the clusters\n+   * @param point a target point\n+   * @return an index of the array of clusters\n+   */\n+  private[mllib]\n+  def findClosestCenter(metric: Function2[BV[Double], BV[Double], Double])\n+        (centers: Seq[BV[Double]])(point: BV[Double]): Int = {\n+    val (closestCenter, closestIndex) =\n+      centers.zipWithIndex.map { case (center, idx) => (metric(center, point), idx)}.minBy(_._1)\n+    closestIndex\n+  }\n+}\n+\n+/**\n+ * This is a divisive hierarchical clustering algorithm based on bi-sect k-means algorithm.\n+ *\n+ * The main idea of this algorithm is based on \"A comparison of document clustering techniques\",\n+ * M. Steinbach, G. Karypis and V. Kumar. Workshop on Text Mining, KDD, 2000.\n+ * http://cs.fit.edu/~pkc/classes/ml-internet/papers/steinbach00tr.pdf\n+ *\n+ * @param numClusters tne number of clusters you want\n+ * @param clusterMap the pairs of cluster and its index as Map\n+ * @param maxIterations the number of maximal iterations\n+ * @param maxRetries the number of maximum retries\n+ * @param seed a random seed\n+ */\n+class HierarchicalClustering private (\n+  private var numClusters: Int,\n+  private var clusterMap: Map[Long, ClusterTree],\n+  private var maxIterations: Int,\n+  private var maxRetries: Int,\n+  private var seed: Long) extends Logging {\n+\n+  /**\n+   * Constructs with the default configuration\n+   */\n+  def this() = this(20, mutable.ListMap.empty[Long, ClusterTree], 20, 10, 1)\n+\n+  /**\n+   * Sets the number of clusters you want\n+   */\n+  def setNumClusters(numClusters: Int): this.type = {\n+    this.numClusters = numClusters\n+    this\n+  }\n+\n+  def getNumClusters: Int = this.numClusters\n+\n+  /**\n+   * Sets the number of maximal iterations in each clustering step\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  def getSubIterations: Int = this.maxIterations\n+\n+  /**\n+   * Sets the number of maximum retries of each clustering step\n+   */\n+  def setMaxRetries(maxRetries: Int): this.type = {\n+    this.maxRetries = maxRetries\n+    this\n+  }\n+\n+  def getMaxRetries: Int = this.maxRetries\n+\n+  /**\n+   * Sets the random seed\n+   */\n+  def setSeed(seed: Long): this.type = {\n+    this.seed = seed\n+    this\n+  }\n+\n+  def getSeed: Long = this.seed\n+\n+  /**\n+   * Runs the hierarchical clustering algorithm\n+   * @param input RDD of vectors\n+   * @return model for the hierarchical clustering\n+   */\n+  def run(input: RDD[Vector]): HierarchicalClusteringModel = {\n+    val sc = input.sparkContext\n+    log.info(s\"${sc.appName} starts a hierarchical clustering algorithm\")\n+\n+    var data = initData(input).cache()\n+    val startTime = System.currentTimeMillis()\n+\n+    // `clusters` is described as binary tree structure\n+    // `clusters(1)` means the root of a binary tree\n+    var clusters = summarizeAsClusters(data)\n+    var leafClusters = clusters\n+    var step = 1\n+    var numDividedClusters = 0\n+    var noMoreDividable = false\n+    var rddArray = Array.empty[RDD[(Long, BV[Double])]]\n+    // the number of maximum nodes of a binary tree by given parameter\n+    val multiplier = math.ceil(math.log10(this.numClusters) / math.log10(2.0)) + 1\n+    val maxAllNodesInTree = math.pow(2, multiplier).toInt\n+\n+    while (clusters.size < maxAllNodesInTree && noMoreDividable == false) {\n+      log.info(s\"${sc.appName} starts step ${step}\")\n+\n+      // enough to be clustered if the number of divided clusters is equal to 0\n+      val divided = getDividedClusters(data, leafClusters)\n+      if (divided.size == 0) {\n+        noMoreDividable = true\n+      }\n+      else {\n+        // update each index\n+        val newData = updateClusterIndex(data, divided).cache()\n+        rddArray = rddArray ++ Array(data)\n+        data = newData\n+\n+        // keep recent 2 cached RDDs in order to run more quickly\n+        if (rddArray.size > 1) {\n+          val head = rddArray.head\n+          head.unpersist()\n+          rddArray = rddArray.filterNot(_.hashCode() == head.hashCode())\n+        }\n+\n+        // merge the divided clusters with the map as the cluster tree\n+        clusters = clusters ++ divided\n+        numDividedClusters = data.map(_._1).distinct().count().toInt\n+        leafClusters = divided\n+        step += 1\n+\n+        log.info(s\"${sc.appName} adding ${divided.size} new clusters at step:${step}\")\n+      }\n+    }\n+    // unpersist kept RDDs\n+    rddArray.foreach(_.unpersist())\n+\n+    // build a cluster tree by Map class which is expressed\n+    log.info(s\"Building the cluster tree is started in ${sc.appName}\")\n+    val root = buildTree(clusters, HierarchicalClustering.ROOT_INDEX_KEY, this.numClusters)\n+    if (root == None) {\n+      new SparkException(\"Failed to build a cluster tree from a Map type of clusters\")\n+    }\n+\n+    // set the elapsed time for training\n+    val finishTime = (System.currentTimeMillis() - startTime) / 1000.0\n+    log.info(s\"Elapsed Time for Hierarchical Clustering Training: ${finishTime} [sec]\")\n+\n+    // make a hierarchical clustering model\n+    val model = new HierarchicalClusteringModel(root.get)\n+    val leavesNodes = model.getClusters()\n+    if (leavesNodes.size < this.numClusters) {\n+      log.warn(s\"# clusters is less than you have expected: ${leavesNodes.size} / ${numClusters}. \")\n+    }\n+    model\n+  }\n+\n+  /**\n+   * Assigns the initial cluster index id to all data\n+   */\n+  private[clustering]\n+  def initData(data: RDD[Vector]): RDD[(Long, BV[Double])] = {\n+    data.map { v: Vector => (HierarchicalClustering.ROOT_INDEX_KEY, v.toBreeze)}.cache\n+  }\n+\n+  /**\n+   * Summarizes data by each cluster as ClusterTree2 classes\n+   */\n+  private[clustering]\n+  def summarizeAsClusters(data: RDD[(Long, BV[Double])]): Map[Long, ClusterTree] = {\n+    // summarize input data\n+    val stats = summarize(data)\n+\n+    // convert statistics to ClusterTree class\n+    stats.map { case (i, (sum, n, sumOfSquares)) =>\n+      val center = Vectors.fromBreeze(sum :/ n)\n+      val variances = n match {\n+        case n if n > 1 => Vectors.fromBreeze(sumOfSquares.:*(n) - (sum :* sum) :/ (n * (n - 1.0)))\n+        case _ => Vectors.zeros(sum.size)\n+      }\n+      (i, new ClusterTree(center, n.toLong, variances))\n+    }.toMap\n+  }\n+\n+  /**\n+   * Summarizes data by each cluster as Map\n+   */\n+  private[clustering]\n+  def summarize(data: RDD[(Long, BV[Double])]): Map[Long, (BV[Double], Double, BV[Double])] = {\n+    data.mapPartitions { iter =>\n+      // calculate the accumulation of the all point in a partition and count the rows\n+      val map = mutable.Map.empty[Long, (BV[Double], Double, BV[Double])]\n+      iter.foreach { case (idx: Long, point: BV[Double]) =>\n+        // get a map value or else get a sparse vector\n+        val (sumBV, n, sumOfSquares) = map.get(idx)\n+            .getOrElse(BSV.zeros[Double](point.size), 0.0, BSV.zeros[Double](point.size))\n+        map(idx) = (sumBV + point, n + 1.0, sumOfSquares + (point :* point))\n+      }\n+      map.toIterator\n+    }.reduceByKey { case ((sum1, n1, sumOfSquares1), (sum2, n2, sumOfSquares2)) =>\n+      // sum the accumulation and the count in the all partition\n+      (sum1 + sum2, n1 + n2, sumOfSquares1 + sumOfSquares2)\n+    }.collect().toMap\n+  }\n+\n+  /**\n+   * Gets the initial centers for bi-sect k-means\n+   */\n+  private[clustering]\n+  def initChildrenCenter(clusters: Map[Long, BV[Double]]): Map[Long, BV[Double]] = {\n+    val rand = new XORShiftRandom()\n+    rand.setSeed(this.seed)\n+\n+    clusters.flatMap { case (idx, center) =>\n+      val childrenIndexes = Array(2 * idx, 2 * idx + 1)\n+      val relativeErrorCoefficient = 0.001\n+      Array(\n+        (2 * idx, center.map(elm => elm - (elm * relativeErrorCoefficient * rand.nextDouble()))),\n+        (2 * idx + 1, center.map(elm => elm + (elm * relativeErrorCoefficient * rand.nextDouble())))\n+      )\n+    }.toMap\n+  }\n+\n+  /**\n+   * Gets the new divided centers\n+   */\n+  private[clustering]\n+  def getDividedClusters(data: RDD[(Long, BV[Double])],\n+    dividedClusters: Map[Long, ClusterTree]): Map[Long, ClusterTree] = {\n+    val sc = data.sparkContext\n+    val appName = sc.appName\n+\n+    // get keys of dividable clusters\n+    val dividableKeys = dividedClusters.filter { case (idx, cluster) =>\n+      cluster.variances.toArray.sum > 0.0 && cluster.records >= 2\n+    }.keySet\n+    if (dividableKeys.size == 0) {\n+      log.info(s\"There is no dividable clusters in ${appName}.\")\n+      return Map.empty[Long, ClusterTree]\n+    }\n+\n+    // divide input data\n+    var dividableData = data.filter { case (idx, point) => dividableKeys.contains(idx)}\n+    var dividableClusters = dividedClusters.filter { case (k, v) => dividableKeys.contains(k)}\n+    val idealIndexes = dividableKeys.flatMap(idx => Array(2 * idx, 2 * idx + 1).toIterator)\n+    var stats = divide(data, dividableClusters)\n+\n+    // if there is clusters which is failed to be divided,\n+    // retry to divide only failed clusters again and again\n+    var tryTimes = 1\n+    while (stats.size < dividableKeys.size * 2 && tryTimes <= this.maxRetries) {\n+      // get the indexes of clusters which is failed to be divided\n+      val failedIndexes = idealIndexes.filterNot(stats.keySet.contains).map(idx => (idx / 2).toLong)\n+      val failedCenters = dividedClusters.filter { case (idx, clstr) => failedIndexes.contains(idx)}\n+      log.info(s\"# failed clusters is ${failedCenters.size} of ${dividableKeys.size}\" +\n+          s\"at ${tryTimes} times in ${appName}\")\n+\n+      // divide the failed clusters again\n+      sc.broadcast(failedIndexes)\n+      dividableData = data.filter { case (idx, point) => failedIndexes.contains(idx)}\n+      val missingStats = divide(dividableData, failedCenters)\n+      stats = stats ++ missingStats\n+      tryTimes += 1\n+    }\n+\n+    // make children clusters\n+    stats.filter { case (i, (sum, n, sumOfSquares)) => n > 0}\n+        .map { case (i, (sum, n, sumOfSquares)) =>\n+      val center = Vectors.fromBreeze(sum :/ n)\n+      val variances = n match {\n+        case 1 => Vectors.sparse(sum.size, Array(), Array())\n+        case _ => Vectors.fromBreeze(sumOfSquares.:*(n) - (sum :* sum) :/ (n * (n - 1.0)))\n+      }\n+      val child = new ClusterTree(center, n.toLong, variances)\n+      (i, child)\n+    }.toMap\n+  }\n+\n+  /**\n+   * Builds a cluster tree from a Map of clusters\n+   *\n+   * @param treeMap divided clusters as a Map class\n+   * @param rootIndex index you want to start\n+   * @param numClusters the number of clusters you want\n+   * @return a built cluster tree\n+   */\n+  private[clustering]\n+  def buildTree(treeMap: Map[Long, ClusterTree],\n+    rootIndex: Long,\n+    numClusters: Int): Option[ClusterTree] = {\n+\n+    // if there is no index in the Map\n+    if (!treeMap.contains(rootIndex)) return None\n+\n+    // build a cluster tree if the queue is empty or until the number of leaves clusters is enough\n+    var numLeavesClusters = 1\n+    val root = treeMap(rootIndex)\n+    var leavesQueue = Map(rootIndex -> root)\n+    while (leavesQueue.size > 0 && numLeavesClusters < numClusters) {\n+      // pick up the cluster whose variance is the maximum in the queue\n+      val mostScattered = leavesQueue.maxBy(_._2.variancesNorm)\n+      val mostScatteredKey = mostScattered._1\n+      val mostScatteredCluster = mostScattered._2\n+\n+      // relate the most scattered cluster to its children clusters\n+      val childrenIndexes = Array(2 * mostScatteredKey, 2 * mostScatteredKey + 1)\n+      if (childrenIndexes.forall(i => treeMap.contains(i))) {\n+        // insert children to the most scattered cluster\n+        val children = childrenIndexes.map(i => treeMap(i))\n+        mostScatteredCluster.insert(children)\n+\n+        // calculate the local dendrogram height\n+        // TODO Supports distance metrics other Euclidean distance metric\n+        val metric = (bv1: BV[Double], bv2: BV[Double]) => breezeNorm(bv1 - bv2, 2.0)\n+        val localHeight = children\n+            .map(child => metric(child.center.toBreeze, mostScatteredCluster.center.toBreeze)).max\n+        mostScatteredCluster.setLocalHeight(localHeight)\n+\n+        // update the queue\n+        leavesQueue = leavesQueue ++ childrenIndexes.map(i => (i -> treeMap(i))).toMap\n+        numLeavesClusters += 1\n+      }\n+\n+      // remove the cluster which is involved to the cluster tree\n+      leavesQueue = leavesQueue.filterNot(_ == mostScattered)\n+\n+      log.info(s\"Total Leaves Clusters: ${numLeavesClusters} / ${numClusters}. \" +\n+          s\"Cluster ${childrenIndexes.mkString(\",\")} are merged.\")\n+    }\n+    Some(root)\n+  }\n+\n+  /**\n+   * Divides the input data\n+   *\n+   * @param data the pairs of cluster index and point which you want to divide\n+   * @param clusters the clusters you want to divide AS a Map class\n+   * @return divided clusters as Map\n+   */\n+  private[clustering]\n+  def divide(data: RDD[(Long, BV[Double])],\n+    clusters: Map[Long, ClusterTree]): Map[Long, (BV[Double], Double, BV[Double])] = {\n+\n+    val sc = data.sparkContext\n+    val centers = clusters.map { case (idx, cluster) => (idx, cluster.center.toBreeze)}\n+    var newCenters = initChildrenCenter(centers)\n+    if (newCenters.size == 0) {\n+      return Map.empty[Long, (BV[Double], Double, BV[Double])]\n+    }\n+    sc.broadcast(newCenters)"
  }],
  "prId": 5267
}, {
  "comments": [{
    "author": {
      "login": "freeman-lab"
    },
    "body": "What is \"ClusterTree2\"? Did you mean \"ClusterTree\"?\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-04-27T05:20:30Z",
    "diffHunk": "@@ -0,0 +1,574 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+object HierarchicalClustering extends Logging {\n+\n+  private[clustering] val ROOT_INDEX_KEY: Long = 1\n+\n+  /**\n+   * Finds the closes cluster's center\n+   *\n+   * @param metric a distance metric\n+   * @param centers centers of the clusters\n+   * @param point a target point\n+   * @return an index of the array of clusters\n+   */\n+  private[mllib]\n+  def findClosestCenter(metric: Function2[BV[Double], BV[Double], Double])\n+        (centers: Seq[BV[Double]])(point: BV[Double]): Int = {\n+    val (closestCenter, closestIndex) =\n+      centers.zipWithIndex.map { case (center, idx) => (metric(center, point), idx)}.minBy(_._1)\n+    closestIndex\n+  }\n+}\n+\n+/**\n+ * This is a divisive hierarchical clustering algorithm based on bi-sect k-means algorithm.\n+ *\n+ * The main idea of this algorithm is based on \"A comparison of document clustering techniques\",\n+ * M. Steinbach, G. Karypis and V. Kumar. Workshop on Text Mining, KDD, 2000.\n+ * http://cs.fit.edu/~pkc/classes/ml-internet/papers/steinbach00tr.pdf\n+ *\n+ * @param numClusters tne number of clusters you want\n+ * @param clusterMap the pairs of cluster and its index as Map\n+ * @param maxIterations the number of maximal iterations\n+ * @param maxRetries the number of maximum retries\n+ * @param seed a random seed\n+ */\n+class HierarchicalClustering private (\n+  private var numClusters: Int,\n+  private var clusterMap: Map[Long, ClusterTree],\n+  private var maxIterations: Int,\n+  private var maxRetries: Int,\n+  private var seed: Long) extends Logging {\n+\n+  /**\n+   * Constructs with the default configuration\n+   */\n+  def this() = this(20, mutable.ListMap.empty[Long, ClusterTree], 20, 10, 1)\n+\n+  /**\n+   * Sets the number of clusters you want\n+   */\n+  def setNumClusters(numClusters: Int): this.type = {\n+    this.numClusters = numClusters\n+    this\n+  }\n+\n+  def getNumClusters: Int = this.numClusters\n+\n+  /**\n+   * Sets the number of maximal iterations in each clustering step\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  def getSubIterations: Int = this.maxIterations\n+\n+  /**\n+   * Sets the number of maximum retries of each clustering step\n+   */\n+  def setMaxRetries(maxRetries: Int): this.type = {\n+    this.maxRetries = maxRetries\n+    this\n+  }\n+\n+  def getMaxRetries: Int = this.maxRetries\n+\n+  /**\n+   * Sets the random seed\n+   */\n+  def setSeed(seed: Long): this.type = {\n+    this.seed = seed\n+    this\n+  }\n+\n+  def getSeed: Long = this.seed\n+\n+  /**\n+   * Runs the hierarchical clustering algorithm\n+   * @param input RDD of vectors\n+   * @return model for the hierarchical clustering\n+   */\n+  def run(input: RDD[Vector]): HierarchicalClusteringModel = {\n+    val sc = input.sparkContext\n+    log.info(s\"${sc.appName} starts a hierarchical clustering algorithm\")\n+\n+    var data = initData(input).cache()\n+    val startTime = System.currentTimeMillis()\n+\n+    // `clusters` is described as binary tree structure\n+    // `clusters(1)` means the root of a binary tree\n+    var clusters = summarizeAsClusters(data)\n+    var leafClusters = clusters\n+    var step = 1\n+    var numDividedClusters = 0\n+    var noMoreDividable = false\n+    var rddArray = Array.empty[RDD[(Long, BV[Double])]]\n+    // the number of maximum nodes of a binary tree by given parameter\n+    val multiplier = math.ceil(math.log10(this.numClusters) / math.log10(2.0)) + 1\n+    val maxAllNodesInTree = math.pow(2, multiplier).toInt\n+\n+    while (clusters.size < maxAllNodesInTree && noMoreDividable == false) {\n+      log.info(s\"${sc.appName} starts step ${step}\")\n+\n+      // enough to be clustered if the number of divided clusters is equal to 0\n+      val divided = getDividedClusters(data, leafClusters)\n+      if (divided.size == 0) {\n+        noMoreDividable = true\n+      }\n+      else {\n+        // update each index\n+        val newData = updateClusterIndex(data, divided).cache()\n+        rddArray = rddArray ++ Array(data)\n+        data = newData\n+\n+        // keep recent 2 cached RDDs in order to run more quickly\n+        if (rddArray.size > 1) {\n+          val head = rddArray.head\n+          head.unpersist()\n+          rddArray = rddArray.filterNot(_.hashCode() == head.hashCode())\n+        }\n+\n+        // merge the divided clusters with the map as the cluster tree\n+        clusters = clusters ++ divided\n+        numDividedClusters = data.map(_._1).distinct().count().toInt\n+        leafClusters = divided\n+        step += 1\n+\n+        log.info(s\"${sc.appName} adding ${divided.size} new clusters at step:${step}\")\n+      }\n+    }\n+    // unpersist kept RDDs\n+    rddArray.foreach(_.unpersist())\n+\n+    // build a cluster tree by Map class which is expressed\n+    log.info(s\"Building the cluster tree is started in ${sc.appName}\")\n+    val root = buildTree(clusters, HierarchicalClustering.ROOT_INDEX_KEY, this.numClusters)\n+    if (root == None) {\n+      new SparkException(\"Failed to build a cluster tree from a Map type of clusters\")\n+    }\n+\n+    // set the elapsed time for training\n+    val finishTime = (System.currentTimeMillis() - startTime) / 1000.0\n+    log.info(s\"Elapsed Time for Hierarchical Clustering Training: ${finishTime} [sec]\")\n+\n+    // make a hierarchical clustering model\n+    val model = new HierarchicalClusteringModel(root.get)\n+    val leavesNodes = model.getClusters()\n+    if (leavesNodes.size < this.numClusters) {\n+      log.warn(s\"# clusters is less than you have expected: ${leavesNodes.size} / ${numClusters}. \")\n+    }\n+    model\n+  }\n+\n+  /**\n+   * Assigns the initial cluster index id to all data\n+   */\n+  private[clustering]\n+  def initData(data: RDD[Vector]): RDD[(Long, BV[Double])] = {\n+    data.map { v: Vector => (HierarchicalClustering.ROOT_INDEX_KEY, v.toBreeze)}.cache\n+  }\n+\n+  /**\n+   * Summarizes data by each cluster as ClusterTree2 classes"
  }],
  "prId": 5267
}, {
  "comments": [{
    "author": {
      "login": "freeman-lab"
    },
    "body": "This algorithm contains a lot of `cacheing` and `unpersisting`. Can we add a more detailed note in the docstrings as to how much of a data set will be cached? In other words, what is the expected memory footprint of this algorithm relative to the size of the data set? This will be useful for users in considering how much RAM to have available for this algorithm.\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-04-27T05:22:46Z",
    "diffHunk": "@@ -0,0 +1,574 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+object HierarchicalClustering extends Logging {\n+\n+  private[clustering] val ROOT_INDEX_KEY: Long = 1\n+\n+  /**\n+   * Finds the closes cluster's center\n+   *\n+   * @param metric a distance metric\n+   * @param centers centers of the clusters\n+   * @param point a target point\n+   * @return an index of the array of clusters\n+   */\n+  private[mllib]\n+  def findClosestCenter(metric: Function2[BV[Double], BV[Double], Double])\n+        (centers: Seq[BV[Double]])(point: BV[Double]): Int = {\n+    val (closestCenter, closestIndex) =\n+      centers.zipWithIndex.map { case (center, idx) => (metric(center, point), idx)}.minBy(_._1)\n+    closestIndex\n+  }\n+}\n+\n+/**\n+ * This is a divisive hierarchical clustering algorithm based on bi-sect k-means algorithm.\n+ *\n+ * The main idea of this algorithm is based on \"A comparison of document clustering techniques\",\n+ * M. Steinbach, G. Karypis and V. Kumar. Workshop on Text Mining, KDD, 2000.\n+ * http://cs.fit.edu/~pkc/classes/ml-internet/papers/steinbach00tr.pdf\n+ *\n+ * @param numClusters tne number of clusters you want\n+ * @param clusterMap the pairs of cluster and its index as Map\n+ * @param maxIterations the number of maximal iterations\n+ * @param maxRetries the number of maximum retries\n+ * @param seed a random seed\n+ */\n+class HierarchicalClustering private (\n+  private var numClusters: Int,\n+  private var clusterMap: Map[Long, ClusterTree],\n+  private var maxIterations: Int,\n+  private var maxRetries: Int,\n+  private var seed: Long) extends Logging {\n+\n+  /**\n+   * Constructs with the default configuration\n+   */\n+  def this() = this(20, mutable.ListMap.empty[Long, ClusterTree], 20, 10, 1)\n+\n+  /**\n+   * Sets the number of clusters you want\n+   */\n+  def setNumClusters(numClusters: Int): this.type = {\n+    this.numClusters = numClusters\n+    this\n+  }\n+\n+  def getNumClusters: Int = this.numClusters\n+\n+  /**\n+   * Sets the number of maximal iterations in each clustering step\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  def getSubIterations: Int = this.maxIterations\n+\n+  /**\n+   * Sets the number of maximum retries of each clustering step\n+   */\n+  def setMaxRetries(maxRetries: Int): this.type = {\n+    this.maxRetries = maxRetries\n+    this\n+  }\n+\n+  def getMaxRetries: Int = this.maxRetries\n+\n+  /**\n+   * Sets the random seed\n+   */\n+  def setSeed(seed: Long): this.type = {\n+    this.seed = seed\n+    this\n+  }\n+\n+  def getSeed: Long = this.seed\n+\n+  /**\n+   * Runs the hierarchical clustering algorithm\n+   * @param input RDD of vectors\n+   * @return model for the hierarchical clustering\n+   */\n+  def run(input: RDD[Vector]): HierarchicalClusteringModel = {\n+    val sc = input.sparkContext\n+    log.info(s\"${sc.appName} starts a hierarchical clustering algorithm\")\n+\n+    var data = initData(input).cache()"
  }],
  "prId": 5267
}, {
  "comments": [{
    "author": {
      "login": "freeman-lab"
    },
    "body": "What are the cases where we'd want this to be anything other than 1?\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-06-12T21:16:57Z",
    "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+object HierarchicalClustering extends Logging {\n+\n+  private[clustering] val ROOT_INDEX_KEY: Long = 1"
  }, {
    "author": {
      "login": "yu-iskw"
    },
    "body": "I think there is no case we'd like to set another value other than 1. It is a just constant in order not to avoid a magic number. Have I got your question right?\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-06-12T22:31:37Z",
    "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+object HierarchicalClustering extends Logging {\n+\n+  private[clustering] val ROOT_INDEX_KEY: Long = 1"
  }],
  "prId": 5267
}, {
  "comments": [{
    "author": {
      "login": "freeman-lab"
    },
    "body": "Change `bi-sect` -> `bisect`\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-06-12T21:17:20Z",
    "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+object HierarchicalClustering extends Logging {\n+\n+  private[clustering] val ROOT_INDEX_KEY: Long = 1\n+\n+  /**\n+   * Finds the closes cluster's center\n+   *\n+   * @param metric a distance metric\n+   * @param centers centers of the clusters\n+   * @param point a target point\n+   * @return an index of the array of clusters\n+   */\n+  private[mllib]\n+  def findClosestCenter(metric: Function2[BV[Double], BV[Double], Double])\n+        (centers: Seq[BV[Double]])(point: BV[Double]): Int = {\n+    val (closestCenter, closestIndex) =\n+      centers.zipWithIndex.map { case (center, idx) => (metric(center, point), idx)}.minBy(_._1)\n+    closestIndex\n+  }\n+}\n+\n+/**\n+ * This is a divisive hierarchical clustering algorithm based on bi-sect k-means algorithm."
  }],
  "prId": 5267
}, {
  "comments": [{
    "author": {
      "login": "freeman-lab"
    },
    "body": "Change `bi-sect` -> `bisect`\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-06-12T21:17:37Z",
    "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+object HierarchicalClustering extends Logging {\n+\n+  private[clustering] val ROOT_INDEX_KEY: Long = 1\n+\n+  /**\n+   * Finds the closes cluster's center\n+   *\n+   * @param metric a distance metric\n+   * @param centers centers of the clusters\n+   * @param point a target point\n+   * @return an index of the array of clusters\n+   */\n+  private[mllib]\n+  def findClosestCenter(metric: Function2[BV[Double], BV[Double], Double])\n+        (centers: Seq[BV[Double]])(point: BV[Double]): Int = {\n+    val (closestCenter, closestIndex) =\n+      centers.zipWithIndex.map { case (center, idx) => (metric(center, point), idx)}.minBy(_._1)\n+    closestIndex\n+  }\n+}\n+\n+/**\n+ * This is a divisive hierarchical clustering algorithm based on bi-sect k-means algorithm.\n+ *\n+ * The main idea of this algorithm is based on \"A comparison of document clustering techniques\",\n+ * M. Steinbach, G. Karypis and V. Kumar. Workshop on Text Mining, KDD, 2000.\n+ * http://cs.fit.edu/~pkc/classes/ml-internet/papers/steinbach00tr.pdf\n+ *\n+ * @param numClusters tne number of clusters you want\n+ * @param clusterMap the pairs of cluster and its index as Map\n+ * @param maxIterations the number of maximal iterations\n+ * @param maxRetries the number of maximum retries\n+ * @param seed a random seed\n+ */\n+class HierarchicalClustering private (\n+  private var numClusters: Int,\n+  private var clusterMap: Map[Long, ClusterTree],\n+  private var maxIterations: Int,\n+  private var maxRetries: Int,\n+  private var seed: Long) extends Logging {\n+\n+  /**\n+   * Constructs with the default configuration\n+   */\n+  def this() = this(20, mutable.ListMap.empty[Long, ClusterTree], 20, 10, 1)\n+\n+  /**\n+   * Sets the number of clusters you want\n+   */\n+  def setNumClusters(numClusters: Int): this.type = {\n+    this.numClusters = numClusters\n+    this\n+  }\n+\n+  def getNumClusters: Int = this.numClusters\n+\n+  /**\n+   * Sets the number of maximal iterations in each clustering step\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  def getMaxIterations: Int = this.maxIterations\n+\n+  /**\n+   * Sets the number of maximum retries of each clustering step\n+   */\n+  def setMaxRetries(maxRetries: Int): this.type = {\n+    this.maxRetries = maxRetries\n+    this\n+  }\n+\n+  def getMaxRetries: Int = this.maxRetries\n+\n+  /**\n+   * Sets the random seed\n+   */\n+  def setSeed(seed: Long): this.type = {\n+    this.seed = seed\n+    this\n+  }\n+\n+  def getSeed: Long = this.seed\n+\n+  /**\n+   * Runs the hierarchical clustering algorithm\n+   * @param input RDD of vectors\n+   * @return model for the hierarchical clustering\n+   */\n+  def run(input: RDD[Vector]): HierarchicalClusteringModel = {\n+    val sc = input.sparkContext\n+    log.info(s\"${sc.appName} starts a hierarchical clustering algorithm\")\n+\n+    var data = initData(input).cache()\n+    val startTime = System.currentTimeMillis()\n+\n+    // `clusters` is described as binary tree structure\n+    // `clusters(1)` means the root of a binary tree\n+    var clusters = summarizeAsClusters(data)\n+    var leafClusters = clusters\n+    var step = 1\n+    var numDividedClusters = 0\n+    var noMoreDividable = false\n+    var rddArray = Array.empty[RDD[(Long, BV[Double])]]\n+    // the number of maximum nodes of a binary tree by given parameter\n+    val multiplier = math.ceil(math.log10(this.numClusters) / math.log10(2.0)) + 1\n+    val maxAllNodesInTree = math.pow(2, multiplier).toInt\n+\n+    while (clusters.size < maxAllNodesInTree && noMoreDividable == false) {\n+      log.info(s\"${sc.appName} starts step ${step}\")\n+\n+      // enough to be clustered if the number of divided clusters is equal to 0\n+      val divided = getDividedClusters(data, leafClusters)\n+      if (divided.size == 0) {\n+        noMoreDividable = true\n+      }\n+      else {\n+        // update each index\n+        val newData = updateClusterIndex(data, divided).cache()\n+        rddArray = rddArray ++ Array(data)\n+        data = newData\n+\n+        // keep recent 2 cached RDDs in order to run more quickly\n+        if (rddArray.size > 1) {\n+          val head = rddArray.head\n+          head.unpersist()\n+          rddArray = rddArray.filterNot(_.hashCode() == head.hashCode())\n+        }\n+\n+        // merge the divided clusters with the map as the cluster tree\n+        clusters = clusters ++ divided\n+        numDividedClusters = data.map(_._1).distinct().count().toInt\n+        leafClusters = divided\n+        step += 1\n+\n+        log.info(s\"${sc.appName} adding ${divided.size} new clusters at step:${step}\")\n+      }\n+    }\n+    // unpersist kept RDDs\n+    rddArray.foreach(_.unpersist())\n+\n+    // build a cluster tree by Map class which is expressed\n+    log.info(s\"Building the cluster tree is started in ${sc.appName}\")\n+    val root = buildTree(clusters, HierarchicalClustering.ROOT_INDEX_KEY, this.numClusters)\n+    if (root == None) {\n+      new SparkException(\"Failed to build a cluster tree from a Map type of clusters\")\n+    }\n+\n+    // set the elapsed time for training\n+    val finishTime = (System.currentTimeMillis() - startTime) / 1000.0\n+    log.info(s\"Elapsed Time for Hierarchical Clustering Training: ${finishTime} [sec]\")\n+\n+    // make a hierarchical clustering model\n+    val model = new HierarchicalClusteringModel(root.get)\n+    val leavesNodes = model.getClusters\n+    if (leavesNodes.size < this.numClusters) {\n+      log.warn(s\"# clusters is less than you have expected: ${leavesNodes.size} / ${numClusters}. \")\n+    }\n+    model\n+  }\n+\n+  /**\n+   * Assigns the initial cluster index id to all data\n+   */\n+  private[clustering]\n+  def initData(data: RDD[Vector]): RDD[(Long, BV[Double])] = {\n+    data.map { v: Vector => (HierarchicalClustering.ROOT_INDEX_KEY, v.toBreeze)}.cache\n+  }\n+\n+  /**\n+   * Summarizes data by each cluster as ClusterTree classes\n+   */\n+  private[clustering]\n+  def summarizeAsClusters(data: RDD[(Long, BV[Double])]): Map[Long, ClusterTree] = {\n+    // summarize input data\n+    val stats = summarize(data)\n+\n+    // convert statistics to ClusterTree class\n+    stats.map { case (i, (sum, n, sumOfSquares)) =>\n+      val center = Vectors.fromBreeze(sum :/ n)\n+      val variances = n match {\n+        case n if n > 1 => Vectors.fromBreeze(sumOfSquares.:*(n) - (sum :* sum) :/ (n * (n - 1.0)))\n+        case _ => Vectors.zeros(sum.size)\n+      }\n+      (i, new ClusterTree(center, n.toLong, variances))\n+    }.toMap\n+  }\n+\n+  /**\n+   * Summarizes data by each cluster as Map\n+   */\n+  private[clustering]\n+  def summarize(data: RDD[(Long, BV[Double])]): Map[Long, (BV[Double], Double, BV[Double])] = {\n+    data.mapPartitions { iter =>\n+      // calculate the accumulation of the all point in a partition and count the rows\n+      val map = mutable.Map.empty[Long, (BV[Double], Double, BV[Double])]\n+      iter.foreach { case (idx: Long, point: BV[Double]) =>\n+        // get a map value or else get a sparse vector\n+        val (sumBV, n, sumOfSquares) = map.get(idx)\n+            .getOrElse(BSV.zeros[Double](point.size), 0.0, BSV.zeros[Double](point.size))\n+        map(idx) = (sumBV + point, n + 1.0, sumOfSquares + (point :* point))\n+      }\n+      map.toIterator\n+    }.reduceByKey { case ((sum1, n1, sumOfSquares1), (sum2, n2, sumOfSquares2)) =>\n+      // sum the accumulation and the count in the all partition\n+      (sum1 + sum2, n1 + n2, sumOfSquares1 + sumOfSquares2)\n+    }.collect().toMap\n+  }\n+\n+  /**\n+   * Gets the new divided centers\n+   */\n+  private[clustering]\n+  def getDividedClusters(data: RDD[(Long, BV[Double])],\n+    dividedClusters: Map[Long, ClusterTree]): Map[Long, ClusterTree] = {\n+    val sc = data.sparkContext\n+    val appName = sc.appName\n+\n+    // get keys of dividable clusters\n+    val dividableKeys = dividedClusters.filter { case (idx, cluster) =>\n+      cluster.variances.toArray.sum > 0.0 && cluster.records >= 2\n+    }.keySet\n+    if (dividableKeys.size == 0) {\n+      log.info(s\"There is no dividable clusters in ${appName}.\")\n+      return Map.empty[Long, ClusterTree]\n+    }\n+\n+    // divide input data\n+    var dividableData = data.filter { case (idx, point) => dividableKeys.contains(idx)}\n+    var dividableClusters = dividedClusters.filter { case (k, v) => dividableKeys.contains(k)}\n+    val idealIndexes = dividableKeys.flatMap(idx => Array(2 * idx, 2 * idx + 1).toIterator)\n+    var stats = divide(data, dividableClusters)\n+\n+    // if there is clusters which is failed to be divided,\n+    // retry to divide only failed clusters again and again\n+    var tryTimes = 1\n+    while (stats.size < dividableKeys.size * 2 && tryTimes <= this.maxRetries) {\n+      // get the indexes of clusters which is failed to be divided\n+      val failedIndexes = idealIndexes.filterNot(stats.keySet.contains).map(idx => (idx / 2).toLong)\n+      val failedCenters = dividedClusters.filter { case (idx, clstr) => failedIndexes.contains(idx)}\n+      log.info(s\"# failed clusters is ${failedCenters.size} of ${dividableKeys.size}\" +\n+          s\"at ${tryTimes} times in ${appName}\")\n+\n+      // divide the failed clusters again\n+      val bcFailedIndexes = sc.broadcast(failedIndexes)\n+      dividableData = data.filter { case (idx, point) => bcFailedIndexes.value.contains(idx)}\n+      val missingStats = divide(dividableData, failedCenters)\n+      stats = stats ++ missingStats\n+      tryTimes += 1\n+    }\n+\n+    // make children clusters\n+    stats.filter { case (i, (sum, n, sumOfSquares)) => n > 0}\n+        .map { case (i, (sum, n, sumOfSquares)) =>\n+      val center = Vectors.fromBreeze(sum :/ n)\n+      val variances = n match {\n+        case 1 => Vectors.sparse(sum.size, Array(), Array())\n+        case _ => Vectors.fromBreeze(sumOfSquares.:*(n) - (sum :* sum) :/ (n * (n - 1.0)))\n+      }\n+      val child = new ClusterTree(center, n.toLong, variances)\n+      (i, child)\n+    }.toMap\n+  }\n+\n+  /**\n+   * Divides the input data\n+   *\n+   * @param data the pairs of cluster index and point which you want to divide\n+   * @param clusters the clusters you want to divide AS a Map class\n+   * @return divided clusters as Map\n+   */\n+  private[clustering]\n+  def divide(data: RDD[(Long, BV[Double])],\n+    clusters: Map[Long, ClusterTree]): Map[Long, (BV[Double], Double, BV[Double])] = {\n+\n+    val sc = data.sparkContext\n+    val centers = clusters.map { case (idx, cluster) => (idx, cluster.center.toBreeze)}\n+    var newCenters = initChildrenCenter(centers)\n+    if (newCenters.size == 0) {\n+      return Map.empty[Long, (BV[Double], Double, BV[Double])]\n+    }\n+    var bcNewCenters = sc.broadcast(newCenters)\n+\n+    // TODO Supports distance metrics other Euclidean distance metric\n+    val metric = (bv1: BV[Double], bv2: BV[Double]) => breezeNorm(bv1 - bv2, 2.0)\n+    val bcMetric = sc.broadcast(metric)\n+\n+    val vectorSize = newCenters(newCenters.keySet.min).size\n+    var stats = newCenters.keys.map { idx =>\n+      (idx, (BSV.zeros[Double](vectorSize).toVector, 0.0, BSV.zeros[Double](vectorSize).toVector))\n+    }.toMap\n+\n+    var subIter = 0\n+    var diffVariances = Double.MaxValue\n+    var oldVariances = Double.MaxValue\n+    var variances = Double.MaxValue\n+    while (subIter < this.maxIterations && diffVariances > 10E-4) {\n+      // calculate summary of each cluster\n+      val eachStats = data.mapPartitions { iter =>\n+        val map = mutable.Map.empty[Long, (BV[Double], Double, BV[Double])]\n+        iter.foreach { case (idx, point) =>\n+          // calculate next index number\n+          val childrenCenters = Array(2 * idx, 2 * idx + 1)\n+              .filter(bcNewCenters.value.keySet.contains(_)).map(bcNewCenters.value(_)).toArray\n+          if (childrenCenters.size >= 1) {\n+            val closestIndex =\n+              HierarchicalClustering.findClosestCenter(bcMetric.value)(childrenCenters)(point)\n+            val nextIndex = 2 * idx + closestIndex\n+\n+            // get a map value or else get a sparse vector\n+            val (sumBV, n, sumOfSquares) = map.get(nextIndex)\n+                .getOrElse(BSV.zeros[Double](point.size), 0.0, BSV.zeros[Double](point.size))\n+            map(nextIndex) = (sumBV + point, n + 1.0, sumOfSquares + (point :* point))\n+          }\n+        }\n+        map.toIterator\n+      }.reduceByKey { case ((sv1, n1, sumOfSquares1), (sv2, n2, sumOfSquares2)) =>\n+        // sum the accumulation and the count in the all partition\n+        (sv1 + sv2, n1 + n2, sumOfSquares1 + sumOfSquares2)\n+      }.collect().toMap\n+\n+      // calculate the center of each cluster\n+      newCenters = eachStats.map { case (idx, (sum, n, sumOfSquares)) => (idx, sum :/ n)}\n+      bcNewCenters = sc.broadcast(newCenters)\n+\n+      // update summary of each cluster\n+      stats = eachStats.toMap\n+\n+      variances = stats.map { case (idx, (sum, n, sumOfSquares)) =>\n+        math.pow(sumOfSquares.toArray.sum, 1.0 / sumOfSquares.size)\n+      }.sum\n+      diffVariances = math.abs(oldVariances - variances) / oldVariances\n+      oldVariances = variances\n+      subIter += 1\n+    }\n+    stats\n+  }\n+\n+  /**\n+   * Gets the initial centers for bi-sect k-means"
  }],
  "prId": 5267
}, {
  "comments": [{
    "author": {
      "login": "freeman-lab"
    },
    "body": "grammar nit, change `until the number of leaves clusters` -> `until the number of leaf clusters`\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-06-12T21:18:15Z",
    "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+object HierarchicalClustering extends Logging {\n+\n+  private[clustering] val ROOT_INDEX_KEY: Long = 1\n+\n+  /**\n+   * Finds the closes cluster's center\n+   *\n+   * @param metric a distance metric\n+   * @param centers centers of the clusters\n+   * @param point a target point\n+   * @return an index of the array of clusters\n+   */\n+  private[mllib]\n+  def findClosestCenter(metric: Function2[BV[Double], BV[Double], Double])\n+        (centers: Seq[BV[Double]])(point: BV[Double]): Int = {\n+    val (closestCenter, closestIndex) =\n+      centers.zipWithIndex.map { case (center, idx) => (metric(center, point), idx)}.minBy(_._1)\n+    closestIndex\n+  }\n+}\n+\n+/**\n+ * This is a divisive hierarchical clustering algorithm based on bi-sect k-means algorithm.\n+ *\n+ * The main idea of this algorithm is based on \"A comparison of document clustering techniques\",\n+ * M. Steinbach, G. Karypis and V. Kumar. Workshop on Text Mining, KDD, 2000.\n+ * http://cs.fit.edu/~pkc/classes/ml-internet/papers/steinbach00tr.pdf\n+ *\n+ * @param numClusters tne number of clusters you want\n+ * @param clusterMap the pairs of cluster and its index as Map\n+ * @param maxIterations the number of maximal iterations\n+ * @param maxRetries the number of maximum retries\n+ * @param seed a random seed\n+ */\n+class HierarchicalClustering private (\n+  private var numClusters: Int,\n+  private var clusterMap: Map[Long, ClusterTree],\n+  private var maxIterations: Int,\n+  private var maxRetries: Int,\n+  private var seed: Long) extends Logging {\n+\n+  /**\n+   * Constructs with the default configuration\n+   */\n+  def this() = this(20, mutable.ListMap.empty[Long, ClusterTree], 20, 10, 1)\n+\n+  /**\n+   * Sets the number of clusters you want\n+   */\n+  def setNumClusters(numClusters: Int): this.type = {\n+    this.numClusters = numClusters\n+    this\n+  }\n+\n+  def getNumClusters: Int = this.numClusters\n+\n+  /**\n+   * Sets the number of maximal iterations in each clustering step\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  def getMaxIterations: Int = this.maxIterations\n+\n+  /**\n+   * Sets the number of maximum retries of each clustering step\n+   */\n+  def setMaxRetries(maxRetries: Int): this.type = {\n+    this.maxRetries = maxRetries\n+    this\n+  }\n+\n+  def getMaxRetries: Int = this.maxRetries\n+\n+  /**\n+   * Sets the random seed\n+   */\n+  def setSeed(seed: Long): this.type = {\n+    this.seed = seed\n+    this\n+  }\n+\n+  def getSeed: Long = this.seed\n+\n+  /**\n+   * Runs the hierarchical clustering algorithm\n+   * @param input RDD of vectors\n+   * @return model for the hierarchical clustering\n+   */\n+  def run(input: RDD[Vector]): HierarchicalClusteringModel = {\n+    val sc = input.sparkContext\n+    log.info(s\"${sc.appName} starts a hierarchical clustering algorithm\")\n+\n+    var data = initData(input).cache()\n+    val startTime = System.currentTimeMillis()\n+\n+    // `clusters` is described as binary tree structure\n+    // `clusters(1)` means the root of a binary tree\n+    var clusters = summarizeAsClusters(data)\n+    var leafClusters = clusters\n+    var step = 1\n+    var numDividedClusters = 0\n+    var noMoreDividable = false\n+    var rddArray = Array.empty[RDD[(Long, BV[Double])]]\n+    // the number of maximum nodes of a binary tree by given parameter\n+    val multiplier = math.ceil(math.log10(this.numClusters) / math.log10(2.0)) + 1\n+    val maxAllNodesInTree = math.pow(2, multiplier).toInt\n+\n+    while (clusters.size < maxAllNodesInTree && noMoreDividable == false) {\n+      log.info(s\"${sc.appName} starts step ${step}\")\n+\n+      // enough to be clustered if the number of divided clusters is equal to 0\n+      val divided = getDividedClusters(data, leafClusters)\n+      if (divided.size == 0) {\n+        noMoreDividable = true\n+      }\n+      else {\n+        // update each index\n+        val newData = updateClusterIndex(data, divided).cache()\n+        rddArray = rddArray ++ Array(data)\n+        data = newData\n+\n+        // keep recent 2 cached RDDs in order to run more quickly\n+        if (rddArray.size > 1) {\n+          val head = rddArray.head\n+          head.unpersist()\n+          rddArray = rddArray.filterNot(_.hashCode() == head.hashCode())\n+        }\n+\n+        // merge the divided clusters with the map as the cluster tree\n+        clusters = clusters ++ divided\n+        numDividedClusters = data.map(_._1).distinct().count().toInt\n+        leafClusters = divided\n+        step += 1\n+\n+        log.info(s\"${sc.appName} adding ${divided.size} new clusters at step:${step}\")\n+      }\n+    }\n+    // unpersist kept RDDs\n+    rddArray.foreach(_.unpersist())\n+\n+    // build a cluster tree by Map class which is expressed\n+    log.info(s\"Building the cluster tree is started in ${sc.appName}\")\n+    val root = buildTree(clusters, HierarchicalClustering.ROOT_INDEX_KEY, this.numClusters)\n+    if (root == None) {\n+      new SparkException(\"Failed to build a cluster tree from a Map type of clusters\")\n+    }\n+\n+    // set the elapsed time for training\n+    val finishTime = (System.currentTimeMillis() - startTime) / 1000.0\n+    log.info(s\"Elapsed Time for Hierarchical Clustering Training: ${finishTime} [sec]\")\n+\n+    // make a hierarchical clustering model\n+    val model = new HierarchicalClusteringModel(root.get)\n+    val leavesNodes = model.getClusters\n+    if (leavesNodes.size < this.numClusters) {\n+      log.warn(s\"# clusters is less than you have expected: ${leavesNodes.size} / ${numClusters}. \")\n+    }\n+    model\n+  }\n+\n+  /**\n+   * Assigns the initial cluster index id to all data\n+   */\n+  private[clustering]\n+  def initData(data: RDD[Vector]): RDD[(Long, BV[Double])] = {\n+    data.map { v: Vector => (HierarchicalClustering.ROOT_INDEX_KEY, v.toBreeze)}.cache\n+  }\n+\n+  /**\n+   * Summarizes data by each cluster as ClusterTree classes\n+   */\n+  private[clustering]\n+  def summarizeAsClusters(data: RDD[(Long, BV[Double])]): Map[Long, ClusterTree] = {\n+    // summarize input data\n+    val stats = summarize(data)\n+\n+    // convert statistics to ClusterTree class\n+    stats.map { case (i, (sum, n, sumOfSquares)) =>\n+      val center = Vectors.fromBreeze(sum :/ n)\n+      val variances = n match {\n+        case n if n > 1 => Vectors.fromBreeze(sumOfSquares.:*(n) - (sum :* sum) :/ (n * (n - 1.0)))\n+        case _ => Vectors.zeros(sum.size)\n+      }\n+      (i, new ClusterTree(center, n.toLong, variances))\n+    }.toMap\n+  }\n+\n+  /**\n+   * Summarizes data by each cluster as Map\n+   */\n+  private[clustering]\n+  def summarize(data: RDD[(Long, BV[Double])]): Map[Long, (BV[Double], Double, BV[Double])] = {\n+    data.mapPartitions { iter =>\n+      // calculate the accumulation of the all point in a partition and count the rows\n+      val map = mutable.Map.empty[Long, (BV[Double], Double, BV[Double])]\n+      iter.foreach { case (idx: Long, point: BV[Double]) =>\n+        // get a map value or else get a sparse vector\n+        val (sumBV, n, sumOfSquares) = map.get(idx)\n+            .getOrElse(BSV.zeros[Double](point.size), 0.0, BSV.zeros[Double](point.size))\n+        map(idx) = (sumBV + point, n + 1.0, sumOfSquares + (point :* point))\n+      }\n+      map.toIterator\n+    }.reduceByKey { case ((sum1, n1, sumOfSquares1), (sum2, n2, sumOfSquares2)) =>\n+      // sum the accumulation and the count in the all partition\n+      (sum1 + sum2, n1 + n2, sumOfSquares1 + sumOfSquares2)\n+    }.collect().toMap\n+  }\n+\n+  /**\n+   * Gets the new divided centers\n+   */\n+  private[clustering]\n+  def getDividedClusters(data: RDD[(Long, BV[Double])],\n+    dividedClusters: Map[Long, ClusterTree]): Map[Long, ClusterTree] = {\n+    val sc = data.sparkContext\n+    val appName = sc.appName\n+\n+    // get keys of dividable clusters\n+    val dividableKeys = dividedClusters.filter { case (idx, cluster) =>\n+      cluster.variances.toArray.sum > 0.0 && cluster.records >= 2\n+    }.keySet\n+    if (dividableKeys.size == 0) {\n+      log.info(s\"There is no dividable clusters in ${appName}.\")\n+      return Map.empty[Long, ClusterTree]\n+    }\n+\n+    // divide input data\n+    var dividableData = data.filter { case (idx, point) => dividableKeys.contains(idx)}\n+    var dividableClusters = dividedClusters.filter { case (k, v) => dividableKeys.contains(k)}\n+    val idealIndexes = dividableKeys.flatMap(idx => Array(2 * idx, 2 * idx + 1).toIterator)\n+    var stats = divide(data, dividableClusters)\n+\n+    // if there is clusters which is failed to be divided,\n+    // retry to divide only failed clusters again and again\n+    var tryTimes = 1\n+    while (stats.size < dividableKeys.size * 2 && tryTimes <= this.maxRetries) {\n+      // get the indexes of clusters which is failed to be divided\n+      val failedIndexes = idealIndexes.filterNot(stats.keySet.contains).map(idx => (idx / 2).toLong)\n+      val failedCenters = dividedClusters.filter { case (idx, clstr) => failedIndexes.contains(idx)}\n+      log.info(s\"# failed clusters is ${failedCenters.size} of ${dividableKeys.size}\" +\n+          s\"at ${tryTimes} times in ${appName}\")\n+\n+      // divide the failed clusters again\n+      val bcFailedIndexes = sc.broadcast(failedIndexes)\n+      dividableData = data.filter { case (idx, point) => bcFailedIndexes.value.contains(idx)}\n+      val missingStats = divide(dividableData, failedCenters)\n+      stats = stats ++ missingStats\n+      tryTimes += 1\n+    }\n+\n+    // make children clusters\n+    stats.filter { case (i, (sum, n, sumOfSquares)) => n > 0}\n+        .map { case (i, (sum, n, sumOfSquares)) =>\n+      val center = Vectors.fromBreeze(sum :/ n)\n+      val variances = n match {\n+        case 1 => Vectors.sparse(sum.size, Array(), Array())\n+        case _ => Vectors.fromBreeze(sumOfSquares.:*(n) - (sum :* sum) :/ (n * (n - 1.0)))\n+      }\n+      val child = new ClusterTree(center, n.toLong, variances)\n+      (i, child)\n+    }.toMap\n+  }\n+\n+  /**\n+   * Divides the input data\n+   *\n+   * @param data the pairs of cluster index and point which you want to divide\n+   * @param clusters the clusters you want to divide AS a Map class\n+   * @return divided clusters as Map\n+   */\n+  private[clustering]\n+  def divide(data: RDD[(Long, BV[Double])],\n+    clusters: Map[Long, ClusterTree]): Map[Long, (BV[Double], Double, BV[Double])] = {\n+\n+    val sc = data.sparkContext\n+    val centers = clusters.map { case (idx, cluster) => (idx, cluster.center.toBreeze)}\n+    var newCenters = initChildrenCenter(centers)\n+    if (newCenters.size == 0) {\n+      return Map.empty[Long, (BV[Double], Double, BV[Double])]\n+    }\n+    var bcNewCenters = sc.broadcast(newCenters)\n+\n+    // TODO Supports distance metrics other Euclidean distance metric\n+    val metric = (bv1: BV[Double], bv2: BV[Double]) => breezeNorm(bv1 - bv2, 2.0)\n+    val bcMetric = sc.broadcast(metric)\n+\n+    val vectorSize = newCenters(newCenters.keySet.min).size\n+    var stats = newCenters.keys.map { idx =>\n+      (idx, (BSV.zeros[Double](vectorSize).toVector, 0.0, BSV.zeros[Double](vectorSize).toVector))\n+    }.toMap\n+\n+    var subIter = 0\n+    var diffVariances = Double.MaxValue\n+    var oldVariances = Double.MaxValue\n+    var variances = Double.MaxValue\n+    while (subIter < this.maxIterations && diffVariances > 10E-4) {\n+      // calculate summary of each cluster\n+      val eachStats = data.mapPartitions { iter =>\n+        val map = mutable.Map.empty[Long, (BV[Double], Double, BV[Double])]\n+        iter.foreach { case (idx, point) =>\n+          // calculate next index number\n+          val childrenCenters = Array(2 * idx, 2 * idx + 1)\n+              .filter(bcNewCenters.value.keySet.contains(_)).map(bcNewCenters.value(_)).toArray\n+          if (childrenCenters.size >= 1) {\n+            val closestIndex =\n+              HierarchicalClustering.findClosestCenter(bcMetric.value)(childrenCenters)(point)\n+            val nextIndex = 2 * idx + closestIndex\n+\n+            // get a map value or else get a sparse vector\n+            val (sumBV, n, sumOfSquares) = map.get(nextIndex)\n+                .getOrElse(BSV.zeros[Double](point.size), 0.0, BSV.zeros[Double](point.size))\n+            map(nextIndex) = (sumBV + point, n + 1.0, sumOfSquares + (point :* point))\n+          }\n+        }\n+        map.toIterator\n+      }.reduceByKey { case ((sv1, n1, sumOfSquares1), (sv2, n2, sumOfSquares2)) =>\n+        // sum the accumulation and the count in the all partition\n+        (sv1 + sv2, n1 + n2, sumOfSquares1 + sumOfSquares2)\n+      }.collect().toMap\n+\n+      // calculate the center of each cluster\n+      newCenters = eachStats.map { case (idx, (sum, n, sumOfSquares)) => (idx, sum :/ n)}\n+      bcNewCenters = sc.broadcast(newCenters)\n+\n+      // update summary of each cluster\n+      stats = eachStats.toMap\n+\n+      variances = stats.map { case (idx, (sum, n, sumOfSquares)) =>\n+        math.pow(sumOfSquares.toArray.sum, 1.0 / sumOfSquares.size)\n+      }.sum\n+      diffVariances = math.abs(oldVariances - variances) / oldVariances\n+      oldVariances = variances\n+      subIter += 1\n+    }\n+    stats\n+  }\n+\n+  /**\n+   * Gets the initial centers for bi-sect k-means\n+   */\n+  private[clustering]\n+  def initChildrenCenter(clusters: Map[Long, BV[Double]]): Map[Long, BV[Double]] = {\n+    val rand = new XORShiftRandom()\n+    rand.setSeed(this.seed)\n+\n+    clusters.flatMap { case (idx, center) =>\n+      val childrenIndexes = Array(2 * idx, 2 * idx + 1)\n+      val relativeErrorCoefficient = 0.001\n+      Array(\n+        (2 * idx, center.map(elm => elm - (elm * relativeErrorCoefficient * rand.nextDouble()))),\n+        (2 * idx + 1, center.map(elm => elm + (elm * relativeErrorCoefficient * rand.nextDouble())))\n+      )\n+    }.toMap\n+  }\n+\n+  /**\n+   * Builds a cluster tree from a Map of clusters\n+   *\n+   * @param treeMap divided clusters as a Map class\n+   * @param rootIndex index you want to start\n+   * @param numClusters the number of clusters you want\n+   * @return a built cluster tree\n+   */\n+  private[clustering]\n+  def buildTree(treeMap: Map[Long, ClusterTree],\n+    rootIndex: Long,\n+    numClusters: Int): Option[ClusterTree] = {\n+\n+    // if there is no index in the Map\n+    if (!treeMap.contains(rootIndex)) return None\n+\n+    // build a cluster tree if the queue is empty or until the number of leaves clusters is enough"
  }],
  "prId": 5267
}, {
  "comments": [{
    "author": {
      "login": "freeman-lab"
    },
    "body": "grammar nit, change to `if there are clusters which failed to be divided`\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-06-12T21:19:13Z",
    "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+object HierarchicalClustering extends Logging {\n+\n+  private[clustering] val ROOT_INDEX_KEY: Long = 1\n+\n+  /**\n+   * Finds the closes cluster's center\n+   *\n+   * @param metric a distance metric\n+   * @param centers centers of the clusters\n+   * @param point a target point\n+   * @return an index of the array of clusters\n+   */\n+  private[mllib]\n+  def findClosestCenter(metric: Function2[BV[Double], BV[Double], Double])\n+        (centers: Seq[BV[Double]])(point: BV[Double]): Int = {\n+    val (closestCenter, closestIndex) =\n+      centers.zipWithIndex.map { case (center, idx) => (metric(center, point), idx)}.minBy(_._1)\n+    closestIndex\n+  }\n+}\n+\n+/**\n+ * This is a divisive hierarchical clustering algorithm based on bi-sect k-means algorithm.\n+ *\n+ * The main idea of this algorithm is based on \"A comparison of document clustering techniques\",\n+ * M. Steinbach, G. Karypis and V. Kumar. Workshop on Text Mining, KDD, 2000.\n+ * http://cs.fit.edu/~pkc/classes/ml-internet/papers/steinbach00tr.pdf\n+ *\n+ * @param numClusters tne number of clusters you want\n+ * @param clusterMap the pairs of cluster and its index as Map\n+ * @param maxIterations the number of maximal iterations\n+ * @param maxRetries the number of maximum retries\n+ * @param seed a random seed\n+ */\n+class HierarchicalClustering private (\n+  private var numClusters: Int,\n+  private var clusterMap: Map[Long, ClusterTree],\n+  private var maxIterations: Int,\n+  private var maxRetries: Int,\n+  private var seed: Long) extends Logging {\n+\n+  /**\n+   * Constructs with the default configuration\n+   */\n+  def this() = this(20, mutable.ListMap.empty[Long, ClusterTree], 20, 10, 1)\n+\n+  /**\n+   * Sets the number of clusters you want\n+   */\n+  def setNumClusters(numClusters: Int): this.type = {\n+    this.numClusters = numClusters\n+    this\n+  }\n+\n+  def getNumClusters: Int = this.numClusters\n+\n+  /**\n+   * Sets the number of maximal iterations in each clustering step\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  def getMaxIterations: Int = this.maxIterations\n+\n+  /**\n+   * Sets the number of maximum retries of each clustering step\n+   */\n+  def setMaxRetries(maxRetries: Int): this.type = {\n+    this.maxRetries = maxRetries\n+    this\n+  }\n+\n+  def getMaxRetries: Int = this.maxRetries\n+\n+  /**\n+   * Sets the random seed\n+   */\n+  def setSeed(seed: Long): this.type = {\n+    this.seed = seed\n+    this\n+  }\n+\n+  def getSeed: Long = this.seed\n+\n+  /**\n+   * Runs the hierarchical clustering algorithm\n+   * @param input RDD of vectors\n+   * @return model for the hierarchical clustering\n+   */\n+  def run(input: RDD[Vector]): HierarchicalClusteringModel = {\n+    val sc = input.sparkContext\n+    log.info(s\"${sc.appName} starts a hierarchical clustering algorithm\")\n+\n+    var data = initData(input).cache()\n+    val startTime = System.currentTimeMillis()\n+\n+    // `clusters` is described as binary tree structure\n+    // `clusters(1)` means the root of a binary tree\n+    var clusters = summarizeAsClusters(data)\n+    var leafClusters = clusters\n+    var step = 1\n+    var numDividedClusters = 0\n+    var noMoreDividable = false\n+    var rddArray = Array.empty[RDD[(Long, BV[Double])]]\n+    // the number of maximum nodes of a binary tree by given parameter\n+    val multiplier = math.ceil(math.log10(this.numClusters) / math.log10(2.0)) + 1\n+    val maxAllNodesInTree = math.pow(2, multiplier).toInt\n+\n+    while (clusters.size < maxAllNodesInTree && noMoreDividable == false) {\n+      log.info(s\"${sc.appName} starts step ${step}\")\n+\n+      // enough to be clustered if the number of divided clusters is equal to 0\n+      val divided = getDividedClusters(data, leafClusters)\n+      if (divided.size == 0) {\n+        noMoreDividable = true\n+      }\n+      else {\n+        // update each index\n+        val newData = updateClusterIndex(data, divided).cache()\n+        rddArray = rddArray ++ Array(data)\n+        data = newData\n+\n+        // keep recent 2 cached RDDs in order to run more quickly\n+        if (rddArray.size > 1) {\n+          val head = rddArray.head\n+          head.unpersist()\n+          rddArray = rddArray.filterNot(_.hashCode() == head.hashCode())\n+        }\n+\n+        // merge the divided clusters with the map as the cluster tree\n+        clusters = clusters ++ divided\n+        numDividedClusters = data.map(_._1).distinct().count().toInt\n+        leafClusters = divided\n+        step += 1\n+\n+        log.info(s\"${sc.appName} adding ${divided.size} new clusters at step:${step}\")\n+      }\n+    }\n+    // unpersist kept RDDs\n+    rddArray.foreach(_.unpersist())\n+\n+    // build a cluster tree by Map class which is expressed\n+    log.info(s\"Building the cluster tree is started in ${sc.appName}\")\n+    val root = buildTree(clusters, HierarchicalClustering.ROOT_INDEX_KEY, this.numClusters)\n+    if (root == None) {\n+      new SparkException(\"Failed to build a cluster tree from a Map type of clusters\")\n+    }\n+\n+    // set the elapsed time for training\n+    val finishTime = (System.currentTimeMillis() - startTime) / 1000.0\n+    log.info(s\"Elapsed Time for Hierarchical Clustering Training: ${finishTime} [sec]\")\n+\n+    // make a hierarchical clustering model\n+    val model = new HierarchicalClusteringModel(root.get)\n+    val leavesNodes = model.getClusters\n+    if (leavesNodes.size < this.numClusters) {\n+      log.warn(s\"# clusters is less than you have expected: ${leavesNodes.size} / ${numClusters}. \")\n+    }\n+    model\n+  }\n+\n+  /**\n+   * Assigns the initial cluster index id to all data\n+   */\n+  private[clustering]\n+  def initData(data: RDD[Vector]): RDD[(Long, BV[Double])] = {\n+    data.map { v: Vector => (HierarchicalClustering.ROOT_INDEX_KEY, v.toBreeze)}.cache\n+  }\n+\n+  /**\n+   * Summarizes data by each cluster as ClusterTree classes\n+   */\n+  private[clustering]\n+  def summarizeAsClusters(data: RDD[(Long, BV[Double])]): Map[Long, ClusterTree] = {\n+    // summarize input data\n+    val stats = summarize(data)\n+\n+    // convert statistics to ClusterTree class\n+    stats.map { case (i, (sum, n, sumOfSquares)) =>\n+      val center = Vectors.fromBreeze(sum :/ n)\n+      val variances = n match {\n+        case n if n > 1 => Vectors.fromBreeze(sumOfSquares.:*(n) - (sum :* sum) :/ (n * (n - 1.0)))\n+        case _ => Vectors.zeros(sum.size)\n+      }\n+      (i, new ClusterTree(center, n.toLong, variances))\n+    }.toMap\n+  }\n+\n+  /**\n+   * Summarizes data by each cluster as Map\n+   */\n+  private[clustering]\n+  def summarize(data: RDD[(Long, BV[Double])]): Map[Long, (BV[Double], Double, BV[Double])] = {\n+    data.mapPartitions { iter =>\n+      // calculate the accumulation of the all point in a partition and count the rows\n+      val map = mutable.Map.empty[Long, (BV[Double], Double, BV[Double])]\n+      iter.foreach { case (idx: Long, point: BV[Double]) =>\n+        // get a map value or else get a sparse vector\n+        val (sumBV, n, sumOfSquares) = map.get(idx)\n+            .getOrElse(BSV.zeros[Double](point.size), 0.0, BSV.zeros[Double](point.size))\n+        map(idx) = (sumBV + point, n + 1.0, sumOfSquares + (point :* point))\n+      }\n+      map.toIterator\n+    }.reduceByKey { case ((sum1, n1, sumOfSquares1), (sum2, n2, sumOfSquares2)) =>\n+      // sum the accumulation and the count in the all partition\n+      (sum1 + sum2, n1 + n2, sumOfSquares1 + sumOfSquares2)\n+    }.collect().toMap\n+  }\n+\n+  /**\n+   * Gets the new divided centers\n+   */\n+  private[clustering]\n+  def getDividedClusters(data: RDD[(Long, BV[Double])],\n+    dividedClusters: Map[Long, ClusterTree]): Map[Long, ClusterTree] = {\n+    val sc = data.sparkContext\n+    val appName = sc.appName\n+\n+    // get keys of dividable clusters\n+    val dividableKeys = dividedClusters.filter { case (idx, cluster) =>\n+      cluster.variances.toArray.sum > 0.0 && cluster.records >= 2\n+    }.keySet\n+    if (dividableKeys.size == 0) {\n+      log.info(s\"There is no dividable clusters in ${appName}.\")\n+      return Map.empty[Long, ClusterTree]\n+    }\n+\n+    // divide input data\n+    var dividableData = data.filter { case (idx, point) => dividableKeys.contains(idx)}\n+    var dividableClusters = dividedClusters.filter { case (k, v) => dividableKeys.contains(k)}\n+    val idealIndexes = dividableKeys.flatMap(idx => Array(2 * idx, 2 * idx + 1).toIterator)\n+    var stats = divide(data, dividableClusters)\n+\n+    // if there is clusters which is failed to be divided,"
  }],
  "prId": 5267
}, {
  "comments": [{
    "author": {
      "login": "freeman-lab"
    },
    "body": "what does `which is expressed` mean here?\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-06-12T21:19:51Z",
    "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+object HierarchicalClustering extends Logging {\n+\n+  private[clustering] val ROOT_INDEX_KEY: Long = 1\n+\n+  /**\n+   * Finds the closes cluster's center\n+   *\n+   * @param metric a distance metric\n+   * @param centers centers of the clusters\n+   * @param point a target point\n+   * @return an index of the array of clusters\n+   */\n+  private[mllib]\n+  def findClosestCenter(metric: Function2[BV[Double], BV[Double], Double])\n+        (centers: Seq[BV[Double]])(point: BV[Double]): Int = {\n+    val (closestCenter, closestIndex) =\n+      centers.zipWithIndex.map { case (center, idx) => (metric(center, point), idx)}.minBy(_._1)\n+    closestIndex\n+  }\n+}\n+\n+/**\n+ * This is a divisive hierarchical clustering algorithm based on bi-sect k-means algorithm.\n+ *\n+ * The main idea of this algorithm is based on \"A comparison of document clustering techniques\",\n+ * M. Steinbach, G. Karypis and V. Kumar. Workshop on Text Mining, KDD, 2000.\n+ * http://cs.fit.edu/~pkc/classes/ml-internet/papers/steinbach00tr.pdf\n+ *\n+ * @param numClusters tne number of clusters you want\n+ * @param clusterMap the pairs of cluster and its index as Map\n+ * @param maxIterations the number of maximal iterations\n+ * @param maxRetries the number of maximum retries\n+ * @param seed a random seed\n+ */\n+class HierarchicalClustering private (\n+  private var numClusters: Int,\n+  private var clusterMap: Map[Long, ClusterTree],\n+  private var maxIterations: Int,\n+  private var maxRetries: Int,\n+  private var seed: Long) extends Logging {\n+\n+  /**\n+   * Constructs with the default configuration\n+   */\n+  def this() = this(20, mutable.ListMap.empty[Long, ClusterTree], 20, 10, 1)\n+\n+  /**\n+   * Sets the number of clusters you want\n+   */\n+  def setNumClusters(numClusters: Int): this.type = {\n+    this.numClusters = numClusters\n+    this\n+  }\n+\n+  def getNumClusters: Int = this.numClusters\n+\n+  /**\n+   * Sets the number of maximal iterations in each clustering step\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  def getMaxIterations: Int = this.maxIterations\n+\n+  /**\n+   * Sets the number of maximum retries of each clustering step\n+   */\n+  def setMaxRetries(maxRetries: Int): this.type = {\n+    this.maxRetries = maxRetries\n+    this\n+  }\n+\n+  def getMaxRetries: Int = this.maxRetries\n+\n+  /**\n+   * Sets the random seed\n+   */\n+  def setSeed(seed: Long): this.type = {\n+    this.seed = seed\n+    this\n+  }\n+\n+  def getSeed: Long = this.seed\n+\n+  /**\n+   * Runs the hierarchical clustering algorithm\n+   * @param input RDD of vectors\n+   * @return model for the hierarchical clustering\n+   */\n+  def run(input: RDD[Vector]): HierarchicalClusteringModel = {\n+    val sc = input.sparkContext\n+    log.info(s\"${sc.appName} starts a hierarchical clustering algorithm\")\n+\n+    var data = initData(input).cache()\n+    val startTime = System.currentTimeMillis()\n+\n+    // `clusters` is described as binary tree structure\n+    // `clusters(1)` means the root of a binary tree\n+    var clusters = summarizeAsClusters(data)\n+    var leafClusters = clusters\n+    var step = 1\n+    var numDividedClusters = 0\n+    var noMoreDividable = false\n+    var rddArray = Array.empty[RDD[(Long, BV[Double])]]\n+    // the number of maximum nodes of a binary tree by given parameter\n+    val multiplier = math.ceil(math.log10(this.numClusters) / math.log10(2.0)) + 1\n+    val maxAllNodesInTree = math.pow(2, multiplier).toInt\n+\n+    while (clusters.size < maxAllNodesInTree && noMoreDividable == false) {\n+      log.info(s\"${sc.appName} starts step ${step}\")\n+\n+      // enough to be clustered if the number of divided clusters is equal to 0\n+      val divided = getDividedClusters(data, leafClusters)\n+      if (divided.size == 0) {\n+        noMoreDividable = true\n+      }\n+      else {\n+        // update each index\n+        val newData = updateClusterIndex(data, divided).cache()\n+        rddArray = rddArray ++ Array(data)\n+        data = newData\n+\n+        // keep recent 2 cached RDDs in order to run more quickly\n+        if (rddArray.size > 1) {\n+          val head = rddArray.head\n+          head.unpersist()\n+          rddArray = rddArray.filterNot(_.hashCode() == head.hashCode())\n+        }\n+\n+        // merge the divided clusters with the map as the cluster tree\n+        clusters = clusters ++ divided\n+        numDividedClusters = data.map(_._1).distinct().count().toInt\n+        leafClusters = divided\n+        step += 1\n+\n+        log.info(s\"${sc.appName} adding ${divided.size} new clusters at step:${step}\")\n+      }\n+    }\n+    // unpersist kept RDDs\n+    rddArray.foreach(_.unpersist())\n+\n+    // build a cluster tree by Map class which is expressed"
  }],
  "prId": 5267
}, {
  "comments": [{
    "author": {
      "login": "freeman-lab"
    },
    "body": "grammar nit, change `enough to be clustered` -> `can be clustered`\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-06-12T21:22:19Z",
    "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+object HierarchicalClustering extends Logging {\n+\n+  private[clustering] val ROOT_INDEX_KEY: Long = 1\n+\n+  /**\n+   * Finds the closes cluster's center\n+   *\n+   * @param metric a distance metric\n+   * @param centers centers of the clusters\n+   * @param point a target point\n+   * @return an index of the array of clusters\n+   */\n+  private[mllib]\n+  def findClosestCenter(metric: Function2[BV[Double], BV[Double], Double])\n+        (centers: Seq[BV[Double]])(point: BV[Double]): Int = {\n+    val (closestCenter, closestIndex) =\n+      centers.zipWithIndex.map { case (center, idx) => (metric(center, point), idx)}.minBy(_._1)\n+    closestIndex\n+  }\n+}\n+\n+/**\n+ * This is a divisive hierarchical clustering algorithm based on bi-sect k-means algorithm.\n+ *\n+ * The main idea of this algorithm is based on \"A comparison of document clustering techniques\",\n+ * M. Steinbach, G. Karypis and V. Kumar. Workshop on Text Mining, KDD, 2000.\n+ * http://cs.fit.edu/~pkc/classes/ml-internet/papers/steinbach00tr.pdf\n+ *\n+ * @param numClusters tne number of clusters you want\n+ * @param clusterMap the pairs of cluster and its index as Map\n+ * @param maxIterations the number of maximal iterations\n+ * @param maxRetries the number of maximum retries\n+ * @param seed a random seed\n+ */\n+class HierarchicalClustering private (\n+  private var numClusters: Int,\n+  private var clusterMap: Map[Long, ClusterTree],\n+  private var maxIterations: Int,\n+  private var maxRetries: Int,\n+  private var seed: Long) extends Logging {\n+\n+  /**\n+   * Constructs with the default configuration\n+   */\n+  def this() = this(20, mutable.ListMap.empty[Long, ClusterTree], 20, 10, 1)\n+\n+  /**\n+   * Sets the number of clusters you want\n+   */\n+  def setNumClusters(numClusters: Int): this.type = {\n+    this.numClusters = numClusters\n+    this\n+  }\n+\n+  def getNumClusters: Int = this.numClusters\n+\n+  /**\n+   * Sets the number of maximal iterations in each clustering step\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  def getMaxIterations: Int = this.maxIterations\n+\n+  /**\n+   * Sets the number of maximum retries of each clustering step\n+   */\n+  def setMaxRetries(maxRetries: Int): this.type = {\n+    this.maxRetries = maxRetries\n+    this\n+  }\n+\n+  def getMaxRetries: Int = this.maxRetries\n+\n+  /**\n+   * Sets the random seed\n+   */\n+  def setSeed(seed: Long): this.type = {\n+    this.seed = seed\n+    this\n+  }\n+\n+  def getSeed: Long = this.seed\n+\n+  /**\n+   * Runs the hierarchical clustering algorithm\n+   * @param input RDD of vectors\n+   * @return model for the hierarchical clustering\n+   */\n+  def run(input: RDD[Vector]): HierarchicalClusteringModel = {\n+    val sc = input.sparkContext\n+    log.info(s\"${sc.appName} starts a hierarchical clustering algorithm\")\n+\n+    var data = initData(input).cache()\n+    val startTime = System.currentTimeMillis()\n+\n+    // `clusters` is described as binary tree structure\n+    // `clusters(1)` means the root of a binary tree\n+    var clusters = summarizeAsClusters(data)\n+    var leafClusters = clusters\n+    var step = 1\n+    var numDividedClusters = 0\n+    var noMoreDividable = false\n+    var rddArray = Array.empty[RDD[(Long, BV[Double])]]\n+    // the number of maximum nodes of a binary tree by given parameter\n+    val multiplier = math.ceil(math.log10(this.numClusters) / math.log10(2.0)) + 1\n+    val maxAllNodesInTree = math.pow(2, multiplier).toInt\n+\n+    while (clusters.size < maxAllNodesInTree && noMoreDividable == false) {\n+      log.info(s\"${sc.appName} starts step ${step}\")\n+\n+      // enough to be clustered if the number of divided clusters is equal to 0"
  }],
  "prId": 5267
}, {
  "comments": [{
    "author": {
      "login": "freeman-lab"
    },
    "body": "Probably drop this one line, I think it's unusual to have a `SEE ALSO` if it's from another library.\n",
    "commit": "29ccdf9eaa987530435782d2051acbeda3d3ac36",
    "createdAt": "2015-06-12T22:11:45Z",
    "diffHunk": "@@ -0,0 +1,631 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, norm => breezeNorm}\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+import org.apache.spark.{Logging, SparkException}\n+\n+import scala.collection.{Map, mutable}\n+\n+\n+object HierarchicalClustering extends Logging {\n+\n+  private[clustering] val ROOT_INDEX_KEY: Long = 1\n+\n+  /**\n+   * Finds the closes cluster's center\n+   *\n+   * @param metric a distance metric\n+   * @param centers centers of the clusters\n+   * @param point a target point\n+   * @return an index of the array of clusters\n+   */\n+  private[mllib]\n+  def findClosestCenter(metric: Function2[BV[Double], BV[Double], Double])\n+        (centers: Seq[BV[Double]])(point: BV[Double]): Int = {\n+    val (closestCenter, closestIndex) =\n+      centers.zipWithIndex.map { case (center, idx) => (metric(center, point), idx)}.minBy(_._1)\n+    closestIndex\n+  }\n+}\n+\n+/**\n+ * This is a divisive hierarchical clustering algorithm based on bi-sect k-means algorithm.\n+ *\n+ * The main idea of this algorithm is based on \"A comparison of document clustering techniques\",\n+ * M. Steinbach, G. Karypis and V. Kumar. Workshop on Text Mining, KDD, 2000.\n+ * http://cs.fit.edu/~pkc/classes/ml-internet/papers/steinbach00tr.pdf\n+ *\n+ * @param numClusters tne number of clusters you want\n+ * @param clusterMap the pairs of cluster and its index as Map\n+ * @param maxIterations the number of maximal iterations\n+ * @param maxRetries the number of maximum retries\n+ * @param seed a random seed\n+ */\n+class HierarchicalClustering private (\n+  private var numClusters: Int,\n+  private var clusterMap: Map[Long, ClusterTree],\n+  private var maxIterations: Int,\n+  private var maxRetries: Int,\n+  private var seed: Long) extends Logging {\n+\n+  /**\n+   * Constructs with the default configuration\n+   */\n+  def this() = this(20, mutable.ListMap.empty[Long, ClusterTree], 20, 10, 1)\n+\n+  /**\n+   * Sets the number of clusters you want\n+   */\n+  def setNumClusters(numClusters: Int): this.type = {\n+    this.numClusters = numClusters\n+    this\n+  }\n+\n+  def getNumClusters: Int = this.numClusters\n+\n+  /**\n+   * Sets the number of maximal iterations in each clustering step\n+   */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  def getMaxIterations: Int = this.maxIterations\n+\n+  /**\n+   * Sets the number of maximum retries of each clustering step\n+   */\n+  def setMaxRetries(maxRetries: Int): this.type = {\n+    this.maxRetries = maxRetries\n+    this\n+  }\n+\n+  def getMaxRetries: Int = this.maxRetries\n+\n+  /**\n+   * Sets the random seed\n+   */\n+  def setSeed(seed: Long): this.type = {\n+    this.seed = seed\n+    this\n+  }\n+\n+  def getSeed: Long = this.seed\n+\n+  /**\n+   * Runs the hierarchical clustering algorithm\n+   * @param input RDD of vectors\n+   * @return model for the hierarchical clustering\n+   */\n+  def run(input: RDD[Vector]): HierarchicalClusteringModel = {\n+    val sc = input.sparkContext\n+    log.info(s\"${sc.appName} starts a hierarchical clustering algorithm\")\n+\n+    var data = initData(input).cache()\n+    val startTime = System.currentTimeMillis()\n+\n+    // `clusters` is described as binary tree structure\n+    // `clusters(1)` means the root of a binary tree\n+    var clusters = summarizeAsClusters(data)\n+    var leafClusters = clusters\n+    var step = 1\n+    var numDividedClusters = 0\n+    var noMoreDividable = false\n+    var rddArray = Array.empty[RDD[(Long, BV[Double])]]\n+    // the number of maximum nodes of a binary tree by given parameter\n+    val multiplier = math.ceil(math.log10(this.numClusters) / math.log10(2.0)) + 1\n+    val maxAllNodesInTree = math.pow(2, multiplier).toInt\n+\n+    while (clusters.size < maxAllNodesInTree && noMoreDividable == false) {\n+      log.info(s\"${sc.appName} starts step ${step}\")\n+\n+      // enough to be clustered if the number of divided clusters is equal to 0\n+      val divided = getDividedClusters(data, leafClusters)\n+      if (divided.size == 0) {\n+        noMoreDividable = true\n+      }\n+      else {\n+        // update each index\n+        val newData = updateClusterIndex(data, divided).cache()\n+        rddArray = rddArray ++ Array(data)\n+        data = newData\n+\n+        // keep recent 2 cached RDDs in order to run more quickly\n+        if (rddArray.size > 1) {\n+          val head = rddArray.head\n+          head.unpersist()\n+          rddArray = rddArray.filterNot(_.hashCode() == head.hashCode())\n+        }\n+\n+        // merge the divided clusters with the map as the cluster tree\n+        clusters = clusters ++ divided\n+        numDividedClusters = data.map(_._1).distinct().count().toInt\n+        leafClusters = divided\n+        step += 1\n+\n+        log.info(s\"${sc.appName} adding ${divided.size} new clusters at step:${step}\")\n+      }\n+    }\n+    // unpersist kept RDDs\n+    rddArray.foreach(_.unpersist())\n+\n+    // build a cluster tree by Map class which is expressed\n+    log.info(s\"Building the cluster tree is started in ${sc.appName}\")\n+    val root = buildTree(clusters, HierarchicalClustering.ROOT_INDEX_KEY, this.numClusters)\n+    if (root == None) {\n+      new SparkException(\"Failed to build a cluster tree from a Map type of clusters\")\n+    }\n+\n+    // set the elapsed time for training\n+    val finishTime = (System.currentTimeMillis() - startTime) / 1000.0\n+    log.info(s\"Elapsed Time for Hierarchical Clustering Training: ${finishTime} [sec]\")\n+\n+    // make a hierarchical clustering model\n+    val model = new HierarchicalClusteringModel(root.get)\n+    val leavesNodes = model.getClusters\n+    if (leavesNodes.size < this.numClusters) {\n+      log.warn(s\"# clusters is less than you have expected: ${leavesNodes.size} / ${numClusters}. \")\n+    }\n+    model\n+  }\n+\n+  /**\n+   * Assigns the initial cluster index id to all data\n+   */\n+  private[clustering]\n+  def initData(data: RDD[Vector]): RDD[(Long, BV[Double])] = {\n+    data.map { v: Vector => (HierarchicalClustering.ROOT_INDEX_KEY, v.toBreeze)}.cache\n+  }\n+\n+  /**\n+   * Summarizes data by each cluster as ClusterTree classes\n+   */\n+  private[clustering]\n+  def summarizeAsClusters(data: RDD[(Long, BV[Double])]): Map[Long, ClusterTree] = {\n+    // summarize input data\n+    val stats = summarize(data)\n+\n+    // convert statistics to ClusterTree class\n+    stats.map { case (i, (sum, n, sumOfSquares)) =>\n+      val center = Vectors.fromBreeze(sum :/ n)\n+      val variances = n match {\n+        case n if n > 1 => Vectors.fromBreeze(sumOfSquares.:*(n) - (sum :* sum) :/ (n * (n - 1.0)))\n+        case _ => Vectors.zeros(sum.size)\n+      }\n+      (i, new ClusterTree(center, n.toLong, variances))\n+    }.toMap\n+  }\n+\n+  /**\n+   * Summarizes data by each cluster as Map\n+   */\n+  private[clustering]\n+  def summarize(data: RDD[(Long, BV[Double])]): Map[Long, (BV[Double], Double, BV[Double])] = {\n+    data.mapPartitions { iter =>\n+      // calculate the accumulation of the all point in a partition and count the rows\n+      val map = mutable.Map.empty[Long, (BV[Double], Double, BV[Double])]\n+      iter.foreach { case (idx: Long, point: BV[Double]) =>\n+        // get a map value or else get a sparse vector\n+        val (sumBV, n, sumOfSquares) = map.get(idx)\n+            .getOrElse(BSV.zeros[Double](point.size), 0.0, BSV.zeros[Double](point.size))\n+        map(idx) = (sumBV + point, n + 1.0, sumOfSquares + (point :* point))\n+      }\n+      map.toIterator\n+    }.reduceByKey { case ((sum1, n1, sumOfSquares1), (sum2, n2, sumOfSquares2)) =>\n+      // sum the accumulation and the count in the all partition\n+      (sum1 + sum2, n1 + n2, sumOfSquares1 + sumOfSquares2)\n+    }.collect().toMap\n+  }\n+\n+  /**\n+   * Gets the new divided centers\n+   */\n+  private[clustering]\n+  def getDividedClusters(data: RDD[(Long, BV[Double])],\n+    dividedClusters: Map[Long, ClusterTree]): Map[Long, ClusterTree] = {\n+    val sc = data.sparkContext\n+    val appName = sc.appName\n+\n+    // get keys of dividable clusters\n+    val dividableKeys = dividedClusters.filter { case (idx, cluster) =>\n+      cluster.variances.toArray.sum > 0.0 && cluster.records >= 2\n+    }.keySet\n+    if (dividableKeys.size == 0) {\n+      log.info(s\"There is no dividable clusters in ${appName}.\")\n+      return Map.empty[Long, ClusterTree]\n+    }\n+\n+    // divide input data\n+    var dividableData = data.filter { case (idx, point) => dividableKeys.contains(idx)}\n+    var dividableClusters = dividedClusters.filter { case (k, v) => dividableKeys.contains(k)}\n+    val idealIndexes = dividableKeys.flatMap(idx => Array(2 * idx, 2 * idx + 1).toIterator)\n+    var stats = divide(data, dividableClusters)\n+\n+    // if there is clusters which is failed to be divided,\n+    // retry to divide only failed clusters again and again\n+    var tryTimes = 1\n+    while (stats.size < dividableKeys.size * 2 && tryTimes <= this.maxRetries) {\n+      // get the indexes of clusters which is failed to be divided\n+      val failedIndexes = idealIndexes.filterNot(stats.keySet.contains).map(idx => (idx / 2).toLong)\n+      val failedCenters = dividedClusters.filter { case (idx, clstr) => failedIndexes.contains(idx)}\n+      log.info(s\"# failed clusters is ${failedCenters.size} of ${dividableKeys.size}\" +\n+          s\"at ${tryTimes} times in ${appName}\")\n+\n+      // divide the failed clusters again\n+      val bcFailedIndexes = sc.broadcast(failedIndexes)\n+      dividableData = data.filter { case (idx, point) => bcFailedIndexes.value.contains(idx)}\n+      val missingStats = divide(dividableData, failedCenters)\n+      stats = stats ++ missingStats\n+      tryTimes += 1\n+    }\n+\n+    // make children clusters\n+    stats.filter { case (i, (sum, n, sumOfSquares)) => n > 0}\n+        .map { case (i, (sum, n, sumOfSquares)) =>\n+      val center = Vectors.fromBreeze(sum :/ n)\n+      val variances = n match {\n+        case 1 => Vectors.sparse(sum.size, Array(), Array())\n+        case _ => Vectors.fromBreeze(sumOfSquares.:*(n) - (sum :* sum) :/ (n * (n - 1.0)))\n+      }\n+      val child = new ClusterTree(center, n.toLong, variances)\n+      (i, child)\n+    }.toMap\n+  }\n+\n+  /**\n+   * Divides the input data\n+   *\n+   * @param data the pairs of cluster index and point which you want to divide\n+   * @param clusters the clusters you want to divide AS a Map class\n+   * @return divided clusters as Map\n+   */\n+  private[clustering]\n+  def divide(data: RDD[(Long, BV[Double])],\n+    clusters: Map[Long, ClusterTree]): Map[Long, (BV[Double], Double, BV[Double])] = {\n+\n+    val sc = data.sparkContext\n+    val centers = clusters.map { case (idx, cluster) => (idx, cluster.center.toBreeze)}\n+    var newCenters = initChildrenCenter(centers)\n+    if (newCenters.size == 0) {\n+      return Map.empty[Long, (BV[Double], Double, BV[Double])]\n+    }\n+    var bcNewCenters = sc.broadcast(newCenters)\n+\n+    // TODO Supports distance metrics other Euclidean distance metric\n+    val metric = (bv1: BV[Double], bv2: BV[Double]) => breezeNorm(bv1 - bv2, 2.0)\n+    val bcMetric = sc.broadcast(metric)\n+\n+    val vectorSize = newCenters(newCenters.keySet.min).size\n+    var stats = newCenters.keys.map { idx =>\n+      (idx, (BSV.zeros[Double](vectorSize).toVector, 0.0, BSV.zeros[Double](vectorSize).toVector))\n+    }.toMap\n+\n+    var subIter = 0\n+    var diffVariances = Double.MaxValue\n+    var oldVariances = Double.MaxValue\n+    var variances = Double.MaxValue\n+    while (subIter < this.maxIterations && diffVariances > 10E-4) {\n+      // calculate summary of each cluster\n+      val eachStats = data.mapPartitions { iter =>\n+        val map = mutable.Map.empty[Long, (BV[Double], Double, BV[Double])]\n+        iter.foreach { case (idx, point) =>\n+          // calculate next index number\n+          val childrenCenters = Array(2 * idx, 2 * idx + 1)\n+              .filter(bcNewCenters.value.keySet.contains(_)).map(bcNewCenters.value(_)).toArray\n+          if (childrenCenters.size >= 1) {\n+            val closestIndex =\n+              HierarchicalClustering.findClosestCenter(bcMetric.value)(childrenCenters)(point)\n+            val nextIndex = 2 * idx + closestIndex\n+\n+            // get a map value or else get a sparse vector\n+            val (sumBV, n, sumOfSquares) = map.get(nextIndex)\n+                .getOrElse(BSV.zeros[Double](point.size), 0.0, BSV.zeros[Double](point.size))\n+            map(nextIndex) = (sumBV + point, n + 1.0, sumOfSquares + (point :* point))\n+          }\n+        }\n+        map.toIterator\n+      }.reduceByKey { case ((sv1, n1, sumOfSquares1), (sv2, n2, sumOfSquares2)) =>\n+        // sum the accumulation and the count in the all partition\n+        (sv1 + sv2, n1 + n2, sumOfSquares1 + sumOfSquares2)\n+      }.collect().toMap\n+\n+      // calculate the center of each cluster\n+      newCenters = eachStats.map { case (idx, (sum, n, sumOfSquares)) => (idx, sum :/ n)}\n+      bcNewCenters = sc.broadcast(newCenters)\n+\n+      // update summary of each cluster\n+      stats = eachStats.toMap\n+\n+      variances = stats.map { case (idx, (sum, n, sumOfSquares)) =>\n+        math.pow(sumOfSquares.toArray.sum, 1.0 / sumOfSquares.size)\n+      }.sum\n+      diffVariances = math.abs(oldVariances - variances) / oldVariances\n+      oldVariances = variances\n+      subIter += 1\n+    }\n+    stats\n+  }\n+\n+  /**\n+   * Gets the initial centers for bi-sect k-means\n+   */\n+  private[clustering]\n+  def initChildrenCenter(clusters: Map[Long, BV[Double]]): Map[Long, BV[Double]] = {\n+    val rand = new XORShiftRandom()\n+    rand.setSeed(this.seed)\n+\n+    clusters.flatMap { case (idx, center) =>\n+      val childrenIndexes = Array(2 * idx, 2 * idx + 1)\n+      val relativeErrorCoefficient = 0.001\n+      Array(\n+        (2 * idx, center.map(elm => elm - (elm * relativeErrorCoefficient * rand.nextDouble()))),\n+        (2 * idx + 1, center.map(elm => elm + (elm * relativeErrorCoefficient * rand.nextDouble())))\n+      )\n+    }.toMap\n+  }\n+\n+  /**\n+   * Builds a cluster tree from a Map of clusters\n+   *\n+   * @param treeMap divided clusters as a Map class\n+   * @param rootIndex index you want to start\n+   * @param numClusters the number of clusters you want\n+   * @return a built cluster tree\n+   */\n+  private[clustering]\n+  def buildTree(treeMap: Map[Long, ClusterTree],\n+    rootIndex: Long,\n+    numClusters: Int): Option[ClusterTree] = {\n+\n+    // if there is no index in the Map\n+    if (!treeMap.contains(rootIndex)) return None\n+\n+    // build a cluster tree if the queue is empty or until the number of leaves clusters is enough\n+    var numLeavesClusters = 1\n+    val root = treeMap(rootIndex)\n+    var leavesQueue = Map(rootIndex -> root)\n+    while (leavesQueue.size > 0 && numLeavesClusters < numClusters) {\n+      // pick up the cluster whose variance is the maximum in the queue\n+      val mostScattered = leavesQueue.maxBy(_._2.variancesNorm)\n+      val mostScatteredKey = mostScattered._1\n+      val mostScatteredCluster = mostScattered._2\n+\n+      // relate the most scattered cluster to its children clusters\n+      val childrenIndexes = Array(2 * mostScatteredKey, 2 * mostScatteredKey + 1)\n+      if (childrenIndexes.forall(i => treeMap.contains(i))) {\n+        // insert children to the most scattered cluster\n+        val children = childrenIndexes.map(i => treeMap(i))\n+        mostScatteredCluster.insert(children)\n+\n+        // calculate the local dendrogram height\n+        // TODO Supports distance metrics other Euclidean distance metric\n+        val metric = (bv1: BV[Double], bv2: BV[Double]) => breezeNorm(bv1 - bv2, 2.0)\n+        val localHeight = children\n+            .map(child => metric(child.center.toBreeze, mostScatteredCluster.center.toBreeze)).max\n+        mostScatteredCluster.setLocalHeight(localHeight)\n+\n+        // update the queue\n+        leavesQueue = leavesQueue ++ childrenIndexes.map(i => (i -> treeMap(i))).toMap\n+        numLeavesClusters += 1\n+      }\n+\n+      // remove the cluster which is involved to the cluster tree\n+      leavesQueue = leavesQueue.filterNot(_ == mostScattered)\n+\n+      log.info(s\"Total Leaves Clusters: ${numLeavesClusters} / ${numClusters}. \" +\n+          s\"Cluster ${childrenIndexes.mkString(\",\")} are merged.\")\n+    }\n+    Some(root)\n+  }\n+\n+  /**\n+   * Updates the indexes of clusters which is divided to its children indexes\n+   */\n+  private[clustering]\n+  def updateClusterIndex(\n+    data: RDD[(Long, BV[Double])],\n+    dividedClusters: Map[Long, ClusterTree]): RDD[(Long, BV[Double])] = {\n+    // extract the centers of the clusters\n+    val sc = data.sparkContext\n+    var centers = dividedClusters.map { case (idx, cluster) => (idx, cluster.center)}\n+    val bcCenters = sc.broadcast(centers)\n+\n+    // TODO Supports distance metrics other Euclidean distance metric\n+    val metric = (bv1: BV[Double], bv2: BV[Double]) => breezeNorm(bv1 - bv2, 2.0)\n+    val bcMetric = sc.broadcast(metric)\n+\n+    // update the indexes to their children indexes\n+    data.map { case (idx, point) =>\n+      val childrenIndexes = Array(2 * idx, 2 * idx + 1).filter(bcCenters.value.keySet.contains(_))\n+      childrenIndexes.size match {\n+        // stay the index if the number of children is not enough\n+        case s if s < 2 => (idx, point)\n+        // update the indexes\n+        case _ => {\n+          val nextCenters = childrenIndexes.map(bcCenters.value(_)).map(_.toBreeze)\n+          val closestIndex = HierarchicalClustering\n+              .findClosestCenter(bcMetric.value)(nextCenters)(point)\n+          val nextIndex = 2 * idx + closestIndex\n+          (nextIndex, point)\n+        }\n+      }\n+    }\n+  }\n+}\n+\n+/**\n+ * A cluster as a tree node which can have its sub nodes\n+ *\n+ * @param center the center of the cluster\n+ * @param records the number of rows in the cluster\n+ * @param variances variance vectors\n+ * @param variancesNorm the norm of variance vector\n+ * @param localHeight the maximal distance between this node and its children\n+ * @param parent the parent cluster of the cluster\n+ * @param children the children nodes of the cluster\n+ */\n+class ClusterTree private (\n+  val center: Vector,\n+  val records: Long,\n+  val variances: Vector,\n+  val variancesNorm: Double,\n+  private var localHeight: Double,\n+  private var parent: Option[ClusterTree],\n+  private var children: Seq[ClusterTree]) extends Serializable {\n+\n+  require(!variancesNorm.isNaN)\n+\n+  def this(center: Vector, rows: Long, variances: Vector) =\n+    this(center, rows, variances, breezeNorm(variances.toBreeze, 2.0),\n+      0.0, None, Array.empty[ClusterTree])\n+\n+  /**\n+   * Inserts a sub node as its child\n+   *\n+   * @param child inserted sub node\n+   */\n+  def insert(child: ClusterTree) {\n+    insert(Array(child))\n+  }\n+\n+  /**\n+   * Inserts sub nodes as its children\n+   *\n+   * @param children inserted sub nodes\n+   */\n+  def insert(children: Array[ClusterTree]) {\n+    this.children = this.children ++ children\n+    children.foreach(child => child.parent = Some(this))\n+  }\n+\n+  /**\n+   * Converts the tree into Array class\n+   * the sub nodes are recursively expanded\n+   *\n+   * @return an Array class which the cluster tree is expanded\n+   */\n+  def toArray(): Array[ClusterTree] = {\n+    val array = this.children.size match {\n+      case 0 => Array(this)\n+      case _ => Array(this) ++ this.children.flatMap(child => child.toArray().toIterator)\n+    }\n+    array.sortWith { case (a, b) =>\n+      a.getDepth < b.getDepth && a.variances.toArray.sum < b.variances.toArray.sum\n+    }\n+  }\n+\n+  /**\n+   * Gets the depth of the cluster in the tree\n+   *\n+   * @return the depth from the root\n+   */\n+  def getDepth: Int = {\n+    this.parent match {\n+      case None => 0\n+      case _ => 1 + this.parent.get.getDepth\n+    }\n+  }\n+\n+  /**\n+   * Gets the leaves nodes in the cluster tree\n+   */\n+  def getLeavesNodes: Array[ClusterTree] = {\n+    this.toArray().filter(_.isLeaf).sortBy(_.center.toArray.sum)\n+  }\n+\n+  def isLeaf: Boolean = (this.children.size == 0)\n+\n+  def getParent: Option[ClusterTree] = this.parent\n+\n+  def getChildren: Seq[ClusterTree] = this.children\n+\n+  /**\n+   * Gets the dendrogram height of the cluster at the cluster tree.\n+   * A dendrogram height is different from a local height.\n+   * A dendrogram height means a total height of a node in a tree.\n+   * A local height means a maximum distance between a node and its children.\n+   *\n+   * @return the dendrogram height\n+   */\n+  def getHeight: Double = {\n+    this.children.size match {\n+      case 0 => 0.0\n+      case _ => this.localHeight + this.children.map(_.getHeight).max\n+    }\n+  }\n+\n+  private[mllib]\n+  def setLocalHeight(height: Double) = (this.localHeight = height)\n+\n+  /**\n+   * Converts to a adjacency list\n+   *\n+   * @return List[(fromNodeId, toNodeId, distance)]\n+   */\n+  def toAdjacencyList(): Array[(Int, Int, Double)] = {\n+    val nodes = toArray()\n+\n+    var adjacencyList = Array.empty[(Int, Int, Double)]\n+    nodes.foreach { parent =>\n+      if (parent.children.size > 1) {\n+        val parentIndex = nodes.indexOf(parent)\n+        parent.children.foreach { child =>\n+          val childIndex = nodes.indexOf(child)\n+          adjacencyList = adjacencyList :+(parentIndex, childIndex, parent.localHeight)\n+        }\n+      }\n+    }\n+    adjacencyList\n+  }\n+\n+  /**\n+   * Converts to a linkage matrix\n+   * Returned data format is fit for scipy's dendrogram function\n+   * SEE ALSO: scipy.cluster.hierarchy.dendrogram"
  }],
  "prId": 5267
}]