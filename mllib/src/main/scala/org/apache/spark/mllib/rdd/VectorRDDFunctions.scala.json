[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Please wrap the return type to a class called `VectorRDDStatisticalSummary` that provides `mean`, `variance`, `count: Long`, `min`, `max`, and `std`. So later we can add more summary statistics to it. I would also recommend changing the method name to `summarize` or `summarizeStatistics`.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-03-30T17:59:35Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Compute full column-wise statistics for the RDD, including\n+   * {{{\n+   *   Mean:              Vector,\n+   *   Variance:          Vector,\n+   *   Count:             Double,\n+   *   Non-zero count:    Vector,\n+   *   Maximum elements:  Vector,\n+   *   Minimum elements:  Vector.\n+   * }}},\n+   * with the size of Vector as input parameter.\n+   */\n+  def statistics(size: Int): (Vector, Vector, Double, Vector, Vector, Vector) = {"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Use in-place ops and axpy whenever possible for performance. For this line, it is equivalent to \n\n```\ncurrMean = prevMean :* (cnt / (cnt + 1.0))\naxpy(1.0/(cnt+1.0), currData, currMean)\n```\n\nSo you only create one temp array. You can also put this temp storage into `aggregate`'s initial value set to avoid object creation.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-03-30T18:05:28Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Compute full column-wise statistics for the RDD, including\n+   * {{{\n+   *   Mean:              Vector,\n+   *   Variance:          Vector,\n+   *   Count:             Double,\n+   *   Non-zero count:    Vector,\n+   *   Maximum elements:  Vector,\n+   *   Minimum elements:  Vector.\n+   * }}},\n+   * with the size of Vector as input parameter.\n+   */\n+  def statistics(size: Int): (Vector, Vector, Double, Vector, Vector, Vector) = {\n+    val results = self.map(_.toBreeze).aggregate((\n+      BV.zeros[Double](size),\n+      BV.zeros[Double](size),\n+      0.0,\n+      BV.zeros[Double](size),\n+      BV.fill(size){Double.MinValue},\n+      BV.fill(size){Double.MaxValue}))(\n+      seqOp = (c, v) => (c, v) match {\n+        case ((prevMean, prevM2n, cnt, nnzVec, maxVec, minVec), currData) =>\n+          val currMean = ((prevMean :* cnt) + currData) :/ (cnt + 1.0)"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Adding non-zeros directly to `nnzVec` instead of creating another. We need to think about whether we should remove explicit zero values. I suggest we do not count explicit zeros.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-03-30T18:08:15Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Compute full column-wise statistics for the RDD, including\n+   * {{{\n+   *   Mean:              Vector,\n+   *   Variance:          Vector,\n+   *   Count:             Double,\n+   *   Non-zero count:    Vector,\n+   *   Maximum elements:  Vector,\n+   *   Minimum elements:  Vector.\n+   * }}},\n+   * with the size of Vector as input parameter.\n+   */\n+  def statistics(size: Int): (Vector, Vector, Double, Vector, Vector, Vector) = {\n+    val results = self.map(_.toBreeze).aggregate((\n+      BV.zeros[Double](size),\n+      BV.zeros[Double](size),\n+      0.0,\n+      BV.zeros[Double](size),\n+      BV.fill(size){Double.MinValue},\n+      BV.fill(size){Double.MaxValue}))(\n+      seqOp = (c, v) => (c, v) match {\n+        case ((prevMean, prevM2n, cnt, nnzVec, maxVec, minVec), currData) =>\n+          val currMean = ((prevMean :* cnt) + currData) :/ (cnt + 1.0)\n+          val nonZeroCnt = Vectors\n+            .sparse(size, currData.activeKeysIterator.toSeq.map(x => (x, 1.0))).toBreeze"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "If we check explicit zero values, this line can be merged to the next foreach block.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-03-30T18:09:26Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Compute full column-wise statistics for the RDD, including\n+   * {{{\n+   *   Mean:              Vector,\n+   *   Variance:          Vector,\n+   *   Count:             Double,\n+   *   Non-zero count:    Vector,\n+   *   Maximum elements:  Vector,\n+   *   Minimum elements:  Vector.\n+   * }}},\n+   * with the size of Vector as input parameter.\n+   */\n+  def statistics(size: Int): (Vector, Vector, Double, Vector, Vector, Vector) = {\n+    val results = self.map(_.toBreeze).aggregate((\n+      BV.zeros[Double](size),\n+      BV.zeros[Double](size),\n+      0.0,\n+      BV.zeros[Double](size),\n+      BV.fill(size){Double.MinValue},\n+      BV.fill(size){Double.MaxValue}))(\n+      seqOp = (c, v) => (c, v) match {\n+        case ((prevMean, prevM2n, cnt, nnzVec, maxVec, minVec), currData) =>\n+          val currMean = ((prevMean :* cnt) + currData) :/ (cnt + 1.0)\n+          val nonZeroCnt = Vectors\n+            .sparse(size, currData.activeKeysIterator.toSeq.map(x => (x, 1.0))).toBreeze"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`lhs` and `rhs` are used in a context where there is an equation. Let's update the names to `mean1`, `mean2`, etc.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-03-30T18:12:01Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Compute full column-wise statistics for the RDD, including\n+   * {{{\n+   *   Mean:              Vector,\n+   *   Variance:          Vector,\n+   *   Count:             Double,\n+   *   Non-zero count:    Vector,\n+   *   Maximum elements:  Vector,\n+   *   Minimum elements:  Vector.\n+   * }}},\n+   * with the size of Vector as input parameter.\n+   */\n+  def statistics(size: Int): (Vector, Vector, Double, Vector, Vector, Vector) = {\n+    val results = self.map(_.toBreeze).aggregate((\n+      BV.zeros[Double](size),\n+      BV.zeros[Double](size),\n+      0.0,\n+      BV.zeros[Double](size),\n+      BV.fill(size){Double.MinValue},\n+      BV.fill(size){Double.MaxValue}))(\n+      seqOp = (c, v) => (c, v) match {\n+        case ((prevMean, prevM2n, cnt, nnzVec, maxVec, minVec), currData) =>\n+          val currMean = ((prevMean :* cnt) + currData) :/ (cnt + 1.0)\n+          val nonZeroCnt = Vectors\n+            .sparse(size, currData.activeKeysIterator.toSeq.map(x => (x, 1.0))).toBreeze\n+          currData.activeIterator.foreach { case (id, value) =>\n+            if (maxVec(id) < value) maxVec(id) = value\n+            if (minVec(id) > value) minVec(id) = value\n+          }\n+          (currMean,\n+            prevM2n + ((currData - prevMean) :* (currData - currMean)),\n+            cnt + 1.0,\n+            nnzVec + nonZeroCnt,\n+            maxVec,\n+            minVec)\n+      },\n+      combOp = (lhs, rhs) => (lhs, rhs) match {\n+        case (\n+          (lhsMean, lhsM2n, lhsCnt, lhsNNZ, lhsMax, lhsMin),"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "What do you mean by `Ring`?\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-01T16:48:32Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalSummary(\n+    mean: Vector,\n+    variance: Vector,\n+    count: Long,\n+    max: Vector,\n+    min: Vector,\n+    nonZeroCnt: Vector) extends Serializable\n+\n+/**\n+ * Case class of the aggregate value for collecting summary statistics from RDD[Vector]. These\n+ * values are relatively with\n+ * [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]], the\n+ * latter is computed from the former.\n+ */\n+private case class VectorRDDStatisticalRing("
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`fakeMean` is a weird name here. `curMean` looks better to me.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-01T16:49:33Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalSummary(\n+    mean: Vector,\n+    variance: Vector,\n+    count: Long,\n+    max: Vector,\n+    min: Vector,\n+    nonZeroCnt: Vector) extends Serializable\n+\n+/**\n+ * Case class of the aggregate value for collecting summary statistics from RDD[Vector]. These\n+ * values are relatively with\n+ * [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]], the\n+ * latter is computed from the former.\n+ */\n+private case class VectorRDDStatisticalRing(\n+    fakeMean: BV[Double],"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`size` is not necessary. You can get the size from the first element.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-01T16:54:09Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalSummary(\n+    mean: Vector,\n+    variance: Vector,\n+    count: Long,\n+    max: Vector,\n+    min: Vector,\n+    nonZeroCnt: Vector) extends Serializable\n+\n+/**\n+ * Case class of the aggregate value for collecting summary statistics from RDD[Vector]. These\n+ * values are relatively with\n+ * [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]], the\n+ * latter is computed from the former.\n+ */\n+private case class VectorRDDStatisticalRing(\n+    fakeMean: BV[Double],\n+    fakeM2n: BV[Double],\n+    totalCnt: Double,\n+    nnz: BV[Double],\n+    fakeMax: BV[Double],\n+    fakeMin: BV[Double])\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  private def seqOp(\n+      aggregator: VectorRDDStatisticalRing,\n+      currData: BV[Double]): VectorRDDStatisticalRing = {\n+    aggregator match {\n+      case VectorRDDStatisticalRing(prevMean, prevM2n, cnt, nnzVec, maxVec, minVec) =>\n+        currData.activeIterator.foreach {\n+          case (id, value) =>\n+            if (maxVec(id) < value) maxVec(id) = value\n+            if (minVec(id) > value) minVec(id) = value\n+\n+            val tmpPrevMean = prevMean(id)\n+            prevMean(id) = (prevMean(id) * cnt + value) / (cnt + 1.0)\n+            prevM2n(id) += (value - prevMean(id)) * (value - tmpPrevMean)\n+\n+            nnzVec(id) += 1.0\n+        }\n+\n+        VectorRDDStatisticalRing(\n+          prevMean,\n+          prevM2n,\n+          cnt + 1.0,\n+          nnzVec,\n+          maxVec,\n+          minVec)\n+    }\n+  }\n+\n+  /**\n+   * Combine function used for combining intermediate results together from every worker.\n+   */\n+  private def combOp(\n+      statistics1: VectorRDDStatisticalRing,\n+      statistics2: VectorRDDStatisticalRing): VectorRDDStatisticalRing = {\n+    (statistics1, statistics2) match {\n+      case (VectorRDDStatisticalRing(mean1, m2n1, cnt1, nnz1, max1, min1),\n+            VectorRDDStatisticalRing(mean2, m2n2, cnt2, nnz2, max2, min2)) =>\n+        val totalCnt = cnt1 + cnt2\n+        val deltaMean = mean2 - mean1\n+\n+        mean2.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            mean1(id) = (mean1(id) * nnz1(id) + mean2(id) * nnz2(id)) / (nnz1(id) + nnz2(id))\n+        }\n+\n+        m2n2.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            m2n1(id) +=\n+              value + deltaMean(id) * deltaMean(id) * nnz1(id) * nnz2(id) / (nnz1(id)+nnz2(id))\n+        }\n+\n+        max2.activeIterator.foreach {\n+          case (id, value) =>\n+            if (max1(id) < value) max1(id) = value\n+        }\n+\n+        min2.activeIterator.foreach {\n+          case (id, value) =>\n+            if (min1(id) > value) min1(id) = value\n+        }\n+\n+        axpy(1.0, nnz2, nnz1)\n+        VectorRDDStatisticalRing(mean1, m2n1, totalCnt, nnz1, max1, min1)\n+    }\n+  }\n+\n+  /**\n+   * Compute full column-wise statistics for the RDD with the size of Vector as input parameter.\n+   */\n+  def summarizeStatistics(size: Int): VectorRDDStatisticalSummary = {"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "You should check for every index where `nnz(i) != n`. Don't need to handle Double.MinValue / Double.MaxValue explicitly.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-01T16:56:10Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalSummary(\n+    mean: Vector,\n+    variance: Vector,\n+    count: Long,\n+    max: Vector,\n+    min: Vector,\n+    nonZeroCnt: Vector) extends Serializable\n+\n+/**\n+ * Case class of the aggregate value for collecting summary statistics from RDD[Vector]. These\n+ * values are relatively with\n+ * [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]], the\n+ * latter is computed from the former.\n+ */\n+private case class VectorRDDStatisticalRing(\n+    fakeMean: BV[Double],\n+    fakeM2n: BV[Double],\n+    totalCnt: Double,\n+    nnz: BV[Double],\n+    fakeMax: BV[Double],\n+    fakeMin: BV[Double])\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  private def seqOp(\n+      aggregator: VectorRDDStatisticalRing,\n+      currData: BV[Double]): VectorRDDStatisticalRing = {\n+    aggregator match {\n+      case VectorRDDStatisticalRing(prevMean, prevM2n, cnt, nnzVec, maxVec, minVec) =>\n+        currData.activeIterator.foreach {\n+          case (id, value) =>\n+            if (maxVec(id) < value) maxVec(id) = value\n+            if (minVec(id) > value) minVec(id) = value\n+\n+            val tmpPrevMean = prevMean(id)\n+            prevMean(id) = (prevMean(id) * cnt + value) / (cnt + 1.0)\n+            prevM2n(id) += (value - prevMean(id)) * (value - tmpPrevMean)\n+\n+            nnzVec(id) += 1.0\n+        }\n+\n+        VectorRDDStatisticalRing(\n+          prevMean,\n+          prevM2n,\n+          cnt + 1.0,\n+          nnzVec,\n+          maxVec,\n+          minVec)\n+    }\n+  }\n+\n+  /**\n+   * Combine function used for combining intermediate results together from every worker.\n+   */\n+  private def combOp(\n+      statistics1: VectorRDDStatisticalRing,\n+      statistics2: VectorRDDStatisticalRing): VectorRDDStatisticalRing = {\n+    (statistics1, statistics2) match {\n+      case (VectorRDDStatisticalRing(mean1, m2n1, cnt1, nnz1, max1, min1),\n+            VectorRDDStatisticalRing(mean2, m2n2, cnt2, nnz2, max2, min2)) =>\n+        val totalCnt = cnt1 + cnt2\n+        val deltaMean = mean2 - mean1\n+\n+        mean2.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            mean1(id) = (mean1(id) * nnz1(id) + mean2(id) * nnz2(id)) / (nnz1(id) + nnz2(id))\n+        }\n+\n+        m2n2.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            m2n1(id) +=\n+              value + deltaMean(id) * deltaMean(id) * nnz1(id) * nnz2(id) / (nnz1(id)+nnz2(id))\n+        }\n+\n+        max2.activeIterator.foreach {\n+          case (id, value) =>\n+            if (max1(id) < value) max1(id) = value\n+        }\n+\n+        min2.activeIterator.foreach {\n+          case (id, value) =>\n+            if (min1(id) > value) min1(id) = value\n+        }\n+\n+        axpy(1.0, nnz2, nnz1)\n+        VectorRDDStatisticalRing(mean1, m2n1, totalCnt, nnz1, max1, min1)\n+    }\n+  }\n+\n+  /**\n+   * Compute full column-wise statistics for the RDD with the size of Vector as input parameter.\n+   */\n+  def summarizeStatistics(size: Int): VectorRDDStatisticalSummary = {\n+    val zeroValue = VectorRDDStatisticalRing(\n+      BV.zeros[Double](size),\n+      BV.zeros[Double](size),\n+      0.0,\n+      BV.zeros[Double](size),\n+      BV.fill(size)(Double.MinValue),\n+      BV.fill(size)(Double.MaxValue))\n+\n+    val VectorRDDStatisticalRing(fakeMean, fakeM2n, totalCnt, nnz, fakeMax, fakeMin) =\n+      self.map(_.toBreeze).aggregate(zeroValue)(seqOp, combOp)\n+\n+    // solve real mean\n+    val realMean = fakeMean :* nnz :/ totalCnt\n+\n+    // solve real m2n\n+    val deltaMean = fakeMean\n+    val realM2n = fakeM2n - ((deltaMean :* deltaMean) :* (nnz :* (nnz :- totalCnt)) :/ totalCnt)\n+\n+    // remove the initial value in max and min, i.e. the Double.MaxValue or Double.MinValue."
  }, {
    "author": {
      "login": "yinxusen"
    },
    "body": "I think checking for every index where  `nnz(i) == 0` is better. I just set those max/min value to 0.0.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-02T01:52:07Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalSummary(\n+    mean: Vector,\n+    variance: Vector,\n+    count: Long,\n+    max: Vector,\n+    min: Vector,\n+    nonZeroCnt: Vector) extends Serializable\n+\n+/**\n+ * Case class of the aggregate value for collecting summary statistics from RDD[Vector]. These\n+ * values are relatively with\n+ * [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]], the\n+ * latter is computed from the former.\n+ */\n+private case class VectorRDDStatisticalRing(\n+    fakeMean: BV[Double],\n+    fakeM2n: BV[Double],\n+    totalCnt: Double,\n+    nnz: BV[Double],\n+    fakeMax: BV[Double],\n+    fakeMin: BV[Double])\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  private def seqOp(\n+      aggregator: VectorRDDStatisticalRing,\n+      currData: BV[Double]): VectorRDDStatisticalRing = {\n+    aggregator match {\n+      case VectorRDDStatisticalRing(prevMean, prevM2n, cnt, nnzVec, maxVec, minVec) =>\n+        currData.activeIterator.foreach {\n+          case (id, value) =>\n+            if (maxVec(id) < value) maxVec(id) = value\n+            if (minVec(id) > value) minVec(id) = value\n+\n+            val tmpPrevMean = prevMean(id)\n+            prevMean(id) = (prevMean(id) * cnt + value) / (cnt + 1.0)\n+            prevM2n(id) += (value - prevMean(id)) * (value - tmpPrevMean)\n+\n+            nnzVec(id) += 1.0\n+        }\n+\n+        VectorRDDStatisticalRing(\n+          prevMean,\n+          prevM2n,\n+          cnt + 1.0,\n+          nnzVec,\n+          maxVec,\n+          minVec)\n+    }\n+  }\n+\n+  /**\n+   * Combine function used for combining intermediate results together from every worker.\n+   */\n+  private def combOp(\n+      statistics1: VectorRDDStatisticalRing,\n+      statistics2: VectorRDDStatisticalRing): VectorRDDStatisticalRing = {\n+    (statistics1, statistics2) match {\n+      case (VectorRDDStatisticalRing(mean1, m2n1, cnt1, nnz1, max1, min1),\n+            VectorRDDStatisticalRing(mean2, m2n2, cnt2, nnz2, max2, min2)) =>\n+        val totalCnt = cnt1 + cnt2\n+        val deltaMean = mean2 - mean1\n+\n+        mean2.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            mean1(id) = (mean1(id) * nnz1(id) + mean2(id) * nnz2(id)) / (nnz1(id) + nnz2(id))\n+        }\n+\n+        m2n2.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            m2n1(id) +=\n+              value + deltaMean(id) * deltaMean(id) * nnz1(id) * nnz2(id) / (nnz1(id)+nnz2(id))\n+        }\n+\n+        max2.activeIterator.foreach {\n+          case (id, value) =>\n+            if (max1(id) < value) max1(id) = value\n+        }\n+\n+        min2.activeIterator.foreach {\n+          case (id, value) =>\n+            if (min1(id) > value) min1(id) = value\n+        }\n+\n+        axpy(1.0, nnz2, nnz1)\n+        VectorRDDStatisticalRing(mean1, m2n1, totalCnt, nnz1, max1, min1)\n+    }\n+  }\n+\n+  /**\n+   * Compute full column-wise statistics for the RDD with the size of Vector as input parameter.\n+   */\n+  def summarizeStatistics(size: Int): VectorRDDStatisticalSummary = {\n+    val zeroValue = VectorRDDStatisticalRing(\n+      BV.zeros[Double](size),\n+      BV.zeros[Double](size),\n+      0.0,\n+      BV.zeros[Double](size),\n+      BV.fill(size)(Double.MinValue),\n+      BV.fill(size)(Double.MaxValue))\n+\n+    val VectorRDDStatisticalRing(fakeMean, fakeM2n, totalCnt, nnz, fakeMax, fakeMin) =\n+      self.map(_.toBreeze).aggregate(zeroValue)(seqOp, combOp)\n+\n+    // solve real mean\n+    val realMean = fakeMean :* nnz :/ totalCnt\n+\n+    // solve real m2n\n+    val deltaMean = fakeMean\n+    val realM2n = fakeM2n - ((deltaMean :* deltaMean) :* (nnz :* (nnz :- totalCnt)) :/ totalCnt)\n+\n+    // remove the initial value in max and min, i.e. the Double.MaxValue or Double.MinValue."
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Instead of having two classes, we only need one accumulator class. `variance` can be computed on demand. Use `StatCounter` as a reference implementation.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-01T17:03:13Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalSummary("
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Use `numNonzeros`.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-01T17:27:07Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalSummary(\n+    mean: Vector,\n+    variance: Vector,\n+    count: Long,\n+    max: Vector,\n+    min: Vector,\n+    nonZeroCnt: Vector) extends Serializable"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Should skip `value == 0.0`.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-01T17:27:43Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalSummary(\n+    mean: Vector,\n+    variance: Vector,\n+    count: Long,\n+    max: Vector,\n+    min: Vector,\n+    nonZeroCnt: Vector) extends Serializable\n+\n+/**\n+ * Case class of the aggregate value for collecting summary statistics from RDD[Vector]. These\n+ * values are relatively with\n+ * [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]], the\n+ * latter is computed from the former.\n+ */\n+private case class VectorRDDStatisticalRing(\n+    fakeMean: BV[Double],\n+    fakeM2n: BV[Double],\n+    totalCnt: Double,\n+    nnz: BV[Double],\n+    fakeMax: BV[Double],\n+    fakeMin: BV[Double])\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  private def seqOp(\n+      aggregator: VectorRDDStatisticalRing,\n+      currData: BV[Double]): VectorRDDStatisticalRing = {\n+    aggregator match {\n+      case VectorRDDStatisticalRing(prevMean, prevM2n, cnt, nnzVec, maxVec, minVec) =>\n+        currData.activeIterator.foreach {\n+          case (id, value) =>"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "@mengxr I am not sure of the return type. Indeed, I prefer to use `Vector` in Spark as the return type instead of `BV` in Breeze, but if so, I have to use lots of `toBreeze` in my code.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-02T03:15:18Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalAggregator(\n+    mean: BV[Double],\n+    statCounter: BV[Double],\n+    totalCount: Double,\n+    numNonZeros: BV[Double],\n+    max: BV[Double],\n+    min: BV[Double])\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  private def seqOp(\n+      aggregator: VectorRDDStatisticalAggregator,\n+      currData: BV[Double]): VectorRDDStatisticalAggregator = {\n+    aggregator match {\n+      case VectorRDDStatisticalAggregator(prevMean, prevM2n, cnt, nnzVec, maxVec, minVec) =>\n+        currData.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            if (maxVec(id) < value) maxVec(id) = value\n+            if (minVec(id) > value) minVec(id) = value\n+\n+            val tmpPrevMean = prevMean(id)\n+            prevMean(id) = (prevMean(id) * cnt + value) / (cnt + 1.0)\n+            prevM2n(id) += (value - prevMean(id)) * (value - tmpPrevMean)\n+\n+            nnzVec(id) += 1.0\n+        }\n+\n+        VectorRDDStatisticalAggregator(\n+          prevMean,\n+          prevM2n,\n+          cnt + 1.0,\n+          nnzVec,\n+          maxVec,\n+          minVec)\n+    }\n+  }\n+\n+  /**\n+   * Combine function used for combining intermediate results together from every worker.\n+   */\n+  private def combOp(\n+      statistics1: VectorRDDStatisticalAggregator,\n+      statistics2: VectorRDDStatisticalAggregator): VectorRDDStatisticalAggregator = {\n+    (statistics1, statistics2) match {\n+      case (VectorRDDStatisticalAggregator(mean1, m2n1, cnt1, nnz1, max1, min1),\n+            VectorRDDStatisticalAggregator(mean2, m2n2, cnt2, nnz2, max2, min2)) =>\n+        val totalCnt = cnt1 + cnt2\n+        val deltaMean = mean2 - mean1\n+\n+        mean2.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            mean1(id) = (mean1(id) * nnz1(id) + mean2(id) * nnz2(id)) / (nnz1(id) + nnz2(id))\n+        }\n+\n+        m2n2.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            m2n1(id) +=\n+              value + deltaMean(id) * deltaMean(id) * nnz1(id) * nnz2(id) / (nnz1(id)+nnz2(id))\n+        }\n+\n+        max2.activeIterator.foreach {\n+          case (id, value) =>\n+            if (max1(id) < value) max1(id) = value\n+        }\n+\n+        min2.activeIterator.foreach {\n+          case (id, value) =>\n+            if (min1(id) > value) min1(id) = value\n+        }\n+\n+        axpy(1.0, nnz2, nnz1)\n+        VectorRDDStatisticalAggregator(mean1, m2n1, totalCnt, nnz1, max1, min1)\n+    }\n+  }\n+\n+  /**\n+   * Compute full column-wise statistics for the RDD with the size of Vector as input parameter.\n+   */\n+  def summarizeStatistics(): VectorRDDStatisticalAggregator = {\n+    val size = self.take(1).head.size\n+    val zeroValue = VectorRDDStatisticalAggregator(\n+      BV.zeros[Double](size),\n+      BV.zeros[Double](size),\n+      0.0,\n+      BV.zeros[Double](size),\n+      BV.fill(size)(Double.MinValue),\n+      BV.fill(size)(Double.MaxValue))\n+\n+    val VectorRDDStatisticalAggregator(currMean, currM2n, totalCnt, nnz, currMax, currMin) =\n+      self.map(_.toBreeze).aggregate(zeroValue)(seqOp, combOp)\n+\n+    // solve real mean\n+    val realMean = currMean :* nnz :/ totalCnt\n+\n+    // solve real m2n\n+    val deltaMean = currMean\n+    val realM2n = currM2n - ((deltaMean :* deltaMean) :* (nnz :* (nnz :- totalCnt)) :/ totalCnt)\n+\n+    // remove the initial value in max and min, i.e. the Double.MaxValue or Double.MinValue.\n+    nnz.activeIterator.foreach {\n+      case (id, 0.0) =>\n+        currMax(id) = 0.0\n+        currMin(id) = 0.0\n+      case _ =>\n+    }\n+\n+    // get variance\n+    realM2n :/= totalCnt\n+\n+    VectorRDDStatisticalAggregator("
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Do not expose breeze types in public APIs, and don't worry about `toBreeze`, which is a very light operation.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-02T05:53:19Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalAggregator(\n+    mean: BV[Double],\n+    statCounter: BV[Double],\n+    totalCount: Double,\n+    numNonZeros: BV[Double],\n+    max: BV[Double],\n+    min: BV[Double])\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  private def seqOp(\n+      aggregator: VectorRDDStatisticalAggregator,\n+      currData: BV[Double]): VectorRDDStatisticalAggregator = {\n+    aggregator match {\n+      case VectorRDDStatisticalAggregator(prevMean, prevM2n, cnt, nnzVec, maxVec, minVec) =>\n+        currData.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            if (maxVec(id) < value) maxVec(id) = value\n+            if (minVec(id) > value) minVec(id) = value\n+\n+            val tmpPrevMean = prevMean(id)\n+            prevMean(id) = (prevMean(id) * cnt + value) / (cnt + 1.0)\n+            prevM2n(id) += (value - prevMean(id)) * (value - tmpPrevMean)\n+\n+            nnzVec(id) += 1.0\n+        }\n+\n+        VectorRDDStatisticalAggregator(\n+          prevMean,\n+          prevM2n,\n+          cnt + 1.0,\n+          nnzVec,\n+          maxVec,\n+          minVec)\n+    }\n+  }\n+\n+  /**\n+   * Combine function used for combining intermediate results together from every worker.\n+   */\n+  private def combOp(\n+      statistics1: VectorRDDStatisticalAggregator,\n+      statistics2: VectorRDDStatisticalAggregator): VectorRDDStatisticalAggregator = {\n+    (statistics1, statistics2) match {\n+      case (VectorRDDStatisticalAggregator(mean1, m2n1, cnt1, nnz1, max1, min1),\n+            VectorRDDStatisticalAggregator(mean2, m2n2, cnt2, nnz2, max2, min2)) =>\n+        val totalCnt = cnt1 + cnt2\n+        val deltaMean = mean2 - mean1\n+\n+        mean2.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            mean1(id) = (mean1(id) * nnz1(id) + mean2(id) * nnz2(id)) / (nnz1(id) + nnz2(id))\n+        }\n+\n+        m2n2.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            m2n1(id) +=\n+              value + deltaMean(id) * deltaMean(id) * nnz1(id) * nnz2(id) / (nnz1(id)+nnz2(id))\n+        }\n+\n+        max2.activeIterator.foreach {\n+          case (id, value) =>\n+            if (max1(id) < value) max1(id) = value\n+        }\n+\n+        min2.activeIterator.foreach {\n+          case (id, value) =>\n+            if (min1(id) > value) min1(id) = value\n+        }\n+\n+        axpy(1.0, nnz2, nnz1)\n+        VectorRDDStatisticalAggregator(mean1, m2n1, totalCnt, nnz1, max1, min1)\n+    }\n+  }\n+\n+  /**\n+   * Compute full column-wise statistics for the RDD with the size of Vector as input parameter.\n+   */\n+  def summarizeStatistics(): VectorRDDStatisticalAggregator = {\n+    val size = self.take(1).head.size\n+    val zeroValue = VectorRDDStatisticalAggregator(\n+      BV.zeros[Double](size),\n+      BV.zeros[Double](size),\n+      0.0,\n+      BV.zeros[Double](size),\n+      BV.fill(size)(Double.MinValue),\n+      BV.fill(size)(Double.MaxValue))\n+\n+    val VectorRDDStatisticalAggregator(currMean, currM2n, totalCnt, nnz, currMax, currMin) =\n+      self.map(_.toBreeze).aggregate(zeroValue)(seqOp, combOp)\n+\n+    // solve real mean\n+    val realMean = currMean :* nnz :/ totalCnt\n+\n+    // solve real m2n\n+    val deltaMean = currMean\n+    val realM2n = currM2n - ((deltaMean :* deltaMean) :* (nnz :* (nnz :- totalCnt)) :/ totalCnt)\n+\n+    // remove the initial value in max and min, i.e. the Double.MaxValue or Double.MinValue.\n+    nnz.activeIterator.foreach {\n+      case (id, 0.0) =>\n+        currMax(id) = 0.0\n+        currMin(id) = 0.0\n+      case _ =>\n+    }\n+\n+    // get variance\n+    realM2n :/= totalCnt\n+\n+    VectorRDDStatisticalAggregator("
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Instead of a case class, use a normal class and hide breeze vector members.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-02T06:00:30Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalAggregator("
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "For example, you can have a trait:\n\n```\nclass VectorRDDStatisticalSummary {\n  def mean(): Vector\n  def variance(): Vector\n}\n```\n\nSet `computeSummaryStatistics`'s return type to this trait. Let aggregator implement this trait but mark it private.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-02T06:04:12Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalAggregator("
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Put a comment for this filter.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-02T06:06:26Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalAggregator(\n+    mean: BV[Double],\n+    statCounter: BV[Double],\n+    totalCount: Double,\n+    numNonZeros: BV[Double],\n+    max: BV[Double],\n+    min: BV[Double])\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  private def seqOp(\n+      aggregator: VectorRDDStatisticalAggregator,\n+      currData: BV[Double]): VectorRDDStatisticalAggregator = {\n+    aggregator match {\n+      case VectorRDDStatisticalAggregator(prevMean, prevM2n, cnt, nnzVec, maxVec, minVec) =>\n+        currData.activeIterator.foreach {\n+          case (id, 0.0) =>"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Define seqOp and combOp inside Aggregator to have a better separation of code.\n\n```\nclass Aggregator {\n  def add(BV[Double]): this.type\n  def merge(Aggregator): this.type\n}\n```\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-02T06:08:55Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalAggregator(\n+    mean: BV[Double],\n+    statCounter: BV[Double],\n+    totalCount: Double,\n+    numNonZeros: BV[Double],\n+    max: BV[Double],\n+    min: BV[Double])\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  private def seqOp("
  }, {
    "author": {
      "login": "yinxusen"
    },
    "body": "Hmm... that's great! Code could be simplified a lot.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-02T09:20:39Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalAggregator(\n+    mean: BV[Double],\n+    statCounter: BV[Double],\n+    totalCount: Double,\n+    numNonZeros: BV[Double],\n+    max: BV[Double],\n+    min: BV[Double])\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  private def seqOp("
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "I understand that this is convenient, but it creates seven temporary vectors. A while loop might be better. Also, you need to put a comment on what this line does and explain the formula.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-02T06:13:01Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalAggregator(\n+    mean: BV[Double],\n+    statCounter: BV[Double],\n+    totalCount: Double,\n+    numNonZeros: BV[Double],\n+    max: BV[Double],\n+    min: BV[Double])\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  private def seqOp(\n+      aggregator: VectorRDDStatisticalAggregator,\n+      currData: BV[Double]): VectorRDDStatisticalAggregator = {\n+    aggregator match {\n+      case VectorRDDStatisticalAggregator(prevMean, prevM2n, cnt, nnzVec, maxVec, minVec) =>\n+        currData.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            if (maxVec(id) < value) maxVec(id) = value\n+            if (minVec(id) > value) minVec(id) = value\n+\n+            val tmpPrevMean = prevMean(id)\n+            prevMean(id) = (prevMean(id) * cnt + value) / (cnt + 1.0)\n+            prevM2n(id) += (value - prevMean(id)) * (value - tmpPrevMean)\n+\n+            nnzVec(id) += 1.0\n+        }\n+\n+        VectorRDDStatisticalAggregator(\n+          prevMean,\n+          prevM2n,\n+          cnt + 1.0,\n+          nnzVec,\n+          maxVec,\n+          minVec)\n+    }\n+  }\n+\n+  /**\n+   * Combine function used for combining intermediate results together from every worker.\n+   */\n+  private def combOp(\n+      statistics1: VectorRDDStatisticalAggregator,\n+      statistics2: VectorRDDStatisticalAggregator): VectorRDDStatisticalAggregator = {\n+    (statistics1, statistics2) match {\n+      case (VectorRDDStatisticalAggregator(mean1, m2n1, cnt1, nnz1, max1, min1),\n+            VectorRDDStatisticalAggregator(mean2, m2n2, cnt2, nnz2, max2, min2)) =>\n+        val totalCnt = cnt1 + cnt2\n+        val deltaMean = mean2 - mean1\n+\n+        mean2.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            mean1(id) = (mean1(id) * nnz1(id) + mean2(id) * nnz2(id)) / (nnz1(id) + nnz2(id))\n+        }\n+\n+        m2n2.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            m2n1(id) +=\n+              value + deltaMean(id) * deltaMean(id) * nnz1(id) * nnz2(id) / (nnz1(id)+nnz2(id))\n+        }\n+\n+        max2.activeIterator.foreach {\n+          case (id, value) =>\n+            if (max1(id) < value) max1(id) = value\n+        }\n+\n+        min2.activeIterator.foreach {\n+          case (id, value) =>\n+            if (min1(id) > value) min1(id) = value\n+        }\n+\n+        axpy(1.0, nnz2, nnz1)\n+        VectorRDDStatisticalAggregator(mean1, m2n1, totalCnt, nnz1, max1, min1)\n+    }\n+  }\n+\n+  /**\n+   * Compute full column-wise statistics for the RDD with the size of Vector as input parameter.\n+   */\n+  def summarizeStatistics(): VectorRDDStatisticalAggregator = {\n+    val size = self.take(1).head.size\n+    val zeroValue = VectorRDDStatisticalAggregator(\n+      BV.zeros[Double](size),\n+      BV.zeros[Double](size),\n+      0.0,\n+      BV.zeros[Double](size),\n+      BV.fill(size)(Double.MinValue),\n+      BV.fill(size)(Double.MaxValue))\n+\n+    val VectorRDDStatisticalAggregator(currMean, currM2n, totalCnt, nnz, currMax, currMin) =\n+      self.map(_.toBreeze).aggregate(zeroValue)(seqOp, combOp)\n+\n+    // solve real mean\n+    val realMean = currMean :* nnz :/ totalCnt\n+\n+    // solve real m2n\n+    val deltaMean = currMean\n+    val realM2n = currM2n - ((deltaMean :* deltaMean) :* (nnz :* (nnz :- totalCnt)) :/ totalCnt)"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "There are corner cases you don't cover, e.g.,\n\n```\n1\n2\n3\n0\n```\n\nIgnoring `0`, the min value is `1`. Basically, if nnz is smaller than count, you need to update min and max by comparing them to `0`.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-02T06:16:18Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Case class of the summary statistics, including mean, variance, count, max, min, and non-zero\n+ * elements count.\n+ */\n+case class VectorRDDStatisticalAggregator(\n+    mean: BV[Double],\n+    statCounter: BV[Double],\n+    totalCount: Double,\n+    numNonZeros: BV[Double],\n+    max: BV[Double],\n+    min: BV[Double])\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  private def seqOp(\n+      aggregator: VectorRDDStatisticalAggregator,\n+      currData: BV[Double]): VectorRDDStatisticalAggregator = {\n+    aggregator match {\n+      case VectorRDDStatisticalAggregator(prevMean, prevM2n, cnt, nnzVec, maxVec, minVec) =>\n+        currData.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            if (maxVec(id) < value) maxVec(id) = value\n+            if (minVec(id) > value) minVec(id) = value\n+\n+            val tmpPrevMean = prevMean(id)\n+            prevMean(id) = (prevMean(id) * cnt + value) / (cnt + 1.0)\n+            prevM2n(id) += (value - prevMean(id)) * (value - tmpPrevMean)\n+\n+            nnzVec(id) += 1.0\n+        }\n+\n+        VectorRDDStatisticalAggregator(\n+          prevMean,\n+          prevM2n,\n+          cnt + 1.0,\n+          nnzVec,\n+          maxVec,\n+          minVec)\n+    }\n+  }\n+\n+  /**\n+   * Combine function used for combining intermediate results together from every worker.\n+   */\n+  private def combOp(\n+      statistics1: VectorRDDStatisticalAggregator,\n+      statistics2: VectorRDDStatisticalAggregator): VectorRDDStatisticalAggregator = {\n+    (statistics1, statistics2) match {\n+      case (VectorRDDStatisticalAggregator(mean1, m2n1, cnt1, nnz1, max1, min1),\n+            VectorRDDStatisticalAggregator(mean2, m2n2, cnt2, nnz2, max2, min2)) =>\n+        val totalCnt = cnt1 + cnt2\n+        val deltaMean = mean2 - mean1\n+\n+        mean2.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            mean1(id) = (mean1(id) * nnz1(id) + mean2(id) * nnz2(id)) / (nnz1(id) + nnz2(id))\n+        }\n+\n+        m2n2.activeIterator.foreach {\n+          case (id, 0.0) =>\n+          case (id, value) =>\n+            m2n1(id) +=\n+              value + deltaMean(id) * deltaMean(id) * nnz1(id) * nnz2(id) / (nnz1(id)+nnz2(id))\n+        }\n+\n+        max2.activeIterator.foreach {\n+          case (id, value) =>\n+            if (max1(id) < value) max1(id) = value\n+        }\n+\n+        min2.activeIterator.foreach {\n+          case (id, value) =>\n+            if (min1(id) > value) min1(id) = value\n+        }\n+\n+        axpy(1.0, nnz2, nnz1)\n+        VectorRDDStatisticalAggregator(mean1, m2n1, totalCnt, nnz1, max1, min1)\n+    }\n+  }\n+\n+  /**\n+   * Compute full column-wise statistics for the RDD with the size of Vector as input parameter.\n+   */\n+  def summarizeStatistics(): VectorRDDStatisticalAggregator = {\n+    val size = self.take(1).head.size\n+    val zeroValue = VectorRDDStatisticalAggregator(\n+      BV.zeros[Double](size),\n+      BV.zeros[Double](size),\n+      0.0,\n+      BV.zeros[Double](size),\n+      BV.fill(size)(Double.MinValue),\n+      BV.fill(size)(Double.MaxValue))\n+\n+    val VectorRDDStatisticalAggregator(currMean, currM2n, totalCnt, nnz, currMax, currMin) =\n+      self.map(_.toBreeze).aggregate(zeroValue)(seqOp, combOp)\n+\n+    // solve real mean\n+    val realMean = currMean :* nnz :/ totalCnt\n+\n+    // solve real m2n\n+    val deltaMean = currMean\n+    val realM2n = currM2n - ((deltaMean :* deltaMean) :* (nnz :* (nnz :- totalCnt)) :/ totalCnt)\n+\n+    // remove the initial value in max and min, i.e. the Double.MaxValue or Double.MinValue.\n+    nnz.activeIterator.foreach {\n+      case (id, 0.0) =>"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "This is \"nnz += other.nnz\". Remove `axpy` from imports.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T00:55:55Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],\n+    val currM2n: BV[Double],\n+    var totalCnt: Double,\n+    val nnz: BV[Double],\n+    val currMax: BV[Double],\n+    val currMin: BV[Double]) extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below.\n+  override lazy val mean = Vectors.fromBreeze(currMean :* nnz :/ totalCnt)\n+\n+  // Online variance solution used in add() function, while parallel variance solution used in\n+  // merge() function. Reference here:\n+  // http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n+  // Solution here ignoring the zero elements when calling add() and merge(), for decreasing the\n+  // O(n) algorithm to O(nnz). Real variance is computed here after we get other statistics, simply\n+  // by another parallel combination process.\n+  override lazy val variance = {\n+    val deltaMean = currMean\n+    var i = 0\n+    while(i < currM2n.size) {\n+      currM2n(i) += deltaMean(i) * deltaMean(i) * nnz(i) * (totalCnt-nnz(i)) / totalCnt\n+      currM2n(i) /= totalCnt\n+      i += 1\n+    }\n+    Vectors.fromBreeze(currM2n)\n+  }\n+\n+  override lazy val totalCount: Long = totalCnt.toLong\n+\n+  override lazy val numNonZeros: Vector = Vectors.fromBreeze(nnz)\n+\n+  override lazy val max: Vector = {\n+    nnz.iterator.foreach {\n+      case (id, count) =>\n+        if ((count == 0.0) || ((count < totalCnt) && (currMax(id) < 0.0)))  currMax(id) = 0.0\n+    }\n+    Vectors.fromBreeze(currMax)\n+  }\n+\n+  override lazy val min: Vector = {\n+    nnz.iterator.foreach {\n+      case (id, count) =>\n+        if ((count == 0.0) || ((count < totalCnt) && (currMin(id) > 0.0))) currMin(id) = 0.0\n+    }\n+    Vectors.fromBreeze(currMin)\n+  }\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  def add(currData: BV[Double]): this.type = {\n+    currData.activeIterator.foreach {\n+      // this case is used for filtering the zero elements if the vector is a dense one.\n+      case (id, 0.0) =>\n+      case (id, value) =>\n+        if (currMax(id) < value) currMax(id) = value\n+        if (currMin(id) > value) currMin(id) = value\n+\n+        val tmpPrevMean = currMean(id)\n+        currMean(id) = (currMean(id) * nnz(id) + value) / (nnz(id) + 1.0)\n+        currM2n(id) += (value - currMean(id)) * (value - tmpPrevMean)\n+\n+        nnz(id) += 1.0\n+    }\n+\n+    totalCnt += 1.0\n+    this\n+  }\n+\n+  /**\n+   * Combine function used for combining intermediate results together from every worker.\n+   */\n+  def merge(other: Aggregator): this.type = {\n+\n+    totalCnt += other.totalCnt\n+\n+    val deltaMean = currMean - other.currMean\n+\n+    other.currMean.activeIterator.foreach {\n+      case (id, 0.0) =>\n+      case (id, value) =>\n+        currMean(id) =\n+          (currMean(id) * nnz(id) + other.currMean(id) * other.nnz(id)) / (nnz(id) + other.nnz(id))\n+    }\n+\n+    var i = 0\n+    while(i < currM2n.size) {\n+      (nnz(i), other.nnz(i)) match {\n+        case (0.0, 0.0) =>\n+        case _ => currM2n(i) +=\n+          other.currM2n(i) + deltaMean(i) * deltaMean(i) * nnz(i) * other.nnz(i) / (nnz(i)+other.nnz(i))\n+      }\n+      i += 1\n+    }\n+\n+    other.currMax.activeIterator.foreach {\n+      case (id, value) =>\n+        if (currMax(id) < value) currMax(id) = value\n+    }\n+\n+    other.currMin.activeIterator.foreach {\n+      case (id, value) =>\n+        if (currMin(id) > value) currMin(id) = value\n+    }\n+\n+    axpy(1.0, other.nnz, nnz)"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`count` should be sufficient here.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T00:59:11Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Use BDV[Double] because you know they are dense. It might affect which implicit the compiler picks up.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T01:01:23Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "I don't think `lazy val` is useful because user should call this method only once. If the dimension is beyond 1e6, cache this extra mean vector cost several MBs. Same for other lazy values. Also, use while loop instead of chaining vector ops.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T01:04:04Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],\n+    val currM2n: BV[Double],\n+    var totalCnt: Double,\n+    val nnz: BV[Double],\n+    val currMax: BV[Double],\n+    val currMin: BV[Double]) extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below.\n+  override lazy val mean = Vectors.fromBreeze(currMean :* nnz :/ totalCnt)"
  }, {
    "author": {
      "login": "yinxusen"
    },
    "body": "If we remove `lazy`, it is dangerous for end-user to call this method more than once. Should we add a warning in the comment?\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T04:49:01Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],\n+    val currM2n: BV[Double],\n+    var totalCnt: Double,\n+    val nnz: BV[Double],\n+    val currMax: BV[Double],\n+    val currMin: BV[Double]) extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below.\n+  override lazy val mean = Vectors.fromBreeze(currMean :* nnz :/ totalCnt)"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "If the user call this method twice, just generate the vectors twice. We need to keep a small footprint within the aggregator.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T05:03:05Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],\n+    val currM2n: BV[Double],\n+    var totalCnt: Double,\n+    val nnz: BV[Double],\n+    val currMax: BV[Double],\n+    val currMin: BV[Double]) extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below.\n+  override lazy val mean = Vectors.fromBreeze(currMean :* nnz :/ totalCnt)"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "put a space after `while`\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T01:05:11Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],\n+    val currM2n: BV[Double],\n+    var totalCnt: Double,\n+    val nnz: BV[Double],\n+    val currMax: BV[Double],\n+    val currMin: BV[Double]) extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below.\n+  override lazy val mean = Vectors.fromBreeze(currMean :* nnz :/ totalCnt)\n+\n+  // Online variance solution used in add() function, while parallel variance solution used in\n+  // merge() function. Reference here:\n+  // http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n+  // Solution here ignoring the zero elements when calling add() and merge(), for decreasing the\n+  // O(n) algorithm to O(nnz). Real variance is computed here after we get other statistics, simply\n+  // by another parallel combination process.\n+  override lazy val variance = {\n+    val deltaMean = currMean\n+    var i = 0\n+    while(i < currM2n.size) {"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`totalCnt-nnz(i)` -> `totalCnt - nnz(i)` (spaces around `-`)\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T01:05:40Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],\n+    val currM2n: BV[Double],\n+    var totalCnt: Double,\n+    val nnz: BV[Double],\n+    val currMax: BV[Double],\n+    val currMin: BV[Double]) extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below.\n+  override lazy val mean = Vectors.fromBreeze(currMean :* nnz :/ totalCnt)\n+\n+  // Online variance solution used in add() function, while parallel variance solution used in\n+  // merge() function. Reference here:\n+  // http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n+  // Solution here ignoring the zero elements when calling add() and merge(), for decreasing the\n+  // O(n) algorithm to O(nnz). Real variance is computed here after we get other statistics, simply\n+  // by another parallel combination process.\n+  override lazy val variance = {\n+    val deltaMean = currMean\n+    var i = 0\n+    while(i < currM2n.size) {\n+      currM2n(i) += deltaMean(i) * deltaMean(i) * nnz(i) * (totalCnt-nnz(i)) / totalCnt"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Use while loop instead of iterator.foreach.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T01:07:14Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],\n+    val currM2n: BV[Double],\n+    var totalCnt: Double,\n+    val nnz: BV[Double],\n+    val currMax: BV[Double],\n+    val currMin: BV[Double]) extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below.\n+  override lazy val mean = Vectors.fromBreeze(currMean :* nnz :/ totalCnt)\n+\n+  // Online variance solution used in add() function, while parallel variance solution used in\n+  // merge() function. Reference here:\n+  // http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n+  // Solution here ignoring the zero elements when calling add() and merge(), for decreasing the\n+  // O(n) algorithm to O(nnz). Real variance is computed here after we get other statistics, simply\n+  // by another parallel combination process.\n+  override lazy val variance = {\n+    val deltaMean = currMean\n+    var i = 0\n+    while(i < currM2n.size) {\n+      currM2n(i) += deltaMean(i) * deltaMean(i) * nnz(i) * (totalCnt-nnz(i)) / totalCnt\n+      currM2n(i) /= totalCnt\n+      i += 1\n+    }\n+    Vectors.fromBreeze(currM2n)\n+  }\n+\n+  override lazy val totalCount: Long = totalCnt.toLong\n+\n+  override lazy val numNonZeros: Vector = Vectors.fromBreeze(nnz)\n+\n+  override lazy val max: Vector = {\n+    nnz.iterator.foreach {"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "You don't need `count == 0.0`. `currMax(id)` is `Double.MinValue` if `count == 0.0`.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T01:08:40Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],\n+    val currM2n: BV[Double],\n+    var totalCnt: Double,\n+    val nnz: BV[Double],\n+    val currMax: BV[Double],\n+    val currMin: BV[Double]) extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below.\n+  override lazy val mean = Vectors.fromBreeze(currMean :* nnz :/ totalCnt)\n+\n+  // Online variance solution used in add() function, while parallel variance solution used in\n+  // merge() function. Reference here:\n+  // http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n+  // Solution here ignoring the zero elements when calling add() and merge(), for decreasing the\n+  // O(n) algorithm to O(nnz). Real variance is computed here after we get other statistics, simply\n+  // by another parallel combination process.\n+  override lazy val variance = {\n+    val deltaMean = currMean\n+    var i = 0\n+    while(i < currM2n.size) {\n+      currM2n(i) += deltaMean(i) * deltaMean(i) * nnz(i) * (totalCnt-nnz(i)) / totalCnt\n+      currM2n(i) /= totalCnt\n+      i += 1\n+    }\n+    Vectors.fromBreeze(currM2n)\n+  }\n+\n+  override lazy val totalCount: Long = totalCnt.toLong\n+\n+  override lazy val numNonZeros: Vector = Vectors.fromBreeze(nnz)\n+\n+  override lazy val max: Vector = {\n+    nnz.iterator.foreach {\n+      case (id, count) =>\n+        if ((count == 0.0) || ((count < totalCnt) && (currMax(id) < 0.0)))  currMax(id) = 0.0"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Same issues here.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T01:09:20Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],\n+    val currM2n: BV[Double],\n+    var totalCnt: Double,\n+    val nnz: BV[Double],\n+    val currMax: BV[Double],\n+    val currMin: BV[Double]) extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below.\n+  override lazy val mean = Vectors.fromBreeze(currMean :* nnz :/ totalCnt)\n+\n+  // Online variance solution used in add() function, while parallel variance solution used in\n+  // merge() function. Reference here:\n+  // http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n+  // Solution here ignoring the zero elements when calling add() and merge(), for decreasing the\n+  // O(n) algorithm to O(nnz). Real variance is computed here after we get other statistics, simply\n+  // by another parallel combination process.\n+  override lazy val variance = {\n+    val deltaMean = currMean\n+    var i = 0\n+    while(i < currM2n.size) {\n+      currM2n(i) += deltaMean(i) * deltaMean(i) * nnz(i) * (totalCnt-nnz(i)) / totalCnt\n+      currM2n(i) /= totalCnt\n+      i += 1\n+    }\n+    Vectors.fromBreeze(currM2n)\n+  }\n+\n+  override lazy val totalCount: Long = totalCnt.toLong\n+\n+  override lazy val numNonZeros: Vector = Vectors.fromBreeze(nnz)\n+\n+  override lazy val max: Vector = {\n+    nnz.iterator.foreach {\n+      case (id, count) =>\n+        if ((count == 0.0) || ((count < totalCnt) && (currMax(id) < 0.0)))  currMax(id) = 0.0\n+    }\n+    Vectors.fromBreeze(currMax)\n+  }\n+\n+  override lazy val min: Vector = {\n+    nnz.iterator.foreach {\n+      case (id, count) =>\n+        if ((count == 0.0) || ((count < totalCnt) && (currMin(id) > 0.0))) currMin(id) = 0.0"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Sparse vector can also have explicit zero values. So please update the comment.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T01:10:19Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],\n+    val currM2n: BV[Double],\n+    var totalCnt: Double,\n+    val nnz: BV[Double],\n+    val currMax: BV[Double],\n+    val currMin: BV[Double]) extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below.\n+  override lazy val mean = Vectors.fromBreeze(currMean :* nnz :/ totalCnt)\n+\n+  // Online variance solution used in add() function, while parallel variance solution used in\n+  // merge() function. Reference here:\n+  // http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n+  // Solution here ignoring the zero elements when calling add() and merge(), for decreasing the\n+  // O(n) algorithm to O(nnz). Real variance is computed here after we get other statistics, simply\n+  // by another parallel combination process.\n+  override lazy val variance = {\n+    val deltaMean = currMean\n+    var i = 0\n+    while(i < currM2n.size) {\n+      currM2n(i) += deltaMean(i) * deltaMean(i) * nnz(i) * (totalCnt-nnz(i)) / totalCnt\n+      currM2n(i) /= totalCnt\n+      i += 1\n+    }\n+    Vectors.fromBreeze(currM2n)\n+  }\n+\n+  override lazy val totalCount: Long = totalCnt.toLong\n+\n+  override lazy val numNonZeros: Vector = Vectors.fromBreeze(nnz)\n+\n+  override lazy val max: Vector = {\n+    nnz.iterator.foreach {\n+      case (id, count) =>\n+        if ((count == 0.0) || ((count < totalCnt) && (currMax(id) < 0.0)))  currMax(id) = 0.0\n+    }\n+    Vectors.fromBreeze(currMax)\n+  }\n+\n+  override lazy val min: Vector = {\n+    nnz.iterator.foreach {\n+      case (id, count) =>\n+        if ((count == 0.0) || ((count < totalCnt) && (currMin(id) > 0.0))) currMin(id) = 0.0\n+    }\n+    Vectors.fromBreeze(currMin)\n+  }\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  def add(currData: BV[Double]): this.type = {\n+    currData.activeIterator.foreach {\n+      // this case is used for filtering the zero elements if the vector is a dense one."
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Move this comment block to a better please. Either to `add` and `merge` or the class doc.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T01:11:40Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],\n+    val currM2n: BV[Double],\n+    var totalCnt: Double,\n+    val nnz: BV[Double],\n+    val currMax: BV[Double],\n+    val currMin: BV[Double]) extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below.\n+  override lazy val mean = Vectors.fromBreeze(currMean :* nnz :/ totalCnt)\n+\n+  // Online variance solution used in add() function, while parallel variance solution used in\n+  // merge() function. Reference here:\n+  // http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n+  // Solution here ignoring the zero elements when calling add() and merge(), for decreasing the\n+  // O(n) algorithm to O(nnz). Real variance is computed here after we get other statistics, simply\n+  // by another parallel combination process."
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Use a more specific name. I used the name `Aggregator` in my previous comment just for demonstration. If a user calls `getClass`, he/she will see `org.apache.spark.mllib.rdd.Aggregator`, which is confusing.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T01:15:12Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator("
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`currMax` is a dense vector. So use a while loop.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T01:17:48Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],\n+    val currM2n: BV[Double],\n+    var totalCnt: Double,\n+    val nnz: BV[Double],\n+    val currMax: BV[Double],\n+    val currMin: BV[Double]) extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below.\n+  override lazy val mean = Vectors.fromBreeze(currMean :* nnz :/ totalCnt)\n+\n+  // Online variance solution used in add() function, while parallel variance solution used in\n+  // merge() function. Reference here:\n+  // http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n+  // Solution here ignoring the zero elements when calling add() and merge(), for decreasing the\n+  // O(n) algorithm to O(nnz). Real variance is computed here after we get other statistics, simply\n+  // by another parallel combination process.\n+  override lazy val variance = {\n+    val deltaMean = currMean\n+    var i = 0\n+    while(i < currM2n.size) {\n+      currM2n(i) += deltaMean(i) * deltaMean(i) * nnz(i) * (totalCnt-nnz(i)) / totalCnt\n+      currM2n(i) /= totalCnt\n+      i += 1\n+    }\n+    Vectors.fromBreeze(currM2n)\n+  }\n+\n+  override lazy val totalCount: Long = totalCnt.toLong\n+\n+  override lazy val numNonZeros: Vector = Vectors.fromBreeze(nnz)\n+\n+  override lazy val max: Vector = {\n+    nnz.iterator.foreach {\n+      case (id, count) =>\n+        if ((count == 0.0) || ((count < totalCnt) && (currMax(id) < 0.0)))  currMax(id) = 0.0\n+    }\n+    Vectors.fromBreeze(currMax)\n+  }\n+\n+  override lazy val min: Vector = {\n+    nnz.iterator.foreach {\n+      case (id, count) =>\n+        if ((count == 0.0) || ((count < totalCnt) && (currMin(id) > 0.0))) currMin(id) = 0.0\n+    }\n+    Vectors.fromBreeze(currMin)\n+  }\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  def add(currData: BV[Double]): this.type = {\n+    currData.activeIterator.foreach {\n+      // this case is used for filtering the zero elements if the vector is a dense one.\n+      case (id, 0.0) =>\n+      case (id, value) =>\n+        if (currMax(id) < value) currMax(id) = value\n+        if (currMin(id) > value) currMin(id) = value\n+\n+        val tmpPrevMean = currMean(id)\n+        currMean(id) = (currMean(id) * nnz(id) + value) / (nnz(id) + 1.0)\n+        currM2n(id) += (value - currMean(id)) * (value - tmpPrevMean)\n+\n+        nnz(id) += 1.0\n+    }\n+\n+    totalCnt += 1.0\n+    this\n+  }\n+\n+  /**\n+   * Combine function used for combining intermediate results together from every worker.\n+   */\n+  def merge(other: Aggregator): this.type = {\n+\n+    totalCnt += other.totalCnt\n+\n+    val deltaMean = currMean - other.currMean\n+\n+    other.currMean.activeIterator.foreach {\n+      case (id, 0.0) =>\n+      case (id, value) =>\n+        currMean(id) =\n+          (currMean(id) * nnz(id) + other.currMean(id) * other.nnz(id)) / (nnz(id) + other.nnz(id))\n+    }\n+\n+    var i = 0\n+    while(i < currM2n.size) {\n+      (nnz(i), other.nnz(i)) match {\n+        case (0.0, 0.0) =>\n+        case _ => currM2n(i) +=\n+          other.currM2n(i) + deltaMean(i) * deltaMean(i) * nnz(i) * other.nnz(i) / (nnz(i)+other.nnz(i))\n+      }\n+      i += 1\n+    }\n+\n+    other.currMax.activeIterator.foreach {"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`currMean` is a dense vector. So use a while loop.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T01:18:10Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],\n+    val currM2n: BV[Double],\n+    var totalCnt: Double,\n+    val nnz: BV[Double],\n+    val currMax: BV[Double],\n+    val currMin: BV[Double]) extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below.\n+  override lazy val mean = Vectors.fromBreeze(currMean :* nnz :/ totalCnt)\n+\n+  // Online variance solution used in add() function, while parallel variance solution used in\n+  // merge() function. Reference here:\n+  // http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n+  // Solution here ignoring the zero elements when calling add() and merge(), for decreasing the\n+  // O(n) algorithm to O(nnz). Real variance is computed here after we get other statistics, simply\n+  // by another parallel combination process.\n+  override lazy val variance = {\n+    val deltaMean = currMean\n+    var i = 0\n+    while(i < currM2n.size) {\n+      currM2n(i) += deltaMean(i) * deltaMean(i) * nnz(i) * (totalCnt-nnz(i)) / totalCnt\n+      currM2n(i) /= totalCnt\n+      i += 1\n+    }\n+    Vectors.fromBreeze(currM2n)\n+  }\n+\n+  override lazy val totalCount: Long = totalCnt.toLong\n+\n+  override lazy val numNonZeros: Vector = Vectors.fromBreeze(nnz)\n+\n+  override lazy val max: Vector = {\n+    nnz.iterator.foreach {\n+      case (id, count) =>\n+        if ((count == 0.0) || ((count < totalCnt) && (currMax(id) < 0.0)))  currMax(id) = 0.0\n+    }\n+    Vectors.fromBreeze(currMax)\n+  }\n+\n+  override lazy val min: Vector = {\n+    nnz.iterator.foreach {\n+      case (id, count) =>\n+        if ((count == 0.0) || ((count < totalCnt) && (currMin(id) > 0.0))) currMin(id) = 0.0\n+    }\n+    Vectors.fromBreeze(currMin)\n+  }\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  def add(currData: BV[Double]): this.type = {\n+    currData.activeIterator.foreach {\n+      // this case is used for filtering the zero elements if the vector is a dense one.\n+      case (id, 0.0) =>\n+      case (id, value) =>\n+        if (currMax(id) < value) currMax(id) = value\n+        if (currMin(id) > value) currMin(id) = value\n+\n+        val tmpPrevMean = currMean(id)\n+        currMean(id) = (currMean(id) * nnz(id) + value) / (nnz(id) + 1.0)\n+        currM2n(id) += (value - currMean(id)) * (value - tmpPrevMean)\n+\n+        nnz(id) += 1.0\n+    }\n+\n+    totalCnt += 1.0\n+    this\n+  }\n+\n+  /**\n+   * Combine function used for combining intermediate results together from every worker.\n+   */\n+  def merge(other: Aggregator): this.type = {\n+\n+    totalCnt += other.totalCnt\n+\n+    val deltaMean = currMean - other.currMean\n+\n+    other.currMean.activeIterator.foreach {"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`take(1).head` = `first()`\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T01:19:24Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],\n+    val currM2n: BV[Double],\n+    var totalCnt: Double,\n+    val nnz: BV[Double],\n+    val currMax: BV[Double],\n+    val currMin: BV[Double]) extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below.\n+  override lazy val mean = Vectors.fromBreeze(currMean :* nnz :/ totalCnt)\n+\n+  // Online variance solution used in add() function, while parallel variance solution used in\n+  // merge() function. Reference here:\n+  // http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n+  // Solution here ignoring the zero elements when calling add() and merge(), for decreasing the\n+  // O(n) algorithm to O(nnz). Real variance is computed here after we get other statistics, simply\n+  // by another parallel combination process.\n+  override lazy val variance = {\n+    val deltaMean = currMean\n+    var i = 0\n+    while(i < currM2n.size) {\n+      currM2n(i) += deltaMean(i) * deltaMean(i) * nnz(i) * (totalCnt-nnz(i)) / totalCnt\n+      currM2n(i) /= totalCnt\n+      i += 1\n+    }\n+    Vectors.fromBreeze(currM2n)\n+  }\n+\n+  override lazy val totalCount: Long = totalCnt.toLong\n+\n+  override lazy val numNonZeros: Vector = Vectors.fromBreeze(nnz)\n+\n+  override lazy val max: Vector = {\n+    nnz.iterator.foreach {\n+      case (id, count) =>\n+        if ((count == 0.0) || ((count < totalCnt) && (currMax(id) < 0.0)))  currMax(id) = 0.0\n+    }\n+    Vectors.fromBreeze(currMax)\n+  }\n+\n+  override lazy val min: Vector = {\n+    nnz.iterator.foreach {\n+      case (id, count) =>\n+        if ((count == 0.0) || ((count < totalCnt) && (currMin(id) > 0.0))) currMin(id) = 0.0\n+    }\n+    Vectors.fromBreeze(currMin)\n+  }\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  def add(currData: BV[Double]): this.type = {\n+    currData.activeIterator.foreach {\n+      // this case is used for filtering the zero elements if the vector is a dense one.\n+      case (id, 0.0) =>\n+      case (id, value) =>\n+        if (currMax(id) < value) currMax(id) = value\n+        if (currMin(id) > value) currMin(id) = value\n+\n+        val tmpPrevMean = currMean(id)\n+        currMean(id) = (currMean(id) * nnz(id) + value) / (nnz(id) + 1.0)\n+        currM2n(id) += (value - currMean(id)) * (value - tmpPrevMean)\n+\n+        nnz(id) += 1.0\n+    }\n+\n+    totalCnt += 1.0\n+    this\n+  }\n+\n+  /**\n+   * Combine function used for combining intermediate results together from every worker.\n+   */\n+  def merge(other: Aggregator): this.type = {\n+\n+    totalCnt += other.totalCnt\n+\n+    val deltaMean = currMean - other.currMean\n+\n+    other.currMean.activeIterator.foreach {\n+      case (id, 0.0) =>\n+      case (id, value) =>\n+        currMean(id) =\n+          (currMean(id) * nnz(id) + other.currMean(id) * other.nnz(id)) / (nnz(id) + other.nnz(id))\n+    }\n+\n+    var i = 0\n+    while(i < currM2n.size) {\n+      (nnz(i), other.nnz(i)) match {\n+        case (0.0, 0.0) =>\n+        case _ => currM2n(i) +=\n+          other.currM2n(i) + deltaMean(i) * deltaMean(i) * nnz(i) * other.nnz(i) / (nnz(i)+other.nnz(i))\n+      }\n+      i += 1\n+    }\n+\n+    other.currMax.activeIterator.foreach {\n+      case (id, value) =>\n+        if (currMax(id) < value) currMax(id) = value\n+    }\n+\n+    other.currMin.activeIterator.foreach {\n+      case (id, value) =>\n+        if (currMin(id) > value) currMin(id) = value\n+    }\n+\n+    axpy(1.0, other.nnz, nnz)\n+    this\n+  }\n+}\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Compute full column-wise statistics for the RDD with the size of Vector as input parameter.\n+   */\n+  def summarizeStatistics(): VectorRDDStatisticalSummary = {\n+    val size = self.take(1).head.size"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Do you think `computeSummaryStatistics` is a better name?\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T01:22:00Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{axpy, Vector => BV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def totalCount: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function.\n+ */\n+private class Aggregator(\n+    val currMean: BV[Double],\n+    val currM2n: BV[Double],\n+    var totalCnt: Double,\n+    val nnz: BV[Double],\n+    val currMax: BV[Double],\n+    val currMin: BV[Double]) extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below.\n+  override lazy val mean = Vectors.fromBreeze(currMean :* nnz :/ totalCnt)\n+\n+  // Online variance solution used in add() function, while parallel variance solution used in\n+  // merge() function. Reference here:\n+  // http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance\n+  // Solution here ignoring the zero elements when calling add() and merge(), for decreasing the\n+  // O(n) algorithm to O(nnz). Real variance is computed here after we get other statistics, simply\n+  // by another parallel combination process.\n+  override lazy val variance = {\n+    val deltaMean = currMean\n+    var i = 0\n+    while(i < currM2n.size) {\n+      currM2n(i) += deltaMean(i) * deltaMean(i) * nnz(i) * (totalCnt-nnz(i)) / totalCnt\n+      currM2n(i) /= totalCnt\n+      i += 1\n+    }\n+    Vectors.fromBreeze(currM2n)\n+  }\n+\n+  override lazy val totalCount: Long = totalCnt.toLong\n+\n+  override lazy val numNonZeros: Vector = Vectors.fromBreeze(nnz)\n+\n+  override lazy val max: Vector = {\n+    nnz.iterator.foreach {\n+      case (id, count) =>\n+        if ((count == 0.0) || ((count < totalCnt) && (currMax(id) < 0.0)))  currMax(id) = 0.0\n+    }\n+    Vectors.fromBreeze(currMax)\n+  }\n+\n+  override lazy val min: Vector = {\n+    nnz.iterator.foreach {\n+      case (id, count) =>\n+        if ((count == 0.0) || ((count < totalCnt) && (currMin(id) > 0.0))) currMin(id) = 0.0\n+    }\n+    Vectors.fromBreeze(currMin)\n+  }\n+\n+  /**\n+   * Aggregate function used for aggregating elements in a worker together.\n+   */\n+  def add(currData: BV[Double]): this.type = {\n+    currData.activeIterator.foreach {\n+      // this case is used for filtering the zero elements if the vector is a dense one.\n+      case (id, 0.0) =>\n+      case (id, value) =>\n+        if (currMax(id) < value) currMax(id) = value\n+        if (currMin(id) > value) currMin(id) = value\n+\n+        val tmpPrevMean = currMean(id)\n+        currMean(id) = (currMean(id) * nnz(id) + value) / (nnz(id) + 1.0)\n+        currM2n(id) += (value - currMean(id)) * (value - tmpPrevMean)\n+\n+        nnz(id) += 1.0\n+    }\n+\n+    totalCnt += 1.0\n+    this\n+  }\n+\n+  /**\n+   * Combine function used for combining intermediate results together from every worker.\n+   */\n+  def merge(other: Aggregator): this.type = {\n+\n+    totalCnt += other.totalCnt\n+\n+    val deltaMean = currMean - other.currMean\n+\n+    other.currMean.activeIterator.foreach {\n+      case (id, 0.0) =>\n+      case (id, value) =>\n+        currMean(id) =\n+          (currMean(id) * nnz(id) + other.currMean(id) * other.nnz(id)) / (nnz(id) + other.nnz(id))\n+    }\n+\n+    var i = 0\n+    while(i < currM2n.size) {\n+      (nnz(i), other.nnz(i)) match {\n+        case (0.0, 0.0) =>\n+        case _ => currM2n(i) +=\n+          other.currM2n(i) + deltaMean(i) * deltaMean(i) * nnz(i) * other.nnz(i) / (nnz(i)+other.nnz(i))\n+      }\n+      i += 1\n+    }\n+\n+    other.currMax.activeIterator.foreach {\n+      case (id, value) =>\n+        if (currMax(id) < value) currMax(id) = value\n+    }\n+\n+    other.currMin.activeIterator.foreach {\n+      case (id, value) =>\n+        if (currMin(id) > value) currMin(id) = value\n+    }\n+\n+    axpy(1.0, other.nnz, nnz)\n+    this\n+  }\n+}\n+\n+/**\n+ * Extra functions available on RDDs of [[org.apache.spark.mllib.linalg.Vector Vector]] through an\n+ * implicit conversion. Import `org.apache.spark.MLContext._` at the top of your program to use\n+ * these functions.\n+ */\n+class VectorRDDFunctions(self: RDD[Vector]) extends Serializable {\n+\n+  /**\n+   * Compute full column-wise statistics for the RDD with the size of Vector as input parameter.\n+   */\n+  def summarizeStatistics(): VectorRDDStatisticalSummary = {"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove comment\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T20:32:30Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector\n+  def variance: Vector\n+  def count: Long\n+  def numNonZeros: Vector\n+  def max: Vector\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function. Online variance solution used in add() function, while\n+ * parallel variance solution used in merge() function. Reference here:\n+ * [[http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance variance-wiki]]. Solution here\n+ * ignoring the zero elements when calling add() and merge(), for decreasing the O(n) algorithm to\n+ * O(nnz). Real variance is computed here after we get other statistics, simply by another parallel\n+ * combination process.\n+ */\n+private class VectorRDDStatisticsAggregator(\n+    val currMean: BDV[Double],\n+    val currM2n: BDV[Double],\n+    var totalCnt: Double,\n+    val nnz: BDV[Double],\n+    val currMax: BDV[Double],\n+    val currMin: BDV[Double])\n+  extends VectorRDDStatisticalSummary with Serializable {\n+\n+  // lazy val is used for computing only once time. Same below."
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Add docs to each method and put one line between method declaration.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-03T20:33:05Z",
    "diffHunk": "@@ -0,0 +1,185 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+  def mean: Vector"
  }],
  "prId": 268
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "@mengxr I use `totalCnt - 1.0` here to compute sample variance.\n\nAny other questions, feel free to inform me. :)\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-08T09:15:44Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+\n+  /**\n+   * Computes the mean of columns in RDD[Vector].\n+   */\n+  def mean: Vector\n+\n+  /**\n+   * Computes the sample variance of columns in RDD[Vector].\n+   */\n+  def variance: Vector\n+\n+  /**\n+   * Computes number of vectors in RDD[Vector].\n+   */\n+  def count: Long\n+\n+  /**\n+   * Computes the number of non-zero elements in each column of RDD[Vector].\n+   */\n+  def numNonZeros: Vector\n+\n+  /**\n+   * Computes the maximum of each column in RDD[Vector].\n+   */\n+  def max: Vector\n+\n+  /**\n+   * Computes the minimum of each column in RDD[Vector].\n+   */\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function. Online variance solution used in add() function, while\n+ * parallel variance solution used in merge() function. Reference here:\n+ * [[http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance variance-wiki]]. Solution here\n+ * ignoring the zero elements when calling add() and merge(), for decreasing the O(n) algorithm to\n+ * O(nnz). Real variance is computed here after we get other statistics, simply by another parallel\n+ * combination process.\n+ */\n+private class VectorRDDStatisticsAggregator(\n+    val currMean: BDV[Double],\n+    val currM2n: BDV[Double],\n+    var totalCnt: Double,\n+    val nnz: BDV[Double],\n+    val currMax: BDV[Double],\n+    val currMin: BDV[Double])\n+  extends VectorRDDStatisticalSummary with Serializable {\n+\n+  override def mean = {\n+    val realMean = BDV.zeros[Double](currMean.length)\n+    var i = 0\n+    while (i < currMean.length) {\n+      realMean(i) = currMean(i) * nnz(i) / totalCnt\n+      i += 1\n+    }\n+    Vectors.fromBreeze(realMean)\n+  }\n+\n+  override def variance = {\n+    val realVariance = BDV.zeros[Double](currM2n.length)\n+    val deltaMean = currMean\n+    var i = 0\n+    while (i < currM2n.size) {\n+      realVariance(i) =\n+        currM2n(i) + deltaMean(i) * deltaMean(i) * nnz(i) * (totalCnt - nnz(i)) / totalCnt\n+      realVariance(i) /= (totalCnt - 1.0)"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "What if `totalCnt == 1`? R's `var` returns `NA` for a sample of size 1.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-09T08:54:22Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+\n+  /**\n+   * Computes the mean of columns in RDD[Vector].\n+   */\n+  def mean: Vector\n+\n+  /**\n+   * Computes the sample variance of columns in RDD[Vector].\n+   */\n+  def variance: Vector\n+\n+  /**\n+   * Computes number of vectors in RDD[Vector].\n+   */\n+  def count: Long\n+\n+  /**\n+   * Computes the number of non-zero elements in each column of RDD[Vector].\n+   */\n+  def numNonZeros: Vector\n+\n+  /**\n+   * Computes the maximum of each column in RDD[Vector].\n+   */\n+  def max: Vector\n+\n+  /**\n+   * Computes the minimum of each column in RDD[Vector].\n+   */\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function. Online variance solution used in add() function, while\n+ * parallel variance solution used in merge() function. Reference here:\n+ * [[http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance variance-wiki]]. Solution here\n+ * ignoring the zero elements when calling add() and merge(), for decreasing the O(n) algorithm to\n+ * O(nnz). Real variance is computed here after we get other statistics, simply by another parallel\n+ * combination process.\n+ */\n+private class VectorRDDStatisticsAggregator(\n+    val currMean: BDV[Double],\n+    val currM2n: BDV[Double],\n+    var totalCnt: Double,\n+    val nnz: BDV[Double],\n+    val currMax: BDV[Double],\n+    val currMin: BDV[Double])\n+  extends VectorRDDStatisticalSummary with Serializable {\n+\n+  override def mean = {\n+    val realMean = BDV.zeros[Double](currMean.length)\n+    var i = 0\n+    while (i < currMean.length) {\n+      realMean(i) = currMean(i) * nnz(i) / totalCnt\n+      i += 1\n+    }\n+    Vectors.fromBreeze(realMean)\n+  }\n+\n+  override def variance = {\n+    val realVariance = BDV.zeros[Double](currM2n.length)\n+    val deltaMean = currMean\n+    var i = 0\n+    while (i < currM2n.size) {\n+      realVariance(i) =\n+        currM2n(i) + deltaMean(i) * deltaMean(i) * nnz(i) * (totalCnt - nnz(i)) / totalCnt\n+      realVariance(i) /= (totalCnt - 1.0)"
  }, {
    "author": {
      "login": "yinxusen"
    },
    "body": "Ah.. yes. if `totalCnt == 1`, then R's VAR should be `0`.\n",
    "commit": "d61363f9965b6792efb40d4af6f7932d219dfd4d",
    "createdAt": "2014-04-09T08:59:07Z",
    "diffHunk": "@@ -0,0 +1,208 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.rdd\n+\n+import breeze.linalg.{Vector => BV, DenseVector => BDV}\n+\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * Trait of the summary statistics, including mean, variance, count, max, min, and non-zero elements\n+ * count.\n+ */\n+trait VectorRDDStatisticalSummary {\n+\n+  /**\n+   * Computes the mean of columns in RDD[Vector].\n+   */\n+  def mean: Vector\n+\n+  /**\n+   * Computes the sample variance of columns in RDD[Vector].\n+   */\n+  def variance: Vector\n+\n+  /**\n+   * Computes number of vectors in RDD[Vector].\n+   */\n+  def count: Long\n+\n+  /**\n+   * Computes the number of non-zero elements in each column of RDD[Vector].\n+   */\n+  def numNonZeros: Vector\n+\n+  /**\n+   * Computes the maximum of each column in RDD[Vector].\n+   */\n+  def max: Vector\n+\n+  /**\n+   * Computes the minimum of each column in RDD[Vector].\n+   */\n+  def min: Vector\n+}\n+\n+/**\n+ * Aggregates [[org.apache.spark.mllib.rdd.VectorRDDStatisticalSummary VectorRDDStatisticalSummary]]\n+ * together with add() and merge() function. Online variance solution used in add() function, while\n+ * parallel variance solution used in merge() function. Reference here:\n+ * [[http://en.wikipedia.org/wiki/Algorithms_for_calculating_variance variance-wiki]]. Solution here\n+ * ignoring the zero elements when calling add() and merge(), for decreasing the O(n) algorithm to\n+ * O(nnz). Real variance is computed here after we get other statistics, simply by another parallel\n+ * combination process.\n+ */\n+private class VectorRDDStatisticsAggregator(\n+    val currMean: BDV[Double],\n+    val currM2n: BDV[Double],\n+    var totalCnt: Double,\n+    val nnz: BDV[Double],\n+    val currMax: BDV[Double],\n+    val currMin: BDV[Double])\n+  extends VectorRDDStatisticalSummary with Serializable {\n+\n+  override def mean = {\n+    val realMean = BDV.zeros[Double](currMean.length)\n+    var i = 0\n+    while (i < currMean.length) {\n+      realMean(i) = currMean(i) * nnz(i) / totalCnt\n+      i += 1\n+    }\n+    Vectors.fromBreeze(realMean)\n+  }\n+\n+  override def variance = {\n+    val realVariance = BDV.zeros[Double](currM2n.length)\n+    val deltaMean = currMean\n+    var i = 0\n+    while (i < currM2n.size) {\n+      realVariance(i) =\n+        currM2n(i) + deltaMean(i) * deltaMean(i) * nnz(i) * (totalCnt - nnz(i)) / totalCnt\n+      realVariance(i) /= (totalCnt - 1.0)"
  }],
  "prId": 268
}]