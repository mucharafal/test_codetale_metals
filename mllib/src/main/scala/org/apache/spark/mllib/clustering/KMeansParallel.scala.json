[{
  "comments": [{
    "author": {
      "login": "derrickburns"
    },
    "body": "Uses the general k-means algorithm here on the sample set instead of a \"LocalKMeans\" implementation.\n",
    "commit": "d6f7c66dae95a0b6967637d1132bb7327ab41d8c",
    "createdAt": "2014-09-17T00:37:55Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.mllib.base.{ PointOps, FP, Zero }\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+private[mllib] class KMeansParallel[P <: FP: ClassTag, C <: FP: ClassTag](\n+  pointOps: PointOps[P, C],\n+  k: Int,\n+  runs: Int,\n+  initializationSteps: Int,\n+  numPartitions: Int)\n+  extends KMeansInitializer[P, C] with Logging {\n+\n+  /**\n+   * Initialize `runs` sets of cluster centers using the k-means|| algorithm by Bahmani et al.\n+   * (Bahmani et al., Scalable K-Means++, VLDB 2012). This is a variant of k-means++ that tries\n+   * to find  dissimilar cluster centers by starting with a random center and then doing\n+   * passes where more centers are chosen with probability proportional to their squared distance\n+   * to the current cluster set. It results in a provable approximation to an optimal clustering.\n+   *\n+   * The original paper can be found at http://theory.stanford.edu/~sergei/papers/vldb12-kmpar.pdf.\n+   *\n+   * @param data the RDD of points\n+   * @param seed the random number generator seed\n+   * @return\n+   */\n+  def init(data: RDD[P], seed: Int): Array[Array[C]] = {\n+    log.debug(\"k-means parallel on {} points\" + data.count())\n+\n+    // randomly select one center per run, putting each into a separate array buffer\n+    val sample = data.takeSample(true, runs, seed).toSeq.map(pointOps.pointToCenter)\n+    val centers: Array[ArrayBuffer[C]] = Array.tabulate(runs)(r => ArrayBuffer(sample(r)))\n+\n+    // add at most 2k points per step\n+    for (step <- 0 until initializationSteps) {\n+      if (log.isInfoEnabled) showCenters(centers, step)\n+      val centerArrays = centers.map { x: ArrayBuffer[C] => x.toArray }\n+      val bcCenters = data.sparkContext.broadcast(centerArrays)\n+      for ((r, p) <- choose(data, seed, step, bcCenters)) {\n+        centers(r) += pointOps.pointToCenter(p)\n+      }\n+      bcCenters.unpersist()\n+    }\n+\n+    val bcCenters = data.sparkContext.broadcast(centers.map(_.toArray))\n+    val result = finalCenters(data, bcCenters, seed)\n+    bcCenters.unpersist()\n+    result\n+  }\n+\n+  def showCenters(centers: Array[ArrayBuffer[C]], step: Int) {\n+    log.info(\"step {}\", step)\n+    for (run <- 0 until runs) {\n+      log.info(\"final: run {} has {} centers\", run, centers.length)\n+    }\n+  }\n+\n+  /**\n+   * Randomly choose at most 2 * k  additional cluster centers by weighting them by their distance\n+   * to the current closest cluster\n+   *\n+   * @param data  the RDD of points\n+   * @param seed  random generator seed\n+   * @param step  which step of the selection process\n+   * @return  array of (run, point)\n+   */\n+  def choose(data: RDD[P], seed: Int, step: Int, bcCenters: Broadcast[Array[Array[C]]])\n+  : Array[(Int, P)] = {\n+    // compute the weighted distortion for each run\n+    val sumCosts = data.flatMap {\n+      point =>\n+        val centers = bcCenters.value\n+        for (r <- 0 until runs) yield {\n+          (r, point.weight * pointOps.pointCost(centers(r), point))\n+        }\n+    }.reduceByKey(_ + _).collectAsMap()\n+\n+    // choose points in proportion to ratio of weighted cost to weighted distortion\n+    data.mapPartitionsWithIndex {\n+      (index, points: Iterator[P]) =>\n+        val centers = bcCenters.value\n+        val range = 0 until runs\n+        val rand = new XORShiftRandom(seed ^ (step << 16) ^ index)\n+        points.flatMap { p =>\n+          range.filter { r =>\n+            rand.nextDouble() < 2.0 * p.weight * pointOps.pointCost(centers(r), p) * k / sumCosts(r)\n+          }.map((_, p))\n+        }\n+    }.collect()\n+  }\n+\n+  /**\n+   * Reduce sets of candidate cluster centers to at most k points per set using KMeansPlusPlus.\n+   * Weight the points by the distance to the closest cluster center.\n+   *\n+   * @param data  original points\n+   * @param bcCenters  array of sets of candidate centers\n+   * @param seed  random number seed\n+   * @return  array of sets of cluster centers\n+   */\n+  def finalCenters(data: RDD[P], bcCenters: Broadcast[Array[Array[C]]], seed: Int)\n+  : Array[Array[C]] = {\n+    // for each (run, cluster) compute the sum of the weights of the points in the cluster\n+    val weightMap = data.flatMap {\n+      point =>\n+        val centers = bcCenters.value\n+        for (r <- 0 until runs) yield {\n+          ((r, pointOps.findClosest(centers(r), point)._1), point.weight)\n+        }\n+    }.reduceByKey(_ + _).collectAsMap()\n+\n+    val centers = bcCenters.value\n+    val kmeansPlusPlus = new KMeansPlusPlus(pointOps)\n+    val trackingKmeans = new MultiKMeans(pointOps, 30)\n+    val finalCenters = (0 until runs).map {\n+      r =>\n+        val myCenters = centers(r).toArray\n+        log.info(\"run {} has {} centers\", r, myCenters.length)\n+        val weights = (0 until myCenters.length).map(i => weightMap.getOrElse((r, i), Zero)).toArray\n+        val kx = if (k > myCenters.length) myCenters.length else k\n+        val sc = data.sparkContext\n+        val initial = kmeansPlusPlus.getCenters(sc, seed, myCenters, weights, kx, numPartitions, 1)\n+        trackingKmeans.cluster(data.sparkContext.parallelize(myCenters.map(pointOps.centerToPoint)),"
  }],
  "prId": 2419
}]