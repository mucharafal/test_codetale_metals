[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Should it be \"Revisiting k-means: New Algorithms via Bayesian Nonparametrics\" instead? http://machinelearning.wustl.edu/mlpapers/papers/ICML2012Kulis_291\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-07-01T23:41:59Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Though we implement the distributed version described in the MLbase paper, it is still worth citing the original paper of DP-means.\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:09:59Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Should be private.\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-07-01T23:42:02Z",
    "diffHunk": "@@ -0,0 +1,248 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda:1 , convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData\n+          .mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambda)\n+          globalCenters ++= newGlobalCenters\n+        }\n+      } while (covered == false)\n+\n+      // Find the sum and count of points belonging to each cluster\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+        val dims = activeCenters(0).vector.size\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) +=1\n+        }\n+\n+        val result = for (i <- 0 until k) yield {\n+          (i, (sums(i), counts(i)))\n+        }\n+        result.iterator\n+      }.reduceByKey(mergeClusters).collectAsMap()\n+\n+      // Update the cluster centers\n+      var changed = false\n+      var j = 0\n+      val currentK = clusterStat.size\n+      while (j < currentK) {\n+        val (sumOfPoints, count) = clusterStat(j)\n+        if (count != 0) {\n+          scal(1.0 / count, sumOfPoints)\n+          val newCenter = new VectorWithNorm(sumOfPoints)\n+          // Check for convergence\n+          globalCenters.length match {\n+            case currentK => if (DpMeans.squaredDistance(newCenter, globalCenters(j)) >\n+              convergenceTol * convergenceTol) {\n+              changed = true\n+              }\n+            case _ => changed = true\n+          }\n+          globalCenters(j) = newCenter\n+        }\n+        j += 1\n+      }\n+      if (!changed) {\n+        converged = true\n+        logInfo(\"DpMeans clustering finished in \" + (iteration + 1) + \" iterations\")\n+      }\n+      iteration += 1\n+      norms.unpersist()\n+    }\n+\n+    if (iteration == maxIterations) {\n+      logInfo(s\"DPMeans reached the max number of iterations: $maxIterations.\")\n+    } else {\n+      logInfo(s\"DPMeans converged in $iteration iterations\")\n+    }\n+    new DpMeansModel(globalCenters.toArray.map(_.vector))\n+  }\n+}\n+/**\n+ * Core methods of  DP means clustering.\n+ */\n+object DpMeans {"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "There are two issues with using `lambda` as the parameter name:\n1. `lambda` is a keyword in Python.\n2. It is used in other algorithms as the regularization parameter, while it requires users understand the formulation.\n\nWe can use a more descriptive name like `clusterPenalty`, `maxRadius`, or `distanceThreshold`\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:10:02Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation."
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "space after `.`\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:10:34Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`Set` -> `Returns`\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:10:36Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "wrong doc. Btw, use `returns`/`sets` in API doc instead of `return`/`set`. Please fix other API doc as well.\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:10:38Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "defining a case class would help code readability\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:11:13Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "For merge operations, the contract used in Spark is that the first argument is mutable but not the second. So we should add `y` to `x` and return `x`.\n\nIt might be better to move this method near `reduceByKey`. See my comments about `reduceByKey`.\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:11:14Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`while (!covered) {`\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:11:18Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "It is useful to log the number of `localClusters` and then number of `newGlobalClusters` in the while loop.\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:11:22Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "It is also useful to check the number of new clusters from each partition. If there are millions of new centers from one partition, which happens if `lambda` is small, we should throw an error directly instead of collecting all of them.\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T20:19:32Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))"
  }, {
    "author": {
      "login": "FlytxtRnD"
    },
    "body": "@mengxr Is it a good idea to let the user input a expected cluster count as a parameter which can be used as an upper bound for the number of clusters expected from each partition? Otherwise what is a fair default value ?\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-11T09:36:37Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`var` -> `val`\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:11:52Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "I'm a little worried about when this while loop ends when I read the code. It would be useful to have some comments about its complexity.\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:12:37Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambda)\n+          globalCenters ++= newGlobalCenters\n+        }\n+      } while (covered == false)"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "space after `+=`\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:12:40Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambda)\n+          globalCenters ++= newGlobalCenters\n+        }\n+      } while (covered == false)\n+\n+      // Find the sum and count of points belonging to each cluster\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+        val dims = activeCenters(0).vector.size\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) +=1"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`Iterator.tabulate(k) { i => (i, (sums(i), counts(i))) }`\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:13:43Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambda)\n+          globalCenters ++= newGlobalCenters\n+        }\n+      } while (covered == false)\n+\n+      // Find the sum and count of points belonging to each cluster\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+        val dims = activeCenters(0).vector.size\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) +=1\n+        }\n+\n+        val result = for (i <- 0 until k) yield {"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "We should use `aggregateByKey` instead of `reduceByKey` because `reduceByKey` shouldn't modify the items in place. It might be okay to define `mergeClusters` inline with pattern matching to improve readability.\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:13:53Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambda)\n+          globalCenters ++= newGlobalCenters\n+        }\n+      } while (covered == false)\n+\n+      // Find the sum and count of points belonging to each cluster\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+        val dims = activeCenters(0).vector.size\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) +=1\n+        }\n+\n+        val result = for (i <- 0 until k) yield {\n+          (i, (sums(i), counts(i)))\n+        }\n+        result.iterator\n+      }.reduceByKey(mergeClusters).collectAsMap()"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Why do we need this `match`?\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:14:02Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambda)\n+          globalCenters ++= newGlobalCenters\n+        }\n+      } while (covered == false)\n+\n+      // Find the sum and count of points belonging to each cluster\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+        val dims = activeCenters(0).vector.size\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) +=1\n+        }\n+\n+        val result = for (i <- 0 until k) yield {\n+          (i, (sums(i), counts(i)))\n+        }\n+        result.iterator\n+      }.reduceByKey(mergeClusters).collectAsMap()\n+\n+      // Update the cluster centers\n+      var changed = false\n+      var j = 0\n+      val currentK = clusterStat.size\n+      while (j < currentK) {\n+        val (sumOfPoints, count) = clusterStat(j)\n+        if (count != 0) {\n+          scal(1.0 / count, sumOfPoints)\n+          val newCenter = new VectorWithNorm(sumOfPoints)\n+          // Check for convergence\n+          globalCenters.length match {"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "fix indentation\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:14:03Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambda)\n+          globalCenters ++= newGlobalCenters\n+        }\n+      } while (covered == false)\n+\n+      // Find the sum and count of points belonging to each cluster\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+        val dims = activeCenters(0).vector.size\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) +=1\n+        }\n+\n+        val result = for (i <- 0 until k) yield {\n+          (i, (sums(i), counts(i)))\n+        }\n+        result.iterator\n+      }.reduceByKey(mergeClusters).collectAsMap()\n+\n+      // Update the cluster centers\n+      var changed = false\n+      var j = 0\n+      val currentK = clusterStat.size\n+      while (j < currentK) {\n+        val (sumOfPoints, count) = clusterStat(j)\n+        if (count != 0) {\n+          scal(1.0 / count, sumOfPoints)\n+          val newCenter = new VectorWithNorm(sumOfPoints)\n+          // Check for convergence\n+          globalCenters.length match {\n+            case currentK => if (DpMeans.squaredDistance(newCenter, globalCenters(j)) >\n+              convergenceTol * convergenceTol) {\n+              changed = true\n+              }"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Should not unpersist the input vector norms, which do not change during iterations.\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:14:05Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambda)\n+          globalCenters ++= newGlobalCenters\n+        }\n+      } while (covered == false)\n+\n+      // Find the sum and count of points belonging to each cluster\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+        val dims = activeCenters(0).vector.size\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) +=1\n+        }\n+\n+        val result = for (i <- 0 until k) yield {\n+          (i, (sums(i), counts(i)))\n+        }\n+        result.iterator\n+      }.reduceByKey(mergeClusters).collectAsMap()\n+\n+      // Update the cluster centers\n+      var changed = false\n+      var j = 0\n+      val currentK = clusterStat.size\n+      while (j < currentK) {\n+        val (sumOfPoints, count) = clusterStat(j)\n+        if (count != 0) {\n+          scal(1.0 / count, sumOfPoints)\n+          val newCenter = new VectorWithNorm(sumOfPoints)\n+          // Check for convergence\n+          globalCenters.length match {\n+            case currentK => if (DpMeans.squaredDistance(newCenter, globalCenters(j)) >\n+              convergenceTol * convergenceTol) {\n+              changed = true\n+              }\n+            case _ => changed = true\n+          }\n+          globalCenters(j) = newCenter\n+        }\n+        j += 1\n+      }\n+      if (!changed) {\n+        converged = true\n+        logInfo(\"DpMeans clustering finished in \" + (iteration + 1) + \" iterations\")\n+      }\n+      iteration += 1\n+      norms.unpersist()"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove extra space before `DP`\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:14:07Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambda)\n+          globalCenters ++= newGlobalCenters\n+        }\n+      } while (covered == false)\n+\n+      // Find the sum and count of points belonging to each cluster\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+        val dims = activeCenters(0).vector.size\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) +=1\n+        }\n+\n+        val result = for (i <- 0 until k) yield {\n+          (i, (sums(i), counts(i)))\n+        }\n+        result.iterator\n+      }.reduceByKey(mergeClusters).collectAsMap()\n+\n+      // Update the cluster centers\n+      var changed = false\n+      var j = 0\n+      val currentK = clusterStat.size\n+      while (j < currentK) {\n+        val (sumOfPoints, count) = clusterStat(j)\n+        if (count != 0) {\n+          scal(1.0 / count, sumOfPoints)\n+          val newCenter = new VectorWithNorm(sumOfPoints)\n+          // Check for convergence\n+          globalCenters.length match {\n+            case currentK => if (DpMeans.squaredDistance(newCenter, globalCenters(j)) >\n+              convergenceTol * convergenceTol) {\n+              changed = true\n+              }\n+            case _ => changed = true\n+          }\n+          globalCenters(j) = newCenter\n+        }\n+        j += 1\n+      }\n+      if (!changed) {\n+        converged = true\n+        logInfo(\"DpMeans clustering finished in \" + (iteration + 1) + \" iterations\")\n+      }\n+      iteration += 1\n+      norms.unpersist()\n+    }\n+\n+    if (iteration == maxIterations) {\n+      logInfo(s\"DPMeans reached the max number of iterations: $maxIterations.\")\n+    } else {\n+      logInfo(s\"DPMeans converged in $iteration iterations\")\n+    }\n+    new DpMeansModel(globalCenters.toArray.map(_.vector))\n+  }\n+}\n+/**\n+ * Core methods of  DP means clustering."
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Missing doc for params and, more importantly, the return type. The API doc reads like that the method returns a boolean instead.\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:14:24Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambda)\n+          globalCenters ++= newGlobalCenters\n+        }\n+      } while (covered == false)\n+\n+      // Find the sum and count of points belonging to each cluster\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+        val dims = activeCenters(0).vector.size\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) +=1\n+        }\n+\n+        val result = for (i <- 0 until k) yield {\n+          (i, (sums(i), counts(i)))\n+        }\n+        result.iterator\n+      }.reduceByKey(mergeClusters).collectAsMap()\n+\n+      // Update the cluster centers\n+      var changed = false\n+      var j = 0\n+      val currentK = clusterStat.size\n+      while (j < currentK) {\n+        val (sumOfPoints, count) = clusterStat(j)\n+        if (count != 0) {\n+          scal(1.0 / count, sumOfPoints)\n+          val newCenter = new VectorWithNorm(sumOfPoints)\n+          // Check for convergence\n+          globalCenters.length match {\n+            case currentK => if (DpMeans.squaredDistance(newCenter, globalCenters(j)) >\n+              convergenceTol * convergenceTol) {\n+              changed = true\n+              }\n+            case _ => changed = true\n+          }\n+          globalCenters(j) = newCenter\n+        }\n+        j += 1\n+      }\n+      if (!changed) {\n+        converged = true\n+        logInfo(\"DpMeans clustering finished in \" + (iteration + 1) + \" iterations\")\n+      }\n+      iteration += 1\n+      norms.unpersist()\n+    }\n+\n+    if (iteration == maxIterations) {\n+      logInfo(s\"DPMeans reached the max number of iterations: $maxIterations.\")\n+    } else {\n+      logInfo(s\"DPMeans converged in $iteration iterations\")\n+    }\n+    new DpMeansModel(globalCenters.toArray.map(_.vector))\n+  }\n+}\n+/**\n+ * Core methods of  DP means clustering.\n+ */\n+private object DpMeans {\n+  /**\n+   * A data point is said to be \"covered\" by a cluster `c` if the distance from the point\n+   * to the cluster center of `c` is less than a given lambda value."
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "The method name taken from the paper is not self-explanatory. `generateNewCenters` would be better here. We can leave a comment in the doc, mentioning that this is the `cover` method in the paper.\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:16:07Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambda)\n+          globalCenters ++= newGlobalCenters\n+        }\n+      } while (covered == false)\n+\n+      // Find the sum and count of points belonging to each cluster\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+        val dims = activeCenters(0).vector.size\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) +=1\n+        }\n+\n+        val result = for (i <- 0 until k) yield {\n+          (i, (sums(i), counts(i)))\n+        }\n+        result.iterator\n+      }.reduceByKey(mergeClusters).collectAsMap()\n+\n+      // Update the cluster centers\n+      var changed = false\n+      var j = 0\n+      val currentK = clusterStat.size\n+      while (j < currentK) {\n+        val (sumOfPoints, count) = clusterStat(j)\n+        if (count != 0) {\n+          scal(1.0 / count, sumOfPoints)\n+          val newCenter = new VectorWithNorm(sumOfPoints)\n+          // Check for convergence\n+          globalCenters.length match {\n+            case currentK => if (DpMeans.squaredDistance(newCenter, globalCenters(j)) >\n+              convergenceTol * convergenceTol) {\n+              changed = true\n+              }\n+            case _ => changed = true\n+          }\n+          globalCenters(j) = newCenter\n+        }\n+        j += 1\n+      }\n+      if (!changed) {\n+        converged = true\n+        logInfo(\"DpMeans clustering finished in \" + (iteration + 1) + \" iterations\")\n+      }\n+      iteration += 1\n+      norms.unpersist()\n+    }\n+\n+    if (iteration == maxIterations) {\n+      logInfo(s\"DPMeans reached the max number of iterations: $maxIterations.\")\n+    } else {\n+      logInfo(s\"DPMeans converged in $iteration iterations\")\n+    }\n+    new DpMeansModel(globalCenters.toArray.map(_.vector))\n+  }\n+}\n+/**\n+ * Core methods of  DP means clustering.\n+ */\n+private object DpMeans {\n+  /**\n+   * A data point is said to be \"covered\" by a cluster `c` if the distance from the point\n+   * to the cluster center of `c` is less than a given lambda value.\n+   */\n+  def cover("
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "space before `(` and after `)`\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:16:09Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambda)\n+          globalCenters ++= newGlobalCenters\n+        }\n+      } while (covered == false)\n+\n+      // Find the sum and count of points belonging to each cluster\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+        val dims = activeCenters(0).vector.size\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) +=1\n+        }\n+\n+        val result = for (i <- 0 until k) yield {\n+          (i, (sums(i), counts(i)))\n+        }\n+        result.iterator\n+      }.reduceByKey(mergeClusters).collectAsMap()\n+\n+      // Update the cluster centers\n+      var changed = false\n+      var j = 0\n+      val currentK = clusterStat.size\n+      while (j < currentK) {\n+        val (sumOfPoints, count) = clusterStat(j)\n+        if (count != 0) {\n+          scal(1.0 / count, sumOfPoints)\n+          val newCenter = new VectorWithNorm(sumOfPoints)\n+          // Check for convergence\n+          globalCenters.length match {\n+            case currentK => if (DpMeans.squaredDistance(newCenter, globalCenters(j)) >\n+              convergenceTol * convergenceTol) {\n+              changed = true\n+              }\n+            case _ => changed = true\n+          }\n+          globalCenters(j) = newCenter\n+        }\n+        j += 1\n+      }\n+      if (!changed) {\n+        converged = true\n+        logInfo(\"DpMeans clustering finished in \" + (iteration + 1) + \" iterations\")\n+      }\n+      iteration += 1\n+      norms.unpersist()\n+    }\n+\n+    if (iteration == maxIterations) {\n+      logInfo(s\"DPMeans reached the max number of iterations: $maxIterations.\")\n+    } else {\n+      logInfo(s\"DPMeans converged in $iteration iterations\")\n+    }\n+    new DpMeansModel(globalCenters.toArray.map(_.vector))\n+  }\n+}\n+/**\n+ * Core methods of  DP means clustering.\n+ */\n+private object DpMeans {\n+  /**\n+   * A data point is said to be \"covered\" by a cluster `c` if the distance from the point\n+   * to the cluster center of `c` is less than a given lambda value.\n+   */\n+  def cover(\n+      points: Iterator[VectorWithNorm],\n+      centers: ArrayBuffer[VectorWithNorm],\n+      lambda: Double): Iterator[VectorWithNorm] = {\n+    var newCenters = ArrayBuffer.empty[VectorWithNorm]\n+    if(!points.isEmpty){"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "fix indentation\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:16:11Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambda)\n+          globalCenters ++= newGlobalCenters\n+        }\n+      } while (covered == false)\n+\n+      // Find the sum and count of points belonging to each cluster\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+        val dims = activeCenters(0).vector.size\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) +=1\n+        }\n+\n+        val result = for (i <- 0 until k) yield {\n+          (i, (sums(i), counts(i)))\n+        }\n+        result.iterator\n+      }.reduceByKey(mergeClusters).collectAsMap()\n+\n+      // Update the cluster centers\n+      var changed = false\n+      var j = 0\n+      val currentK = clusterStat.size\n+      while (j < currentK) {\n+        val (sumOfPoints, count) = clusterStat(j)\n+        if (count != 0) {\n+          scal(1.0 / count, sumOfPoints)\n+          val newCenter = new VectorWithNorm(sumOfPoints)\n+          // Check for convergence\n+          globalCenters.length match {\n+            case currentK => if (DpMeans.squaredDistance(newCenter, globalCenters(j)) >\n+              convergenceTol * convergenceTol) {\n+              changed = true\n+              }\n+            case _ => changed = true\n+          }\n+          globalCenters(j) = newCenter\n+        }\n+        j += 1\n+      }\n+      if (!changed) {\n+        converged = true\n+        logInfo(\"DpMeans clustering finished in \" + (iteration + 1) + \" iterations\")\n+      }\n+      iteration += 1\n+      norms.unpersist()\n+    }\n+\n+    if (iteration == maxIterations) {\n+      logInfo(s\"DPMeans reached the max number of iterations: $maxIterations.\")\n+    } else {\n+      logInfo(s\"DPMeans converged in $iteration iterations\")\n+    }\n+    new DpMeansModel(globalCenters.toArray.map(_.vector))\n+  }\n+}\n+/**\n+ * Core methods of  DP means clustering.\n+ */\n+private object DpMeans {\n+  /**\n+   * A data point is said to be \"covered\" by a cluster `c` if the distance from the point\n+   * to the cluster center of `c` is less than a given lambda value.\n+   */\n+  def cover(\n+      points: Iterator[VectorWithNorm],\n+      centers: ArrayBuffer[VectorWithNorm],\n+      lambda: Double): Iterator[VectorWithNorm] = {\n+    var newCenters = ArrayBuffer.empty[VectorWithNorm]\n+    if(!points.isEmpty){\n+      if (centers.length == 0) newCenters += points.next\n+      points.foreach { z =>\n+        val dist = newCenters.union(centers).map { center => squaredDistance(z, center) }\n+        if (dist.min > lambda) newCenters += z\n+      }\n+    }\n+    newCenters.iterator\n+  }\n+\n+  /**\n+   * Each data point is assigned to the nearest cluster. This method returns\n+   * the corresponding cluster label and the distance from the center to the\n+   * cluster center.\n+   */\n+    def assignCluster("
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "This makes three passes to `dist`: 1 `indexOf` and 2 `min`. One pass should be sufficient and we don't need to create `dist` array. See https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/clustering/KMeans.scala#L549.\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-08T17:16:15Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambda)\n+          globalCenters ++= newGlobalCenters\n+        }\n+      } while (covered == false)\n+\n+      // Find the sum and count of points belonging to each cluster\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+        val dims = activeCenters(0).vector.size\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) +=1\n+        }\n+\n+        val result = for (i <- 0 until k) yield {\n+          (i, (sums(i), counts(i)))\n+        }\n+        result.iterator\n+      }.reduceByKey(mergeClusters).collectAsMap()\n+\n+      // Update the cluster centers\n+      var changed = false\n+      var j = 0\n+      val currentK = clusterStat.size\n+      while (j < currentK) {\n+        val (sumOfPoints, count) = clusterStat(j)\n+        if (count != 0) {\n+          scal(1.0 / count, sumOfPoints)\n+          val newCenter = new VectorWithNorm(sumOfPoints)\n+          // Check for convergence\n+          globalCenters.length match {\n+            case currentK => if (DpMeans.squaredDistance(newCenter, globalCenters(j)) >\n+              convergenceTol * convergenceTol) {\n+              changed = true\n+              }\n+            case _ => changed = true\n+          }\n+          globalCenters(j) = newCenter\n+        }\n+        j += 1\n+      }\n+      if (!changed) {\n+        converged = true\n+        logInfo(\"DpMeans clustering finished in \" + (iteration + 1) + \" iterations\")\n+      }\n+      iteration += 1\n+      norms.unpersist()\n+    }\n+\n+    if (iteration == maxIterations) {\n+      logInfo(s\"DPMeans reached the max number of iterations: $maxIterations.\")\n+    } else {\n+      logInfo(s\"DPMeans converged in $iteration iterations\")\n+    }\n+    new DpMeansModel(globalCenters.toArray.map(_.vector))\n+  }\n+}\n+/**\n+ * Core methods of  DP means clustering.\n+ */\n+private object DpMeans {\n+  /**\n+   * A data point is said to be \"covered\" by a cluster `c` if the distance from the point\n+   * to the cluster center of `c` is less than a given lambda value.\n+   */\n+  def cover(\n+      points: Iterator[VectorWithNorm],\n+      centers: ArrayBuffer[VectorWithNorm],\n+      lambda: Double): Iterator[VectorWithNorm] = {\n+    var newCenters = ArrayBuffer.empty[VectorWithNorm]\n+    if(!points.isEmpty){\n+      if (centers.length == 0) newCenters += points.next\n+      points.foreach { z =>\n+        val dist = newCenters.union(centers).map { center => squaredDistance(z, center) }\n+        if (dist.min > lambda) newCenters += z\n+      }\n+    }\n+    newCenters.iterator\n+  }\n+\n+  /**\n+   * Each data point is assigned to the nearest cluster. This method returns\n+   * the corresponding cluster label and the distance from the center to the\n+   * cluster center.\n+   */\n+    def assignCluster(\n+        centers: ArrayBuffer[VectorWithNorm],\n+        x: VectorWithNorm): (Int, Double) = {\n+      val dist = centers.map { c => squaredDistance(x, c) }\n+      (dist.indexOf(dist.min), dist.min)"
  }, {
    "author": {
      "login": "FlytxtRnD"
    },
    "body": "@mengxr Do you want me to reuse the findClosest function in Kmeans instead of defining something similar here?\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-09-18T06:40:09Z",
    "diffHunk": "@@ -0,0 +1,247 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"MLbase: Distributed Machine Learning Made Easy\" by\n+ * Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambda The distance threshold value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambda: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance.The default parameters are {lambda: 1, convergenceTol: 0.01,\n+   * maxIterations: 20}.\n+   */\n+  def this() = this(1, 0.01, 20)\n+\n+  /** Set the distance threshold that controls cluster creation. Default: 1 */\n+  def getLambda(): Double = lambda\n+\n+  /** Return the lambda. */\n+  def setLambda(lambda: Double): this.type = {\n+    this.lambda = lambda\n+    this\n+  }\n+\n+  /** Set the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Return the threshold value at which convergence is considered to have occurred. */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Set the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Return the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /**\n+   * Perform DP means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP means algorithm.\n+    var iteration = 0\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+      type WeightedPoint = (Vector, Long)\n+      def mergeClusters(x: WeightedPoint, y: WeightedPoint): WeightedPoint = {\n+        axpy(1.0, x._1, y._1)\n+        (y._1, x._2 + y._2)\n+      }\n+\n+      // Loop until all data points are covered by some cluster center\n+      do {\n+        localCenters = zippedData.mapPartitions(h => DpMeans.cover(h, globalCenters, lambda))\n+          .collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          var newGlobalCenters = DpMeans.cover(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambda)\n+          globalCenters ++= newGlobalCenters\n+        }\n+      } while (covered == false)\n+\n+      // Find the sum and count of points belonging to each cluster\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+        val dims = activeCenters(0).vector.size\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) +=1\n+        }\n+\n+        val result = for (i <- 0 until k) yield {\n+          (i, (sums(i), counts(i)))\n+        }\n+        result.iterator\n+      }.reduceByKey(mergeClusters).collectAsMap()\n+\n+      // Update the cluster centers\n+      var changed = false\n+      var j = 0\n+      val currentK = clusterStat.size\n+      while (j < currentK) {\n+        val (sumOfPoints, count) = clusterStat(j)\n+        if (count != 0) {\n+          scal(1.0 / count, sumOfPoints)\n+          val newCenter = new VectorWithNorm(sumOfPoints)\n+          // Check for convergence\n+          globalCenters.length match {\n+            case currentK => if (DpMeans.squaredDistance(newCenter, globalCenters(j)) >\n+              convergenceTol * convergenceTol) {\n+              changed = true\n+              }\n+            case _ => changed = true\n+          }\n+          globalCenters(j) = newCenter\n+        }\n+        j += 1\n+      }\n+      if (!changed) {\n+        converged = true\n+        logInfo(\"DpMeans clustering finished in \" + (iteration + 1) + \" iterations\")\n+      }\n+      iteration += 1\n+      norms.unpersist()\n+    }\n+\n+    if (iteration == maxIterations) {\n+      logInfo(s\"DPMeans reached the max number of iterations: $maxIterations.\")\n+    } else {\n+      logInfo(s\"DPMeans converged in $iteration iterations\")\n+    }\n+    new DpMeansModel(globalCenters.toArray.map(_.vector))\n+  }\n+}\n+/**\n+ * Core methods of  DP means clustering.\n+ */\n+private object DpMeans {\n+  /**\n+   * A data point is said to be \"covered\" by a cluster `c` if the distance from the point\n+   * to the cluster center of `c` is less than a given lambda value.\n+   */\n+  def cover(\n+      points: Iterator[VectorWithNorm],\n+      centers: ArrayBuffer[VectorWithNorm],\n+      lambda: Double): Iterator[VectorWithNorm] = {\n+    var newCenters = ArrayBuffer.empty[VectorWithNorm]\n+    if(!points.isEmpty){\n+      if (centers.length == 0) newCenters += points.next\n+      points.foreach { z =>\n+        val dist = newCenters.union(centers).map { center => squaredDistance(z, center) }\n+        if (dist.min > lambda) newCenters += z\n+      }\n+    }\n+    newCenters.iterator\n+  }\n+\n+  /**\n+   * Each data point is assigned to the nearest cluster. This method returns\n+   * the corresponding cluster label and the distance from the center to the\n+   * cluster center.\n+   */\n+    def assignCluster(\n+        centers: ArrayBuffer[VectorWithNorm],\n+        x: VectorWithNorm): (Int, Double) = {\n+      val dist = centers.map { c => squaredDistance(x, c) }\n+      (dist.indexOf(dist.min), dist.min)"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "yu-iskw"
    },
    "body": "I think we could also implement this with a single `aggregateByKey`. It makes this more simple and more efficient.\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-11-02T00:06:49Z",
    "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP-means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"Revisiting k-means: New Algorithms via Bayesian Nonparametrics\"\n+ * by Brian Kulis, Michael I. Jordan. This implementation is based on \"MLbase: Distributed\n+ * Machine Learning Made Easy\" by Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambdaValue distance value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ * // TODO\n+ * @param maxClusterCount The maximum expected number of clusters.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambdaValue: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int,\n+    private var maxClusterCount: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance with default parameters: {lambdaValue: 1,\n+   * convergenceTol: 0.01, maxIterations: 20, maxClusterCount: 1000}.\n+   */\n+  def this() = this(1, 0.01, 20, 1000)\n+\n+  /** Sets the value for the lambda parameter, which controls the cluster creation. Default: 1 */\n+  def setLambdaValue(lambdaValue: Double): this.type = {\n+    this.lambdaValue = lambdaValue\n+    this\n+  }\n+\n+  /** Returns the lambda value */\n+  def getLambdaValue: Double = lambdaValue\n+\n+  /** Sets the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Returns the convergence threshold value . */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Sets the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Returns the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /** Sets the maximum number of clusters expected. Default: 1000 */\n+  def setMaxClusterCount(maxClusterCount: Int): this.type = {\n+    this.maxClusterCount = maxClusterCount\n+    this\n+  }\n+\n+  /** Returns the maximum number of clusters expected. */\n+  def getMaxClusterCount: Int = maxClusterCount\n+\n+  /**\n+   * Perform DP-means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP-means algorithm.\n+    var iteration = 0\n+    var previousK = 1\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+\n+      // This loop simulates the cluster assignment step by iterating through the data\n+      // points until no new clusters are generated. A new cluster is formed whenever\n+      // the distance from a data point to all the existing cluster centroids is greater\n+      // than lambdaValue. The cluster centers returned by generateNewCenters(), called\n+      // localCenters, are sent to the master. The cluster creation process is performed\n+      // locally on these localCenters and the resulting cluster centers are added to\n+      // the existing set of global centers.\n+      while (!covered) {\n+        val findLocalCenters = zippedData\n+          .mapPartitions(h =>\n+              DpMeans.generateNewCenters(h, globalCenters, lambdaValue)\n+          ).persist()\n+        val localCentersCount = findLocalCenters.count()\n+        logInfo(\" Local centers count :: \" + localCentersCount)\n+        if (localCentersCount > getMaxClusterCount) {\n+          logInfo(\" Local centers count \" + localCentersCount + \" exceeds user expectation\")\n+          sys.exit()\n+        }\n+        localCenters = findLocalCenters.collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          val newGlobalCenters = DpMeans.generateNewCenters(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambdaValue)\n+          globalCenters ++= newGlobalCenters\n+          logInfo(\" Number of global centers :: \" + globalCenters.length)\n+        }\n+      }\n+\n+      // Find the sum and count of points belonging to each cluster\n+      case class WeightedPoint(vector: Vector, count: Long)\n+      val mergeClusters = (x: WeightedPoint, y: WeightedPoint) => {\n+        axpy(1.0, y.vector, x.vector)\n+        WeightedPoint(x.vector, x.count + y.count)\n+      }\n+\n+      val dims = globalCenters(0).vector.size\n+      val clusterStat = zippedData.mapPartitions { points =>"
  }, {
    "author": {
      "login": "FlytxtRnD"
    },
    "body": "@yu-iskw Could you please give more inputs on this note?\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-11-02T12:20:05Z",
    "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP-means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"Revisiting k-means: New Algorithms via Bayesian Nonparametrics\"\n+ * by Brian Kulis, Michael I. Jordan. This implementation is based on \"MLbase: Distributed\n+ * Machine Learning Made Easy\" by Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambdaValue distance value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ * // TODO\n+ * @param maxClusterCount The maximum expected number of clusters.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambdaValue: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int,\n+    private var maxClusterCount: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance with default parameters: {lambdaValue: 1,\n+   * convergenceTol: 0.01, maxIterations: 20, maxClusterCount: 1000}.\n+   */\n+  def this() = this(1, 0.01, 20, 1000)\n+\n+  /** Sets the value for the lambda parameter, which controls the cluster creation. Default: 1 */\n+  def setLambdaValue(lambdaValue: Double): this.type = {\n+    this.lambdaValue = lambdaValue\n+    this\n+  }\n+\n+  /** Returns the lambda value */\n+  def getLambdaValue: Double = lambdaValue\n+\n+  /** Sets the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Returns the convergence threshold value . */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Sets the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Returns the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /** Sets the maximum number of clusters expected. Default: 1000 */\n+  def setMaxClusterCount(maxClusterCount: Int): this.type = {\n+    this.maxClusterCount = maxClusterCount\n+    this\n+  }\n+\n+  /** Returns the maximum number of clusters expected. */\n+  def getMaxClusterCount: Int = maxClusterCount\n+\n+  /**\n+   * Perform DP-means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP-means algorithm.\n+    var iteration = 0\n+    var previousK = 1\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+\n+      // This loop simulates the cluster assignment step by iterating through the data\n+      // points until no new clusters are generated. A new cluster is formed whenever\n+      // the distance from a data point to all the existing cluster centroids is greater\n+      // than lambdaValue. The cluster centers returned by generateNewCenters(), called\n+      // localCenters, are sent to the master. The cluster creation process is performed\n+      // locally on these localCenters and the resulting cluster centers are added to\n+      // the existing set of global centers.\n+      while (!covered) {\n+        val findLocalCenters = zippedData\n+          .mapPartitions(h =>\n+              DpMeans.generateNewCenters(h, globalCenters, lambdaValue)\n+          ).persist()\n+        val localCentersCount = findLocalCenters.count()\n+        logInfo(\" Local centers count :: \" + localCentersCount)\n+        if (localCentersCount > getMaxClusterCount) {\n+          logInfo(\" Local centers count \" + localCentersCount + \" exceeds user expectation\")\n+          sys.exit()\n+        }\n+        localCenters = findLocalCenters.collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          val newGlobalCenters = DpMeans.generateNewCenters(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambdaValue)\n+          globalCenters ++= newGlobalCenters\n+          logInfo(\" Number of global centers :: \" + globalCenters.length)\n+        }\n+      }\n+\n+      // Find the sum and count of points belonging to each cluster\n+      case class WeightedPoint(vector: Vector, count: Long)\n+      val mergeClusters = (x: WeightedPoint, y: WeightedPoint) => {\n+        axpy(1.0, y.vector, x.vector)\n+        WeightedPoint(x.vector, x.count + y.count)\n+      }\n+\n+      val dims = globalCenters(0).vector.size\n+      val clusterStat = zippedData.mapPartitions { points =>"
  }, {
    "author": {
      "login": "yu-iskw"
    },
    "body": "@FlytxtRnD sure! This would work like this. However, I haven't confirmed it carefully yet.\nhttps://github.com/yu-iskw/spark/commit/4070ae666eade692a129a95f5635047b39ccbed7\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-11-02T23:14:20Z",
    "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP-means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"Revisiting k-means: New Algorithms via Bayesian Nonparametrics\"\n+ * by Brian Kulis, Michael I. Jordan. This implementation is based on \"MLbase: Distributed\n+ * Machine Learning Made Easy\" by Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambdaValue distance value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ * // TODO\n+ * @param maxClusterCount The maximum expected number of clusters.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambdaValue: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int,\n+    private var maxClusterCount: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance with default parameters: {lambdaValue: 1,\n+   * convergenceTol: 0.01, maxIterations: 20, maxClusterCount: 1000}.\n+   */\n+  def this() = this(1, 0.01, 20, 1000)\n+\n+  /** Sets the value for the lambda parameter, which controls the cluster creation. Default: 1 */\n+  def setLambdaValue(lambdaValue: Double): this.type = {\n+    this.lambdaValue = lambdaValue\n+    this\n+  }\n+\n+  /** Returns the lambda value */\n+  def getLambdaValue: Double = lambdaValue\n+\n+  /** Sets the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Returns the convergence threshold value . */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Sets the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Returns the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /** Sets the maximum number of clusters expected. Default: 1000 */\n+  def setMaxClusterCount(maxClusterCount: Int): this.type = {\n+    this.maxClusterCount = maxClusterCount\n+    this\n+  }\n+\n+  /** Returns the maximum number of clusters expected. */\n+  def getMaxClusterCount: Int = maxClusterCount\n+\n+  /**\n+   * Perform DP-means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP-means algorithm.\n+    var iteration = 0\n+    var previousK = 1\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+\n+      // This loop simulates the cluster assignment step by iterating through the data\n+      // points until no new clusters are generated. A new cluster is formed whenever\n+      // the distance from a data point to all the existing cluster centroids is greater\n+      // than lambdaValue. The cluster centers returned by generateNewCenters(), called\n+      // localCenters, are sent to the master. The cluster creation process is performed\n+      // locally on these localCenters and the resulting cluster centers are added to\n+      // the existing set of global centers.\n+      while (!covered) {\n+        val findLocalCenters = zippedData\n+          .mapPartitions(h =>\n+              DpMeans.generateNewCenters(h, globalCenters, lambdaValue)\n+          ).persist()\n+        val localCentersCount = findLocalCenters.count()\n+        logInfo(\" Local centers count :: \" + localCentersCount)\n+        if (localCentersCount > getMaxClusterCount) {\n+          logInfo(\" Local centers count \" + localCentersCount + \" exceeds user expectation\")\n+          sys.exit()\n+        }\n+        localCenters = findLocalCenters.collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          val newGlobalCenters = DpMeans.generateNewCenters(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambdaValue)\n+          globalCenters ++= newGlobalCenters\n+          logInfo(\" Number of global centers :: \" + globalCenters.length)\n+        }\n+      }\n+\n+      // Find the sum and count of points belonging to each cluster\n+      case class WeightedPoint(vector: Vector, count: Long)\n+      val mergeClusters = (x: WeightedPoint, y: WeightedPoint) => {\n+        axpy(1.0, y.vector, x.vector)\n+        WeightedPoint(x.vector, x.count + y.count)\n+      }\n+\n+      val dims = globalCenters(0).vector.size\n+      val clusterStat = zippedData.mapPartitions { points =>"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "yu-iskw"
    },
    "body": "I think It should throw a `SparkException`, instead of `sys.exit()`.\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-11-02T00:34:04Z",
    "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP-means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"Revisiting k-means: New Algorithms via Bayesian Nonparametrics\"\n+ * by Brian Kulis, Michael I. Jordan. This implementation is based on \"MLbase: Distributed\n+ * Machine Learning Made Easy\" by Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambdaValue distance value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ * // TODO\n+ * @param maxClusterCount The maximum expected number of clusters.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambdaValue: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int,\n+    private var maxClusterCount: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance with default parameters: {lambdaValue: 1,\n+   * convergenceTol: 0.01, maxIterations: 20, maxClusterCount: 1000}.\n+   */\n+  def this() = this(1, 0.01, 20, 1000)\n+\n+  /** Sets the value for the lambda parameter, which controls the cluster creation. Default: 1 */\n+  def setLambdaValue(lambdaValue: Double): this.type = {\n+    this.lambdaValue = lambdaValue\n+    this\n+  }\n+\n+  /** Returns the lambda value */\n+  def getLambdaValue: Double = lambdaValue\n+\n+  /** Sets the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Returns the convergence threshold value . */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Sets the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Returns the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /** Sets the maximum number of clusters expected. Default: 1000 */\n+  def setMaxClusterCount(maxClusterCount: Int): this.type = {\n+    this.maxClusterCount = maxClusterCount\n+    this\n+  }\n+\n+  /** Returns the maximum number of clusters expected. */\n+  def getMaxClusterCount: Int = maxClusterCount\n+\n+  /**\n+   * Perform DP-means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP-means algorithm.\n+    var iteration = 0\n+    var previousK = 1\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+\n+      // This loop simulates the cluster assignment step by iterating through the data\n+      // points until no new clusters are generated. A new cluster is formed whenever\n+      // the distance from a data point to all the existing cluster centroids is greater\n+      // than lambdaValue. The cluster centers returned by generateNewCenters(), called\n+      // localCenters, are sent to the master. The cluster creation process is performed\n+      // locally on these localCenters and the resulting cluster centers are added to\n+      // the existing set of global centers.\n+      while (!covered) {\n+        val findLocalCenters = zippedData\n+          .mapPartitions(h =>\n+              DpMeans.generateNewCenters(h, globalCenters, lambdaValue)\n+          ).persist()\n+        val localCentersCount = findLocalCenters.count()\n+        logInfo(\" Local centers count :: \" + localCentersCount)\n+        if (localCentersCount > getMaxClusterCount) {\n+          logInfo(\" Local centers count \" + localCentersCount + \" exceeds user expectation\")\n+          sys.exit()"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "yu-iskw"
    },
    "body": "need a more descriptive name\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-11-02T00:52:15Z",
    "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP-means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"Revisiting k-means: New Algorithms via Bayesian Nonparametrics\"\n+ * by Brian Kulis, Michael I. Jordan. This implementation is based on \"MLbase: Distributed\n+ * Machine Learning Made Easy\" by Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambdaValue distance value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ * // TODO\n+ * @param maxClusterCount The maximum expected number of clusters.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambdaValue: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int,\n+    private var maxClusterCount: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance with default parameters: {lambdaValue: 1,\n+   * convergenceTol: 0.01, maxIterations: 20, maxClusterCount: 1000}.\n+   */\n+  def this() = this(1, 0.01, 20, 1000)\n+\n+  /** Sets the value for the lambda parameter, which controls the cluster creation. Default: 1 */\n+  def setLambdaValue(lambdaValue: Double): this.type = {\n+    this.lambdaValue = lambdaValue\n+    this\n+  }\n+\n+  /** Returns the lambda value */\n+  def getLambdaValue: Double = lambdaValue\n+\n+  /** Sets the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Returns the convergence threshold value . */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Sets the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Returns the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /** Sets the maximum number of clusters expected. Default: 1000 */\n+  def setMaxClusterCount(maxClusterCount: Int): this.type = {\n+    this.maxClusterCount = maxClusterCount\n+    this\n+  }\n+\n+  /** Returns the maximum number of clusters expected. */\n+  def getMaxClusterCount: Int = maxClusterCount\n+\n+  /**\n+   * Perform DP-means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP-means algorithm.\n+    var iteration = 0\n+    var previousK = 1\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+\n+      // This loop simulates the cluster assignment step by iterating through the data\n+      // points until no new clusters are generated. A new cluster is formed whenever\n+      // the distance from a data point to all the existing cluster centroids is greater\n+      // than lambdaValue. The cluster centers returned by generateNewCenters(), called\n+      // localCenters, are sent to the master. The cluster creation process is performed\n+      // locally on these localCenters and the resulting cluster centers are added to\n+      // the existing set of global centers.\n+      while (!covered) {\n+        val findLocalCenters = zippedData\n+          .mapPartitions(h =>\n+              DpMeans.generateNewCenters(h, globalCenters, lambdaValue)\n+          ).persist()\n+        val localCentersCount = findLocalCenters.count()\n+        logInfo(\" Local centers count :: \" + localCentersCount)\n+        if (localCentersCount > getMaxClusterCount) {\n+          logInfo(\" Local centers count \" + localCentersCount + \" exceeds user expectation\")\n+          sys.exit()\n+        }\n+        localCenters = findLocalCenters.collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          val newGlobalCenters = DpMeans.generateNewCenters(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambdaValue)\n+          globalCenters ++= newGlobalCenters\n+          logInfo(\" Number of global centers :: \" + globalCenters.length)\n+        }\n+      }\n+\n+      // Find the sum and count of points belonging to each cluster\n+      case class WeightedPoint(vector: Vector, count: Long)\n+      val mergeClusters = (x: WeightedPoint, y: WeightedPoint) => {\n+        axpy(1.0, y.vector, x.vector)\n+        WeightedPoint(x.vector, x.count + y.count)\n+      }\n+\n+      val dims = globalCenters(0).vector.size\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) += 1\n+        }\n+        val result = Iterator.tabulate(k) { i => (i, WeightedPoint(sums(i), counts(i))) }\n+        result\n+      }.aggregateByKey(WeightedPoint(Vectors.zeros(dims), 0L))(mergeClusters, mergeClusters)\n+        .collectAsMap()\n+\n+      // Update the cluster centers and convergence check\n+      var changed = false"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "yu-iskw"
    },
    "body": "replace `!points.isEmpty` with `points.nonEmpty`\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-11-02T01:23:49Z",
    "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP-means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"Revisiting k-means: New Algorithms via Bayesian Nonparametrics\"\n+ * by Brian Kulis, Michael I. Jordan. This implementation is based on \"MLbase: Distributed\n+ * Machine Learning Made Easy\" by Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambdaValue distance value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ * // TODO\n+ * @param maxClusterCount The maximum expected number of clusters.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambdaValue: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int,\n+    private var maxClusterCount: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance with default parameters: {lambdaValue: 1,\n+   * convergenceTol: 0.01, maxIterations: 20, maxClusterCount: 1000}.\n+   */\n+  def this() = this(1, 0.01, 20, 1000)\n+\n+  /** Sets the value for the lambda parameter, which controls the cluster creation. Default: 1 */\n+  def setLambdaValue(lambdaValue: Double): this.type = {\n+    this.lambdaValue = lambdaValue\n+    this\n+  }\n+\n+  /** Returns the lambda value */\n+  def getLambdaValue: Double = lambdaValue\n+\n+  /** Sets the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Returns the convergence threshold value . */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Sets the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Returns the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /** Sets the maximum number of clusters expected. Default: 1000 */\n+  def setMaxClusterCount(maxClusterCount: Int): this.type = {\n+    this.maxClusterCount = maxClusterCount\n+    this\n+  }\n+\n+  /** Returns the maximum number of clusters expected. */\n+  def getMaxClusterCount: Int = maxClusterCount\n+\n+  /**\n+   * Perform DP-means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP-means algorithm.\n+    var iteration = 0\n+    var previousK = 1\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+\n+      // This loop simulates the cluster assignment step by iterating through the data\n+      // points until no new clusters are generated. A new cluster is formed whenever\n+      // the distance from a data point to all the existing cluster centroids is greater\n+      // than lambdaValue. The cluster centers returned by generateNewCenters(), called\n+      // localCenters, are sent to the master. The cluster creation process is performed\n+      // locally on these localCenters and the resulting cluster centers are added to\n+      // the existing set of global centers.\n+      while (!covered) {\n+        val findLocalCenters = zippedData\n+          .mapPartitions(h =>\n+              DpMeans.generateNewCenters(h, globalCenters, lambdaValue)\n+          ).persist()\n+        val localCentersCount = findLocalCenters.count()\n+        logInfo(\" Local centers count :: \" + localCentersCount)\n+        if (localCentersCount > getMaxClusterCount) {\n+          logInfo(\" Local centers count \" + localCentersCount + \" exceeds user expectation\")\n+          sys.exit()\n+        }\n+        localCenters = findLocalCenters.collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          val newGlobalCenters = DpMeans.generateNewCenters(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambdaValue)\n+          globalCenters ++= newGlobalCenters\n+          logInfo(\" Number of global centers :: \" + globalCenters.length)\n+        }\n+      }\n+\n+      // Find the sum and count of points belonging to each cluster\n+      case class WeightedPoint(vector: Vector, count: Long)\n+      val mergeClusters = (x: WeightedPoint, y: WeightedPoint) => {\n+        axpy(1.0, y.vector, x.vector)\n+        WeightedPoint(x.vector, x.count + y.count)\n+      }\n+\n+      val dims = globalCenters(0).vector.size\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) += 1\n+        }\n+        val result = Iterator.tabulate(k) { i => (i, WeightedPoint(sums(i), counts(i))) }\n+        result\n+      }.aggregateByKey(WeightedPoint(Vectors.zeros(dims), 0L))(mergeClusters, mergeClusters)\n+        .collectAsMap()\n+\n+      // Update the cluster centers and convergence check\n+      var changed = false\n+      var j = 0\n+      val currentK = clusterStat.size\n+\n+      while (j < currentK) {\n+        val (sumOfPoints, count) = (clusterStat(j).vector, clusterStat(j).count)\n+        if (count != 0) {\n+          scal(1.0 / count, sumOfPoints)\n+          val newCenter = new VectorWithNorm(sumOfPoints)\n+          // Check for convergence\n+          if (previousK == currentK) {\n+            if (DpMeans.squaredDistance(newCenter, globalCenters(j))\n+              > convergenceTol * convergenceTol) {\n+              changed = true\n+            }\n+          }\n+          else {\n+            changed = true\n+          }\n+          globalCenters(j) = newCenter\n+        }\n+        previousK = currentK\n+        j += 1\n+      }\n+      if (!changed) {\n+        converged = true\n+        logInfo(\"DpMeans clustering finished in \" + (iteration + 1) + \" iterations\")\n+      }\n+      previousK = currentK\n+      iteration += 1\n+    }\n+\n+    if (iteration == maxIterations) {\n+      logInfo(s\"DPMeans reached the max number of iterations: $maxIterations.\")\n+    } else {\n+      logInfo(s\"DPMeans converged in $iteration iterations\")\n+    }\n+    new DpMeansModel(globalCenters.toArray.map(_.vector), lambdaValue)\n+  }\n+}\n+\n+/**\n+ * Core methods of DP-means clustering.\n+ */\n+private object DpMeans {\n+\n+  /**\n+   * A new cluster is formed whenever the distance from a data point to\n+   * all the existing cluster centroids is greater than lambdaValue.\n+   * @param points an iterator to the input data points\n+   * @param centers current centers\n+   * @param lambdaValue threshold value which decides the cluster creation\n+   * @return an iterator to the computed new centers\n+   */\n+  def generateNewCenters(\n+      points: Iterator[VectorWithNorm],\n+      centers: ArrayBuffer[VectorWithNorm],\n+      lambdaValue: Double): Iterator[VectorWithNorm] = {\n+    var newCenters = ArrayBuffer.empty[VectorWithNorm]\n+    if (!points.isEmpty) {"
  }],
  "prId": 6880
}, {
  "comments": [{
    "author": {
      "login": "yu-iskw"
    },
    "body": "replace `centers.length == 0` with `centers.isEmpty`\n",
    "commit": "c25eae2eacacf867c666d730e05bc6daa3fe7a78",
    "createdAt": "2015-11-02T01:24:27Z",
    "diffHunk": "@@ -0,0 +1,279 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.mllib.clustering\n+\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS.{axpy, scal}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * The Dirichlet process (DP) is a popular non-parametric Bayesian mixture\n+ * model that allows for flexible clustering of data without having to\n+ * determine the number of clusters in advance.\n+ *\n+ * Given a set of data points, this class performs cluster creation process,\n+ * based on DP-means algorithm, iterating until the maximum number of iterations\n+ * is reached or the convergence criteria is satisfied. With the current\n+ * global set of centers, it locally creates a new cluster centered at `x`\n+ * whenever it encounters an uncovered data point `x`. In a similar manner,\n+ * a local cluster center is promoted to a global center whenever an uncovered\n+ * local cluster center is found. A data point is said to be \"covered\" by\n+ * a cluster `c` if the distance from the point to the cluster center of `c`\n+ * is less than a given lambda value.\n+ *\n+ * The original paper is \"Revisiting k-means: New Algorithms via Bayesian Nonparametrics\"\n+ * by Brian Kulis, Michael I. Jordan. This implementation is based on \"MLbase: Distributed\n+ * Machine Learning Made Easy\" by Xinghao Pan, Evan R. Sparks, Andre Wibisono\n+ *\n+ * @param lambdaValue distance value that controls cluster creation.\n+ * @param convergenceTol The threshold value at which convergence is considered to have occurred.\n+ * @param maxIterations The maximum number of iterations to perform.\n+ * // TODO\n+ * @param maxClusterCount The maximum expected number of clusters.\n+ */\n+\n+@Experimental\n+class DpMeans private (\n+    private var lambdaValue: Double,\n+    private var convergenceTol: Double,\n+    private var maxIterations: Int,\n+    private var maxClusterCount: Int) extends Serializable with Logging {\n+\n+  /**\n+   * Constructs a default instance with default parameters: {lambdaValue: 1,\n+   * convergenceTol: 0.01, maxIterations: 20, maxClusterCount: 1000}.\n+   */\n+  def this() = this(1, 0.01, 20, 1000)\n+\n+  /** Sets the value for the lambda parameter, which controls the cluster creation. Default: 1 */\n+  def setLambdaValue(lambdaValue: Double): this.type = {\n+    this.lambdaValue = lambdaValue\n+    this\n+  }\n+\n+  /** Returns the lambda value */\n+  def getLambdaValue: Double = lambdaValue\n+\n+  /** Sets the threshold value at which convergence is considered to have occurred. Default: 0.01 */\n+  def setConvergenceTol(convergenceTol: Double): this.type = {\n+    this.convergenceTol = convergenceTol\n+    this\n+  }\n+\n+  /** Returns the convergence threshold value . */\n+  def getConvergenceTol: Double = convergenceTol\n+\n+  /** Sets the maximum number of iterations. Default: 20 */\n+  def setMaxIterations(maxIterations: Int): this.type = {\n+    this.maxIterations = maxIterations\n+    this\n+  }\n+\n+  /** Returns the maximum number of iterations. */\n+  def getMaxIterations: Int = maxIterations\n+\n+  /** Sets the maximum number of clusters expected. Default: 1000 */\n+  def setMaxClusterCount(maxClusterCount: Int): this.type = {\n+    this.maxClusterCount = maxClusterCount\n+    this\n+  }\n+\n+  /** Returns the maximum number of clusters expected. */\n+  def getMaxClusterCount: Int = maxClusterCount\n+\n+  /**\n+   * Perform DP-means clustering\n+   */\n+  def run(data: RDD[Vector]): DpMeansModel = {\n+    if (data.getStorageLevel == StorageLevel.NONE) {\n+      logWarning(\"The input data is not directly cached, which may hurt performance if its\"\n+        + \" parent RDDs are also uncached.\")\n+    }\n+\n+    // Compute squared norms and cache them.\n+    val norms = data.map(Vectors.norm(_, 2.0))\n+    norms.persist()\n+    val zippedData = data.zip(norms).map {\n+      case (v, norm) => new VectorWithNorm(v, norm)\n+    }\n+\n+    // Implementation of DP-means algorithm.\n+    var iteration = 0\n+    var previousK = 1\n+    var covered = false\n+    var converged = false\n+    var localCenters = Array.empty[VectorWithNorm]\n+    val globalCenters = ArrayBuffer.empty[VectorWithNorm]\n+\n+    // Execute clustering until the maximum number of iterations is reached\n+    // or the cluster centers have converged.\n+    while (iteration < maxIterations && !converged) {\n+\n+      // This loop simulates the cluster assignment step by iterating through the data\n+      // points until no new clusters are generated. A new cluster is formed whenever\n+      // the distance from a data point to all the existing cluster centroids is greater\n+      // than lambdaValue. The cluster centers returned by generateNewCenters(), called\n+      // localCenters, are sent to the master. The cluster creation process is performed\n+      // locally on these localCenters and the resulting cluster centers are added to\n+      // the existing set of global centers.\n+      while (!covered) {\n+        val findLocalCenters = zippedData\n+          .mapPartitions(h =>\n+              DpMeans.generateNewCenters(h, globalCenters, lambdaValue)\n+          ).persist()\n+        val localCentersCount = findLocalCenters.count()\n+        logInfo(\" Local centers count :: \" + localCentersCount)\n+        if (localCentersCount > getMaxClusterCount) {\n+          logInfo(\" Local centers count \" + localCentersCount + \" exceeds user expectation\")\n+          sys.exit()\n+        }\n+        localCenters = findLocalCenters.collect()\n+        if (localCenters.isEmpty) {\n+          covered = true\n+        }\n+        // Promote a local cluster center to a global center\n+        else {\n+          val newGlobalCenters = DpMeans.generateNewCenters(localCenters.iterator,\n+            ArrayBuffer.empty[VectorWithNorm], lambdaValue)\n+          globalCenters ++= newGlobalCenters\n+          logInfo(\" Number of global centers :: \" + globalCenters.length)\n+        }\n+      }\n+\n+      // Find the sum and count of points belonging to each cluster\n+      case class WeightedPoint(vector: Vector, count: Long)\n+      val mergeClusters = (x: WeightedPoint, y: WeightedPoint) => {\n+        axpy(1.0, y.vector, x.vector)\n+        WeightedPoint(x.vector, x.count + y.count)\n+      }\n+\n+      val dims = globalCenters(0).vector.size\n+      val clusterStat = zippedData.mapPartitions { points =>\n+        val activeCenters = globalCenters\n+        val k = activeCenters.length\n+\n+        val sums = Array.fill(k)(Vectors.zeros(dims))\n+        val counts = Array.fill(k)(0L)\n+        val totalCost = Array.fill(k)(0.0D)\n+        points.foreach { point =>\n+          val (currentCenter, cost) = DpMeans.assignCluster(activeCenters, point)\n+          totalCost(currentCenter) += cost\n+          val currentSum = sums(currentCenter)\n+          axpy(1.0, point.vector, currentSum)\n+          counts(currentCenter) += 1\n+        }\n+        val result = Iterator.tabulate(k) { i => (i, WeightedPoint(sums(i), counts(i))) }\n+        result\n+      }.aggregateByKey(WeightedPoint(Vectors.zeros(dims), 0L))(mergeClusters, mergeClusters)\n+        .collectAsMap()\n+\n+      // Update the cluster centers and convergence check\n+      var changed = false\n+      var j = 0\n+      val currentK = clusterStat.size\n+\n+      while (j < currentK) {\n+        val (sumOfPoints, count) = (clusterStat(j).vector, clusterStat(j).count)\n+        if (count != 0) {\n+          scal(1.0 / count, sumOfPoints)\n+          val newCenter = new VectorWithNorm(sumOfPoints)\n+          // Check for convergence\n+          if (previousK == currentK) {\n+            if (DpMeans.squaredDistance(newCenter, globalCenters(j))\n+              > convergenceTol * convergenceTol) {\n+              changed = true\n+            }\n+          }\n+          else {\n+            changed = true\n+          }\n+          globalCenters(j) = newCenter\n+        }\n+        previousK = currentK\n+        j += 1\n+      }\n+      if (!changed) {\n+        converged = true\n+        logInfo(\"DpMeans clustering finished in \" + (iteration + 1) + \" iterations\")\n+      }\n+      previousK = currentK\n+      iteration += 1\n+    }\n+\n+    if (iteration == maxIterations) {\n+      logInfo(s\"DPMeans reached the max number of iterations: $maxIterations.\")\n+    } else {\n+      logInfo(s\"DPMeans converged in $iteration iterations\")\n+    }\n+    new DpMeansModel(globalCenters.toArray.map(_.vector), lambdaValue)\n+  }\n+}\n+\n+/**\n+ * Core methods of DP-means clustering.\n+ */\n+private object DpMeans {\n+\n+  /**\n+   * A new cluster is formed whenever the distance from a data point to\n+   * all the existing cluster centroids is greater than lambdaValue.\n+   * @param points an iterator to the input data points\n+   * @param centers current centers\n+   * @param lambdaValue threshold value which decides the cluster creation\n+   * @return an iterator to the computed new centers\n+   */\n+  def generateNewCenters(\n+      points: Iterator[VectorWithNorm],\n+      centers: ArrayBuffer[VectorWithNorm],\n+      lambdaValue: Double): Iterator[VectorWithNorm] = {\n+    var newCenters = ArrayBuffer.empty[VectorWithNorm]\n+    if (!points.isEmpty) {\n+      if (centers.length == 0) newCenters += points.next"
  }],
  "prId": 6880
}]