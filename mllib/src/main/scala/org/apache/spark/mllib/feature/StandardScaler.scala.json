[{
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "nit: this can go on one line",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-20T10:16:30Z",
    "diffHunk": "@@ -134,77 +136,49 @@ class StandardScalerModel @Since(\"1.3.0\") (\n   @Since(\"1.1.0\")\n   override def transform(vector: Vector): Vector = {\n     require(mean.size == vector.size)\n-    if (withMean) {\n-      // Must have a copy of the values since it will be modified in place\n-      val values = vector match {\n-        // specially handle DenseVector because its toArray does not clone already\n-        case d: DenseVector => d.values.clone()\n-        case v: Vector => v.toArray\n-      }\n-      val newValues = transformWithMean(values)\n-      Vectors.dense(newValues)\n-    } else if (withStd) {\n-      vector match {\n-        case DenseVector(values) =>\n-          val newValues = transformDenseWithStd(values)\n-          Vectors.dense(newValues)\n-        case SparseVector(size, indices, values) =>\n-          val (newIndices, newValues) = transformSparseWithStd(indices, values)\n-          Vectors.sparse(size, newIndices, newValues)\n-        case other =>\n-          throw new UnsupportedOperationException(\n-            s\"Only sparse and dense vectors are supported but got ${other.getClass}.\")\n-      }\n-    } else {\n-      // Note that it's safe since we always assume that the data in RDD should be immutable.\n-      vector\n-    }\n-  }\n-\n-  private[spark] def transformWithMean(values: Array[Double]): Array[Double] = {\n-    // By default, Scala generates Java methods for member variables. So every time when\n-    // the member variables are accessed, `invokespecial` will be called which is expensive.\n-    // This can be avoid by having a local reference of `shift`.\n-    val localShift = shift\n-    val size = values.length\n-    if (withStd) {\n-      var i = 0\n-      while (i < size) {\n-        values(i) = if (std(i) != 0.0) (values(i) - localShift(i)) * (1.0 / std(i)) else 0.0\n-        i += 1\n-      }\n-    } else {\n-      var i = 0\n-      while (i < size) {\n-        values(i) -= localShift(i)\n-        i += 1\n-      }\n-    }\n-    values\n-  }\n-\n-  private[spark] def transformDenseWithStd(values: Array[Double]): Array[Double] = {\n-    val size = values.length\n-    val newValues = values.clone()\n-    var i = 0\n-    while(i < size) {\n-      newValues(i) *= (if (std(i) != 0.0) 1.0 / std(i) else 0.0)\n-      i += 1\n-    }\n-    newValues\n-  }\n \n-  private[spark] def transformSparseWithStd(indices: Array[Int],\n-                                            values: Array[Double]): (Array[Int], Array[Double]) = {\n-    // For sparse vector, the `index` array inside sparse vector object will not be changed,\n-    // so we can re-use it to save memory.\n-    val nnz = values.length\n-    val newValues = values.clone()\n-    var i = 0\n-    while (i < nnz) {\n-      newValues(i) *= (if (std(indices(i)) != 0.0) 1.0 / std(indices(i)) else 0.0)\n-      i += 1\n+    (withMean, withStd) match {\n+      case (true, true) =>\n+        // By default, Scala generates Java methods for member variables. So every time when\n+        // the member variables are accessed, `invokespecial` will be called which is expensive.\n+        // This can be avoid by having a local reference of `shift`.\n+        val localShift = shift\n+        val localScale = scale\n+        val values = vector match {\n+          // specially handle DenseVector because its toArray does not clone already\n+          case d: DenseVector => d.values.clone()\n+          case v: Vector => v.toArray\n+        }\n+        val newValues = NewStandardScalerModel\n+          .transformWithBoth(localShift, localScale, values)\n+        Vectors.dense(newValues)\n+\n+      case (true, false) =>\n+        val localShift = shift\n+        val values = vector match {\n+          case d: DenseVector => d.values.clone()\n+          case v: Vector => v.toArray\n+        }\n+        val newValues = NewStandardScalerModel",
    "line": 114
  }],
  "prId": 25160
}]