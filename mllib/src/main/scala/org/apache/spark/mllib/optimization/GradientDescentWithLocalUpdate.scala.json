[{
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Move the right brace to the end of the last line\n",
    "commit": "5f8220a343a6ebbeff907745c6b29f64c3610620",
    "createdAt": "2014-03-18T09:51:48Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+import org.jblas.DoubleMatrix\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.Random\n+\n+/**\n+ * Class used to solve an optimization problem using Gradient Descent.\n+ * @param gradient Gradient function to be used.\n+ * @param updater Updater to be used to update weights after every iteration.\n+ */\n+class GradientDescentWithLocalUpdate(gradient: Gradient, updater: Updater)\n+  extends GradientDescent(gradient, updater) with Logging\n+{"
  }],
  "prId": 166
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Please reorder the imports according to [Spark coding convention](https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide#SparkCodeStyleGuide-Imports).\n",
    "commit": "5f8220a343a6ebbeff907745c6b29f64c3610620",
    "createdAt": "2014-03-18T09:53:57Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+import org.jblas.DoubleMatrix\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.Random"
  }],
  "prId": 166
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Hmm... since BSP+ is not a well known concept and there's no related references (yet), maybe we should not use this term? Any suggestions @mengxr?\n",
    "commit": "5f8220a343a6ebbeff907745c6b29f64c3610620",
    "createdAt": "2014-03-18T09:56:55Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+import org.jblas.DoubleMatrix\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.Random\n+\n+/**\n+ * Class used to solve an optimization problem using Gradient Descent.\n+ * @param gradient Gradient function to be used.\n+ * @param updater Updater to be used to update weights after every iteration.\n+ */\n+class GradientDescentWithLocalUpdate(gradient: Gradient, updater: Updater)\n+  extends GradientDescent(gradient, updater) with Logging\n+{\n+  private var numLocalIterations: Int = 1\n+\n+  /**\n+   * Set the number of local iterations. Default 1.\n+   */\n+  def setNumLocalIterations(numLocalIter: Int): this.type = {\n+    this.numLocalIterations = numLocalIter\n+    this\n+  }\n+\n+  override def optimize(data: RDD[(Double, Array[Double])], initialWeights: Array[Double])\n+    : Array[Double] = {\n+\n+    val (weights, stochasticLossHistory) = GradientDescentWithLocalUpdate.runMiniBatchSGD(\n+        data,\n+        gradient,\n+        updater,\n+        stepSize,\n+        numIterations,\n+        numLocalIterations,\n+        regParam,\n+        miniBatchFraction,\n+        initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+// Top-level method to run gradient descent.\n+object GradientDescentWithLocalUpdate extends Logging {\n+   /**\n+   * Run BSP+ gradient descent in parallel using mini batches."
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "I agree. We need a reference here or at least explain `BSP+`.\n",
    "commit": "5f8220a343a6ebbeff907745c6b29f64c3610620",
    "createdAt": "2014-03-18T22:23:04Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+import org.jblas.DoubleMatrix\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.Random\n+\n+/**\n+ * Class used to solve an optimization problem using Gradient Descent.\n+ * @param gradient Gradient function to be used.\n+ * @param updater Updater to be used to update weights after every iteration.\n+ */\n+class GradientDescentWithLocalUpdate(gradient: Gradient, updater: Updater)\n+  extends GradientDescent(gradient, updater) with Logging\n+{\n+  private var numLocalIterations: Int = 1\n+\n+  /**\n+   * Set the number of local iterations. Default 1.\n+   */\n+  def setNumLocalIterations(numLocalIter: Int): this.type = {\n+    this.numLocalIterations = numLocalIter\n+    this\n+  }\n+\n+  override def optimize(data: RDD[(Double, Array[Double])], initialWeights: Array[Double])\n+    : Array[Double] = {\n+\n+    val (weights, stochasticLossHistory) = GradientDescentWithLocalUpdate.runMiniBatchSGD(\n+        data,\n+        gradient,\n+        updater,\n+        stepSize,\n+        numIterations,\n+        numLocalIterations,\n+        regParam,\n+        miniBatchFraction,\n+        initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+// Top-level method to run gradient descent.\n+object GradientDescentWithLocalUpdate extends Logging {\n+   /**\n+   * Run BSP+ gradient descent in parallel using mini batches."
  }],
  "prId": 166
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Remove the space before the colon.\n",
    "commit": "5f8220a343a6ebbeff907745c6b29f64c3610620",
    "createdAt": "2014-03-18T09:57:31Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+import org.jblas.DoubleMatrix\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.Random\n+\n+/**\n+ * Class used to solve an optimization problem using Gradient Descent.\n+ * @param gradient Gradient function to be used.\n+ * @param updater Updater to be used to update weights after every iteration.\n+ */\n+class GradientDescentWithLocalUpdate(gradient: Gradient, updater: Updater)\n+  extends GradientDescent(gradient, updater) with Logging\n+{\n+  private var numLocalIterations: Int = 1\n+\n+  /**\n+   * Set the number of local iterations. Default 1.\n+   */\n+  def setNumLocalIterations(numLocalIter: Int): this.type = {\n+    this.numLocalIterations = numLocalIter\n+    this\n+  }\n+\n+  override def optimize(data: RDD[(Double, Array[Double])], initialWeights: Array[Double])\n+    : Array[Double] = {\n+\n+    val (weights, stochasticLossHistory) = GradientDescentWithLocalUpdate.runMiniBatchSGD(\n+        data,\n+        gradient,\n+        updater,\n+        stepSize,\n+        numIterations,\n+        numLocalIterations,\n+        regParam,\n+        miniBatchFraction,\n+        initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+// Top-level method to run gradient descent.\n+object GradientDescentWithLocalUpdate extends Logging {\n+   /**\n+   * Run BSP+ gradient descent in parallel using mini batches.\n+   *\n+   * @param data - Input data for SGD. RDD of form (label, [feature values]).\n+   * @param gradient - Gradient object that will be used to compute the gradient.\n+   * @param updater - Updater object that will be used to update the model.\n+   * @param stepSize - stepSize to be used during update.\n+   * @param numOuterIterations - number of outer iterations that SGD should be run.\n+   * @param numInnerIterations - number of inner iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the stochastic\n+   *         loss computed for every iteration.\n+   */\n+  def runMiniBatchSGD(\n+      data: RDD[(Double, Array[Double])],\n+      gradient: Gradient,\n+      updater: Updater,\n+      stepSize: Double,\n+      numOuterIterations: Int,\n+      numInnerIterations: Int,\n+      regParam: Double,\n+      miniBatchFraction: Double,\n+      initialWeights: Array[Double]) : (Array[Double], Array[Double]) = {"
  }],
  "prId": 166
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "We can simply return `gradient.compute(...)` here without introducing `grad` and `loss`.\n",
    "commit": "5f8220a343a6ebbeff907745c6b29f64c3610620",
    "createdAt": "2014-03-18T10:09:36Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+import org.jblas.DoubleMatrix\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.Random\n+\n+/**\n+ * Class used to solve an optimization problem using Gradient Descent.\n+ * @param gradient Gradient function to be used.\n+ * @param updater Updater to be used to update weights after every iteration.\n+ */\n+class GradientDescentWithLocalUpdate(gradient: Gradient, updater: Updater)\n+  extends GradientDescent(gradient, updater) with Logging\n+{\n+  private var numLocalIterations: Int = 1\n+\n+  /**\n+   * Set the number of local iterations. Default 1.\n+   */\n+  def setNumLocalIterations(numLocalIter: Int): this.type = {\n+    this.numLocalIterations = numLocalIter\n+    this\n+  }\n+\n+  override def optimize(data: RDD[(Double, Array[Double])], initialWeights: Array[Double])\n+    : Array[Double] = {\n+\n+    val (weights, stochasticLossHistory) = GradientDescentWithLocalUpdate.runMiniBatchSGD(\n+        data,\n+        gradient,\n+        updater,\n+        stepSize,\n+        numIterations,\n+        numLocalIterations,\n+        regParam,\n+        miniBatchFraction,\n+        initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+// Top-level method to run gradient descent.\n+object GradientDescentWithLocalUpdate extends Logging {\n+   /**\n+   * Run BSP+ gradient descent in parallel using mini batches.\n+   *\n+   * @param data - Input data for SGD. RDD of form (label, [feature values]).\n+   * @param gradient - Gradient object that will be used to compute the gradient.\n+   * @param updater - Updater object that will be used to update the model.\n+   * @param stepSize - stepSize to be used during update.\n+   * @param numOuterIterations - number of outer iterations that SGD should be run.\n+   * @param numInnerIterations - number of inner iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the stochastic\n+   *         loss computed for every iteration.\n+   */\n+  def runMiniBatchSGD(\n+      data: RDD[(Double, Array[Double])],\n+      gradient: Gradient,\n+      updater: Updater,\n+      stepSize: Double,\n+      numOuterIterations: Int,\n+      numInnerIterations: Int,\n+      regParam: Double,\n+      miniBatchFraction: Double,\n+      initialWeights: Array[Double]) : (Array[Double], Array[Double]) = {\n+\n+    val stochasticLossHistory = new ArrayBuffer[Double](numOuterIterations)\n+\n+    val numExamples: Long = data.count()\n+    val numPartition = data.partitions.length\n+    val miniBatchSize = numExamples * miniBatchFraction / numPartition\n+\n+    // Initialize weights as a column vector\n+    var weights = new DoubleMatrix(initialWeights.length, 1, initialWeights: _*)\n+    var regVal = 0.0\n+\n+    for (i <- 1 to numOuterIterations) {\n+      val weightsAndLosses = data.mapPartitions { iter =>\n+        var iterReserved = iter\n+        val localLossHistory = new ArrayBuffer[Double](numInnerIterations)\n+\n+        for (j <- 1 to numInnerIterations) {\n+          val (iterCurrent, iterNext) = iterReserved.duplicate\n+          val rand = new Random(42 + i * numOuterIterations + j)\n+          val sampled = iterCurrent.filter(x => rand.nextDouble() <= miniBatchFraction)\n+          val (gradientSum, lossSum) = sampled.map { case (y, features) =>\n+            val featuresCol = new DoubleMatrix(features.length, 1, features: _*)\n+            val (grad, loss) = gradient.compute(featuresCol, y, weights)\n+            (grad, loss)"
  }],
  "prId": 166
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "An edge case: do we need to consider empty partition here (`reduce` complains about empty collection)? We can use `reduceOption` together with default values for `gradientSum` and `lossSum`.\n",
    "commit": "5f8220a343a6ebbeff907745c6b29f64c3610620",
    "createdAt": "2014-03-18T10:13:32Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+import org.jblas.DoubleMatrix\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.Random\n+\n+/**\n+ * Class used to solve an optimization problem using Gradient Descent.\n+ * @param gradient Gradient function to be used.\n+ * @param updater Updater to be used to update weights after every iteration.\n+ */\n+class GradientDescentWithLocalUpdate(gradient: Gradient, updater: Updater)\n+  extends GradientDescent(gradient, updater) with Logging\n+{\n+  private var numLocalIterations: Int = 1\n+\n+  /**\n+   * Set the number of local iterations. Default 1.\n+   */\n+  def setNumLocalIterations(numLocalIter: Int): this.type = {\n+    this.numLocalIterations = numLocalIter\n+    this\n+  }\n+\n+  override def optimize(data: RDD[(Double, Array[Double])], initialWeights: Array[Double])\n+    : Array[Double] = {\n+\n+    val (weights, stochasticLossHistory) = GradientDescentWithLocalUpdate.runMiniBatchSGD(\n+        data,\n+        gradient,\n+        updater,\n+        stepSize,\n+        numIterations,\n+        numLocalIterations,\n+        regParam,\n+        miniBatchFraction,\n+        initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+// Top-level method to run gradient descent.\n+object GradientDescentWithLocalUpdate extends Logging {\n+   /**\n+   * Run BSP+ gradient descent in parallel using mini batches.\n+   *\n+   * @param data - Input data for SGD. RDD of form (label, [feature values]).\n+   * @param gradient - Gradient object that will be used to compute the gradient.\n+   * @param updater - Updater object that will be used to update the model.\n+   * @param stepSize - stepSize to be used during update.\n+   * @param numOuterIterations - number of outer iterations that SGD should be run.\n+   * @param numInnerIterations - number of inner iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the stochastic\n+   *         loss computed for every iteration.\n+   */\n+  def runMiniBatchSGD(\n+      data: RDD[(Double, Array[Double])],\n+      gradient: Gradient,\n+      updater: Updater,\n+      stepSize: Double,\n+      numOuterIterations: Int,\n+      numInnerIterations: Int,\n+      regParam: Double,\n+      miniBatchFraction: Double,\n+      initialWeights: Array[Double]) : (Array[Double], Array[Double]) = {\n+\n+    val stochasticLossHistory = new ArrayBuffer[Double](numOuterIterations)\n+\n+    val numExamples: Long = data.count()\n+    val numPartition = data.partitions.length\n+    val miniBatchSize = numExamples * miniBatchFraction / numPartition\n+\n+    // Initialize weights as a column vector\n+    var weights = new DoubleMatrix(initialWeights.length, 1, initialWeights: _*)\n+    var regVal = 0.0\n+\n+    for (i <- 1 to numOuterIterations) {\n+      val weightsAndLosses = data.mapPartitions { iter =>\n+        var iterReserved = iter\n+        val localLossHistory = new ArrayBuffer[Double](numInnerIterations)\n+\n+        for (j <- 1 to numInnerIterations) {\n+          val (iterCurrent, iterNext) = iterReserved.duplicate\n+          val rand = new Random(42 + i * numOuterIterations + j)\n+          val sampled = iterCurrent.filter(x => rand.nextDouble() <= miniBatchFraction)\n+          val (gradientSum, lossSum) = sampled.map { case (y, features) =>\n+            val featuresCol = new DoubleMatrix(features.length, 1, features: _*)\n+            val (grad, loss) = gradient.compute(featuresCol, y, weights)\n+            (grad, loss)\n+          }.reduce((a, b) => (a._1.addi(b._1), a._2 + b._2))"
  }, {
    "author": {
      "login": "yinxusen"
    },
    "body": "Hmm... Sounds good. I should take care of it, especially when the data in each partition is little and the `miniBatchFraction` is low, a blank partition could occur.\n",
    "commit": "5f8220a343a6ebbeff907745c6b29f64c3610620",
    "createdAt": "2014-03-19T02:13:17Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+import org.jblas.DoubleMatrix\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.Random\n+\n+/**\n+ * Class used to solve an optimization problem using Gradient Descent.\n+ * @param gradient Gradient function to be used.\n+ * @param updater Updater to be used to update weights after every iteration.\n+ */\n+class GradientDescentWithLocalUpdate(gradient: Gradient, updater: Updater)\n+  extends GradientDescent(gradient, updater) with Logging\n+{\n+  private var numLocalIterations: Int = 1\n+\n+  /**\n+   * Set the number of local iterations. Default 1.\n+   */\n+  def setNumLocalIterations(numLocalIter: Int): this.type = {\n+    this.numLocalIterations = numLocalIter\n+    this\n+  }\n+\n+  override def optimize(data: RDD[(Double, Array[Double])], initialWeights: Array[Double])\n+    : Array[Double] = {\n+\n+    val (weights, stochasticLossHistory) = GradientDescentWithLocalUpdate.runMiniBatchSGD(\n+        data,\n+        gradient,\n+        updater,\n+        stepSize,\n+        numIterations,\n+        numLocalIterations,\n+        regParam,\n+        miniBatchFraction,\n+        initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+// Top-level method to run gradient descent.\n+object GradientDescentWithLocalUpdate extends Logging {\n+   /**\n+   * Run BSP+ gradient descent in parallel using mini batches.\n+   *\n+   * @param data - Input data for SGD. RDD of form (label, [feature values]).\n+   * @param gradient - Gradient object that will be used to compute the gradient.\n+   * @param updater - Updater object that will be used to update the model.\n+   * @param stepSize - stepSize to be used during update.\n+   * @param numOuterIterations - number of outer iterations that SGD should be run.\n+   * @param numInnerIterations - number of inner iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the stochastic\n+   *         loss computed for every iteration.\n+   */\n+  def runMiniBatchSGD(\n+      data: RDD[(Double, Array[Double])],\n+      gradient: Gradient,\n+      updater: Updater,\n+      stepSize: Double,\n+      numOuterIterations: Int,\n+      numInnerIterations: Int,\n+      regParam: Double,\n+      miniBatchFraction: Double,\n+      initialWeights: Array[Double]) : (Array[Double], Array[Double]) = {\n+\n+    val stochasticLossHistory = new ArrayBuffer[Double](numOuterIterations)\n+\n+    val numExamples: Long = data.count()\n+    val numPartition = data.partitions.length\n+    val miniBatchSize = numExamples * miniBatchFraction / numPartition\n+\n+    // Initialize weights as a column vector\n+    var weights = new DoubleMatrix(initialWeights.length, 1, initialWeights: _*)\n+    var regVal = 0.0\n+\n+    for (i <- 1 to numOuterIterations) {\n+      val weightsAndLosses = data.mapPartitions { iter =>\n+        var iterReserved = iter\n+        val localLossHistory = new ArrayBuffer[Double](numInnerIterations)\n+\n+        for (j <- 1 to numInnerIterations) {\n+          val (iterCurrent, iterNext) = iterReserved.duplicate\n+          val rand = new Random(42 + i * numOuterIterations + j)\n+          val sampled = iterCurrent.filter(x => rand.nextDouble() <= miniBatchFraction)\n+          val (gradientSum, lossSum) = sampled.map { case (y, features) =>\n+            val featuresCol = new DoubleMatrix(features.length, 1, features: _*)\n+            val (grad, loss) = gradient.compute(featuresCol, y, weights)\n+            (grad, loss)\n+          }.reduce((a, b) => (a._1.addi(b._1), a._2 + b._2))"
  }],
  "prId": 166
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "How about changing 10 to \"a few\"? Length of `stochasticLossHistory` may be less than 10 :)\n",
    "commit": "5f8220a343a6ebbeff907745c6b29f64c3610620",
    "createdAt": "2014-03-18T10:19:57Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+import org.jblas.DoubleMatrix\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.Random\n+\n+/**\n+ * Class used to solve an optimization problem using Gradient Descent.\n+ * @param gradient Gradient function to be used.\n+ * @param updater Updater to be used to update weights after every iteration.\n+ */\n+class GradientDescentWithLocalUpdate(gradient: Gradient, updater: Updater)\n+  extends GradientDescent(gradient, updater) with Logging\n+{\n+  private var numLocalIterations: Int = 1\n+\n+  /**\n+   * Set the number of local iterations. Default 1.\n+   */\n+  def setNumLocalIterations(numLocalIter: Int): this.type = {\n+    this.numLocalIterations = numLocalIter\n+    this\n+  }\n+\n+  override def optimize(data: RDD[(Double, Array[Double])], initialWeights: Array[Double])\n+    : Array[Double] = {\n+\n+    val (weights, stochasticLossHistory) = GradientDescentWithLocalUpdate.runMiniBatchSGD(\n+        data,\n+        gradient,\n+        updater,\n+        stepSize,\n+        numIterations,\n+        numLocalIterations,\n+        regParam,\n+        miniBatchFraction,\n+        initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+// Top-level method to run gradient descent.\n+object GradientDescentWithLocalUpdate extends Logging {\n+   /**\n+   * Run BSP+ gradient descent in parallel using mini batches.\n+   *\n+   * @param data - Input data for SGD. RDD of form (label, [feature values]).\n+   * @param gradient - Gradient object that will be used to compute the gradient.\n+   * @param updater - Updater object that will be used to update the model.\n+   * @param stepSize - stepSize to be used during update.\n+   * @param numOuterIterations - number of outer iterations that SGD should be run.\n+   * @param numInnerIterations - number of inner iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the stochastic\n+   *         loss computed for every iteration.\n+   */\n+  def runMiniBatchSGD(\n+      data: RDD[(Double, Array[Double])],\n+      gradient: Gradient,\n+      updater: Updater,\n+      stepSize: Double,\n+      numOuterIterations: Int,\n+      numInnerIterations: Int,\n+      regParam: Double,\n+      miniBatchFraction: Double,\n+      initialWeights: Array[Double]) : (Array[Double], Array[Double]) = {\n+\n+    val stochasticLossHistory = new ArrayBuffer[Double](numOuterIterations)\n+\n+    val numExamples: Long = data.count()\n+    val numPartition = data.partitions.length\n+    val miniBatchSize = numExamples * miniBatchFraction / numPartition\n+\n+    // Initialize weights as a column vector\n+    var weights = new DoubleMatrix(initialWeights.length, 1, initialWeights: _*)\n+    var regVal = 0.0\n+\n+    for (i <- 1 to numOuterIterations) {\n+      val weightsAndLosses = data.mapPartitions { iter =>\n+        var iterReserved = iter\n+        val localLossHistory = new ArrayBuffer[Double](numInnerIterations)\n+\n+        for (j <- 1 to numInnerIterations) {\n+          val (iterCurrent, iterNext) = iterReserved.duplicate\n+          val rand = new Random(42 + i * numOuterIterations + j)\n+          val sampled = iterCurrent.filter(x => rand.nextDouble() <= miniBatchFraction)\n+          val (gradientSum, lossSum) = sampled.map { case (y, features) =>\n+            val featuresCol = new DoubleMatrix(features.length, 1, features: _*)\n+            val (grad, loss) = gradient.compute(featuresCol, y, weights)\n+            (grad, loss)\n+          }.reduce((a, b) => (a._1.addi(b._1), a._2 + b._2))\n+\n+          localLossHistory += lossSum / miniBatchSize + regVal\n+\n+          val update = updater.compute(weights, gradientSum.div(miniBatchSize),\n+            stepSize, (i - 1) + numOuterIterations + j, regParam)\n+\n+          weights = update._1\n+          regVal = update._2\n+\n+          iterReserved = iterNext\n+        }\n+\n+        List((weights, localLossHistory.toArray)).iterator\n+      }\n+\n+      val c = weightsAndLosses.collect()\n+      val (ws, ls) = c.unzip\n+\n+      stochasticLossHistory.append(ls.head.reduce(_ + _) / ls.head.size)\n+\n+      val weightsSum = ws.reduce(_ addi _)\n+      weights = weightsSum.divi(c.size)\n+    }\n+\n+    logInfo(\"GradientDescentWithLocalUpdate finished. Last 10 stochastic losses %s\".format("
  }],
  "prId": 166
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "I'm not very familiar with how `duplicate` is implemented. Scala doc says \"The implementation may allocate temporary storage for elements iterated by one iterator but not yet by the other.\" Is there a risk of running out of memory here?\n",
    "commit": "5f8220a343a6ebbeff907745c6b29f64c3610620",
    "createdAt": "2014-03-18T22:30:04Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+import org.jblas.DoubleMatrix\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.Random\n+\n+/**\n+ * Class used to solve an optimization problem using Gradient Descent.\n+ * @param gradient Gradient function to be used.\n+ * @param updater Updater to be used to update weights after every iteration.\n+ */\n+class GradientDescentWithLocalUpdate(gradient: Gradient, updater: Updater)\n+  extends GradientDescent(gradient, updater) with Logging\n+{\n+  private var numLocalIterations: Int = 1\n+\n+  /**\n+   * Set the number of local iterations. Default 1.\n+   */\n+  def setNumLocalIterations(numLocalIter: Int): this.type = {\n+    this.numLocalIterations = numLocalIter\n+    this\n+  }\n+\n+  override def optimize(data: RDD[(Double, Array[Double])], initialWeights: Array[Double])\n+    : Array[Double] = {\n+\n+    val (weights, stochasticLossHistory) = GradientDescentWithLocalUpdate.runMiniBatchSGD(\n+        data,\n+        gradient,\n+        updater,\n+        stepSize,\n+        numIterations,\n+        numLocalIterations,\n+        regParam,\n+        miniBatchFraction,\n+        initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+// Top-level method to run gradient descent.\n+object GradientDescentWithLocalUpdate extends Logging {\n+   /**\n+   * Run BSP+ gradient descent in parallel using mini batches.\n+   *\n+   * @param data - Input data for SGD. RDD of form (label, [feature values]).\n+   * @param gradient - Gradient object that will be used to compute the gradient.\n+   * @param updater - Updater object that will be used to update the model.\n+   * @param stepSize - stepSize to be used during update.\n+   * @param numOuterIterations - number of outer iterations that SGD should be run.\n+   * @param numInnerIterations - number of inner iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the stochastic\n+   *         loss computed for every iteration.\n+   */\n+  def runMiniBatchSGD(\n+      data: RDD[(Double, Array[Double])],\n+      gradient: Gradient,\n+      updater: Updater,\n+      stepSize: Double,\n+      numOuterIterations: Int,\n+      numInnerIterations: Int,\n+      regParam: Double,\n+      miniBatchFraction: Double,\n+      initialWeights: Array[Double]) : (Array[Double], Array[Double]) = {\n+\n+    val stochasticLossHistory = new ArrayBuffer[Double](numOuterIterations)\n+\n+    val numExamples: Long = data.count()\n+    val numPartition = data.partitions.length\n+    val miniBatchSize = numExamples * miniBatchFraction / numPartition\n+\n+    // Initialize weights as a column vector\n+    var weights = new DoubleMatrix(initialWeights.length, 1, initialWeights: _*)\n+    var regVal = 0.0\n+\n+    for (i <- 1 to numOuterIterations) {\n+      val weightsAndLosses = data.mapPartitions { iter =>\n+        var iterReserved = iter\n+        val localLossHistory = new ArrayBuffer[Double](numInnerIterations)\n+\n+        for (j <- 1 to numInnerIterations) {\n+          val (iterCurrent, iterNext) = iterReserved.duplicate"
  }, {
    "author": {
      "login": "yinxusen"
    },
    "body": "Good question, I look into [`duplicate`](https://github.com/scala/scala/blob/master/src/library/scala/collection/Iterator.scala#L1060) method just now. It uses a `scala.collection.mutuable.Queue` to mimic an `iterator`, and the elements iterated by one iterator but not yet by the other is stored there. I am shocked by that...\n\nI have no idea of the memory cost by the `Queue`, but it seems the only way to duplicate an `iterator`. We have already tested before that the method is really fast than `iterator.toArray`. @liancheng Do you know about that?\n",
    "commit": "5f8220a343a6ebbeff907745c6b29f64c3610620",
    "createdAt": "2014-03-19T03:08:31Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+import org.jblas.DoubleMatrix\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.Random\n+\n+/**\n+ * Class used to solve an optimization problem using Gradient Descent.\n+ * @param gradient Gradient function to be used.\n+ * @param updater Updater to be used to update weights after every iteration.\n+ */\n+class GradientDescentWithLocalUpdate(gradient: Gradient, updater: Updater)\n+  extends GradientDescent(gradient, updater) with Logging\n+{\n+  private var numLocalIterations: Int = 1\n+\n+  /**\n+   * Set the number of local iterations. Default 1.\n+   */\n+  def setNumLocalIterations(numLocalIter: Int): this.type = {\n+    this.numLocalIterations = numLocalIter\n+    this\n+  }\n+\n+  override def optimize(data: RDD[(Double, Array[Double])], initialWeights: Array[Double])\n+    : Array[Double] = {\n+\n+    val (weights, stochasticLossHistory) = GradientDescentWithLocalUpdate.runMiniBatchSGD(\n+        data,\n+        gradient,\n+        updater,\n+        stepSize,\n+        numIterations,\n+        numLocalIterations,\n+        regParam,\n+        miniBatchFraction,\n+        initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+// Top-level method to run gradient descent.\n+object GradientDescentWithLocalUpdate extends Logging {\n+   /**\n+   * Run BSP+ gradient descent in parallel using mini batches.\n+   *\n+   * @param data - Input data for SGD. RDD of form (label, [feature values]).\n+   * @param gradient - Gradient object that will be used to compute the gradient.\n+   * @param updater - Updater object that will be used to update the model.\n+   * @param stepSize - stepSize to be used during update.\n+   * @param numOuterIterations - number of outer iterations that SGD should be run.\n+   * @param numInnerIterations - number of inner iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the stochastic\n+   *         loss computed for every iteration.\n+   */\n+  def runMiniBatchSGD(\n+      data: RDD[(Double, Array[Double])],\n+      gradient: Gradient,\n+      updater: Updater,\n+      stepSize: Double,\n+      numOuterIterations: Int,\n+      numInnerIterations: Int,\n+      regParam: Double,\n+      miniBatchFraction: Double,\n+      initialWeights: Array[Double]) : (Array[Double], Array[Double]) = {\n+\n+    val stochasticLossHistory = new ArrayBuffer[Double](numOuterIterations)\n+\n+    val numExamples: Long = data.count()\n+    val numPartition = data.partitions.length\n+    val miniBatchSize = numExamples * miniBatchFraction / numPartition\n+\n+    // Initialize weights as a column vector\n+    var weights = new DoubleMatrix(initialWeights.length, 1, initialWeights: _*)\n+    var regVal = 0.0\n+\n+    for (i <- 1 to numOuterIterations) {\n+      val weightsAndLosses = data.mapPartitions { iter =>\n+        var iterReserved = iter\n+        val localLossHistory = new ArrayBuffer[Double](numInnerIterations)\n+\n+        for (j <- 1 to numInnerIterations) {\n+          val (iterCurrent, iterNext) = iterReserved.duplicate"
  }, {
    "author": {
      "login": "liancheng"
    },
    "body": "We're using `Iterator.duplicate` to iterate the dataset multiple times without calling `Iterator.toArray`. According to implementation of `duplicate`, it does consume more memory and generates more temporary objects. I didn't notice that before. But according to previous experiments, `duplicate` is much more GC-friendly than `toArray`. I think the reason is that the underlying implementation of the `mutable.Queue` used in `duplicate` is actually a `mutable.LinkedList`, which doesn't require large amount of _continuous_ memory, and thus may trigger full-GC less frequently.\n\nIf my guess is right, instead of using `duplicate` maybe we can simply call `Iterator.toList` to reduce full-GC frequency?\n",
    "commit": "5f8220a343a6ebbeff907745c6b29f64c3610620",
    "createdAt": "2014-03-19T06:42:31Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+import org.jblas.DoubleMatrix\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.Random\n+\n+/**\n+ * Class used to solve an optimization problem using Gradient Descent.\n+ * @param gradient Gradient function to be used.\n+ * @param updater Updater to be used to update weights after every iteration.\n+ */\n+class GradientDescentWithLocalUpdate(gradient: Gradient, updater: Updater)\n+  extends GradientDescent(gradient, updater) with Logging\n+{\n+  private var numLocalIterations: Int = 1\n+\n+  /**\n+   * Set the number of local iterations. Default 1.\n+   */\n+  def setNumLocalIterations(numLocalIter: Int): this.type = {\n+    this.numLocalIterations = numLocalIter\n+    this\n+  }\n+\n+  override def optimize(data: RDD[(Double, Array[Double])], initialWeights: Array[Double])\n+    : Array[Double] = {\n+\n+    val (weights, stochasticLossHistory) = GradientDescentWithLocalUpdate.runMiniBatchSGD(\n+        data,\n+        gradient,\n+        updater,\n+        stepSize,\n+        numIterations,\n+        numLocalIterations,\n+        regParam,\n+        miniBatchFraction,\n+        initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+// Top-level method to run gradient descent.\n+object GradientDescentWithLocalUpdate extends Logging {\n+   /**\n+   * Run BSP+ gradient descent in parallel using mini batches.\n+   *\n+   * @param data - Input data for SGD. RDD of form (label, [feature values]).\n+   * @param gradient - Gradient object that will be used to compute the gradient.\n+   * @param updater - Updater object that will be used to update the model.\n+   * @param stepSize - stepSize to be used during update.\n+   * @param numOuterIterations - number of outer iterations that SGD should be run.\n+   * @param numInnerIterations - number of inner iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the stochastic\n+   *         loss computed for every iteration.\n+   */\n+  def runMiniBatchSGD(\n+      data: RDD[(Double, Array[Double])],\n+      gradient: Gradient,\n+      updater: Updater,\n+      stepSize: Double,\n+      numOuterIterations: Int,\n+      numInnerIterations: Int,\n+      regParam: Double,\n+      miniBatchFraction: Double,\n+      initialWeights: Array[Double]) : (Array[Double], Array[Double]) = {\n+\n+    val stochasticLossHistory = new ArrayBuffer[Double](numOuterIterations)\n+\n+    val numExamples: Long = data.count()\n+    val numPartition = data.partitions.length\n+    val miniBatchSize = numExamples * miniBatchFraction / numPartition\n+\n+    // Initialize weights as a column vector\n+    var weights = new DoubleMatrix(initialWeights.length, 1, initialWeights: _*)\n+    var regVal = 0.0\n+\n+    for (i <- 1 to numOuterIterations) {\n+      val weightsAndLosses = data.mapPartitions { iter =>\n+        var iterReserved = iter\n+        val localLossHistory = new ArrayBuffer[Double](numInnerIterations)\n+\n+        for (j <- 1 to numInnerIterations) {\n+          val (iterCurrent, iterNext) = iterReserved.duplicate"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "@yinxusen I think this approach will certainly run OOM if data is too big to fit into memory. You can set a small executor memory and test some data without caching.\n",
    "commit": "5f8220a343a6ebbeff907745c6b29f64c3610620",
    "createdAt": "2014-03-19T08:07:54Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+import org.jblas.DoubleMatrix\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.Random\n+\n+/**\n+ * Class used to solve an optimization problem using Gradient Descent.\n+ * @param gradient Gradient function to be used.\n+ * @param updater Updater to be used to update weights after every iteration.\n+ */\n+class GradientDescentWithLocalUpdate(gradient: Gradient, updater: Updater)\n+  extends GradientDescent(gradient, updater) with Logging\n+{\n+  private var numLocalIterations: Int = 1\n+\n+  /**\n+   * Set the number of local iterations. Default 1.\n+   */\n+  def setNumLocalIterations(numLocalIter: Int): this.type = {\n+    this.numLocalIterations = numLocalIter\n+    this\n+  }\n+\n+  override def optimize(data: RDD[(Double, Array[Double])], initialWeights: Array[Double])\n+    : Array[Double] = {\n+\n+    val (weights, stochasticLossHistory) = GradientDescentWithLocalUpdate.runMiniBatchSGD(\n+        data,\n+        gradient,\n+        updater,\n+        stepSize,\n+        numIterations,\n+        numLocalIterations,\n+        regParam,\n+        miniBatchFraction,\n+        initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+// Top-level method to run gradient descent.\n+object GradientDescentWithLocalUpdate extends Logging {\n+   /**\n+   * Run BSP+ gradient descent in parallel using mini batches.\n+   *\n+   * @param data - Input data for SGD. RDD of form (label, [feature values]).\n+   * @param gradient - Gradient object that will be used to compute the gradient.\n+   * @param updater - Updater object that will be used to update the model.\n+   * @param stepSize - stepSize to be used during update.\n+   * @param numOuterIterations - number of outer iterations that SGD should be run.\n+   * @param numInnerIterations - number of inner iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the stochastic\n+   *         loss computed for every iteration.\n+   */\n+  def runMiniBatchSGD(\n+      data: RDD[(Double, Array[Double])],\n+      gradient: Gradient,\n+      updater: Updater,\n+      stepSize: Double,\n+      numOuterIterations: Int,\n+      numInnerIterations: Int,\n+      regParam: Double,\n+      miniBatchFraction: Double,\n+      initialWeights: Array[Double]) : (Array[Double], Array[Double]) = {\n+\n+    val stochasticLossHistory = new ArrayBuffer[Double](numOuterIterations)\n+\n+    val numExamples: Long = data.count()\n+    val numPartition = data.partitions.length\n+    val miniBatchSize = numExamples * miniBatchFraction / numPartition\n+\n+    // Initialize weights as a column vector\n+    var weights = new DoubleMatrix(initialWeights.length, 1, initialWeights: _*)\n+    var regVal = 0.0\n+\n+    for (i <- 1 to numOuterIterations) {\n+      val weightsAndLosses = data.mapPartitions { iter =>\n+        var iterReserved = iter\n+        val localLossHistory = new ArrayBuffer[Double](numInnerIterations)\n+\n+        for (j <- 1 to numInnerIterations) {\n+          val (iterCurrent, iterNext) = iterReserved.duplicate"
  }, {
    "author": {
      "login": "yinxusen"
    },
    "body": "@mengxr, I absolutely agree with you. I am trying another way now, and will have a test result tomorrow.\n",
    "commit": "5f8220a343a6ebbeff907745c6b29f64c3610620",
    "createdAt": "2014-03-20T09:31:27Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.optimization\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.rdd.RDD\n+\n+import org.jblas.DoubleMatrix\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.util.Random\n+\n+/**\n+ * Class used to solve an optimization problem using Gradient Descent.\n+ * @param gradient Gradient function to be used.\n+ * @param updater Updater to be used to update weights after every iteration.\n+ */\n+class GradientDescentWithLocalUpdate(gradient: Gradient, updater: Updater)\n+  extends GradientDescent(gradient, updater) with Logging\n+{\n+  private var numLocalIterations: Int = 1\n+\n+  /**\n+   * Set the number of local iterations. Default 1.\n+   */\n+  def setNumLocalIterations(numLocalIter: Int): this.type = {\n+    this.numLocalIterations = numLocalIter\n+    this\n+  }\n+\n+  override def optimize(data: RDD[(Double, Array[Double])], initialWeights: Array[Double])\n+    : Array[Double] = {\n+\n+    val (weights, stochasticLossHistory) = GradientDescentWithLocalUpdate.runMiniBatchSGD(\n+        data,\n+        gradient,\n+        updater,\n+        stepSize,\n+        numIterations,\n+        numLocalIterations,\n+        regParam,\n+        miniBatchFraction,\n+        initialWeights)\n+    weights\n+  }\n+\n+}\n+\n+// Top-level method to run gradient descent.\n+object GradientDescentWithLocalUpdate extends Logging {\n+   /**\n+   * Run BSP+ gradient descent in parallel using mini batches.\n+   *\n+   * @param data - Input data for SGD. RDD of form (label, [feature values]).\n+   * @param gradient - Gradient object that will be used to compute the gradient.\n+   * @param updater - Updater object that will be used to update the model.\n+   * @param stepSize - stepSize to be used during update.\n+   * @param numOuterIterations - number of outer iterations that SGD should be run.\n+   * @param numInnerIterations - number of inner iterations that SGD should be run.\n+   * @param regParam - regularization parameter\n+   * @param miniBatchFraction - fraction of the input data set that should be used for\n+   *                            one iteration of SGD. Default value 1.0.\n+   *\n+   * @return A tuple containing two elements. The first element is a column matrix containing\n+   *         weights for every feature, and the second element is an array containing the stochastic\n+   *         loss computed for every iteration.\n+   */\n+  def runMiniBatchSGD(\n+      data: RDD[(Double, Array[Double])],\n+      gradient: Gradient,\n+      updater: Updater,\n+      stepSize: Double,\n+      numOuterIterations: Int,\n+      numInnerIterations: Int,\n+      regParam: Double,\n+      miniBatchFraction: Double,\n+      initialWeights: Array[Double]) : (Array[Double], Array[Double]) = {\n+\n+    val stochasticLossHistory = new ArrayBuffer[Double](numOuterIterations)\n+\n+    val numExamples: Long = data.count()\n+    val numPartition = data.partitions.length\n+    val miniBatchSize = numExamples * miniBatchFraction / numPartition\n+\n+    // Initialize weights as a column vector\n+    var weights = new DoubleMatrix(initialWeights.length, 1, initialWeights: _*)\n+    var regVal = 0.0\n+\n+    for (i <- 1 to numOuterIterations) {\n+      val weightsAndLosses = data.mapPartitions { iter =>\n+        var iterReserved = iter\n+        val localLossHistory = new ArrayBuffer[Double](numInnerIterations)\n+\n+        for (j <- 1 to numInnerIterations) {\n+          val (iterCurrent, iterNext) = iterReserved.duplicate"
  }],
  "prId": 166
}]