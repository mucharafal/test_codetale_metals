[{
  "comments": [{
    "author": {
      "login": "huifeidemaer"
    },
    "body": "if you want to know more about the above codes, you can refer to the following formula:\nFirst), the original sampling formula is :<img src=\"http://www.forkosh.com/mathtex.cgi? P(z^{(d)}_{n}|W, Z_{\\backslash d,n}, \\alpha u, \\beta u)\\propto P(w^{(d)}_{n}|z^{(d)}_{n},W_{\\backslash d,n}, Z_{\\backslash d,n}, \\beta u) P(z^{(d)}_{n}|Z_{\\backslash d,n}, \\alpha u)\">     (1)\nSecond), using the Asymmetric Dirichlet Priors, the second term of formula (1) can be written as following:\n<img src=\"http://www.forkosh.com/mathtex.cgi?  P(z^{(d)}_{N_{d+1}}=t|Z, \\alpha, \\alpha^{'}u)=\\int dm P(z^{(d)}_{N_{d+1}}=t|Z, \\alpha m)P(m|Z, \\alpha^{'}u)\">    (2)\n",
    "commit": "68360c48445572218cf601903bb77fc8002c22ab",
    "createdAt": "2014-09-30T02:14:20Z",
    "diffHunk": "@@ -0,0 +1,818 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import java.util.Random\n+\n+import breeze.collection.mutable.SparseArray\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, sum => bsum}\n+import com.esotericsoftware.kryo.{Kryo, KryoException}\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.Logging\n+import org.apache.spark.graphx._\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.serializer.KryoRegistrator\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.linalg.{DenseVector => SDV, SparseVector => SSV, Vector => SV}\n+import org.apache.spark.mllib.linalg.Vectors\n+import org.apache.spark.rdd.RDD\n+\n+object TopicModeling {\n+\n+  type DocId = VertexId\n+  type WordId = VertexId\n+  type Count = Int\n+  type VD = (BV[Count], Option[(BV[Double], BV[Double])])\n+  type ED = Array[Count]\n+\n+  def train(docs: RDD[(DocId, SSV)],\n+    numTopics: Int = 2048,\n+    totalIter: Int = 150,\n+    burnInIter: Int = 135,\n+    alpha: Double = 0.1,\n+    beta: Double = 0.01): TopicModel = {\n+    val topicModeling = new TopicModeling(docs, numTopics, alpha, beta)\n+    val numTerms = topicModeling.numTerms\n+    val topicModel = TopicModel(numTopics, numTerms, alpha, beta)\n+    topicModeling.runGibbsSampling(topicModel, totalIter, burnInIter)\n+    topicModel\n+  }\n+\n+  private[mllib] def merge(a: BV[Count], b: BV[Count]): BV[Count] = {\n+    assert(a.size == b.size)\n+    a :+ b\n+  }\n+\n+  private[mllib] def update(a: BV[Count], t: Int, inc: Int): BV[Count] = {\n+    a(t) += inc\n+    a\n+  }\n+\n+  private[mllib] def zeros(numTopics: Int, isDense: Boolean = false): BV[Count] = {\n+    if (isDense) {\n+      BDV.zeros(numTopics)\n+    }\n+    else {\n+      BSV.zeros(numTopics)\n+    }\n+  }\n+\n+  private[mllib] def collectTermTopicDist(graph: Graph[VD, ED],\n+    totalTopicCounter: BV[Count],\n+    sumTerms: Long,\n+    numTerms: Int,\n+    numTopics: Int,\n+    alpha: Double,\n+    beta: Double): Graph[VD, ED] = {\n+    graph.mapVertices[VD]((vertexId, counter) => {\n+      if (vertexId >= 0) {\n+        val termTopicCounter = counter._1\n+        val w = BSV.zeros[Double](numTopics)\n+        val w1 = BSV.zeros[Double](numTopics)\n+        var wi = 0D\n+\n+        termTopicCounter.activeIterator.foreach { case (i, v) =>\n+          var adjustment = 0D\n+          w(i) = v * ((totalTopicCounter(i) * (alpha * numTopics)) +\n+            (alpha * numTopics) * (adjustment + alpha) +\n+            adjustment * (sumTerms - 1 + (alpha * numTopics))) /\n+            (totalTopicCounter(i) + (numTerms * beta))\n+\n+          adjustment = -1D\n+          w1(i) = v * ((totalTopicCounter(i) * (alpha * numTopics)) +\n+            (alpha * numTopics) * (adjustment + alpha) +\n+            adjustment * (sumTerms - 1 + (alpha * numTopics))) /\n+            (totalTopicCounter(i) + (numTerms * beta)) - w(i)\n+\n+          wi = w(i) + wi\n+          w(i) = wi\n+        }\n+\n+        w(numTopics - 1) = wi\n+        (termTopicCounter, Some(w, w1))\n+      }\n+      else {\n+        counter\n+      }\n+    })\n+  }\n+\n+  @inline private[mllib] def collectDocTopicDist(\n+    totalTopicCounter: BV[Count],\n+    termTopicCounter: BV[Count],\n+    docTopicCounter: BV[Count],\n+    d: BDV[Double],\n+    d1: BDV[Double],\n+    sumTerms: Long,\n+    numTerms: Int,\n+    numTopics: Int,\n+    alpha: Double,\n+    beta: Double): (BV[Double], BV[Double]) = {\n+    assert(totalTopicCounter.size == numTopics)\n+    var di = 0D\n+    docTopicCounter.activeIterator.foreach { case (i, v) =>\n+\n+      var adjustment = 0D\n+      d(i) = v * (termTopicCounter(i) * (sumTerms - 1 + alpha * numTopics) +\n+        (adjustment + beta) * (sumTerms - 1 + alpha * numTopics)) /\n+        (totalTopicCounter(i) + adjustment + numTerms * beta)\n+\n+      adjustment = -1D\n+      d1(i) = v * (termTopicCounter(i) * (sumTerms - 1 + alpha * numTopics) +\n+        (adjustment + beta) * (sumTerms - 1 + alpha * numTopics)) /\n+        (totalTopicCounter(i) + adjustment + numTerms * beta) - d(i)\n+\n+      di = d(i) + di\n+      d(i) = di\n+    }\n+\n+    d(numTopics - 1) = di\n+\n+    (d, d1)\n+  }\n+\n+  private[mllib] def collectGlobalTopicDist(totalTopicCounter: BV[Count],\n+    sumTerms: Long,\n+    numTerms: Int,\n+    numTopics: Int,\n+    alpha: Double,\n+    beta: Double): (BV[Double], BV[Double]) = {\n+    assert(totalTopicCounter.size == numTopics)\n+    var i = 0\n+    val t = BDV.zeros[Double](numTopics)\n+    val t1 = BDV.zeros[Double](numTopics)\n+    var ti = 0D\n+\n+    while (i < numTopics) {\n+      var adjustment = 0D\n+      t(i) = (adjustment + beta) * (totalTopicCounter(i) * (alpha * numTopics) +\n+        alpha * numTopics * (adjustment + alpha) +\n+        adjustment * (sumTerms - 1 + (alpha * numTopics))) /\n+        (totalTopicCounter(i) + (adjustment + numTerms * beta))\n+\n+      adjustment = -1D\n+      t1(i) = (adjustment + beta) * (totalTopicCounter(i) * (alpha * numTopics) +\n+        alpha * numTopics * (adjustment + alpha) +\n+        adjustment * (sumTerms - 1 + (alpha * numTopics))) /\n+        (totalTopicCounter(i) + (adjustment + numTerms * beta)) - t(i)\n+\n+      ti = t(i) + ti\n+      t(i) = ti\n+\n+      i += 1\n+    }\n+    (t, t1)\n+  }\n+\n+  private[mllib] def sampleTopics(\n+    graph: Graph[VD, ED],\n+    totalTopicCounter: BV[Count],\n+    sumTerms: Long,\n+    innerIter: Long,\n+    numTerms: Int,\n+    numTopics: Int,\n+    alpha: Double,\n+    beta: Double\n+  ): Graph[VD, ED] = {\n+    val parts = graph.edges.partitions.size\n+    val (t, t1) = TopicModeling.collectGlobalTopicDist(totalTopicCounter, sumTerms, numTerms,\n+      numTopics, alpha, beta)\n+    val sampleTopics = (gen: java.util.Random, d: BDV[Double], d1: BDV[Double],\n+    triplet: EdgeTriplet[VD, ED]) => {\n+      assert(triplet.srcId >= 0)\n+      val (termCounter, Some((w, w1))) = triplet.srcAttr\n+      val (docTopicCounter, _) = triplet.dstAttr\n+      TopicModeling.collectDocTopicDist(totalTopicCounter, termCounter,\n+        docTopicCounter, d, d1, sumTerms, numTerms, numTopics, alpha, beta)\n+\n+      val topics = triplet.attr\n+      var i = 0\n+      while (i < topics.length) {\n+        val oldTopic = topics(i)\n+        val newTopic = TopicModeling.multinomialDistSampler(gen, d, w, t, d1(oldTopic),\n+          w1(oldTopic), t1(oldTopic), oldTopic)\n+        topics(i) = newTopic\n+        i += 1\n+      }\n+      topics\n+    }\n+\n+    graph.mapTriplets {\n+      (pid, iter) =>\n+        val gen = new java.util.Random(parts * pid + innerIter)\n+        val d = BDV.zeros[Double](numTopics)\n+        val d1 = BDV.zeros[Double](numTopics)\n+        iter.map {\n+          token =>\n+            sampleTopics(gen, d, d1, token)\n+        }\n+    }\n+  }\n+\n+  private[mllib] def updateCounter(graph: Graph[VD, ED], numTopics: Int): Graph[VD, ED] = {\n+    val newCounter = graph.mapReduceTriplets[BV[Int]](e => {\n+      val docId = e.dstId\n+      val wordId = e.srcId\n+      val newTopics = e.attr\n+      val vector = zeros(numTopics)\n+      var i = 0\n+      while (i < newTopics.length) {\n+        val newTopic = newTopics(i)\n+        vector(newTopic) += 1\n+        i += 1\n+      }\n+      Iterator((docId, vector), (wordId, vector))\n+\n+    }, merge)\n+    graph.joinVertices(newCounter)((_, _, n) => (n, None))\n+  }\n+\n+  private[mllib] def collectGlobalCounter(graph: Graph[VD, ED],\n+    numTopics: Int): BV[Count] = {\n+    graph.vertices.filter(t => t._1 >= 0).map(_._2._1)\n+      .aggregate(zeros(numTopics, isDense = true))(merge, merge)\n+  }\n+\n+  /**\n+   * A multinomial distribution sampler, using roulette method to sample an Int back.\n+   */\n+  @inline private[mllib] def multinomialDistSampler(rand: Random, d: BV[Double], w: BV[Double],\n+    t: BV[Double], d1: Double, w1: Double, t1: Double, currentTopic: Int): Int = {\n+    /**\n+     * Asymmetric Dirichlet Priors you can refer to the paper:\n+     * \"Rethinking LDA: Why Priors Matter\", available at\n+     * [[http://people.ee.duke.edu/~lcarin/Eric3.5.2010.pdf]]\n+     *\n+     * var topicThisTerm = BDV.zeros[Double](numTopics)\n+     * while (i < numTopics) {\n+     * val adjustment = if (i == currentTopic) -1 else 0\n+     * val ratio = (globalTopicCounter(i) + alpha) / (sumTerms + adjustment + (alpha * numTopics))\n+     * val asPrior = ratio * (alpha * numTopics)\n+     * topicThisTerm(i) = (termTopicCounter(i) + adjustment + beta) /\n+     * (globalTopicCounter(i) + adjustment + (numTerms * beta)) *\n+     * (docTopicCounter(i) + adjustment + asPrior) /\n+     * (bsum(docTopicCounter) + adjustment + alpha * numTopics)\n+     *\n+     * }\n+     *"
  }, {
    "author": {
      "login": "witgo"
    },
    "body": "Thank you very much. I will add to the code.\n",
    "commit": "68360c48445572218cf601903bb77fc8002c22ab",
    "createdAt": "2014-09-30T03:56:37Z",
    "diffHunk": "@@ -0,0 +1,818 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import java.util.Random\n+\n+import breeze.collection.mutable.SparseArray\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, sum => bsum}\n+import com.esotericsoftware.kryo.{Kryo, KryoException}\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.Logging\n+import org.apache.spark.graphx._\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.serializer.KryoRegistrator\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.linalg.{DenseVector => SDV, SparseVector => SSV, Vector => SV}\n+import org.apache.spark.mllib.linalg.Vectors\n+import org.apache.spark.rdd.RDD\n+\n+object TopicModeling {\n+\n+  type DocId = VertexId\n+  type WordId = VertexId\n+  type Count = Int\n+  type VD = (BV[Count], Option[(BV[Double], BV[Double])])\n+  type ED = Array[Count]\n+\n+  def train(docs: RDD[(DocId, SSV)],\n+    numTopics: Int = 2048,\n+    totalIter: Int = 150,\n+    burnInIter: Int = 135,\n+    alpha: Double = 0.1,\n+    beta: Double = 0.01): TopicModel = {\n+    val topicModeling = new TopicModeling(docs, numTopics, alpha, beta)\n+    val numTerms = topicModeling.numTerms\n+    val topicModel = TopicModel(numTopics, numTerms, alpha, beta)\n+    topicModeling.runGibbsSampling(topicModel, totalIter, burnInIter)\n+    topicModel\n+  }\n+\n+  private[mllib] def merge(a: BV[Count], b: BV[Count]): BV[Count] = {\n+    assert(a.size == b.size)\n+    a :+ b\n+  }\n+\n+  private[mllib] def update(a: BV[Count], t: Int, inc: Int): BV[Count] = {\n+    a(t) += inc\n+    a\n+  }\n+\n+  private[mllib] def zeros(numTopics: Int, isDense: Boolean = false): BV[Count] = {\n+    if (isDense) {\n+      BDV.zeros(numTopics)\n+    }\n+    else {\n+      BSV.zeros(numTopics)\n+    }\n+  }\n+\n+  private[mllib] def collectTermTopicDist(graph: Graph[VD, ED],\n+    totalTopicCounter: BV[Count],\n+    sumTerms: Long,\n+    numTerms: Int,\n+    numTopics: Int,\n+    alpha: Double,\n+    beta: Double): Graph[VD, ED] = {\n+    graph.mapVertices[VD]((vertexId, counter) => {\n+      if (vertexId >= 0) {\n+        val termTopicCounter = counter._1\n+        val w = BSV.zeros[Double](numTopics)\n+        val w1 = BSV.zeros[Double](numTopics)\n+        var wi = 0D\n+\n+        termTopicCounter.activeIterator.foreach { case (i, v) =>\n+          var adjustment = 0D\n+          w(i) = v * ((totalTopicCounter(i) * (alpha * numTopics)) +\n+            (alpha * numTopics) * (adjustment + alpha) +\n+            adjustment * (sumTerms - 1 + (alpha * numTopics))) /\n+            (totalTopicCounter(i) + (numTerms * beta))\n+\n+          adjustment = -1D\n+          w1(i) = v * ((totalTopicCounter(i) * (alpha * numTopics)) +\n+            (alpha * numTopics) * (adjustment + alpha) +\n+            adjustment * (sumTerms - 1 + (alpha * numTopics))) /\n+            (totalTopicCounter(i) + (numTerms * beta)) - w(i)\n+\n+          wi = w(i) + wi\n+          w(i) = wi\n+        }\n+\n+        w(numTopics - 1) = wi\n+        (termTopicCounter, Some(w, w1))\n+      }\n+      else {\n+        counter\n+      }\n+    })\n+  }\n+\n+  @inline private[mllib] def collectDocTopicDist(\n+    totalTopicCounter: BV[Count],\n+    termTopicCounter: BV[Count],\n+    docTopicCounter: BV[Count],\n+    d: BDV[Double],\n+    d1: BDV[Double],\n+    sumTerms: Long,\n+    numTerms: Int,\n+    numTopics: Int,\n+    alpha: Double,\n+    beta: Double): (BV[Double], BV[Double]) = {\n+    assert(totalTopicCounter.size == numTopics)\n+    var di = 0D\n+    docTopicCounter.activeIterator.foreach { case (i, v) =>\n+\n+      var adjustment = 0D\n+      d(i) = v * (termTopicCounter(i) * (sumTerms - 1 + alpha * numTopics) +\n+        (adjustment + beta) * (sumTerms - 1 + alpha * numTopics)) /\n+        (totalTopicCounter(i) + adjustment + numTerms * beta)\n+\n+      adjustment = -1D\n+      d1(i) = v * (termTopicCounter(i) * (sumTerms - 1 + alpha * numTopics) +\n+        (adjustment + beta) * (sumTerms - 1 + alpha * numTopics)) /\n+        (totalTopicCounter(i) + adjustment + numTerms * beta) - d(i)\n+\n+      di = d(i) + di\n+      d(i) = di\n+    }\n+\n+    d(numTopics - 1) = di\n+\n+    (d, d1)\n+  }\n+\n+  private[mllib] def collectGlobalTopicDist(totalTopicCounter: BV[Count],\n+    sumTerms: Long,\n+    numTerms: Int,\n+    numTopics: Int,\n+    alpha: Double,\n+    beta: Double): (BV[Double], BV[Double]) = {\n+    assert(totalTopicCounter.size == numTopics)\n+    var i = 0\n+    val t = BDV.zeros[Double](numTopics)\n+    val t1 = BDV.zeros[Double](numTopics)\n+    var ti = 0D\n+\n+    while (i < numTopics) {\n+      var adjustment = 0D\n+      t(i) = (adjustment + beta) * (totalTopicCounter(i) * (alpha * numTopics) +\n+        alpha * numTopics * (adjustment + alpha) +\n+        adjustment * (sumTerms - 1 + (alpha * numTopics))) /\n+        (totalTopicCounter(i) + (adjustment + numTerms * beta))\n+\n+      adjustment = -1D\n+      t1(i) = (adjustment + beta) * (totalTopicCounter(i) * (alpha * numTopics) +\n+        alpha * numTopics * (adjustment + alpha) +\n+        adjustment * (sumTerms - 1 + (alpha * numTopics))) /\n+        (totalTopicCounter(i) + (adjustment + numTerms * beta)) - t(i)\n+\n+      ti = t(i) + ti\n+      t(i) = ti\n+\n+      i += 1\n+    }\n+    (t, t1)\n+  }\n+\n+  private[mllib] def sampleTopics(\n+    graph: Graph[VD, ED],\n+    totalTopicCounter: BV[Count],\n+    sumTerms: Long,\n+    innerIter: Long,\n+    numTerms: Int,\n+    numTopics: Int,\n+    alpha: Double,\n+    beta: Double\n+  ): Graph[VD, ED] = {\n+    val parts = graph.edges.partitions.size\n+    val (t, t1) = TopicModeling.collectGlobalTopicDist(totalTopicCounter, sumTerms, numTerms,\n+      numTopics, alpha, beta)\n+    val sampleTopics = (gen: java.util.Random, d: BDV[Double], d1: BDV[Double],\n+    triplet: EdgeTriplet[VD, ED]) => {\n+      assert(triplet.srcId >= 0)\n+      val (termCounter, Some((w, w1))) = triplet.srcAttr\n+      val (docTopicCounter, _) = triplet.dstAttr\n+      TopicModeling.collectDocTopicDist(totalTopicCounter, termCounter,\n+        docTopicCounter, d, d1, sumTerms, numTerms, numTopics, alpha, beta)\n+\n+      val topics = triplet.attr\n+      var i = 0\n+      while (i < topics.length) {\n+        val oldTopic = topics(i)\n+        val newTopic = TopicModeling.multinomialDistSampler(gen, d, w, t, d1(oldTopic),\n+          w1(oldTopic), t1(oldTopic), oldTopic)\n+        topics(i) = newTopic\n+        i += 1\n+      }\n+      topics\n+    }\n+\n+    graph.mapTriplets {\n+      (pid, iter) =>\n+        val gen = new java.util.Random(parts * pid + innerIter)\n+        val d = BDV.zeros[Double](numTopics)\n+        val d1 = BDV.zeros[Double](numTopics)\n+        iter.map {\n+          token =>\n+            sampleTopics(gen, d, d1, token)\n+        }\n+    }\n+  }\n+\n+  private[mllib] def updateCounter(graph: Graph[VD, ED], numTopics: Int): Graph[VD, ED] = {\n+    val newCounter = graph.mapReduceTriplets[BV[Int]](e => {\n+      val docId = e.dstId\n+      val wordId = e.srcId\n+      val newTopics = e.attr\n+      val vector = zeros(numTopics)\n+      var i = 0\n+      while (i < newTopics.length) {\n+        val newTopic = newTopics(i)\n+        vector(newTopic) += 1\n+        i += 1\n+      }\n+      Iterator((docId, vector), (wordId, vector))\n+\n+    }, merge)\n+    graph.joinVertices(newCounter)((_, _, n) => (n, None))\n+  }\n+\n+  private[mllib] def collectGlobalCounter(graph: Graph[VD, ED],\n+    numTopics: Int): BV[Count] = {\n+    graph.vertices.filter(t => t._1 >= 0).map(_._2._1)\n+      .aggregate(zeros(numTopics, isDense = true))(merge, merge)\n+  }\n+\n+  /**\n+   * A multinomial distribution sampler, using roulette method to sample an Int back.\n+   */\n+  @inline private[mllib] def multinomialDistSampler(rand: Random, d: BV[Double], w: BV[Double],\n+    t: BV[Double], d1: Double, w1: Double, t1: Double, currentTopic: Int): Int = {\n+    /**\n+     * Asymmetric Dirichlet Priors you can refer to the paper:\n+     * \"Rethinking LDA: Why Priors Matter\", available at\n+     * [[http://people.ee.duke.edu/~lcarin/Eric3.5.2010.pdf]]\n+     *\n+     * var topicThisTerm = BDV.zeros[Double](numTopics)\n+     * while (i < numTopics) {\n+     * val adjustment = if (i == currentTopic) -1 else 0\n+     * val ratio = (globalTopicCounter(i) + alpha) / (sumTerms + adjustment + (alpha * numTopics))\n+     * val asPrior = ratio * (alpha * numTopics)\n+     * topicThisTerm(i) = (termTopicCounter(i) + adjustment + beta) /\n+     * (globalTopicCounter(i) + adjustment + (numTerms * beta)) *\n+     * (docTopicCounter(i) + adjustment + asPrior) /\n+     * (bsum(docTopicCounter) + adjustment + alpha * numTopics)\n+     *\n+     * }\n+     *"
  }],
  "prId": 2388
}, {
  "comments": [{
    "author": {
      "login": "huifeidemaer"
    },
    "body": "if you want to know more about the above codes, you can refer to the following formula:\nFirst), the original Gibbis sampling formula is :<img src=\"http://www.forkosh.com/mathtex.cgi? P(z^{(d)}_{n}|W, Z_{\\backslash d,n}, \\alpha u, \\beta u)\\propto P(w^{(d)}_{n}|z^{(d)}_{n},W_{\\backslash d,n}, Z_{\\backslash d,n}, \\beta u) P(z^{(d)}_{n}|Z_{\\backslash d,n}, \\alpha u)\">     (1)\nSecond), using the Asymmetric Dirichlet Priors, the second term of formula (1) can be written as following:\n<img src=\"http://www.forkosh.com/mathtex.cgi?  P(z^{(d)}_{N_{d+1}}=t|Z, \\alpha, \\alpha^{'}u)=\\int dm P(z^{(d)}_{N_{d+1}}=t|Z, \\alpha m)P(m|Z, \\alpha^{'}u)=\\frac{N_{t|d}+\\alpha \\frac{\\widehat{N}_{T}+\\frac{\\alpha^{'}}{T}}{\\Sigma_{t}\\widehat{N}_{t}+ \\alpha ^{'}}}{N_{d}+\\alpha}\">    (2)\nThird), in this code, we set the <img src=\"http://www.forkosh.com/mathtex.cgi? \\alpha=\\alpha^{'}\">, you can set different value for them. Additionally, in our code the parameter \"alpha\" is equal to <img src=\"http://www.forkosh.com/mathtex.cgi?\\alpha * T\">;  \"adjustment\" denote that if this is the current topic, you need to reduce number one from the corresponding term; <img src=\"http://www.forkosh.com/mathtex.cgi? ratio=\\frac{\\widehat{N}_{t}+\\frac{\\alpha^{'}}{T}}{\\Sigma _{t}\\widehat{N}_{t}+\\alpha^{'}} \\qquad asPrior = ratio * (alpha * numTopics)\">; \nFinally),  we put them into formula (1) to get the final Asymmetric Dirichlet Priors Gibbs sampling formula.\n",
    "commit": "68360c48445572218cf601903bb77fc8002c22ab",
    "createdAt": "2014-09-30T03:00:49Z",
    "diffHunk": "@@ -0,0 +1,818 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.mllib.clustering\n+\n+import java.util.Random\n+\n+import breeze.collection.mutable.SparseArray\n+import breeze.linalg.{DenseVector => BDV, SparseVector => BSV, Vector => BV, sum => bsum}\n+import com.esotericsoftware.kryo.{Kryo, KryoException}\n+\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.Logging\n+import org.apache.spark.graphx._\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.serializer.KryoRegistrator\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.mllib.linalg.{DenseVector => SDV, SparseVector => SSV, Vector => SV}\n+import org.apache.spark.mllib.linalg.Vectors\n+import org.apache.spark.rdd.RDD\n+\n+object TopicModeling {\n+\n+  type DocId = VertexId\n+  type WordId = VertexId\n+  type Count = Int\n+  type VD = (BV[Count], Option[(BV[Double], BV[Double])])\n+  type ED = Array[Count]\n+\n+  def train(docs: RDD[(DocId, SSV)],\n+    numTopics: Int = 2048,\n+    totalIter: Int = 150,\n+    burnInIter: Int = 135,\n+    alpha: Double = 0.1,\n+    beta: Double = 0.01): TopicModel = {\n+    val topicModeling = new TopicModeling(docs, numTopics, alpha, beta)\n+    val numTerms = topicModeling.numTerms\n+    val topicModel = TopicModel(numTopics, numTerms, alpha, beta)\n+    topicModeling.runGibbsSampling(topicModel, totalIter, burnInIter)\n+    topicModel\n+  }\n+\n+  private[mllib] def merge(a: BV[Count], b: BV[Count]): BV[Count] = {\n+    assert(a.size == b.size)\n+    a :+ b\n+  }\n+\n+  private[mllib] def update(a: BV[Count], t: Int, inc: Int): BV[Count] = {\n+    a(t) += inc\n+    a\n+  }\n+\n+  private[mllib] def zeros(numTopics: Int, isDense: Boolean = false): BV[Count] = {\n+    if (isDense) {\n+      BDV.zeros(numTopics)\n+    }\n+    else {\n+      BSV.zeros(numTopics)\n+    }\n+  }\n+\n+  private[mllib] def collectTermTopicDist(graph: Graph[VD, ED],\n+    totalTopicCounter: BV[Count],\n+    sumTerms: Long,\n+    numTerms: Int,\n+    numTopics: Int,\n+    alpha: Double,\n+    beta: Double): Graph[VD, ED] = {\n+    graph.mapVertices[VD]((vertexId, counter) => {\n+      if (vertexId >= 0) {\n+        val termTopicCounter = counter._1\n+        val w = BSV.zeros[Double](numTopics)\n+        val w1 = BSV.zeros[Double](numTopics)\n+        var wi = 0D\n+\n+        termTopicCounter.activeIterator.foreach { case (i, v) =>\n+          var adjustment = 0D\n+          w(i) = v * ((totalTopicCounter(i) * (alpha * numTopics)) +\n+            (alpha * numTopics) * (adjustment + alpha) +\n+            adjustment * (sumTerms - 1 + (alpha * numTopics))) /\n+            (totalTopicCounter(i) + (numTerms * beta))\n+\n+          adjustment = -1D\n+          w1(i) = v * ((totalTopicCounter(i) * (alpha * numTopics)) +\n+            (alpha * numTopics) * (adjustment + alpha) +\n+            adjustment * (sumTerms - 1 + (alpha * numTopics))) /\n+            (totalTopicCounter(i) + (numTerms * beta)) - w(i)\n+\n+          wi = w(i) + wi\n+          w(i) = wi\n+        }\n+\n+        w(numTopics - 1) = wi\n+        (termTopicCounter, Some(w, w1))\n+      }\n+      else {\n+        counter\n+      }\n+    })\n+  }\n+\n+  @inline private[mllib] def collectDocTopicDist(\n+    totalTopicCounter: BV[Count],\n+    termTopicCounter: BV[Count],\n+    docTopicCounter: BV[Count],\n+    d: BDV[Double],\n+    d1: BDV[Double],\n+    sumTerms: Long,\n+    numTerms: Int,\n+    numTopics: Int,\n+    alpha: Double,\n+    beta: Double): (BV[Double], BV[Double]) = {\n+    assert(totalTopicCounter.size == numTopics)\n+    var di = 0D\n+    docTopicCounter.activeIterator.foreach { case (i, v) =>\n+\n+      var adjustment = 0D\n+      d(i) = v * (termTopicCounter(i) * (sumTerms - 1 + alpha * numTopics) +\n+        (adjustment + beta) * (sumTerms - 1 + alpha * numTopics)) /\n+        (totalTopicCounter(i) + adjustment + numTerms * beta)\n+\n+      adjustment = -1D\n+      d1(i) = v * (termTopicCounter(i) * (sumTerms - 1 + alpha * numTopics) +\n+        (adjustment + beta) * (sumTerms - 1 + alpha * numTopics)) /\n+        (totalTopicCounter(i) + adjustment + numTerms * beta) - d(i)\n+\n+      di = d(i) + di\n+      d(i) = di\n+    }\n+\n+    d(numTopics - 1) = di\n+\n+    (d, d1)\n+  }\n+\n+  private[mllib] def collectGlobalTopicDist(totalTopicCounter: BV[Count],\n+    sumTerms: Long,\n+    numTerms: Int,\n+    numTopics: Int,\n+    alpha: Double,\n+    beta: Double): (BV[Double], BV[Double]) = {\n+    assert(totalTopicCounter.size == numTopics)\n+    var i = 0\n+    val t = BDV.zeros[Double](numTopics)\n+    val t1 = BDV.zeros[Double](numTopics)\n+    var ti = 0D\n+\n+    while (i < numTopics) {\n+      var adjustment = 0D\n+      t(i) = (adjustment + beta) * (totalTopicCounter(i) * (alpha * numTopics) +\n+        alpha * numTopics * (adjustment + alpha) +\n+        adjustment * (sumTerms - 1 + (alpha * numTopics))) /\n+        (totalTopicCounter(i) + (adjustment + numTerms * beta))\n+\n+      adjustment = -1D\n+      t1(i) = (adjustment + beta) * (totalTopicCounter(i) * (alpha * numTopics) +\n+        alpha * numTopics * (adjustment + alpha) +\n+        adjustment * (sumTerms - 1 + (alpha * numTopics))) /\n+        (totalTopicCounter(i) + (adjustment + numTerms * beta)) - t(i)\n+\n+      ti = t(i) + ti\n+      t(i) = ti\n+\n+      i += 1\n+    }\n+    (t, t1)\n+  }\n+\n+  private[mllib] def sampleTopics(\n+    graph: Graph[VD, ED],\n+    totalTopicCounter: BV[Count],\n+    sumTerms: Long,\n+    innerIter: Long,\n+    numTerms: Int,\n+    numTopics: Int,\n+    alpha: Double,\n+    beta: Double\n+  ): Graph[VD, ED] = {\n+    val parts = graph.edges.partitions.size\n+    val (t, t1) = TopicModeling.collectGlobalTopicDist(totalTopicCounter, sumTerms, numTerms,\n+      numTopics, alpha, beta)\n+    val sampleTopics = (gen: java.util.Random, d: BDV[Double], d1: BDV[Double],\n+    triplet: EdgeTriplet[VD, ED]) => {\n+      assert(triplet.srcId >= 0)\n+      val (termCounter, Some((w, w1))) = triplet.srcAttr\n+      val (docTopicCounter, _) = triplet.dstAttr\n+      TopicModeling.collectDocTopicDist(totalTopicCounter, termCounter,\n+        docTopicCounter, d, d1, sumTerms, numTerms, numTopics, alpha, beta)\n+\n+      val topics = triplet.attr\n+      var i = 0\n+      while (i < topics.length) {\n+        val oldTopic = topics(i)\n+        val newTopic = TopicModeling.multinomialDistSampler(gen, d, w, t, d1(oldTopic),\n+          w1(oldTopic), t1(oldTopic), oldTopic)\n+        topics(i) = newTopic\n+        i += 1\n+      }\n+      topics\n+    }\n+\n+    graph.mapTriplets {\n+      (pid, iter) =>\n+        val gen = new java.util.Random(parts * pid + innerIter)\n+        val d = BDV.zeros[Double](numTopics)\n+        val d1 = BDV.zeros[Double](numTopics)\n+        iter.map {\n+          token =>\n+            sampleTopics(gen, d, d1, token)\n+        }\n+    }\n+  }\n+\n+  private[mllib] def updateCounter(graph: Graph[VD, ED], numTopics: Int): Graph[VD, ED] = {\n+    val newCounter = graph.mapReduceTriplets[BV[Int]](e => {\n+      val docId = e.dstId\n+      val wordId = e.srcId\n+      val newTopics = e.attr\n+      val vector = zeros(numTopics)\n+      var i = 0\n+      while (i < newTopics.length) {\n+        val newTopic = newTopics(i)\n+        vector(newTopic) += 1\n+        i += 1\n+      }\n+      Iterator((docId, vector), (wordId, vector))\n+\n+    }, merge)\n+    graph.joinVertices(newCounter)((_, _, n) => (n, None))\n+  }\n+\n+  private[mllib] def collectGlobalCounter(graph: Graph[VD, ED],\n+    numTopics: Int): BV[Count] = {\n+    graph.vertices.filter(t => t._1 >= 0).map(_._2._1)\n+      .aggregate(zeros(numTopics, isDense = true))(merge, merge)\n+  }\n+\n+  /**\n+   * A multinomial distribution sampler, using roulette method to sample an Int back.\n+   */\n+  @inline private[mllib] def multinomialDistSampler(rand: Random, d: BV[Double], w: BV[Double],\n+    t: BV[Double], d1: Double, w1: Double, t1: Double, currentTopic: Int): Int = {\n+    /**\n+     * Asymmetric Dirichlet Priors you can refer to the paper:\n+     * \"Rethinking LDA: Why Priors Matter\", available at\n+     * [[http://people.ee.duke.edu/~lcarin/Eric3.5.2010.pdf]]\n+     *\n+     * var topicThisTerm = BDV.zeros[Double](numTopics)\n+     * while (i < numTopics) {\n+     * val adjustment = if (i == currentTopic) -1 else 0\n+     * val ratio = (globalTopicCounter(i) + adjustment + alpha) / (sumTerms - 1 + (alpha * numTopics))\n+     * val asPrior = ratio * (alpha * numTopics)\n+     * topicThisTerm(i) = (termTopicCounter(i) + adjustment + beta) /\n+     * (globalTopicCounter(i) + adjustment + (numTerms * beta)) *\n+     * (docTopicCounter(i) + adjustment + asPrior) /\n+     * (bsum(docTopicCounter) - 1 + alpha * numTopics)\n+     *\n+     * }\n+     *"
  }],
  "prId": 2388
}]