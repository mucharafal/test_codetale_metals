[{
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "Most of this file should be `private[ann]`\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2015-10-22T19:16:14Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.ann\n+\n+import java.util.Random\n+\n+import breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV, sum => Bsum}\n+import breeze.numerics.{log => Blog}\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+\n+/**\n+ * Trait for loss function\n+ */\n+trait LossFunction {"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "No need for `{...}` when declaring empty trait\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2015-10-22T19:30:13Z",
    "diffHunk": "@@ -0,0 +1,117 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.ann\n+\n+import java.util.Random\n+\n+import breeze.linalg.{DenseMatrix => BDM, DenseVector => BDV, sum => Bsum}\n+import breeze.numerics.{log => Blog}\n+import org.apache.spark.mllib.linalg.{Vectors, Vector}\n+\n+/**\n+ * Trait for loss function\n+ */\n+trait LossFunction {\n+  /**\n+   * Loss function\n+   * @param output actual output\n+   * @param target target output\n+   * @param delta output delta to write to\n+   * @return\n+   */\n+  def loss(output: BDM[Double], target: BDM[Double], delta: BDM[Double]): Double\n+}\n+\n+trait InPlace {\n+"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`Blog` is ambiguous. We can use `brzLog`.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:53Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.ann\n+\n+import java.util.Random\n+\n+import breeze.linalg.{sum => Bsum, DenseMatrix => BDM, DenseVector => BDV}\n+import breeze.numerics.{log => Blog}"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "- missing doc\n- need to explain `delta`\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:55Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.ann\n+\n+import java.util.Random\n+\n+import breeze.linalg.{sum => Bsum, DenseMatrix => BDM, DenseVector => BDV}\n+import breeze.numerics.{log => Blog}\n+\n+/**\n+ * Trait for loss function\n+ */\n+private[ann] trait LossFunction {\n+  /**\n+   * Loss function\n+   * @param output actual output\n+   * @param target target output\n+   * @param delta output delta to write to\n+   * @return"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "leave an empty line between method declarations\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:57Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.ann\n+\n+import java.util.Random\n+\n+import breeze.linalg.{sum => Bsum, DenseMatrix => BDM, DenseVector => BDV}\n+import breeze.numerics.{log => Blog}\n+\n+/**\n+ * Trait for loss function\n+ */\n+private[ann] trait LossFunction {\n+  /**\n+   * Loss function\n+   * @param output actual output\n+   * @param target target output\n+   * @param delta output delta to write to\n+   * @return\n+   */\n+  def loss(output: BDM[Double], target: BDM[Double], delta: BDM[Double]): Double\n+}\n+\n+private[ann] class SigmoidLayerWithSquaredError extends Layer {\n+  override val weightSize = 0",
    "line": 45
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "need comment\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:59Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.ann\n+\n+import java.util.Random\n+\n+import breeze.linalg.{sum => Bsum, DenseMatrix => BDM, DenseVector => BDV}\n+import breeze.numerics.{log => Blog}\n+\n+/**\n+ * Trait for loss function\n+ */\n+private[ann] trait LossFunction {\n+  /**\n+   * Loss function\n+   * @param output actual output\n+   * @param target target output\n+   * @param delta output delta to write to\n+   * @return\n+   */\n+  def loss(output: BDM[Double], target: BDM[Double], delta: BDM[Double]): Double\n+}\n+\n+private[ann] class SigmoidLayerWithSquaredError extends Layer {\n+  override val weightSize = 0\n+  override def outputSize(inputSize: Int): Int = inputSize\n+  override val inPlace = true\n+  override def model(weights: BDV[Double]): LayerModel = new SigmoidLayerModelWithSquaredError()\n+  override def initModel(weights: BDV[Double], random: Random): LayerModel =\n+    new SigmoidLayerModelWithSquaredError()\n+}\n+\n+private[ann] class SigmoidLayerModelWithSquaredError\n+  extends FunctionalLayerModel(new FunctionalLayer(new SigmoidFunction)) with LossFunction {\n+  override def loss(output: BDM[Double], target: BDM[Double], delta: BDM[Double]): Double = {\n+    UniversalFunction(output, target, delta, (o: Double, t: Double) => o - t)\n+    val error = Bsum(delta :* delta) / 2 / output.cols\n+    UniversalFunction(delta, output, delta, (x: Double, o: Double) => x * (o - o * o))\n+    error\n+  }\n+}\n+\n+private[ann] class SoftmaxLayerWithCrossEntropyLoss extends Layer {\n+  override val weightSize = 0\n+  override def outputSize(inputSize: Int): Int = inputSize\n+  override val inPlace = true\n+  override def model(weights: BDV[Double]): LayerModel =\n+    new SoftmaxLayerModelWithCrossEntropyLoss()\n+  override def initModel(weights: BDV[Double], random: Random): LayerModel =\n+    new SoftmaxLayerModelWithCrossEntropyLoss()\n+}\n+\n+private[ann] class SoftmaxLayerModelWithCrossEntropyLoss extends LayerModel with LossFunction {\n+\n+  val weights = new BDV[Double](0)"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "- missing doc\n- should it be private?\n- Maybe we should merge it with `eval` because they are the same now.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:01Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.ann\n+\n+import java.util.Random\n+\n+import breeze.linalg.{sum => Bsum, DenseMatrix => BDM, DenseVector => BDV}\n+import breeze.numerics.{log => Blog}\n+\n+/**\n+ * Trait for loss function\n+ */\n+private[ann] trait LossFunction {\n+  /**\n+   * Loss function\n+   * @param output actual output\n+   * @param target target output\n+   * @param delta output delta to write to\n+   * @return\n+   */\n+  def loss(output: BDM[Double], target: BDM[Double], delta: BDM[Double]): Double\n+}\n+\n+private[ann] class SigmoidLayerWithSquaredError extends Layer {\n+  override val weightSize = 0\n+  override def outputSize(inputSize: Int): Int = inputSize\n+  override val inPlace = true\n+  override def model(weights: BDV[Double]): LayerModel = new SigmoidLayerModelWithSquaredError()\n+  override def initModel(weights: BDV[Double], random: Random): LayerModel =\n+    new SigmoidLayerModelWithSquaredError()\n+}\n+\n+private[ann] class SigmoidLayerModelWithSquaredError\n+  extends FunctionalLayerModel(new FunctionalLayer(new SigmoidFunction)) with LossFunction {\n+  override def loss(output: BDM[Double], target: BDM[Double], delta: BDM[Double]): Double = {\n+    UniversalFunction(output, target, delta, (o: Double, t: Double) => o - t)\n+    val error = Bsum(delta :* delta) / 2 / output.cols\n+    UniversalFunction(delta, output, delta, (x: Double, o: Double) => x * (o - o * o))\n+    error\n+  }\n+}\n+\n+private[ann] class SoftmaxLayerWithCrossEntropyLoss extends Layer {\n+  override val weightSize = 0\n+  override def outputSize(inputSize: Int): Int = inputSize\n+  override val inPlace = true\n+  override def model(weights: BDV[Double]): LayerModel =\n+    new SoftmaxLayerModelWithCrossEntropyLoss()\n+  override def initModel(weights: BDV[Double], random: Random): LayerModel =\n+    new SoftmaxLayerModelWithCrossEntropyLoss()\n+}\n+\n+private[ann] class SoftmaxLayerModelWithCrossEntropyLoss extends LayerModel with LossFunction {\n+\n+  val weights = new BDV[Double](0)\n+\n+  def inplaceEval(x: BDM[Double], y: BDM[Double]): Unit = {"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`math`\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:05Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.ann\n+\n+import java.util.Random\n+\n+import breeze.linalg.{sum => Bsum, DenseMatrix => BDM, DenseVector => BDV}\n+import breeze.numerics.{log => Blog}\n+\n+/**\n+ * Trait for loss function\n+ */\n+private[ann] trait LossFunction {\n+  /**\n+   * Loss function\n+   * @param output actual output\n+   * @param target target output\n+   * @param delta output delta to write to\n+   * @return\n+   */\n+  def loss(output: BDM[Double], target: BDM[Double], delta: BDM[Double]): Double\n+}\n+\n+private[ann] class SigmoidLayerWithSquaredError extends Layer {\n+  override val weightSize = 0\n+  override def outputSize(inputSize: Int): Int = inputSize\n+  override val inPlace = true\n+  override def model(weights: BDV[Double]): LayerModel = new SigmoidLayerModelWithSquaredError()\n+  override def initModel(weights: BDV[Double], random: Random): LayerModel =\n+    new SigmoidLayerModelWithSquaredError()\n+}\n+\n+private[ann] class SigmoidLayerModelWithSquaredError\n+  extends FunctionalLayerModel(new FunctionalLayer(new SigmoidFunction)) with LossFunction {\n+  override def loss(output: BDM[Double], target: BDM[Double], delta: BDM[Double]): Double = {\n+    UniversalFunction(output, target, delta, (o: Double, t: Double) => o - t)\n+    val error = Bsum(delta :* delta) / 2 / output.cols\n+    UniversalFunction(delta, output, delta, (x: Double, o: Double) => x * (o - o * o))\n+    error\n+  }\n+}\n+\n+private[ann] class SoftmaxLayerWithCrossEntropyLoss extends Layer {\n+  override val weightSize = 0\n+  override def outputSize(inputSize: Int): Int = inputSize\n+  override val inPlace = true\n+  override def model(weights: BDV[Double]): LayerModel =\n+    new SoftmaxLayerModelWithCrossEntropyLoss()\n+  override def initModel(weights: BDV[Double], random: Random): LayerModel =\n+    new SoftmaxLayerModelWithCrossEntropyLoss()\n+}\n+\n+private[ann] class SoftmaxLayerModelWithCrossEntropyLoss extends LayerModel with LossFunction {\n+\n+  val weights = new BDV[Double](0)\n+\n+  def inplaceEval(x: BDM[Double], y: BDM[Double]): Unit = {\n+    var j = 0\n+    // find max value to make sure later that exponent is computable\n+    while (j < x.cols) {\n+      var i = 0\n+      var max = Double.MinValue\n+      while (i < x.rows) {\n+        if (x(i, j) > max) {\n+          max = x(i, j)\n+        }\n+        i += 1\n+      }\n+      var sum = 0.0\n+      i = 0\n+      while (i < x.rows) {\n+        val res = Math.exp(x(i, j) - max)"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "`math`\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:08Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.ann\n+\n+import java.util.Random\n+\n+import breeze.linalg.{sum => Bsum, DenseMatrix => BDM, DenseVector => BDV}\n+import breeze.numerics.{log => Blog}\n+\n+/**\n+ * Trait for loss function\n+ */\n+private[ann] trait LossFunction {\n+  /**\n+   * Loss function\n+   * @param output actual output\n+   * @param target target output\n+   * @param delta output delta to write to\n+   * @return\n+   */\n+  def loss(output: BDM[Double], target: BDM[Double], delta: BDM[Double]): Double\n+}\n+\n+private[ann] class SigmoidLayerWithSquaredError extends Layer {\n+  override val weightSize = 0\n+  override def outputSize(inputSize: Int): Int = inputSize\n+  override val inPlace = true\n+  override def model(weights: BDV[Double]): LayerModel = new SigmoidLayerModelWithSquaredError()\n+  override def initModel(weights: BDV[Double], random: Random): LayerModel =\n+    new SigmoidLayerModelWithSquaredError()\n+}\n+\n+private[ann] class SigmoidLayerModelWithSquaredError\n+  extends FunctionalLayerModel(new FunctionalLayer(new SigmoidFunction)) with LossFunction {\n+  override def loss(output: BDM[Double], target: BDM[Double], delta: BDM[Double]): Double = {\n+    UniversalFunction(output, target, delta, (o: Double, t: Double) => o - t)\n+    val error = Bsum(delta :* delta) / 2 / output.cols\n+    UniversalFunction(delta, output, delta, (x: Double, o: Double) => x * (o - o * o))\n+    error\n+  }\n+}\n+\n+private[ann] class SoftmaxLayerWithCrossEntropyLoss extends Layer {\n+  override val weightSize = 0\n+  override def outputSize(inputSize: Int): Int = inputSize\n+  override val inPlace = true\n+  override def model(weights: BDV[Double]): LayerModel =\n+    new SoftmaxLayerModelWithCrossEntropyLoss()\n+  override def initModel(weights: BDV[Double], random: Random): LayerModel =\n+    new SoftmaxLayerModelWithCrossEntropyLoss()\n+}\n+\n+private[ann] class SoftmaxLayerModelWithCrossEntropyLoss extends LayerModel with LossFunction {\n+\n+  val weights = new BDV[Double](0)\n+\n+  def inplaceEval(x: BDM[Double], y: BDM[Double]): Unit = {\n+    var j = 0\n+    // find max value to make sure later that exponent is computable\n+    while (j < x.cols) {\n+      var i = 0\n+      var max = Double.MinValue\n+      while (i < x.rows) {\n+        if (x(i, j) > max) {\n+          max = x(i, j)\n+        }\n+        i += 1\n+      }\n+      var sum = 0.0\n+      i = 0\n+      while (i < x.rows) {\n+        val res = Math.exp(x(i, j) - max)"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Need comment on the implementation (inside `{...}`)\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:10Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.ann\n+\n+import java.util.Random\n+\n+import breeze.linalg.{sum => Bsum, DenseMatrix => BDM, DenseVector => BDV}\n+import breeze.numerics.{log => Blog}\n+\n+/**\n+ * Trait for loss function\n+ */\n+private[ann] trait LossFunction {\n+  /**\n+   * Loss function\n+   * @param output actual output\n+   * @param target target output\n+   * @param delta output delta to write to\n+   * @return\n+   */\n+  def loss(output: BDM[Double], target: BDM[Double], delta: BDM[Double]): Double\n+}\n+\n+private[ann] class SigmoidLayerWithSquaredError extends Layer {\n+  override val weightSize = 0\n+  override def outputSize(inputSize: Int): Int = inputSize\n+  override val inPlace = true\n+  override def model(weights: BDV[Double]): LayerModel = new SigmoidLayerModelWithSquaredError()\n+  override def initModel(weights: BDV[Double], random: Random): LayerModel =\n+    new SigmoidLayerModelWithSquaredError()\n+}\n+\n+private[ann] class SigmoidLayerModelWithSquaredError\n+  extends FunctionalLayerModel(new FunctionalLayer(new SigmoidFunction)) with LossFunction {\n+  override def loss(output: BDM[Double], target: BDM[Double], delta: BDM[Double]): Double = {\n+    UniversalFunction(output, target, delta, (o: Double, t: Double) => o - t)\n+    val error = Bsum(delta :* delta) / 2 / output.cols\n+    UniversalFunction(delta, output, delta, (x: Double, o: Double) => x * (o - o * o))\n+    error\n+  }\n+}\n+\n+private[ann] class SoftmaxLayerWithCrossEntropyLoss extends Layer {\n+  override val weightSize = 0\n+  override def outputSize(inputSize: Int): Int = inputSize\n+  override val inPlace = true\n+  override def model(weights: BDV[Double]): LayerModel =\n+    new SoftmaxLayerModelWithCrossEntropyLoss()\n+  override def initModel(weights: BDV[Double], random: Random): LayerModel =\n+    new SoftmaxLayerModelWithCrossEntropyLoss()\n+}\n+\n+private[ann] class SoftmaxLayerModelWithCrossEntropyLoss extends LayerModel with LossFunction {\n+\n+  val weights = new BDV[Double](0)\n+\n+  def inplaceEval(x: BDM[Double], y: BDM[Double]): Unit = {\n+    var j = 0\n+    // find max value to make sure later that exponent is computable\n+    while (j < x.cols) {\n+      var i = 0\n+      var max = Double.MinValue\n+      while (i < x.rows) {\n+        if (x(i, j) > max) {\n+          max = x(i, j)\n+        }\n+        i += 1\n+      }\n+      var sum = 0.0\n+      i = 0\n+      while (i < x.rows) {\n+        val res = Math.exp(x(i, j) - max)\n+        y(i, j) = res\n+        sum += res\n+        i += 1\n+      }\n+      i = 0\n+      while (i < x.rows) {\n+        y(i, j) /= sum\n+        i += 1\n+      }\n+      j += 1\n+    }\n+  }\n+\n+  override def eval(data: BDM[Double], output: BDM[Double]): Unit = {\n+    inplaceEval(data, output)\n+  }\n+  override def prevDelta(nextDelta: BDM[Double], input: BDM[Double], delta: BDM[Double]): Unit = {}"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "ditto\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:11Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.ann\n+\n+import java.util.Random\n+\n+import breeze.linalg.{sum => Bsum, DenseMatrix => BDM, DenseVector => BDV}\n+import breeze.numerics.{log => Blog}\n+\n+/**\n+ * Trait for loss function\n+ */\n+private[ann] trait LossFunction {\n+  /**\n+   * Loss function\n+   * @param output actual output\n+   * @param target target output\n+   * @param delta output delta to write to\n+   * @return\n+   */\n+  def loss(output: BDM[Double], target: BDM[Double], delta: BDM[Double]): Double\n+}\n+\n+private[ann] class SigmoidLayerWithSquaredError extends Layer {\n+  override val weightSize = 0\n+  override def outputSize(inputSize: Int): Int = inputSize\n+  override val inPlace = true\n+  override def model(weights: BDV[Double]): LayerModel = new SigmoidLayerModelWithSquaredError()\n+  override def initModel(weights: BDV[Double], random: Random): LayerModel =\n+    new SigmoidLayerModelWithSquaredError()\n+}\n+\n+private[ann] class SigmoidLayerModelWithSquaredError\n+  extends FunctionalLayerModel(new FunctionalLayer(new SigmoidFunction)) with LossFunction {\n+  override def loss(output: BDM[Double], target: BDM[Double], delta: BDM[Double]): Double = {\n+    UniversalFunction(output, target, delta, (o: Double, t: Double) => o - t)\n+    val error = Bsum(delta :* delta) / 2 / output.cols\n+    UniversalFunction(delta, output, delta, (x: Double, o: Double) => x * (o - o * o))\n+    error\n+  }\n+}\n+\n+private[ann] class SoftmaxLayerWithCrossEntropyLoss extends Layer {\n+  override val weightSize = 0\n+  override def outputSize(inputSize: Int): Int = inputSize\n+  override val inPlace = true\n+  override def model(weights: BDV[Double]): LayerModel =\n+    new SoftmaxLayerModelWithCrossEntropyLoss()\n+  override def initModel(weights: BDV[Double], random: Random): LayerModel =\n+    new SoftmaxLayerModelWithCrossEntropyLoss()\n+}\n+\n+private[ann] class SoftmaxLayerModelWithCrossEntropyLoss extends LayerModel with LossFunction {\n+\n+  val weights = new BDV[Double](0)\n+\n+  def inplaceEval(x: BDM[Double], y: BDM[Double]): Unit = {\n+    var j = 0\n+    // find max value to make sure later that exponent is computable\n+    while (j < x.cols) {\n+      var i = 0\n+      var max = Double.MinValue\n+      while (i < x.rows) {\n+        if (x(i, j) > max) {\n+          max = x(i, j)\n+        }\n+        i += 1\n+      }\n+      var sum = 0.0\n+      i = 0\n+      while (i < x.rows) {\n+        val res = Math.exp(x(i, j) - max)\n+        y(i, j) = res\n+        sum += res\n+        i += 1\n+      }\n+      i = 0\n+      while (i < x.rows) {\n+        y(i, j) /= sum\n+        i += 1\n+      }\n+      j += 1\n+    }\n+  }\n+\n+  override def eval(data: BDM[Double], output: BDM[Double]): Unit = {\n+    inplaceEval(data, output)\n+  }\n+  override def prevDelta(nextDelta: BDM[Double], input: BDM[Double], delta: BDM[Double]): Unit = {}\n+\n+  override def grad(delta: BDM[Double], input: BDM[Double], cumGrad: BDV[Double]): Unit = {}"
  }],
  "prId": 9229
}]