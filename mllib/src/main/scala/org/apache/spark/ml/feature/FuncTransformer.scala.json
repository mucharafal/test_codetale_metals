[{
  "comments": [{
    "author": {
      "login": "hhbyyh"
    },
    "body": "After loading, the `FuncTransformer[Nothing, Nothing]` will cause warning during `validateInputType`. The warning will not interfere with `transform` process, but it would be better if we can fully restore the `FuncTransformer` with original Type. \r\n\r\nI'll keep checking, but any suggestion is welcome.",
    "commit": "2f436f4f78ae61fae4b8e5cd66066e2aa800c26b",
    "createdAt": "2017-06-12T05:29:59Z",
    "diffHunk": "@@ -0,0 +1,150 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import java.io.{ByteArrayInputStream, ByteArrayOutputStream, ObjectInputStream, ObjectOutputStream}\n+\n+import scala.reflect.runtime.universe.TypeTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{DeveloperApi, Since}\n+import org.apache.spark.ml.UnaryTransformer\n+import org.apache.spark.ml.feature.FuncTransformer.FuncTransformerWriter\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.util._\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.catalyst.ScalaReflection\n+import org.apache.spark.sql.types.DataType\n+\n+/**\n+ * :: DeveloperApi ::\n+ * FuncTransformer allows easily creation of a custom feature transformer for DataFrame, such like\n+ * conditional conversion(if...else...), type conversion, array indexing and many string ops.\n+ * Note that FuncTransformer supports serialization via scala ObjectOutputStream and may not\n+ * guarantee save/load compatibility between different scala version.\n+ */\n+@DeveloperApi\n+@Since(\"2.3.0\")\n+class FuncTransformer [IN: TypeTag, OUT: TypeTag] @Since(\"2.3.0\") (\n+    @Since(\"2.3.0\") override val uid: String,\n+    @Since(\"2.3.0\") val func: IN => OUT,\n+    @Since(\"2.3.0\") val outputDataType: DataType\n+  ) extends UnaryTransformer[IN, OUT, FuncTransformer[IN, OUT]] with DefaultParamsWritable {\n+\n+  /**\n+   * Create a FuncTransformer with specific function and output data type.\n+   * @param fx function which converts an input object to output object.\n+   * @param outputDataType specific output data type\n+   */\n+  @Since(\"2.3.0\")\n+  def this(fx: IN => OUT, outputDataType: DataType) =\n+    this(Identifiable.randomUID(\"FuncTransformer\"), fx, outputDataType)\n+\n+  /**\n+   * Create a FuncTransformer with specific function and automatically infer the output data type.\n+   * If the output data type cannot be automatically inferred, an exception will be thrown.\n+   * @param fx function which converts an input object to output object.\n+   */\n+  @Since(\"2.3.0\")\n+  def this(fx: IN => OUT) = this(Identifiable.randomUID(\"FuncTransformer\"), fx,\n+    try {\n+      ScalaReflection.schemaFor[OUT].dataType\n+    } catch {\n+      case _: UnsupportedOperationException => throw new UnsupportedOperationException(\n+        s\"FuncTransformer outputDataType cannot be automatically inferred, please try\" +\n+          s\" the constructor with specific outputDataType\")\n+    }\n+   )\n+\n+  setDefault(inputCol -> \"input\", outputCol -> \"output\")\n+\n+  @Since(\"2.3.0\")\n+  override def createTransformFunc: IN => OUT = func\n+\n+  @Since(\"2.3.0\")\n+  override def write: MLWriter = new FuncTransformerWriter(\n+    this.asInstanceOf[FuncTransformer[Nothing, Nothing]])\n+\n+  @Since(\"2.3.0\")\n+  override def copy(extra: ParamMap): FuncTransformer[IN, OUT] = {\n+    copyValues(new FuncTransformer(uid, func, outputDataType), extra)\n+  }\n+\n+  override protected def validateInputType(inputType: DataType): Unit = {\n+    try {\n+      val funcINType = ScalaReflection.schemaFor[IN].dataType\n+      require(inputType.equals(funcINType),\n+        s\"$uid only accept input type $funcINType but got $inputType.\")\n+    } catch {\n+      case _: UnsupportedOperationException =>\n+        // cannot infer the output data type, log warning but do not block transform\n+        logWarning(s\"FuncTransformer input Type cannot be automatically inferred,\" +\n+          s\"Type check omitted for $uid\")\n+    }\n+  }\n+}\n+\n+/**\n+ * :: DeveloperApi ::\n+ * Companion object for FuncTransformer with save and load function.\n+ */\n+@DeveloperApi\n+@Since(\"2.3.0\")\n+object FuncTransformer extends DefaultParamsReadable[FuncTransformer[Nothing, Nothing]] {\n+\n+  private[FuncTransformer]\n+  class FuncTransformerWriter(instance: FuncTransformer[Nothing, Nothing]) extends MLWriter {\n+\n+    private case class Data(func: Array[Byte], dataType: String)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val bo = new ByteArrayOutputStream()\n+      new ObjectOutputStream(bo).writeObject(instance.func)\n+      val data = Data(bo.toByteArray, instance.outputDataType.json)\n+      val dataPath = new Path(path, \"data\").toString\n+      sparkSession.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class FuncTransformerReader extends MLReader[FuncTransformer[Nothing, Nothing]] {\n+\n+    private val className = classOf[FuncTransformer[Nothing, Nothing]].getName\n+\n+    override def load(path: String): FuncTransformer[Nothing, Nothing] = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sparkSession.read.parquet(dataPath)\n+      val Row(funcBytes: Array[Byte], dataType: String) = data\n+          .select(\"func\", \"dataType\")\n+          .head()\n+      val func = new ObjectInputStream(new ByteArrayInputStream(funcBytes)).readObject()\n+      val model = new FuncTransformer(\n+        metadata.uid, func.asInstanceOf[Function[Any, Any]], DataType.fromJson(dataType))\n+      DefaultParamsReader.getAndSetParams(model, metadata)\n+      model.asInstanceOf[FuncTransformer[Nothing, Nothing]]"
  }],
  "prId": 17583
}, {
  "comments": [{
    "author": {
      "login": "WeichenXu123"
    },
    "body": "Save a function object seems to be unsafe ?\r\nWhat I concern includes:\r\n1) When func is a closure, can it handle correctly?\r\n2) After save the transformer, when loading model in another spark app, it seems we still need this function class dependency in the new app ?",
    "commit": "2f436f4f78ae61fae4b8e5cd66066e2aa800c26b",
    "createdAt": "2017-08-08T22:44:19Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import java.io.{ByteArrayInputStream, ByteArrayOutputStream, ObjectInputStream, ObjectOutputStream}\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.Transformer\n+import org.apache.spark.ml.feature.FuncTransformer.FuncTransformerWriter\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.sql.{DataFrame, Dataset, Row}\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * :: Experimental ::\n+ * FuncTransformer helps create a custom feature transformer easily for DataFrame, such like\n+ * conditional conversion(if...else...), type conversion, array indexing and many string ops.\n+ * Note that FuncTransformer supports serialization via Scala ObjectOutputStream and may not\n+ * guarantee save/load compatibility between different Scala version.\n+ * @param func a custom UserDefinedFunction to map from inputCol to outputCol e.g.\n+ *             udf { (i: Double) => i + 1 }. Only udf with one input is supported for now.\n+ */\n+@Experimental\n+@Since(\"2.3.0\")\n+class FuncTransformer @Since(\"2.3.0\") (\n+    @Since(\"2.3.0\") override val uid: String,\n+    @Since(\"2.3.0\") val func: UserDefinedFunction\n+  ) extends Transformer with HasInputCol with HasOutputCol with DefaultParamsWritable {\n+\n+  @Since(\"2.3.0\")\n+  def this(func: UserDefinedFunction) = this(Identifiable.randomUID(\"FuncTransformer\"), func)\n+\n+  setDefault(inputCol -> \"input\", outputCol -> \"output\")\n+\n+  /** @group setParam */\n+  @Since(\"2.3.0\")\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.3.0\")\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  @Since(\"2.3.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    dataset.withColumn($(outputCol), func(col($(inputCol))))\n+  }\n+\n+  @Since(\"2.3.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    func.inputTypes match {\n+      case Some(funcInputType) =>\n+        require(funcInputType.length == 1, \"FuncTransformer only supports udf with one input\")\n+        val dataType = schema($(inputCol)).dataType\n+        require(dataType == funcInputType.head, s\"data type mismatch: udf input type\" +\n+          s\" ${funcInputType.head}; inputCol ${$(inputCol)} data type $dataType \")\n+      case None =>\n+        val dataType = schema($(inputCol)).dataType\n+        require(dataType.isInstanceOf[StructType], s\"When func input types is None,\" +\n+          s\" FuncTransformer only supports StructType. ${$(inputCol)} is $dataType\")\n+    }\n+    val outputFields = schema.fields :+ StructField($(outputCol), func.dataType, false)\n+    StructType(outputFields)\n+  }\n+\n+  @Since(\"2.3.0\")\n+  override def copy(extra: ParamMap): FuncTransformer = {\n+    val copied = new FuncTransformer(uid, func)\n+    copyValues(copied, extra)\n+  }\n+\n+  @Since(\"2.3.0\")\n+  override def write: MLWriter = new FuncTransformerWriter(this)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Companion object for FuncTransformer with save and load function.\n+ */\n+@Experimental\n+@Since(\"2.3.0\")\n+object FuncTransformer extends DefaultParamsReadable[FuncTransformer] {\n+\n+  private[FuncTransformer]\n+  class FuncTransformerWriter(instance: FuncTransformer) extends MLWriter {\n+\n+    private case class Data(func: Array[Byte])\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val bo = new ByteArrayOutputStream()\n+      new ObjectOutputStream(bo).writeObject(instance.func)",
    "line": 113
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "Thanks @WeichenXu123 Those are good questions and at least we should cover the questions in the document.\r\n\r\n1. I will test it, but perhaps with a small closure.\r\n2. I imagine we'll still need the class dependency, yet IMO it's an expected behavior.\r\n\r\nI'd like to learn if there's a better suggestion about how to save the udf object?",
    "commit": "2f436f4f78ae61fae4b8e5cd66066e2aa800c26b",
    "createdAt": "2017-08-08T23:29:08Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import java.io.{ByteArrayInputStream, ByteArrayOutputStream, ObjectInputStream, ObjectOutputStream}\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.Transformer\n+import org.apache.spark.ml.feature.FuncTransformer.FuncTransformerWriter\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.sql.{DataFrame, Dataset, Row}\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * :: Experimental ::\n+ * FuncTransformer helps create a custom feature transformer easily for DataFrame, such like\n+ * conditional conversion(if...else...), type conversion, array indexing and many string ops.\n+ * Note that FuncTransformer supports serialization via Scala ObjectOutputStream and may not\n+ * guarantee save/load compatibility between different Scala version.\n+ * @param func a custom UserDefinedFunction to map from inputCol to outputCol e.g.\n+ *             udf { (i: Double) => i + 1 }. Only udf with one input is supported for now.\n+ */\n+@Experimental\n+@Since(\"2.3.0\")\n+class FuncTransformer @Since(\"2.3.0\") (\n+    @Since(\"2.3.0\") override val uid: String,\n+    @Since(\"2.3.0\") val func: UserDefinedFunction\n+  ) extends Transformer with HasInputCol with HasOutputCol with DefaultParamsWritable {\n+\n+  @Since(\"2.3.0\")\n+  def this(func: UserDefinedFunction) = this(Identifiable.randomUID(\"FuncTransformer\"), func)\n+\n+  setDefault(inputCol -> \"input\", outputCol -> \"output\")\n+\n+  /** @group setParam */\n+  @Since(\"2.3.0\")\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.3.0\")\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  @Since(\"2.3.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    dataset.withColumn($(outputCol), func(col($(inputCol))))\n+  }\n+\n+  @Since(\"2.3.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    func.inputTypes match {\n+      case Some(funcInputType) =>\n+        require(funcInputType.length == 1, \"FuncTransformer only supports udf with one input\")\n+        val dataType = schema($(inputCol)).dataType\n+        require(dataType == funcInputType.head, s\"data type mismatch: udf input type\" +\n+          s\" ${funcInputType.head}; inputCol ${$(inputCol)} data type $dataType \")\n+      case None =>\n+        val dataType = schema($(inputCol)).dataType\n+        require(dataType.isInstanceOf[StructType], s\"When func input types is None,\" +\n+          s\" FuncTransformer only supports StructType. ${$(inputCol)} is $dataType\")\n+    }\n+    val outputFields = schema.fields :+ StructField($(outputCol), func.dataType, false)\n+    StructType(outputFields)\n+  }\n+\n+  @Since(\"2.3.0\")\n+  override def copy(extra: ParamMap): FuncTransformer = {\n+    val copied = new FuncTransformer(uid, func)\n+    copyValues(copied, extra)\n+  }\n+\n+  @Since(\"2.3.0\")\n+  override def write: MLWriter = new FuncTransformerWriter(this)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Companion object for FuncTransformer with save and load function.\n+ */\n+@Experimental\n+@Since(\"2.3.0\")\n+object FuncTransformer extends DefaultParamsReadable[FuncTransformer] {\n+\n+  private[FuncTransformer]\n+  class FuncTransformerWriter(instance: FuncTransformer) extends MLWriter {\n+\n+    private case class Data(func: Array[Byte])\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val bo = new ByteArrayOutputStream()\n+      new ObjectOutputStream(bo).writeObject(instance.func)",
    "line": 113
  }, {
    "author": {
      "login": "WeichenXu123"
    },
    "body": "cc @liancheng @cloud-fan Any suggestions/potential risks about saving udf ?",
    "commit": "2f436f4f78ae61fae4b8e5cd66066e2aa800c26b",
    "createdAt": "2017-08-08T23:32:34Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import java.io.{ByteArrayInputStream, ByteArrayOutputStream, ObjectInputStream, ObjectOutputStream}\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.Transformer\n+import org.apache.spark.ml.feature.FuncTransformer.FuncTransformerWriter\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.sql.{DataFrame, Dataset, Row}\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * :: Experimental ::\n+ * FuncTransformer helps create a custom feature transformer easily for DataFrame, such like\n+ * conditional conversion(if...else...), type conversion, array indexing and many string ops.\n+ * Note that FuncTransformer supports serialization via Scala ObjectOutputStream and may not\n+ * guarantee save/load compatibility between different Scala version.\n+ * @param func a custom UserDefinedFunction to map from inputCol to outputCol e.g.\n+ *             udf { (i: Double) => i + 1 }. Only udf with one input is supported for now.\n+ */\n+@Experimental\n+@Since(\"2.3.0\")\n+class FuncTransformer @Since(\"2.3.0\") (\n+    @Since(\"2.3.0\") override val uid: String,\n+    @Since(\"2.3.0\") val func: UserDefinedFunction\n+  ) extends Transformer with HasInputCol with HasOutputCol with DefaultParamsWritable {\n+\n+  @Since(\"2.3.0\")\n+  def this(func: UserDefinedFunction) = this(Identifiable.randomUID(\"FuncTransformer\"), func)\n+\n+  setDefault(inputCol -> \"input\", outputCol -> \"output\")\n+\n+  /** @group setParam */\n+  @Since(\"2.3.0\")\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.3.0\")\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  @Since(\"2.3.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    dataset.withColumn($(outputCol), func(col($(inputCol))))\n+  }\n+\n+  @Since(\"2.3.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    func.inputTypes match {\n+      case Some(funcInputType) =>\n+        require(funcInputType.length == 1, \"FuncTransformer only supports udf with one input\")\n+        val dataType = schema($(inputCol)).dataType\n+        require(dataType == funcInputType.head, s\"data type mismatch: udf input type\" +\n+          s\" ${funcInputType.head}; inputCol ${$(inputCol)} data type $dataType \")\n+      case None =>\n+        val dataType = schema($(inputCol)).dataType\n+        require(dataType.isInstanceOf[StructType], s\"When func input types is None,\" +\n+          s\" FuncTransformer only supports StructType. ${$(inputCol)} is $dataType\")\n+    }\n+    val outputFields = schema.fields :+ StructField($(outputCol), func.dataType, false)\n+    StructType(outputFields)\n+  }\n+\n+  @Since(\"2.3.0\")\n+  override def copy(extra: ParamMap): FuncTransformer = {\n+    val copied = new FuncTransformer(uid, func)\n+    copyValues(copied, extra)\n+  }\n+\n+  @Since(\"2.3.0\")\n+  override def write: MLWriter = new FuncTransformerWriter(this)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Companion object for FuncTransformer with save and load function.\n+ */\n+@Experimental\n+@Since(\"2.3.0\")\n+object FuncTransformer extends DefaultParamsReadable[FuncTransformer] {\n+\n+  private[FuncTransformer]\n+  class FuncTransformerWriter(instance: FuncTransformer) extends MLWriter {\n+\n+    private case class Data(func: Array[Byte])\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val bo = new ByteArrayOutputStream()\n+      new ObjectOutputStream(bo).writeObject(instance.func)",
    "line": 113
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "unit test added for closure",
    "commit": "2f436f4f78ae61fae4b8e5cd66066e2aa800c26b",
    "createdAt": "2017-08-11T01:35:12Z",
    "diffHunk": "@@ -0,0 +1,141 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import java.io.{ByteArrayInputStream, ByteArrayOutputStream, ObjectInputStream, ObjectOutputStream}\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.Transformer\n+import org.apache.spark.ml.feature.FuncTransformer.FuncTransformerWriter\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.sql.{DataFrame, Dataset, Row}\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * :: Experimental ::\n+ * FuncTransformer helps create a custom feature transformer easily for DataFrame, such like\n+ * conditional conversion(if...else...), type conversion, array indexing and many string ops.\n+ * Note that FuncTransformer supports serialization via Scala ObjectOutputStream and may not\n+ * guarantee save/load compatibility between different Scala version.\n+ * @param func a custom UserDefinedFunction to map from inputCol to outputCol e.g.\n+ *             udf { (i: Double) => i + 1 }. Only udf with one input is supported for now.\n+ */\n+@Experimental\n+@Since(\"2.3.0\")\n+class FuncTransformer @Since(\"2.3.0\") (\n+    @Since(\"2.3.0\") override val uid: String,\n+    @Since(\"2.3.0\") val func: UserDefinedFunction\n+  ) extends Transformer with HasInputCol with HasOutputCol with DefaultParamsWritable {\n+\n+  @Since(\"2.3.0\")\n+  def this(func: UserDefinedFunction) = this(Identifiable.randomUID(\"FuncTransformer\"), func)\n+\n+  setDefault(inputCol -> \"input\", outputCol -> \"output\")\n+\n+  /** @group setParam */\n+  @Since(\"2.3.0\")\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.3.0\")\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  @Since(\"2.3.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    dataset.withColumn($(outputCol), func(col($(inputCol))))\n+  }\n+\n+  @Since(\"2.3.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    func.inputTypes match {\n+      case Some(funcInputType) =>\n+        require(funcInputType.length == 1, \"FuncTransformer only supports udf with one input\")\n+        val dataType = schema($(inputCol)).dataType\n+        require(dataType == funcInputType.head, s\"data type mismatch: udf input type\" +\n+          s\" ${funcInputType.head}; inputCol ${$(inputCol)} data type $dataType \")\n+      case None =>\n+        val dataType = schema($(inputCol)).dataType\n+        require(dataType.isInstanceOf[StructType], s\"When func input types is None,\" +\n+          s\" FuncTransformer only supports StructType. ${$(inputCol)} is $dataType\")\n+    }\n+    val outputFields = schema.fields :+ StructField($(outputCol), func.dataType, false)\n+    StructType(outputFields)\n+  }\n+\n+  @Since(\"2.3.0\")\n+  override def copy(extra: ParamMap): FuncTransformer = {\n+    val copied = new FuncTransformer(uid, func)\n+    copyValues(copied, extra)\n+  }\n+\n+  @Since(\"2.3.0\")\n+  override def write: MLWriter = new FuncTransformerWriter(this)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Companion object for FuncTransformer with save and load function.\n+ */\n+@Experimental\n+@Since(\"2.3.0\")\n+object FuncTransformer extends DefaultParamsReadable[FuncTransformer] {\n+\n+  private[FuncTransformer]\n+  class FuncTransformerWriter(instance: FuncTransformer) extends MLWriter {\n+\n+    private case class Data(func: Array[Byte])\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val bo = new ByteArrayOutputStream()\n+      new ObjectOutputStream(bo).writeObject(instance.func)",
    "line": 113
  }],
  "prId": 17583
}]