[{
  "comments": [{
    "author": {
      "login": "WeichenXu123"
    },
    "body": "The var name `activeNodes`, `activeNonLeafs` are not accurate I think.\r\nHere the `activeNodes` are actually \"next level nodes\", including \"probably splittable nodes(active nodes)\" and \"leaf nodes\".",
    "commit": "d86dd18e47451c2e4463c68db441f92a898ac765",
    "createdAt": "2017-10-16T08:50:52Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import org.apache.spark.ml.tree._\n+import org.apache.spark.mllib.tree.model.ImpurityStats\n+\n+/** Object exposing methods for local training of decision trees */\n+private[ml] object LocalDecisionTree {\n+\n+  /**\n+   * Fully splits the passed-in node on the provided local dataset, returning\n+   * an InternalNode/LeafNode corresponding to the root of the resulting tree.\n+   *\n+   * @param node LearningNode to use as the root of the subtree fit on the passed-in dataset\n+   * @param metadata learning and dataset metadata for DecisionTree\n+   * @param splits splits(i) = array of splits for feature i\n+   */\n+  private[ml] def fitNode(\n+      input: Array[TreePoint],\n+      instanceWeights: Array[Double],\n+      node: LearningNode,\n+      metadata: DecisionTreeMetadata,\n+      splits: Array[Array[Split]]): Node = {\n+\n+    // The case with 1 node (depth = 0) is handled separately.\n+    // This allows all iterations in the depth > 0 case to use the same code.\n+    // TODO: Check that learning works when maxDepth > 0 but learning stops at 1 node (because of\n+    //       other parameters).\n+    if (metadata.maxDepth == 0) {\n+      return node.toNode\n+    }\n+\n+    // Prepare column store.\n+    //   Note: rowToColumnStoreDense checks to make sure numRows < Int.MaxValue.\n+    val colStoreInit: Array[Array[Int]]\n+    = LocalDecisionTreeUtils.rowToColumnStoreDense(input.map(_.binnedFeatures))\n+    val labels = input.map(_.label)\n+\n+    // Fit a regression model on the dataset, throwing an error if metadata indicates that\n+    // we should train a classifier.\n+    // TODO: Add support for training classifiers\n+    if (metadata.numClasses > 1 && metadata.numClasses <= 32) {\n+      throw new UnsupportedOperationException(\"Local training of a decision tree classifier is \" +\n+        \"unsupported; currently, only regression is supported\")\n+    } else {\n+      trainRegressor(node, colStoreInit, instanceWeights, labels, metadata, splits)\n+    }\n+  }\n+\n+  /**\n+   * Locally fits a decision tree regressor.\n+   * TODO(smurching): Logic for fitting a classifier & regressor is the same; only difference\n+   * is impurity metric. Use the same logic for fitting a classifier.\n+   *\n+   * @param rootNode Node to use as root of the tree fit on the passed-in dataset\n+   * @param colStoreInit Array of columns of training data\n+   * @param instanceWeights Array of weights for each training example\n+   * @param metadata learning and dataset metadata for DecisionTree\n+   * @param splits splits(i) = Array of possible splits for feature i\n+   * @return LeafNode or InternalNode representation of rootNode\n+   */\n+  private[ml] def trainRegressor(\n+      rootNode: LearningNode,\n+      colStoreInit: Array[Array[Int]],\n+      instanceWeights: Array[Double],\n+      labels: Array[Double],\n+      metadata: DecisionTreeMetadata,\n+      splits: Array[Array[Split]]): Node = {\n+\n+    // Sort each column by decision tree node.\n+    val colStore: Array[FeatureVector] = colStoreInit.zipWithIndex.map { case (col, featureIndex) =>\n+      val featureArity: Int = metadata.featureArity.getOrElse(featureIndex, 0)\n+      FeatureVector(featureIndex, featureArity, col)\n+    }\n+\n+    val numRows = colStore.headOption match {\n+      case None => 0\n+      case Some(column) => column.values.length\n+    }\n+\n+    // Create a new PartitionInfo describing the status of our partially-trained subtree\n+    // at each iteration of training\n+    var trainingInfo: TrainingInfo = TrainingInfo(colStore, instanceWeights,\n+      nodeOffsets = Array[(Int, Int)]((0, numRows)), activeNodes = Array(rootNode))\n+\n+    // Iteratively learn, one level of the tree at a time.\n+    // Note: We do not use node IDs.\n+    var currentLevel = 0\n+    var doneLearning = false\n+\n+    while (currentLevel < metadata.maxDepth && !doneLearning) {\n+      // Splits each active node if possible, returning an array of new active nodes\n+      val activeNodes: Array[LearningNode] =\n+        computeBestSplits(trainingInfo, labels, metadata, splits)\n+      // Filter active node periphery by impurity.\n+      val estimatedRemainingActive = activeNodes.count(_.stats.impurity > 0.0)\n+      // TODO: Check to make sure we split something, and stop otherwise.\n+      doneLearning = currentLevel + 1 >= metadata.maxDepth || estimatedRemainingActive == 0\n+      if (!doneLearning) {\n+        // Obtain a new trainingInfo instance describing our current training status\n+        trainingInfo = trainingInfo.update(splits, activeNodes)\n+      }\n+      currentLevel += 1\n+    }\n+\n+    // Done with learning\n+    rootNode.toNode\n+  }\n+\n+  /**\n+   * Iterate over feature values and labels for a specific (node, feature), updating stats\n+   * aggregator for the current node.\n+   */\n+  private[impl] def updateAggregator(\n+      statsAggregator: DTStatsAggregator,\n+      col: FeatureVector,\n+      instanceWeights: Array[Double],\n+      labels: Array[Double],\n+      from: Int,\n+      to: Int,\n+      featureIndexIdx: Int,\n+      splits: Array[Array[Split]]): Unit = {\n+    val metadata = statsAggregator.metadata\n+    if (metadata.isUnordered(col.featureIndex)) {\n+      from.until(to).foreach { idx =>\n+        val rowIndex = col.indices(idx)\n+        AggUpdateUtils.updateUnorderedFeature(statsAggregator, col.values(idx), labels(rowIndex),\n+          featureIndex = col.featureIndex, featureIndexIdx, splits,\n+          instanceWeight = instanceWeights(rowIndex))\n+      }\n+    } else {\n+      from.until(to).foreach { idx =>\n+        val rowIndex = col.indices(idx)\n+        AggUpdateUtils.updateOrderedFeature(statsAggregator, col.values(idx), labels(rowIndex),\n+          featureIndex = col.featureIndex, featureIndexIdx,\n+          instanceWeight = instanceWeights(rowIndex))\n+      }\n+    }\n+  }\n+\n+  /**\n+   * Find the best splits for all active nodes\n+   *\n+   * @param trainingInfo Contains node offset info for current set of active nodes\n+   * @return  Array of new active nodes formed by splitting the current set of active nodes.\n+   */\n+  private def computeBestSplits(\n+      trainingInfo: TrainingInfo,\n+      labels: Array[Double],\n+      metadata: DecisionTreeMetadata,\n+      splits: Array[Array[Split]]) = {\n+    // For each node, select the best split across all features\n+    trainingInfo match {\n+      case TrainingInfo(columns: Array[FeatureVector], instanceWeights: Array[Double],\n+      nodeOffsets: Array[(Int, Int)], activeNodes: Array[LearningNode]) => {\n+        // Filter out leaf nodes from the previous iteration\n+        val activeNonLeafs = activeNodes.zipWithIndex.filterNot(_._1.isLeaf)\n+        // Iterate over the active nodes in the current level.\n+        activeNonLeafs.flatMap { case (node: LearningNode, nodeIndex: Int) =>"
  }],
  "prId": 19433
}, {
  "comments": [{
    "author": {
      "login": "WeichenXu123"
    },
    "body": "Use `activeNodes.count(_.isLeaf)` instead. Make code simpler.\r\nAnd as mentioned above, the `activeNodes` is better to be renamed to `nextLevelNodes`.",
    "commit": "d86dd18e47451c2e4463c68db441f92a898ac765",
    "createdAt": "2017-10-16T08:54:42Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import org.apache.spark.ml.tree._\n+import org.apache.spark.mllib.tree.model.ImpurityStats\n+\n+/** Object exposing methods for local training of decision trees */\n+private[ml] object LocalDecisionTree {\n+\n+  /**\n+   * Fully splits the passed-in node on the provided local dataset, returning\n+   * an InternalNode/LeafNode corresponding to the root of the resulting tree.\n+   *\n+   * @param node LearningNode to use as the root of the subtree fit on the passed-in dataset\n+   * @param metadata learning and dataset metadata for DecisionTree\n+   * @param splits splits(i) = array of splits for feature i\n+   */\n+  private[ml] def fitNode(\n+      input: Array[TreePoint],\n+      instanceWeights: Array[Double],\n+      node: LearningNode,\n+      metadata: DecisionTreeMetadata,\n+      splits: Array[Array[Split]]): Node = {\n+\n+    // The case with 1 node (depth = 0) is handled separately.\n+    // This allows all iterations in the depth > 0 case to use the same code.\n+    // TODO: Check that learning works when maxDepth > 0 but learning stops at 1 node (because of\n+    //       other parameters).\n+    if (metadata.maxDepth == 0) {\n+      return node.toNode\n+    }\n+\n+    // Prepare column store.\n+    //   Note: rowToColumnStoreDense checks to make sure numRows < Int.MaxValue.\n+    val colStoreInit: Array[Array[Int]]\n+    = LocalDecisionTreeUtils.rowToColumnStoreDense(input.map(_.binnedFeatures))\n+    val labels = input.map(_.label)\n+\n+    // Fit a regression model on the dataset, throwing an error if metadata indicates that\n+    // we should train a classifier.\n+    // TODO: Add support for training classifiers\n+    if (metadata.numClasses > 1 && metadata.numClasses <= 32) {\n+      throw new UnsupportedOperationException(\"Local training of a decision tree classifier is \" +\n+        \"unsupported; currently, only regression is supported\")\n+    } else {\n+      trainRegressor(node, colStoreInit, instanceWeights, labels, metadata, splits)\n+    }\n+  }\n+\n+  /**\n+   * Locally fits a decision tree regressor.\n+   * TODO(smurching): Logic for fitting a classifier & regressor is the same; only difference\n+   * is impurity metric. Use the same logic for fitting a classifier.\n+   *\n+   * @param rootNode Node to use as root of the tree fit on the passed-in dataset\n+   * @param colStoreInit Array of columns of training data\n+   * @param instanceWeights Array of weights for each training example\n+   * @param metadata learning and dataset metadata for DecisionTree\n+   * @param splits splits(i) = Array of possible splits for feature i\n+   * @return LeafNode or InternalNode representation of rootNode\n+   */\n+  private[ml] def trainRegressor(\n+      rootNode: LearningNode,\n+      colStoreInit: Array[Array[Int]],\n+      instanceWeights: Array[Double],\n+      labels: Array[Double],\n+      metadata: DecisionTreeMetadata,\n+      splits: Array[Array[Split]]): Node = {\n+\n+    // Sort each column by decision tree node.\n+    val colStore: Array[FeatureVector] = colStoreInit.zipWithIndex.map { case (col, featureIndex) =>\n+      val featureArity: Int = metadata.featureArity.getOrElse(featureIndex, 0)\n+      FeatureVector(featureIndex, featureArity, col)\n+    }\n+\n+    val numRows = colStore.headOption match {\n+      case None => 0\n+      case Some(column) => column.values.length\n+    }\n+\n+    // Create a new PartitionInfo describing the status of our partially-trained subtree\n+    // at each iteration of training\n+    var trainingInfo: TrainingInfo = TrainingInfo(colStore, instanceWeights,\n+      nodeOffsets = Array[(Int, Int)]((0, numRows)), activeNodes = Array(rootNode))\n+\n+    // Iteratively learn, one level of the tree at a time.\n+    // Note: We do not use node IDs.\n+    var currentLevel = 0\n+    var doneLearning = false\n+\n+    while (currentLevel < metadata.maxDepth && !doneLearning) {\n+      // Splits each active node if possible, returning an array of new active nodes\n+      val activeNodes: Array[LearningNode] =\n+        computeBestSplits(trainingInfo, labels, metadata, splits)\n+      // Filter active node periphery by impurity.\n+      val estimatedRemainingActive = activeNodes.count(_.stats.impurity > 0.0)"
  }, {
    "author": {
      "login": "smurching"
    },
    "body": "Agreed on using `isLeaf` instead of checking for positive impurity, thanks for the suggestion.\r\n\r\nAFAICT at this point in the code `activeNodes` actually does refer to the nodes in the current level; the children of nodes in `activeNodes` are the nodes in the next level, and are returned by `computeBestSplits`. I forgot to include the return type of `computeBestSplit` in its method signature, which probably made this more confusing - my mistake.",
    "commit": "d86dd18e47451c2e4463c68db441f92a898ac765",
    "createdAt": "2017-10-25T00:46:50Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import org.apache.spark.ml.tree._\n+import org.apache.spark.mllib.tree.model.ImpurityStats\n+\n+/** Object exposing methods for local training of decision trees */\n+private[ml] object LocalDecisionTree {\n+\n+  /**\n+   * Fully splits the passed-in node on the provided local dataset, returning\n+   * an InternalNode/LeafNode corresponding to the root of the resulting tree.\n+   *\n+   * @param node LearningNode to use as the root of the subtree fit on the passed-in dataset\n+   * @param metadata learning and dataset metadata for DecisionTree\n+   * @param splits splits(i) = array of splits for feature i\n+   */\n+  private[ml] def fitNode(\n+      input: Array[TreePoint],\n+      instanceWeights: Array[Double],\n+      node: LearningNode,\n+      metadata: DecisionTreeMetadata,\n+      splits: Array[Array[Split]]): Node = {\n+\n+    // The case with 1 node (depth = 0) is handled separately.\n+    // This allows all iterations in the depth > 0 case to use the same code.\n+    // TODO: Check that learning works when maxDepth > 0 but learning stops at 1 node (because of\n+    //       other parameters).\n+    if (metadata.maxDepth == 0) {\n+      return node.toNode\n+    }\n+\n+    // Prepare column store.\n+    //   Note: rowToColumnStoreDense checks to make sure numRows < Int.MaxValue.\n+    val colStoreInit: Array[Array[Int]]\n+    = LocalDecisionTreeUtils.rowToColumnStoreDense(input.map(_.binnedFeatures))\n+    val labels = input.map(_.label)\n+\n+    // Fit a regression model on the dataset, throwing an error if metadata indicates that\n+    // we should train a classifier.\n+    // TODO: Add support for training classifiers\n+    if (metadata.numClasses > 1 && metadata.numClasses <= 32) {\n+      throw new UnsupportedOperationException(\"Local training of a decision tree classifier is \" +\n+        \"unsupported; currently, only regression is supported\")\n+    } else {\n+      trainRegressor(node, colStoreInit, instanceWeights, labels, metadata, splits)\n+    }\n+  }\n+\n+  /**\n+   * Locally fits a decision tree regressor.\n+   * TODO(smurching): Logic for fitting a classifier & regressor is the same; only difference\n+   * is impurity metric. Use the same logic for fitting a classifier.\n+   *\n+   * @param rootNode Node to use as root of the tree fit on the passed-in dataset\n+   * @param colStoreInit Array of columns of training data\n+   * @param instanceWeights Array of weights for each training example\n+   * @param metadata learning and dataset metadata for DecisionTree\n+   * @param splits splits(i) = Array of possible splits for feature i\n+   * @return LeafNode or InternalNode representation of rootNode\n+   */\n+  private[ml] def trainRegressor(\n+      rootNode: LearningNode,\n+      colStoreInit: Array[Array[Int]],\n+      instanceWeights: Array[Double],\n+      labels: Array[Double],\n+      metadata: DecisionTreeMetadata,\n+      splits: Array[Array[Split]]): Node = {\n+\n+    // Sort each column by decision tree node.\n+    val colStore: Array[FeatureVector] = colStoreInit.zipWithIndex.map { case (col, featureIndex) =>\n+      val featureArity: Int = metadata.featureArity.getOrElse(featureIndex, 0)\n+      FeatureVector(featureIndex, featureArity, col)\n+    }\n+\n+    val numRows = colStore.headOption match {\n+      case None => 0\n+      case Some(column) => column.values.length\n+    }\n+\n+    // Create a new PartitionInfo describing the status of our partially-trained subtree\n+    // at each iteration of training\n+    var trainingInfo: TrainingInfo = TrainingInfo(colStore, instanceWeights,\n+      nodeOffsets = Array[(Int, Int)]((0, numRows)), activeNodes = Array(rootNode))\n+\n+    // Iteratively learn, one level of the tree at a time.\n+    // Note: We do not use node IDs.\n+    var currentLevel = 0\n+    var doneLearning = false\n+\n+    while (currentLevel < metadata.maxDepth && !doneLearning) {\n+      // Splits each active node if possible, returning an array of new active nodes\n+      val activeNodes: Array[LearningNode] =\n+        computeBestSplits(trainingInfo, labels, metadata, splits)\n+      // Filter active node periphery by impurity.\n+      val estimatedRemainingActive = activeNodes.count(_.stats.impurity > 0.0)"
  }, {
    "author": {
      "login": "WeichenXu123"
    },
    "body": "Yes. Sorry for confusing you. The change what I said was changing to:\r\n```\r\nval nextLevelNodes: Array[LearningNode] =\r\n        computeBestSplits(trainingInfo, labels, metadata, splits)\r\n```\r\nDoes it look more reasonable ?\r\nAnd change the member name in `trainingInfo`:\r\n`TrainingInfo.activeNodes` ==> `TrainingInfo.currentLevelNodes`",
    "commit": "d86dd18e47451c2e4463c68db441f92a898ac765",
    "createdAt": "2017-10-25T01:32:36Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import org.apache.spark.ml.tree._\n+import org.apache.spark.mllib.tree.model.ImpurityStats\n+\n+/** Object exposing methods for local training of decision trees */\n+private[ml] object LocalDecisionTree {\n+\n+  /**\n+   * Fully splits the passed-in node on the provided local dataset, returning\n+   * an InternalNode/LeafNode corresponding to the root of the resulting tree.\n+   *\n+   * @param node LearningNode to use as the root of the subtree fit on the passed-in dataset\n+   * @param metadata learning and dataset metadata for DecisionTree\n+   * @param splits splits(i) = array of splits for feature i\n+   */\n+  private[ml] def fitNode(\n+      input: Array[TreePoint],\n+      instanceWeights: Array[Double],\n+      node: LearningNode,\n+      metadata: DecisionTreeMetadata,\n+      splits: Array[Array[Split]]): Node = {\n+\n+    // The case with 1 node (depth = 0) is handled separately.\n+    // This allows all iterations in the depth > 0 case to use the same code.\n+    // TODO: Check that learning works when maxDepth > 0 but learning stops at 1 node (because of\n+    //       other parameters).\n+    if (metadata.maxDepth == 0) {\n+      return node.toNode\n+    }\n+\n+    // Prepare column store.\n+    //   Note: rowToColumnStoreDense checks to make sure numRows < Int.MaxValue.\n+    val colStoreInit: Array[Array[Int]]\n+    = LocalDecisionTreeUtils.rowToColumnStoreDense(input.map(_.binnedFeatures))\n+    val labels = input.map(_.label)\n+\n+    // Fit a regression model on the dataset, throwing an error if metadata indicates that\n+    // we should train a classifier.\n+    // TODO: Add support for training classifiers\n+    if (metadata.numClasses > 1 && metadata.numClasses <= 32) {\n+      throw new UnsupportedOperationException(\"Local training of a decision tree classifier is \" +\n+        \"unsupported; currently, only regression is supported\")\n+    } else {\n+      trainRegressor(node, colStoreInit, instanceWeights, labels, metadata, splits)\n+    }\n+  }\n+\n+  /**\n+   * Locally fits a decision tree regressor.\n+   * TODO(smurching): Logic for fitting a classifier & regressor is the same; only difference\n+   * is impurity metric. Use the same logic for fitting a classifier.\n+   *\n+   * @param rootNode Node to use as root of the tree fit on the passed-in dataset\n+   * @param colStoreInit Array of columns of training data\n+   * @param instanceWeights Array of weights for each training example\n+   * @param metadata learning and dataset metadata for DecisionTree\n+   * @param splits splits(i) = Array of possible splits for feature i\n+   * @return LeafNode or InternalNode representation of rootNode\n+   */\n+  private[ml] def trainRegressor(\n+      rootNode: LearningNode,\n+      colStoreInit: Array[Array[Int]],\n+      instanceWeights: Array[Double],\n+      labels: Array[Double],\n+      metadata: DecisionTreeMetadata,\n+      splits: Array[Array[Split]]): Node = {\n+\n+    // Sort each column by decision tree node.\n+    val colStore: Array[FeatureVector] = colStoreInit.zipWithIndex.map { case (col, featureIndex) =>\n+      val featureArity: Int = metadata.featureArity.getOrElse(featureIndex, 0)\n+      FeatureVector(featureIndex, featureArity, col)\n+    }\n+\n+    val numRows = colStore.headOption match {\n+      case None => 0\n+      case Some(column) => column.values.length\n+    }\n+\n+    // Create a new PartitionInfo describing the status of our partially-trained subtree\n+    // at each iteration of training\n+    var trainingInfo: TrainingInfo = TrainingInfo(colStore, instanceWeights,\n+      nodeOffsets = Array[(Int, Int)]((0, numRows)), activeNodes = Array(rootNode))\n+\n+    // Iteratively learn, one level of the tree at a time.\n+    // Note: We do not use node IDs.\n+    var currentLevel = 0\n+    var doneLearning = false\n+\n+    while (currentLevel < metadata.maxDepth && !doneLearning) {\n+      // Splits each active node if possible, returning an array of new active nodes\n+      val activeNodes: Array[LearningNode] =\n+        computeBestSplits(trainingInfo, labels, metadata, splits)\n+      // Filter active node periphery by impurity.\n+      val estimatedRemainingActive = activeNodes.count(_.stats.impurity > 0.0)"
  }, {
    "author": {
      "login": "smurching"
    },
    "body": "Gotcha agreed on the naming change, how about `currentLevelActiveNodes`? Since only the non-leaf nodes from the current level are included.",
    "commit": "d86dd18e47451c2e4463c68db441f92a898ac765",
    "createdAt": "2017-10-25T20:44:17Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import org.apache.spark.ml.tree._\n+import org.apache.spark.mllib.tree.model.ImpurityStats\n+\n+/** Object exposing methods for local training of decision trees */\n+private[ml] object LocalDecisionTree {\n+\n+  /**\n+   * Fully splits the passed-in node on the provided local dataset, returning\n+   * an InternalNode/LeafNode corresponding to the root of the resulting tree.\n+   *\n+   * @param node LearningNode to use as the root of the subtree fit on the passed-in dataset\n+   * @param metadata learning and dataset metadata for DecisionTree\n+   * @param splits splits(i) = array of splits for feature i\n+   */\n+  private[ml] def fitNode(\n+      input: Array[TreePoint],\n+      instanceWeights: Array[Double],\n+      node: LearningNode,\n+      metadata: DecisionTreeMetadata,\n+      splits: Array[Array[Split]]): Node = {\n+\n+    // The case with 1 node (depth = 0) is handled separately.\n+    // This allows all iterations in the depth > 0 case to use the same code.\n+    // TODO: Check that learning works when maxDepth > 0 but learning stops at 1 node (because of\n+    //       other parameters).\n+    if (metadata.maxDepth == 0) {\n+      return node.toNode\n+    }\n+\n+    // Prepare column store.\n+    //   Note: rowToColumnStoreDense checks to make sure numRows < Int.MaxValue.\n+    val colStoreInit: Array[Array[Int]]\n+    = LocalDecisionTreeUtils.rowToColumnStoreDense(input.map(_.binnedFeatures))\n+    val labels = input.map(_.label)\n+\n+    // Fit a regression model on the dataset, throwing an error if metadata indicates that\n+    // we should train a classifier.\n+    // TODO: Add support for training classifiers\n+    if (metadata.numClasses > 1 && metadata.numClasses <= 32) {\n+      throw new UnsupportedOperationException(\"Local training of a decision tree classifier is \" +\n+        \"unsupported; currently, only regression is supported\")\n+    } else {\n+      trainRegressor(node, colStoreInit, instanceWeights, labels, metadata, splits)\n+    }\n+  }\n+\n+  /**\n+   * Locally fits a decision tree regressor.\n+   * TODO(smurching): Logic for fitting a classifier & regressor is the same; only difference\n+   * is impurity metric. Use the same logic for fitting a classifier.\n+   *\n+   * @param rootNode Node to use as root of the tree fit on the passed-in dataset\n+   * @param colStoreInit Array of columns of training data\n+   * @param instanceWeights Array of weights for each training example\n+   * @param metadata learning and dataset metadata for DecisionTree\n+   * @param splits splits(i) = Array of possible splits for feature i\n+   * @return LeafNode or InternalNode representation of rootNode\n+   */\n+  private[ml] def trainRegressor(\n+      rootNode: LearningNode,\n+      colStoreInit: Array[Array[Int]],\n+      instanceWeights: Array[Double],\n+      labels: Array[Double],\n+      metadata: DecisionTreeMetadata,\n+      splits: Array[Array[Split]]): Node = {\n+\n+    // Sort each column by decision tree node.\n+    val colStore: Array[FeatureVector] = colStoreInit.zipWithIndex.map { case (col, featureIndex) =>\n+      val featureArity: Int = metadata.featureArity.getOrElse(featureIndex, 0)\n+      FeatureVector(featureIndex, featureArity, col)\n+    }\n+\n+    val numRows = colStore.headOption match {\n+      case None => 0\n+      case Some(column) => column.values.length\n+    }\n+\n+    // Create a new PartitionInfo describing the status of our partially-trained subtree\n+    // at each iteration of training\n+    var trainingInfo: TrainingInfo = TrainingInfo(colStore, instanceWeights,\n+      nodeOffsets = Array[(Int, Int)]((0, numRows)), activeNodes = Array(rootNode))\n+\n+    // Iteratively learn, one level of the tree at a time.\n+    // Note: We do not use node IDs.\n+    var currentLevel = 0\n+    var doneLearning = false\n+\n+    while (currentLevel < metadata.maxDepth && !doneLearning) {\n+      // Splits each active node if possible, returning an array of new active nodes\n+      val activeNodes: Array[LearningNode] =\n+        computeBestSplits(trainingInfo, labels, metadata, splits)\n+      // Filter active node periphery by impurity.\n+      val estimatedRemainingActive = activeNodes.count(_.stats.impurity > 0.0)"
  }, {
    "author": {
      "login": "WeichenXu123"
    },
    "body": "Wait... I check the code here: `trainingInfo = trainingInfo.update(splits, activeNodes)` So it seems you do not filter out the leaf node from the \"activeNodes\"(which is actually the `nextLevelNode` I mentioned above).\r\nSo I think `TrainingInfo.activeNodes` is still possible to contains leaf node.\r\n",
    "commit": "d86dd18e47451c2e4463c68db441f92a898ac765",
    "createdAt": "2017-10-26T03:04:53Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import org.apache.spark.ml.tree._\n+import org.apache.spark.mllib.tree.model.ImpurityStats\n+\n+/** Object exposing methods for local training of decision trees */\n+private[ml] object LocalDecisionTree {\n+\n+  /**\n+   * Fully splits the passed-in node on the provided local dataset, returning\n+   * an InternalNode/LeafNode corresponding to the root of the resulting tree.\n+   *\n+   * @param node LearningNode to use as the root of the subtree fit on the passed-in dataset\n+   * @param metadata learning and dataset metadata for DecisionTree\n+   * @param splits splits(i) = array of splits for feature i\n+   */\n+  private[ml] def fitNode(\n+      input: Array[TreePoint],\n+      instanceWeights: Array[Double],\n+      node: LearningNode,\n+      metadata: DecisionTreeMetadata,\n+      splits: Array[Array[Split]]): Node = {\n+\n+    // The case with 1 node (depth = 0) is handled separately.\n+    // This allows all iterations in the depth > 0 case to use the same code.\n+    // TODO: Check that learning works when maxDepth > 0 but learning stops at 1 node (because of\n+    //       other parameters).\n+    if (metadata.maxDepth == 0) {\n+      return node.toNode\n+    }\n+\n+    // Prepare column store.\n+    //   Note: rowToColumnStoreDense checks to make sure numRows < Int.MaxValue.\n+    val colStoreInit: Array[Array[Int]]\n+    = LocalDecisionTreeUtils.rowToColumnStoreDense(input.map(_.binnedFeatures))\n+    val labels = input.map(_.label)\n+\n+    // Fit a regression model on the dataset, throwing an error if metadata indicates that\n+    // we should train a classifier.\n+    // TODO: Add support for training classifiers\n+    if (metadata.numClasses > 1 && metadata.numClasses <= 32) {\n+      throw new UnsupportedOperationException(\"Local training of a decision tree classifier is \" +\n+        \"unsupported; currently, only regression is supported\")\n+    } else {\n+      trainRegressor(node, colStoreInit, instanceWeights, labels, metadata, splits)\n+    }\n+  }\n+\n+  /**\n+   * Locally fits a decision tree regressor.\n+   * TODO(smurching): Logic for fitting a classifier & regressor is the same; only difference\n+   * is impurity metric. Use the same logic for fitting a classifier.\n+   *\n+   * @param rootNode Node to use as root of the tree fit on the passed-in dataset\n+   * @param colStoreInit Array of columns of training data\n+   * @param instanceWeights Array of weights for each training example\n+   * @param metadata learning and dataset metadata for DecisionTree\n+   * @param splits splits(i) = Array of possible splits for feature i\n+   * @return LeafNode or InternalNode representation of rootNode\n+   */\n+  private[ml] def trainRegressor(\n+      rootNode: LearningNode,\n+      colStoreInit: Array[Array[Int]],\n+      instanceWeights: Array[Double],\n+      labels: Array[Double],\n+      metadata: DecisionTreeMetadata,\n+      splits: Array[Array[Split]]): Node = {\n+\n+    // Sort each column by decision tree node.\n+    val colStore: Array[FeatureVector] = colStoreInit.zipWithIndex.map { case (col, featureIndex) =>\n+      val featureArity: Int = metadata.featureArity.getOrElse(featureIndex, 0)\n+      FeatureVector(featureIndex, featureArity, col)\n+    }\n+\n+    val numRows = colStore.headOption match {\n+      case None => 0\n+      case Some(column) => column.values.length\n+    }\n+\n+    // Create a new PartitionInfo describing the status of our partially-trained subtree\n+    // at each iteration of training\n+    var trainingInfo: TrainingInfo = TrainingInfo(colStore, instanceWeights,\n+      nodeOffsets = Array[(Int, Int)]((0, numRows)), activeNodes = Array(rootNode))\n+\n+    // Iteratively learn, one level of the tree at a time.\n+    // Note: We do not use node IDs.\n+    var currentLevel = 0\n+    var doneLearning = false\n+\n+    while (currentLevel < metadata.maxDepth && !doneLearning) {\n+      // Splits each active node if possible, returning an array of new active nodes\n+      val activeNodes: Array[LearningNode] =\n+        computeBestSplits(trainingInfo, labels, metadata, splits)\n+      // Filter active node periphery by impurity.\n+      val estimatedRemainingActive = activeNodes.count(_.stats.impurity > 0.0)"
  }, {
    "author": {
      "login": "smurching"
    },
    "body": "Oh true -- I'll reword the doc for `currentLevelActiveNodes` to say:\r\n```\r\n * @param currentLevelActiveNodes  Nodes which are active (could still be split).\r\n *                                 Inactive nodes are known to be leaves in the final tree.\r\n```",
    "commit": "d86dd18e47451c2e4463c68db441f92a898ac765",
    "createdAt": "2017-10-27T01:45:18Z",
    "diffHunk": "@@ -0,0 +1,250 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import org.apache.spark.ml.tree._\n+import org.apache.spark.mllib.tree.model.ImpurityStats\n+\n+/** Object exposing methods for local training of decision trees */\n+private[ml] object LocalDecisionTree {\n+\n+  /**\n+   * Fully splits the passed-in node on the provided local dataset, returning\n+   * an InternalNode/LeafNode corresponding to the root of the resulting tree.\n+   *\n+   * @param node LearningNode to use as the root of the subtree fit on the passed-in dataset\n+   * @param metadata learning and dataset metadata for DecisionTree\n+   * @param splits splits(i) = array of splits for feature i\n+   */\n+  private[ml] def fitNode(\n+      input: Array[TreePoint],\n+      instanceWeights: Array[Double],\n+      node: LearningNode,\n+      metadata: DecisionTreeMetadata,\n+      splits: Array[Array[Split]]): Node = {\n+\n+    // The case with 1 node (depth = 0) is handled separately.\n+    // This allows all iterations in the depth > 0 case to use the same code.\n+    // TODO: Check that learning works when maxDepth > 0 but learning stops at 1 node (because of\n+    //       other parameters).\n+    if (metadata.maxDepth == 0) {\n+      return node.toNode\n+    }\n+\n+    // Prepare column store.\n+    //   Note: rowToColumnStoreDense checks to make sure numRows < Int.MaxValue.\n+    val colStoreInit: Array[Array[Int]]\n+    = LocalDecisionTreeUtils.rowToColumnStoreDense(input.map(_.binnedFeatures))\n+    val labels = input.map(_.label)\n+\n+    // Fit a regression model on the dataset, throwing an error if metadata indicates that\n+    // we should train a classifier.\n+    // TODO: Add support for training classifiers\n+    if (metadata.numClasses > 1 && metadata.numClasses <= 32) {\n+      throw new UnsupportedOperationException(\"Local training of a decision tree classifier is \" +\n+        \"unsupported; currently, only regression is supported\")\n+    } else {\n+      trainRegressor(node, colStoreInit, instanceWeights, labels, metadata, splits)\n+    }\n+  }\n+\n+  /**\n+   * Locally fits a decision tree regressor.\n+   * TODO(smurching): Logic for fitting a classifier & regressor is the same; only difference\n+   * is impurity metric. Use the same logic for fitting a classifier.\n+   *\n+   * @param rootNode Node to use as root of the tree fit on the passed-in dataset\n+   * @param colStoreInit Array of columns of training data\n+   * @param instanceWeights Array of weights for each training example\n+   * @param metadata learning and dataset metadata for DecisionTree\n+   * @param splits splits(i) = Array of possible splits for feature i\n+   * @return LeafNode or InternalNode representation of rootNode\n+   */\n+  private[ml] def trainRegressor(\n+      rootNode: LearningNode,\n+      colStoreInit: Array[Array[Int]],\n+      instanceWeights: Array[Double],\n+      labels: Array[Double],\n+      metadata: DecisionTreeMetadata,\n+      splits: Array[Array[Split]]): Node = {\n+\n+    // Sort each column by decision tree node.\n+    val colStore: Array[FeatureVector] = colStoreInit.zipWithIndex.map { case (col, featureIndex) =>\n+      val featureArity: Int = metadata.featureArity.getOrElse(featureIndex, 0)\n+      FeatureVector(featureIndex, featureArity, col)\n+    }\n+\n+    val numRows = colStore.headOption match {\n+      case None => 0\n+      case Some(column) => column.values.length\n+    }\n+\n+    // Create a new PartitionInfo describing the status of our partially-trained subtree\n+    // at each iteration of training\n+    var trainingInfo: TrainingInfo = TrainingInfo(colStore, instanceWeights,\n+      nodeOffsets = Array[(Int, Int)]((0, numRows)), activeNodes = Array(rootNode))\n+\n+    // Iteratively learn, one level of the tree at a time.\n+    // Note: We do not use node IDs.\n+    var currentLevel = 0\n+    var doneLearning = false\n+\n+    while (currentLevel < metadata.maxDepth && !doneLearning) {\n+      // Splits each active node if possible, returning an array of new active nodes\n+      val activeNodes: Array[LearningNode] =\n+        computeBestSplits(trainingInfo, labels, metadata, splits)\n+      // Filter active node periphery by impurity.\n+      val estimatedRemainingActive = activeNodes.count(_.stats.impurity > 0.0)"
  }],
  "prId": 19433
}]