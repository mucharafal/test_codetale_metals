[{
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "this has to be `ml` as well.\n",
    "commit": "1c230ae23e17905af9ea9655cbcfb5a948e627a9",
    "createdAt": "2016-04-19T08:28:38Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.linalg.udt\n+\n+import org.apache.spark.ml.linalg.{DenseMatrix, Matrix, SparseMatrix}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.GenericMutableRow\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * User-defined type for [[Matrix]] in [[mllib-local]] which allows easy interaction with SQL\n+ * via [[org.apache.spark.sql.Dataset]].\n+ */\n+private[ml] class MatrixUDT extends UserDefinedType[Matrix] {\n+\n+  override def sqlType: StructType = {\n+    // type: 0 = sparse, 1 = dense\n+    // the dense matrix is built by numRows, numCols, values and isTransposed, all of which are\n+    // set as not nullable, except values since in the future, support for binary matrices might\n+    // be added for which values are not needed.\n+    // the sparse matrix needs colPtrs and rowIndices, which are set as\n+    // null, while building the dense matrix.\n+    StructType(Seq(\n+      StructField(\"type\", ByteType, nullable = false),\n+      StructField(\"numRows\", IntegerType, nullable = false),\n+      StructField(\"numCols\", IntegerType, nullable = false),\n+      StructField(\"colPtrs\", ArrayType(IntegerType, containsNull = false), nullable = true),\n+      StructField(\"rowIndices\", ArrayType(IntegerType, containsNull = false), nullable = true),\n+      StructField(\"values\", ArrayType(DoubleType, containsNull = false), nullable = true),\n+      StructField(\"isTransposed\", BooleanType, nullable = false)\n+      ))\n+  }\n+\n+  override def serialize(obj: Matrix): InternalRow = {\n+    val row = new GenericMutableRow(7)\n+    obj match {\n+      case sm: SparseMatrix =>\n+        row.setByte(0, 0)\n+        row.setInt(1, sm.numRows)\n+        row.setInt(2, sm.numCols)\n+        row.update(3, new GenericArrayData(sm.colPtrs.map(_.asInstanceOf[Any])))\n+        row.update(4, new GenericArrayData(sm.rowIndices.map(_.asInstanceOf[Any])))\n+        row.update(5, new GenericArrayData(sm.values.map(_.asInstanceOf[Any])))\n+        row.setBoolean(6, sm.isTransposed)\n+\n+      case dm: DenseMatrix =>\n+        row.setByte(0, 1)\n+        row.setInt(1, dm.numRows)\n+        row.setInt(2, dm.numCols)\n+        row.setNullAt(3)\n+        row.setNullAt(4)\n+        row.update(5, new GenericArrayData(dm.values.map(_.asInstanceOf[Any])))\n+        row.setBoolean(6, dm.isTransposed)\n+    }\n+    row\n+  }\n+\n+  override def deserialize(datum: Any): Matrix = {\n+    datum match {\n+      case row: InternalRow =>\n+        require(row.numFields == 7,\n+          s\"MatrixUDT.deserialize given row with length ${row.numFields} but requires length == 7\")\n+        val tpe = row.getByte(0)\n+        val numRows = row.getInt(1)\n+        val numCols = row.getInt(2)\n+        val values = row.getArray(5).toDoubleArray()\n+        val isTransposed = row.getBoolean(6)\n+        tpe match {\n+          case 0 =>\n+            val colPtrs = row.getArray(3).toIntArray()\n+            val rowIndices = row.getArray(4).toIntArray()\n+            new SparseMatrix(numRows, numCols, colPtrs, rowIndices, values, isTransposed)\n+          case 1 =>\n+            new DenseMatrix(numRows, numCols, values, isTransposed)\n+        }\n+    }\n+  }\n+\n+  override def userClass: Class[Matrix] = classOf[Matrix]\n+\n+  override def equals(o: Any): Boolean = {\n+    o match {\n+      case v: MatrixUDT => true\n+      case _ => false\n+    }\n+  }\n+\n+  // see [SPARK-8647], this achieves the needed constant hash code without constant no.\n+  override def hashCode(): Int = classOf[MatrixUDT].getName.hashCode()\n+\n+  override def typeName: String = \"matrix\"\n+\n+  override def pyUDT: String = \"pyspark.mllib.linalg.MatrixUDT\""
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Changing this will require to change pyspark VectorUDT/MatrixUDT as well. Then we need to copy mllib pyspark VectorUDT/MatrixUDT to ml package. Do we want to do this in this PR? \n",
    "commit": "1c230ae23e17905af9ea9655cbcfb5a948e627a9",
    "createdAt": "2016-04-19T09:11:08Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.linalg.udt\n+\n+import org.apache.spark.ml.linalg.{DenseMatrix, Matrix, SparseMatrix}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.GenericMutableRow\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * User-defined type for [[Matrix]] in [[mllib-local]] which allows easy interaction with SQL\n+ * via [[org.apache.spark.sql.Dataset]].\n+ */\n+private[ml] class MatrixUDT extends UserDefinedType[Matrix] {\n+\n+  override def sqlType: StructType = {\n+    // type: 0 = sparse, 1 = dense\n+    // the dense matrix is built by numRows, numCols, values and isTransposed, all of which are\n+    // set as not nullable, except values since in the future, support for binary matrices might\n+    // be added for which values are not needed.\n+    // the sparse matrix needs colPtrs and rowIndices, which are set as\n+    // null, while building the dense matrix.\n+    StructType(Seq(\n+      StructField(\"type\", ByteType, nullable = false),\n+      StructField(\"numRows\", IntegerType, nullable = false),\n+      StructField(\"numCols\", IntegerType, nullable = false),\n+      StructField(\"colPtrs\", ArrayType(IntegerType, containsNull = false), nullable = true),\n+      StructField(\"rowIndices\", ArrayType(IntegerType, containsNull = false), nullable = true),\n+      StructField(\"values\", ArrayType(DoubleType, containsNull = false), nullable = true),\n+      StructField(\"isTransposed\", BooleanType, nullable = false)\n+      ))\n+  }\n+\n+  override def serialize(obj: Matrix): InternalRow = {\n+    val row = new GenericMutableRow(7)\n+    obj match {\n+      case sm: SparseMatrix =>\n+        row.setByte(0, 0)\n+        row.setInt(1, sm.numRows)\n+        row.setInt(2, sm.numCols)\n+        row.update(3, new GenericArrayData(sm.colPtrs.map(_.asInstanceOf[Any])))\n+        row.update(4, new GenericArrayData(sm.rowIndices.map(_.asInstanceOf[Any])))\n+        row.update(5, new GenericArrayData(sm.values.map(_.asInstanceOf[Any])))\n+        row.setBoolean(6, sm.isTransposed)\n+\n+      case dm: DenseMatrix =>\n+        row.setByte(0, 1)\n+        row.setInt(1, dm.numRows)\n+        row.setInt(2, dm.numCols)\n+        row.setNullAt(3)\n+        row.setNullAt(4)\n+        row.update(5, new GenericArrayData(dm.values.map(_.asInstanceOf[Any])))\n+        row.setBoolean(6, dm.isTransposed)\n+    }\n+    row\n+  }\n+\n+  override def deserialize(datum: Any): Matrix = {\n+    datum match {\n+      case row: InternalRow =>\n+        require(row.numFields == 7,\n+          s\"MatrixUDT.deserialize given row with length ${row.numFields} but requires length == 7\")\n+        val tpe = row.getByte(0)\n+        val numRows = row.getInt(1)\n+        val numCols = row.getInt(2)\n+        val values = row.getArray(5).toDoubleArray()\n+        val isTransposed = row.getBoolean(6)\n+        tpe match {\n+          case 0 =>\n+            val colPtrs = row.getArray(3).toIntArray()\n+            val rowIndices = row.getArray(4).toIntArray()\n+            new SparseMatrix(numRows, numCols, colPtrs, rowIndices, values, isTransposed)\n+          case 1 =>\n+            new DenseMatrix(numRows, numCols, values, isTransposed)\n+        }\n+    }\n+  }\n+\n+  override def userClass: Class[Matrix] = classOf[Matrix]\n+\n+  override def equals(o: Any): Boolean = {\n+    o match {\n+      case v: MatrixUDT => true\n+      case _ => false\n+    }\n+  }\n+\n+  // see [SPARK-8647], this achieves the needed constant hash code without constant no.\n+  override def hashCode(): Int = classOf[MatrixUDT].getName.hashCode()\n+\n+  override def typeName: String = \"matrix\"\n+\n+  override def pyUDT: String = \"pyspark.mllib.linalg.MatrixUDT\""
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Can you create an JIRA for this? I think this is required, otherwise, for pyspark users, it will lead to a conflict when loading a new ML vector.\n",
    "commit": "1c230ae23e17905af9ea9655cbcfb5a948e627a9",
    "createdAt": "2016-04-22T15:28:22Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.linalg.udt\n+\n+import org.apache.spark.ml.linalg.{DenseMatrix, Matrix, SparseMatrix}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.GenericMutableRow\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * User-defined type for [[Matrix]] in [[mllib-local]] which allows easy interaction with SQL\n+ * via [[org.apache.spark.sql.Dataset]].\n+ */\n+private[ml] class MatrixUDT extends UserDefinedType[Matrix] {\n+\n+  override def sqlType: StructType = {\n+    // type: 0 = sparse, 1 = dense\n+    // the dense matrix is built by numRows, numCols, values and isTransposed, all of which are\n+    // set as not nullable, except values since in the future, support for binary matrices might\n+    // be added for which values are not needed.\n+    // the sparse matrix needs colPtrs and rowIndices, which are set as\n+    // null, while building the dense matrix.\n+    StructType(Seq(\n+      StructField(\"type\", ByteType, nullable = false),\n+      StructField(\"numRows\", IntegerType, nullable = false),\n+      StructField(\"numCols\", IntegerType, nullable = false),\n+      StructField(\"colPtrs\", ArrayType(IntegerType, containsNull = false), nullable = true),\n+      StructField(\"rowIndices\", ArrayType(IntegerType, containsNull = false), nullable = true),\n+      StructField(\"values\", ArrayType(DoubleType, containsNull = false), nullable = true),\n+      StructField(\"isTransposed\", BooleanType, nullable = false)\n+      ))\n+  }\n+\n+  override def serialize(obj: Matrix): InternalRow = {\n+    val row = new GenericMutableRow(7)\n+    obj match {\n+      case sm: SparseMatrix =>\n+        row.setByte(0, 0)\n+        row.setInt(1, sm.numRows)\n+        row.setInt(2, sm.numCols)\n+        row.update(3, new GenericArrayData(sm.colPtrs.map(_.asInstanceOf[Any])))\n+        row.update(4, new GenericArrayData(sm.rowIndices.map(_.asInstanceOf[Any])))\n+        row.update(5, new GenericArrayData(sm.values.map(_.asInstanceOf[Any])))\n+        row.setBoolean(6, sm.isTransposed)\n+\n+      case dm: DenseMatrix =>\n+        row.setByte(0, 1)\n+        row.setInt(1, dm.numRows)\n+        row.setInt(2, dm.numCols)\n+        row.setNullAt(3)\n+        row.setNullAt(4)\n+        row.update(5, new GenericArrayData(dm.values.map(_.asInstanceOf[Any])))\n+        row.setBoolean(6, dm.isTransposed)\n+    }\n+    row\n+  }\n+\n+  override def deserialize(datum: Any): Matrix = {\n+    datum match {\n+      case row: InternalRow =>\n+        require(row.numFields == 7,\n+          s\"MatrixUDT.deserialize given row with length ${row.numFields} but requires length == 7\")\n+        val tpe = row.getByte(0)\n+        val numRows = row.getInt(1)\n+        val numCols = row.getInt(2)\n+        val values = row.getArray(5).toDoubleArray()\n+        val isTransposed = row.getBoolean(6)\n+        tpe match {\n+          case 0 =>\n+            val colPtrs = row.getArray(3).toIntArray()\n+            val rowIndices = row.getArray(4).toIntArray()\n+            new SparseMatrix(numRows, numCols, colPtrs, rowIndices, values, isTransposed)\n+          case 1 =>\n+            new DenseMatrix(numRows, numCols, values, isTransposed)\n+        }\n+    }\n+  }\n+\n+  override def userClass: Class[Matrix] = classOf[Matrix]\n+\n+  override def equals(o: Any): Boolean = {\n+    o match {\n+      case v: MatrixUDT => true\n+      case _ => false\n+    }\n+  }\n+\n+  // see [SPARK-8647], this achieves the needed constant hash code without constant no.\n+  override def hashCode(): Int = classOf[MatrixUDT].getName.hashCode()\n+\n+  override def typeName: String = \"matrix\"\n+\n+  override def pyUDT: String = \"pyspark.mllib.linalg.MatrixUDT\""
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Let's change it to `pyspark.ml.linalg.MatrixUDT` for now. Since in python, we don't have any code using the new UDT. We can add this in the followup PR. \n",
    "commit": "1c230ae23e17905af9ea9655cbcfb5a948e627a9",
    "createdAt": "2016-04-22T17:38:14Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.linalg.udt\n+\n+import org.apache.spark.ml.linalg.{DenseMatrix, Matrix, SparseMatrix}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.GenericMutableRow\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * User-defined type for [[Matrix]] in [[mllib-local]] which allows easy interaction with SQL\n+ * via [[org.apache.spark.sql.Dataset]].\n+ */\n+private[ml] class MatrixUDT extends UserDefinedType[Matrix] {\n+\n+  override def sqlType: StructType = {\n+    // type: 0 = sparse, 1 = dense\n+    // the dense matrix is built by numRows, numCols, values and isTransposed, all of which are\n+    // set as not nullable, except values since in the future, support for binary matrices might\n+    // be added for which values are not needed.\n+    // the sparse matrix needs colPtrs and rowIndices, which are set as\n+    // null, while building the dense matrix.\n+    StructType(Seq(\n+      StructField(\"type\", ByteType, nullable = false),\n+      StructField(\"numRows\", IntegerType, nullable = false),\n+      StructField(\"numCols\", IntegerType, nullable = false),\n+      StructField(\"colPtrs\", ArrayType(IntegerType, containsNull = false), nullable = true),\n+      StructField(\"rowIndices\", ArrayType(IntegerType, containsNull = false), nullable = true),\n+      StructField(\"values\", ArrayType(DoubleType, containsNull = false), nullable = true),\n+      StructField(\"isTransposed\", BooleanType, nullable = false)\n+      ))\n+  }\n+\n+  override def serialize(obj: Matrix): InternalRow = {\n+    val row = new GenericMutableRow(7)\n+    obj match {\n+      case sm: SparseMatrix =>\n+        row.setByte(0, 0)\n+        row.setInt(1, sm.numRows)\n+        row.setInt(2, sm.numCols)\n+        row.update(3, new GenericArrayData(sm.colPtrs.map(_.asInstanceOf[Any])))\n+        row.update(4, new GenericArrayData(sm.rowIndices.map(_.asInstanceOf[Any])))\n+        row.update(5, new GenericArrayData(sm.values.map(_.asInstanceOf[Any])))\n+        row.setBoolean(6, sm.isTransposed)\n+\n+      case dm: DenseMatrix =>\n+        row.setByte(0, 1)\n+        row.setInt(1, dm.numRows)\n+        row.setInt(2, dm.numCols)\n+        row.setNullAt(3)\n+        row.setNullAt(4)\n+        row.update(5, new GenericArrayData(dm.values.map(_.asInstanceOf[Any])))\n+        row.setBoolean(6, dm.isTransposed)\n+    }\n+    row\n+  }\n+\n+  override def deserialize(datum: Any): Matrix = {\n+    datum match {\n+      case row: InternalRow =>\n+        require(row.numFields == 7,\n+          s\"MatrixUDT.deserialize given row with length ${row.numFields} but requires length == 7\")\n+        val tpe = row.getByte(0)\n+        val numRows = row.getInt(1)\n+        val numCols = row.getInt(2)\n+        val values = row.getArray(5).toDoubleArray()\n+        val isTransposed = row.getBoolean(6)\n+        tpe match {\n+          case 0 =>\n+            val colPtrs = row.getArray(3).toIntArray()\n+            val rowIndices = row.getArray(4).toIntArray()\n+            new SparseMatrix(numRows, numCols, colPtrs, rowIndices, values, isTransposed)\n+          case 1 =>\n+            new DenseMatrix(numRows, numCols, values, isTransposed)\n+        }\n+    }\n+  }\n+\n+  override def userClass: Class[Matrix] = classOf[Matrix]\n+\n+  override def equals(o: Any): Boolean = {\n+    o match {\n+      case v: MatrixUDT => true\n+      case _ => false\n+    }\n+  }\n+\n+  // see [SPARK-8647], this achieves the needed constant hash code without constant no.\n+  override def hashCode(): Int = classOf[MatrixUDT].getName.hashCode()\n+\n+  override def typeName: String = \"matrix\"\n+\n+  override def pyUDT: String = \"pyspark.mllib.linalg.MatrixUDT\""
  }],
  "prId": 12259
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "Removed `udt` from the package name \n",
    "commit": "1c230ae23e17905af9ea9655cbcfb5a948e627a9",
    "createdAt": "2016-04-22T17:34:44Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.linalg.udt"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "Package name and file path are not consistent. I think it is okay to use `package org.apache.spark.ml.linalg`.\n",
    "commit": "1c230ae23e17905af9ea9655cbcfb5a948e627a9",
    "createdAt": "2016-04-22T18:36:10Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.linalg.udt"
  }],
  "prId": 12259
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Not part of this PR, but we might need to fix this. It seems that it boxes primitive arrays. I created https://issues.apache.org/jira/browse/SPARK-14850 to track the issue. cc: @cloud-fan \n",
    "commit": "1c230ae23e17905af9ea9655cbcfb5a948e627a9",
    "createdAt": "2016-04-22T18:36:14Z",
    "diffHunk": "@@ -0,0 +1,112 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.linalg.udt\n+\n+import org.apache.spark.ml.linalg.{DenseMatrix, Matrix, SparseMatrix}\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.expressions.GenericMutableRow\n+import org.apache.spark.sql.catalyst.util.GenericArrayData\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * User-defined type for [[Matrix]] in [[mllib-local]] which allows easy interaction with SQL\n+ * via [[org.apache.spark.sql.Dataset]].\n+ */\n+private[ml] class MatrixUDT extends UserDefinedType[Matrix] {\n+\n+  override def sqlType: StructType = {\n+    // type: 0 = sparse, 1 = dense\n+    // the dense matrix is built by numRows, numCols, values and isTransposed, all of which are\n+    // set as not nullable, except values since in the future, support for binary matrices might\n+    // be added for which values are not needed.\n+    // the sparse matrix needs colPtrs and rowIndices, which are set as\n+    // null, while building the dense matrix.\n+    StructType(Seq(\n+      StructField(\"type\", ByteType, nullable = false),\n+      StructField(\"numRows\", IntegerType, nullable = false),\n+      StructField(\"numCols\", IntegerType, nullable = false),\n+      StructField(\"colPtrs\", ArrayType(IntegerType, containsNull = false), nullable = true),\n+      StructField(\"rowIndices\", ArrayType(IntegerType, containsNull = false), nullable = true),\n+      StructField(\"values\", ArrayType(DoubleType, containsNull = false), nullable = true),\n+      StructField(\"isTransposed\", BooleanType, nullable = false)\n+      ))\n+  }\n+\n+  override def serialize(obj: Matrix): InternalRow = {\n+    val row = new GenericMutableRow(7)\n+    obj match {\n+      case sm: SparseMatrix =>\n+        row.setByte(0, 0)\n+        row.setInt(1, sm.numRows)\n+        row.setInt(2, sm.numCols)\n+        row.update(3, new GenericArrayData(sm.colPtrs.map(_.asInstanceOf[Any])))",
    "line": 56
  }],
  "prId": 12259
}]