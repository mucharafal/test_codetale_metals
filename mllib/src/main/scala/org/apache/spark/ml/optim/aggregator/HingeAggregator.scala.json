[{
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "should be `HingeAggregator` here and elsewhere in this class",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-07-27T11:28:33Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+\n+/**\n+ * LinearSVCAggregator computes the gradient and loss for loss function (\"hinge\" or\n+ * \"squared_hinge\", as used in binary classification for instances in sparse or dense\n+ * vector in an online fashion.\n+ *\n+ * Two LinearSVCAggregator can be merged together to have a summary of loss and gradient of"
  }],
  "prId": 18315
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "It seems this only support _hinge_ loss currently. BTW, if we support _squared hinge_ in the future, what is the best way? Add a param _loss function_ for ```HingeAggregator``` or just add a new ```SquaredHingeAggregator```? The later way should be more clear, but with more code.",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-08-17T09:26:56Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+\n+/**\n+ * HingeAggregator computes the gradient and loss for loss function (\"hinge\" or\n+ * \"squared_hinge\", as used in binary classification for instances in sparse or dense"
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "I would prefer to use SquaredHingeAggregator. API-wise, it looks more intuitive and consistent to me.  We can continue the review in the other LinearSVC PR.",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-08-20T02:36:58Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+\n+/**\n+ * HingeAggregator computes the gradient and loss for loss function (\"hinge\" or\n+ * \"squared_hinge\", as used in binary classification for instances in sparse or dense"
  }, {
    "author": {
      "login": "yanboliang"
    },
    "body": "Yeah, I agree you for separate ```SquaredHingeAggregator```. Then we should remove ```squared_hinge``` from here?\r\nBTW, you missed right parenthesis here.",
    "commit": "417308409eed00495f0516021549957b01af7ca6",
    "createdAt": "2017-08-23T10:45:47Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+\n+/**\n+ * HingeAggregator computes the gradient and loss for loss function (\"hinge\" or\n+ * \"squared_hinge\", as used in binary classification for instances in sparse or dense"
  }],
  "prId": 18315
}]