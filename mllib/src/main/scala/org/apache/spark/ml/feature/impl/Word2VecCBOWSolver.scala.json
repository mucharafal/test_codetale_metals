[{
  "comments": [{
    "author": {
      "login": "hhbyyh"
    },
    "body": "IMO we should expose batchSize.",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-11-05T19:47:49Z",
    "diffHunk": "@@ -0,0 +1,371 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature.impl\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Word2Vec\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+private [feature] object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000",
    "line": 30
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "albertusk95"
    },
    "body": "I think it'd be better to store `10` in a variable with an intuitive name for the sake of clarity",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2019-07-20T07:33:08Z",
    "diffHunk": "@@ -0,0 +1,371 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature.impl\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Word2Vec\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+private [feature] object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using level 1 and level 2 BLAS for vectorizing operations\n+   * where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val numNegativeSamples = word2Vec.getNumNegativeSamples\n+    val samplingThreshold = word2Vec.getSamplingThreshold\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, samplingThreshold, word2Vec.getUnigramTableSize)\n+    val vocabSize = sampleTable.length\n+\n+    assert(numNegativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($numNegativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (partIndex, sentenceIter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $partIndex\")\n+        val random = new XORShiftRandom(seed ^ ((partIndex + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = sentenceIter.flatMap { s =>\n+          val doSample = samplingThreshold > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(numNegativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val layer2Vectors = new Array[Float](vectorSize * (numNegativeSamples + 1))\n+        val errGradients = new Array[Float](numNegativeSamples + 1)\n+        val layer1Updates = new Array[Float](vectorSize)\n+        val trainingWords = new Array[Int](numNegativeSamples + 1)\n+\n+        val time = System.nanoTime()\n+        var batchTime = System.nanoTime()\n+\n+        for ((batch, idx) <- groupedBatches.zipWithIndex) {\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if((idx + 1) % 10 == 0) {",
    "line": 139
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "albertusk95"
    },
    "body": "`// c in original code, floor at 0`",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2019-07-20T07:38:32Z",
    "diffHunk": "@@ -0,0 +1,371 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature.impl\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Word2Vec\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+private [feature] object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using level 1 and level 2 BLAS for vectorizing operations\n+   * where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val numNegativeSamples = word2Vec.getNumNegativeSamples\n+    val samplingThreshold = word2Vec.getSamplingThreshold\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, samplingThreshold, word2Vec.getUnigramTableSize)\n+    val vocabSize = sampleTable.length\n+\n+    assert(numNegativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($numNegativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (partIndex, sentenceIter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $partIndex\")\n+        val random = new XORShiftRandom(seed ^ ((partIndex + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = sentenceIter.flatMap { s =>\n+          val doSample = samplingThreshold > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(numNegativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val layer2Vectors = new Array[Float](vectorSize * (numNegativeSamples + 1))\n+        val errGradients = new Array[Float](numNegativeSamples + 1)\n+        val layer1Updates = new Array[Float](vectorSize)\n+        val trainingWords = new Array[Int](numNegativeSamples + 1)\n+\n+        val time = System.nanoTime()\n+        var batchTime = System.nanoTime()\n+\n+        for ((batch, idx) <- groupedBatches.zipWithIndex) {\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if((idx + 1) % 10 == 0) {\n+            logInfo(s\"Partition: $partIndex, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime() - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime() - batchTime) / 1e6\n+            batchTime = System.nanoTime()\n+            logDebug(s\"Partition: $partIndex, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $partIndex, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            java.util.Arrays.fill(contextVec, 0.0f)\n+            java.util.Arrays.fill(layer2Vectors, 0.0f)\n+            java.util.Arrays.fill(errGradients, 0.0f)\n+            java.util.Arrays.fill(layer1Updates, 0.0f)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            // sum all of the context word embeddings into a single contextVec\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, numNegativeSamples, trainingWords)\n+\n+            Iterator.range(0, trainingWords.length).foreach { i =>\n+              Array.copy(syn1,\n+                vectorSize * trainingWords(i),\n+                layer2Vectors, vectorSize * i,\n+                vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = numNegativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\",\n+              cols,\n+              rows,\n+              1.0f,\n+              layer2Vectors,\n+              0,\n+              cols,\n+              contextVec,\n+              0,\n+              1,\n+              0.0f,\n+              errGradients,\n+              0,\n+              1)\n+\n+            Iterator.range(0, numNegativeSamples + 1).foreach { i =>\n+              if (errGradients(i) > -MAX_EXP && errGradients(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-errGradients(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update layer 2 vectors\n+                blas\n+                  .saxpy(vectorSize, err, contextVec, 0, 1, syn1, trainingWords(i) * vectorSize, 1)\n+                // accumulate gradients for the cumulative context vector\n+                blas.saxpy(vectorSize, err, layer2Vectors, i * vectorSize, 1, layer1Updates, 0, 1)\n+                errGradients.update(i, err)\n+              } else {\n+                errGradients.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update layer 1 vectors/word embeddings\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, layer1Updates, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            errGradients.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $partIndex, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: $timePerIteration ms\")\n+    }\n+    digitSentences.unpersist()\n+    vocabMapBroadcast.destroy()\n+    unigramTableBroadcast.destroy()\n+    sampleTableBroadcast.destroy()\n+\n+    new feature.Word2VecModel(vocabMap, syn0Global)\n+  }\n+\n+  /**\n+   * Similar to InitUnigramTable in the original code.\n+   * Given the frequency of all words, we create an array that has words in rough proportion\n+   * to their frequencies. Randomly drawing an index from this array, would roughly replicate\n+   * the words frequency distribution\n+   *\n+   * @param normalizedWeights word frequency distribution\n+   * @param tableSize size of the array to create the frequency distribution\n+   * @return array with the frequency distribution\n+   */\n+  private def generateUnigramTable(normalizedWeights: Array[Double], tableSize: Int): Array[Int] = {\n+    val table = new Array[Int](tableSize)\n+    var index = 0\n+    var wordId = 0\n+    while (index < tableSize) {\n+      table.update(index, wordId)\n+      if (index.toFloat / tableSize >= normalizedWeights(wordId)) {\n+        wordId = math.min(normalizedWeights.length - 1, wordId + 1)\n+      }\n+      index += 1\n+    }\n+    table\n+  }\n+\n+  /**\n+   * Generate basic word stats given the input RDD. These include total word count,\n+   * word->frequency map, word frequency distribution array, and sampling table to sample\n+   * high frequency words\n+   */\n+  private def generateVocab[S <: Iterable[String]](\n+      input: RDD[S],\n+      minCount: Int,\n+      sample: Double,\n+      unigramTableSize: Int): Vocabulary = {\n+\n+    val words = input.flatMap(x => x)\n+\n+    val sortedWordCounts = words.map(w => (w, 1L))\n+      .reduceByKey(_ + _)\n+      .filter { case (_, c) => c >= minCount}\n+      .collect()\n+      .sortWith { case ((w1, c1), (w2, c2)) => c1 > c2}\n+      .zipWithIndex\n+\n+    val totalWordCount = sortedWordCounts.map(_._1._2).sum\n+\n+    val vocabMap = sortedWordCounts.map { case ((w, c), i) =>\n+      w -> i\n+    }.toMap\n+\n+    val samplingTable = new Array[Float](vocabMap.size)\n+\n+    if (sample > Double.MinPositiveValue) {\n+      sortedWordCounts.foreach { case ((w, c), i) =>\n+        val samplingRatio = sample * totalWordCount / c\n+        samplingTable.update(i, (math.sqrt(samplingRatio) + samplingRatio).toFloat)\n+      }\n+    }\n+\n+    val weights = sortedWordCounts.map{ case((_, x), _) => scala.math.pow(x, power)}\n+    val totalWeight = weights.sum\n+\n+    val normalizedCumWeights = weights.scanLeft(0.0)(_ + _).tail.map(_ / totalWeight)\n+\n+    val unigramTable = generateUnigramTable(normalizedCumWeights, unigramTableSize)\n+\n+    Vocabulary(totalWordCount, vocabMap, unigramTable, samplingTable)\n+  }\n+\n+  /**\n+   * Generate pairs of contexts and expected output words for use with training\n+   * word-embeddings\n+   */\n+  private def generateContextWordPairs(\n+      sentence: Array[Int],\n+      window: Int,\n+      doSample: Boolean,\n+      samplingTable: Array[Float],\n+      random: XORShiftRandom): Iterator[(Array[Int], Int)] = {\n+    val reducedSentence = if (doSample) {\n+      sentence.filter(i => samplingTable(i) > random.nextFloat)\n+    } else {\n+      sentence\n+    }\n+    reducedSentence.iterator.zipWithIndex.map { case (word, i) =>\n+      val b = window - random.nextInt(window) // (window - a) in original code\n+      // pick b words around the current word index\n+      val start = math.max(0, i - b) // c in original code, floor ar 0",
    "line": 340
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "albertusk95"
    },
    "body": "I think using `word` as a variable name with `Int` as the data type is less relevant",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2019-07-20T07:41:12Z",
    "diffHunk": "@@ -0,0 +1,371 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature.impl\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Word2Vec\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+private [feature] object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using level 1 and level 2 BLAS for vectorizing operations\n+   * where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val numNegativeSamples = word2Vec.getNumNegativeSamples\n+    val samplingThreshold = word2Vec.getSamplingThreshold\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, samplingThreshold, word2Vec.getUnigramTableSize)\n+    val vocabSize = sampleTable.length\n+\n+    assert(numNegativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($numNegativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (partIndex, sentenceIter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $partIndex\")\n+        val random = new XORShiftRandom(seed ^ ((partIndex + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = sentenceIter.flatMap { s =>\n+          val doSample = samplingThreshold > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(numNegativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val layer2Vectors = new Array[Float](vectorSize * (numNegativeSamples + 1))\n+        val errGradients = new Array[Float](numNegativeSamples + 1)\n+        val layer1Updates = new Array[Float](vectorSize)\n+        val trainingWords = new Array[Int](numNegativeSamples + 1)\n+\n+        val time = System.nanoTime()\n+        var batchTime = System.nanoTime()\n+\n+        for ((batch, idx) <- groupedBatches.zipWithIndex) {\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if((idx + 1) % 10 == 0) {\n+            logInfo(s\"Partition: $partIndex, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime() - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime() - batchTime) / 1e6\n+            batchTime = System.nanoTime()\n+            logDebug(s\"Partition: $partIndex, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $partIndex, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            java.util.Arrays.fill(contextVec, 0.0f)\n+            java.util.Arrays.fill(layer2Vectors, 0.0f)\n+            java.util.Arrays.fill(errGradients, 0.0f)\n+            java.util.Arrays.fill(layer1Updates, 0.0f)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            // sum all of the context word embeddings into a single contextVec\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, numNegativeSamples, trainingWords)\n+\n+            Iterator.range(0, trainingWords.length).foreach { i =>\n+              Array.copy(syn1,\n+                vectorSize * trainingWords(i),\n+                layer2Vectors, vectorSize * i,\n+                vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = numNegativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\",\n+              cols,\n+              rows,\n+              1.0f,\n+              layer2Vectors,\n+              0,\n+              cols,\n+              contextVec,\n+              0,\n+              1,\n+              0.0f,\n+              errGradients,\n+              0,\n+              1)\n+\n+            Iterator.range(0, numNegativeSamples + 1).foreach { i =>\n+              if (errGradients(i) > -MAX_EXP && errGradients(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-errGradients(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update layer 2 vectors\n+                blas\n+                  .saxpy(vectorSize, err, contextVec, 0, 1, syn1, trainingWords(i) * vectorSize, 1)\n+                // accumulate gradients for the cumulative context vector\n+                blas.saxpy(vectorSize, err, layer2Vectors, i * vectorSize, 1, layer1Updates, 0, 1)\n+                errGradients.update(i, err)\n+              } else {\n+                errGradients.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update layer 1 vectors/word embeddings\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, layer1Updates, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            errGradients.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $partIndex, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: $timePerIteration ms\")\n+    }\n+    digitSentences.unpersist()\n+    vocabMapBroadcast.destroy()\n+    unigramTableBroadcast.destroy()\n+    sampleTableBroadcast.destroy()\n+\n+    new feature.Word2VecModel(vocabMap, syn0Global)\n+  }\n+\n+  /**\n+   * Similar to InitUnigramTable in the original code.\n+   * Given the frequency of all words, we create an array that has words in rough proportion\n+   * to their frequencies. Randomly drawing an index from this array, would roughly replicate\n+   * the words frequency distribution\n+   *\n+   * @param normalizedWeights word frequency distribution\n+   * @param tableSize size of the array to create the frequency distribution\n+   * @return array with the frequency distribution\n+   */\n+  private def generateUnigramTable(normalizedWeights: Array[Double], tableSize: Int): Array[Int] = {\n+    val table = new Array[Int](tableSize)\n+    var index = 0\n+    var wordId = 0\n+    while (index < tableSize) {\n+      table.update(index, wordId)\n+      if (index.toFloat / tableSize >= normalizedWeights(wordId)) {\n+        wordId = math.min(normalizedWeights.length - 1, wordId + 1)\n+      }\n+      index += 1\n+    }\n+    table\n+  }\n+\n+  /**\n+   * Generate basic word stats given the input RDD. These include total word count,\n+   * word->frequency map, word frequency distribution array, and sampling table to sample\n+   * high frequency words\n+   */\n+  private def generateVocab[S <: Iterable[String]](\n+      input: RDD[S],\n+      minCount: Int,\n+      sample: Double,\n+      unigramTableSize: Int): Vocabulary = {\n+\n+    val words = input.flatMap(x => x)\n+\n+    val sortedWordCounts = words.map(w => (w, 1L))\n+      .reduceByKey(_ + _)\n+      .filter { case (_, c) => c >= minCount}\n+      .collect()\n+      .sortWith { case ((w1, c1), (w2, c2)) => c1 > c2}\n+      .zipWithIndex\n+\n+    val totalWordCount = sortedWordCounts.map(_._1._2).sum\n+\n+    val vocabMap = sortedWordCounts.map { case ((w, c), i) =>\n+      w -> i\n+    }.toMap\n+\n+    val samplingTable = new Array[Float](vocabMap.size)\n+\n+    if (sample > Double.MinPositiveValue) {\n+      sortedWordCounts.foreach { case ((w, c), i) =>\n+        val samplingRatio = sample * totalWordCount / c\n+        samplingTable.update(i, (math.sqrt(samplingRatio) + samplingRatio).toFloat)\n+      }\n+    }\n+\n+    val weights = sortedWordCounts.map{ case((_, x), _) => scala.math.pow(x, power)}\n+    val totalWeight = weights.sum\n+\n+    val normalizedCumWeights = weights.scanLeft(0.0)(_ + _).tail.map(_ / totalWeight)\n+\n+    val unigramTable = generateUnigramTable(normalizedCumWeights, unigramTableSize)\n+\n+    Vocabulary(totalWordCount, vocabMap, unigramTable, samplingTable)\n+  }\n+\n+  /**\n+   * Generate pairs of contexts and expected output words for use with training\n+   * word-embeddings\n+   */\n+  private def generateContextWordPairs(\n+      sentence: Array[Int],\n+      window: Int,\n+      doSample: Boolean,\n+      samplingTable: Array[Float],\n+      random: XORShiftRandom): Iterator[(Array[Int], Int)] = {\n+    val reducedSentence = if (doSample) {\n+      sentence.filter(i => samplingTable(i) > random.nextFloat)\n+    } else {\n+      sentence\n+    }\n+    reducedSentence.iterator.zipWithIndex.map { case (word, i) =>\n+      val b = window - random.nextInt(window) // (window - a) in original code\n+      // pick b words around the current word index\n+      val start = math.max(0, i - b) // c in original code, floor ar 0\n+      val end = math.min(reducedSentence.length, i + b + 1) // cap at sentence length\n+      // make sure current word is not a part of the context\n+      val contextIds = reducedSentence.view.zipWithIndex.slice(start, end)\n+        .filter{case (_, pos) => pos != i}.map(_._1)\n+      (contextIds.toArray, word)\n+    }\n+  }\n+\n+  /**\n+   * This essentially helps translate from uniform distribution to a distribution\n+   * resembling uni-gram frequency distribution.\n+   */\n+  private def generateNegativeSamples(\n+      random: XORShiftRandom,\n+      word: Int,",
    "line": 355
  }],
  "prId": 17673
}]