[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "\"most scikit-learn's\" --> \"most scikit-learn\"\n",
    "commit": "425aad200ec4ccd192492a8f76e2716f337f648a",
    "createdAt": "2015-08-18T01:09:32Z",
    "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml\n+\n+import org.apache.spark.ml.feature.{HashingTF, IDF, IDFModel, VectorAssembler}\n+import org.apache.spark.sql.DataFrame\n+\n+/**\n+ * == Feature transformers ==\n+ *\n+ * The `ml.feature` package provides common feature transformers that help convert raw data or\n+ * features into more suitable forms for model fitting.\n+ * Most feature transformers are implemented as [[Transformer]]s, which transform one [[DataFrame]]\n+ * into another, e.g., [[HashingTF]].\n+ * Some feature transformers are implemented as [[Estimator]]s, because the transformation requires\n+ * some aggregated information of the dataset, e.g., document frequencies in [[IDF]].\n+ * For those feature transformers, calling [[Estimator!.fit]] is required to obtain the model first,\n+ * e.g., [[IDFModel]], in order to apply transformation.\n+ * The transformation is usually done by appending new columns to the input [[DataFrame]], so all\n+ * input columns are carried over.\n+ *\n+ * We try to make each transformer minimal, so it becomes flexible to assemble feature\n+ * transformation pipelines.\n+ * [[Pipeline]] can be used to chain feature transformers, and [[VectorAssembler]] can be used to\n+ * combine multiple feature transformations, for example:\n+ *\n+ * {{{\n+ *   import org.apache.spark.ml.feature._\n+ *   import org.apache.spark.ml.Pipeline\n+ *\n+ *   // a DataFrame with three columns: id (integer), text (string), and rating (double).\n+ *   val df = sqlContext.createDataFrame(Seq(\n+ *     (0, \"Hi I heard about Spark\", 3.0),\n+ *     (1, \"I wish Java could use case classes\", 4.0),\n+ *     (2, \"Logistic regression models are neat\", 4.0)\n+ *   )).toDF(\"id\", \"text\", \"rating\")\n+ *\n+ *   // define feature transformers\n+ *   val tok = new RegexTokenizer()\n+ *     .setInputCol(\"text\")\n+ *     .setOutputCol(\"words\")\n+ *   val sw = new StopWordsRemover()\n+ *     .setInputCol(\"words\")\n+ *     .setOutputCol(\"filtered_words\")\n+ *   val tf = new HashingTF()\n+ *     .setInputCol(\"filtered_words\")\n+ *     .setOutputCol(\"tf\")\n+ *     .setNumFeatures(10000)\n+ *   val idf = new IDF()\n+ *     .setInputCol(\"tf\")\n+ *     .setOutputCol(\"tf_idf\")\n+ *   val assembler = new VectorAssembler()\n+ *     .setInputCols(Array(\"tf_idf\", \"rating\"))\n+ *     .setOutputCol(\"features\")\n+ *\n+ *   // assemble and fit the feature transformation pipeline\n+ *   val pipeline = new Pipeline()\n+ *     .setStages(Array(tok, sw, tf, idf, assembler))\n+ *   val model = pipeline.fit(df)\n+ *\n+ *   // save transformed features with raw data\n+ *   model.transform(df)\n+ *     .select(\"id\", \"text\", \"rating\", \"features\")\n+ *     .write.format(\"parquet\").save(\"/output/path\")\n+ * }}}\n+ *\n+ * Some feature transformers implemented in MLlib are inspired by those implemented in scikit-learn.\n+ * The major difference is that most scikit-learn's feature transformers operate eagerly on the"
  }],
  "prId": 8260
}]