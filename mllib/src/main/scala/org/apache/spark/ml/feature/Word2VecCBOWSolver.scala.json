[{
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "Does it make sense to make this configurable?",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-02T18:22:40Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Yes, I think it does. An expert param maybe.",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T20:38:31Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "not used",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T16:42:26Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            gb.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $i_, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: ${timePerIteration} ms\")\n+    }\n+    digitSentences.unpersist()\n+    vocabMapBroadcast.destroy()\n+    unigramTableBroadcast.destroy()\n+    sampleTableBroadcast.destroy()\n+\n+    new feature.Word2VecModel(vocabMap, syn0Global)\n+  }\n+\n+  /**\n+   * Similar to InitUnigramTable in the original code.\n+   */\n+  private def generateUnigramTable(normalizedWeights: Array[Double], tableSize: Int): Array[Int] = {\n+    val table = new Array[Int](tableSize)\n+    var index = 0\n+    var wordId = 0\n+    while (index < table.length) {\n+      table.update(index, wordId)\n+      if (index.toFloat / table.length >= normalizedWeights(wordId)) {\n+        wordId = math.min(normalizedWeights.length - 1, wordId + 1)\n+      }\n+      index += 1\n+    }\n+    table\n+  }\n+\n+  private def generateVocab[S <: Iterable[String]](\n+      input: RDD[S],\n+      minCount: Int,\n+      sample: Double,\n+      unigramTableSize: Int): Vocabulary = {\n+    val sc = input.context"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "use `tableSize` instead of `table.length`",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T17:41:30Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            gb.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $i_, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: ${timePerIteration} ms\")\n+    }\n+    digitSentences.unpersist()\n+    vocabMapBroadcast.destroy()\n+    unigramTableBroadcast.destroy()\n+    sampleTableBroadcast.destroy()\n+\n+    new feature.Word2VecModel(vocabMap, syn0Global)\n+  }\n+\n+  /**\n+   * Similar to InitUnigramTable in the original code.\n+   */\n+  private def generateUnigramTable(normalizedWeights: Array[Double], tableSize: Int): Array[Int] = {\n+    val table = new Array[Int](tableSize)\n+    var index = 0\n+    var wordId = 0\n+    while (index < table.length) {"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "`sampleTable.length`",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T17:58:14Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size"
  }, {
    "author": {
      "login": "shubhamchopra"
    },
    "body": "Done",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-09T20:22:40Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "parentheses",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T17:59:46Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {"
  }, {
    "author": {
      "login": "shubhamchopra"
    },
    "body": "It's not needed here, is it?",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-09T20:30:31Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "Still looking through algo details, but I find many of the variable names unhelpful, these included. What is syn0, syn1? I know what they represent, syn0 being the word embeddings, not sure what \"syn\" is. I know much of the code is meant to mirror the C implementation, but I'd prefer variable names that are clear, rather than a one-to-one mapping",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T18:30:18Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)"
  }, {
    "author": {
      "login": "shubhamchopra"
    },
    "body": "There are a few places where the implementation seems counter-intuitive, and doesn't directly relate to the papers its based on. I thought it might help if someone wants to cross-check the implementation with the C implementation. Other than that, I do agree with you, and have tried to name things appropriately, other than these key variables around which the implementation is based.",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-09T20:29:55Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "call it `layer2Vectors`?",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T18:30:51Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))"
  }, {
    "author": {
      "login": "shubhamchopra"
    },
    "body": "Done",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-09T20:43:19Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "`numNegativeSamples`",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T18:31:10Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples"
  }, {
    "author": {
      "login": "shubhamchopra"
    },
    "body": "Done",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-09T20:22:15Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "`samplingThreshold`",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T18:32:06Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample"
  }, {
    "author": {
      "login": "shubhamchopra"
    },
    "body": "Done",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-09T20:22:20Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "`case (partIndex, sentenceIter) =>`",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T18:35:40Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>"
  }, {
    "author": {
      "login": "shubhamchopra"
    },
    "body": "Done",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-09T20:33:49Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "Do we even need it though? It's a private method called in one place. An error would be thrown if it weren't large enough so it can't silently fail.",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T18:40:47Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            gb.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $i_, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: ${timePerIteration} ms\")\n+    }\n+    digitSentences.unpersist()\n+    vocabMapBroadcast.destroy()\n+    unigramTableBroadcast.destroy()\n+    sampleTableBroadcast.destroy()\n+\n+    new feature.Word2VecModel(vocabMap, syn0Global)\n+  }\n+\n+  /**\n+   * Similar to InitUnigramTable in the original code.\n+   */\n+  private def generateUnigramTable(normalizedWeights: Array[Double], tableSize: Int): Array[Int] = {\n+    val table = new Array[Int](tableSize)\n+    var index = 0\n+    var wordId = 0\n+    while (index < table.length) {\n+      table.update(index, wordId)\n+      if (index.toFloat / table.length >= normalizedWeights(wordId)) {\n+        wordId = math.min(normalizedWeights.length - 1, wordId + 1)\n+      }\n+      index += 1\n+    }\n+    table\n+  }\n+\n+  private def generateVocab[S <: Iterable[String]](\n+      input: RDD[S],\n+      minCount: Int,\n+      sample: Double,\n+      unigramTableSize: Int): Vocabulary = {\n+    val sc = input.context\n+\n+    val words = input.flatMap(x => x)\n+\n+    val sortedWordCounts = words.map(w => (w, 1L))\n+      .reduceByKey(_ + _)\n+      .filter{case (w, c) => c >= minCount}\n+      .collect()\n+      .sortWith{case ((w1, c1), (w2, c2)) => c1 > c2}\n+      .zipWithIndex\n+\n+    val totalWordCount = sortedWordCounts.map(_._1._2).sum\n+\n+    val vocabMap = sortedWordCounts.map{case ((w, c), i) =>\n+      w -> i\n+    }.toMap\n+\n+    val samplingTable = new Array[Float](vocabMap.size)\n+\n+    if (sample > Double.MinPositiveValue) {\n+      sortedWordCounts.foreach { case ((w, c), i) =>\n+        val samplingRatio = sample * totalWordCount / c\n+        samplingTable.update(i, (math.sqrt(samplingRatio) + samplingRatio).toFloat)\n+      }\n+    }\n+\n+    val weights = sortedWordCounts.map{ case((_, x), _) => scala.math.pow(x, power)}\n+    val totalWeight = weights.sum\n+\n+    val normalizedCumWeights = weights.scanLeft(0.0)(_ + _).tail.map(x => x / totalWeight)\n+\n+    val unigramTable = generateUnigramTable(normalizedCumWeights, unigramTableSize)\n+\n+    Vocabulary(totalWordCount, vocabMap, unigramTable, samplingTable)\n+  }\n+\n+  private def zeroVector(v: Array[Float]): Unit = {\n+    var i = 0\n+    while(i < v.length) {\n+      v.update(i, 0.0f)\n+      i+= 1\n+    }\n+  }\n+\n+  private def generateContextWordPairs(\n+      sentence: Array[Int],\n+      window: Int,\n+      doSample: Boolean,\n+      samplingTable: Array[Float],\n+      random: XORShiftRandom): Iterator[(Array[Int], Int)] = {\n+    val reducedSentence = if (doSample) {\n+      sentence.filter(i => samplingTable(i) > random.nextFloat)\n+    } else {\n+      sentence\n+    }\n+    reducedSentence.iterator.zipWithIndex.map { case (word, i) =>\n+      val b = window - random.nextInt(window) // (window - a) in original code\n+      // pick b words around the current word index\n+      val start = math.max(0, i - b) // c in original code, floor ar 0\n+      val end = math.min(reducedSentence.length, i + b + 1) // cap at sentence length\n+      // make sure current word is not a part of the context\n+      val contextIds = reducedSentence.view.zipWithIndex.slice(start, end)\n+        .filter{case (_, pos) => pos != i}.map(_._1)\n+      (contextIds.toArray, word)\n+    }\n+  }\n+\n+  // This essentially helps translate from uniform distribution to a distribution\n+  // resembling uni-gram frequency distribution.\n+  private def generateNegativeSamples(\n+      random: XORShiftRandom,\n+      word: Int,\n+      unigramTable: Array[Int],\n+      numSamples: Int,\n+      arr: Array[Int]): Unit = {\n+    assert(numSamples + 1 == arr.length,"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "`if ((idx + 1) % 10 == 0)`?",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T19:05:21Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {"
  }, {
    "author": {
      "login": "shubhamchopra"
    },
    "body": "Good catch. Done",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-09T20:45:42Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "`for ((batch, idx) <- groupedBatches.zipWithIndex)`",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T19:07:41Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {"
  }, {
    "author": {
      "login": "shubhamchopra"
    },
    "body": "Done",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-09T20:45:49Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "For random forest, we made a separate `impl` package and put this type of code there. I think that's probably a good way to do it here as well. Also, this should be a `private[feature]` object.",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T19:10:11Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {"
  }, {
    "author": {
      "login": "shubhamchopra"
    },
    "body": "Done",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-09T20:17:12Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "`java.util.Arrays.fill(contextVec, 0f)`, then you can get rid of the extra method.",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T19:23:31Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)"
  }, {
    "author": {
      "login": "shubhamchopra"
    },
    "body": "Done",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-09T20:50:12Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "`// sum all of the context word embeddings into a single contextVec`?",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T19:24:31Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "`trainingWords`? I don't like that much either, but `wordIndices` isn't helpful.",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T19:26:31Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)"
  }, {
    "author": {
      "login": "shubhamchopra"
    },
    "body": "Done",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-09T20:43:26Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "`gb` and `neu1e` can surely have more descriptive names...",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T19:27:19Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "`System.nanoTime()`",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T19:27:50Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "these don't need to be materialized",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T19:33:43Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)"
  }, {
    "author": {
      "login": "shubhamchopra"
    },
    "body": "I instantiate them once and reuse them for the entire iteration. This gave significant speed-ups.",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-09T20:33:45Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "instead of `val err = (negLabels(i) - v) * alpha`, using `val err = ((if (i > 0) 0f else 1f) - v) * alpha`. Then you don't need to allocate the array?",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-09T20:40:40Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "`// accumulate gradients for the cumulative context vector`?",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T20:29:10Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "`// gradient step for layer 2 vectors`",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T20:29:29Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "`// gradient step for word embeddings`",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T20:30:02Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "instead of `for/yield` just use a for loop, and declare a `var errorSum = 0f` outside of it. Then, instead of updating `gb` and taking the sum here, you can just add to `errorSum`. Replace `gb.update(i, err)` with `errorSum += math.abs(err)`, etc...",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T20:37:08Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            gb.map(math.abs).sum / alpha"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "don't use `{}` for a single variable.",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T20:37:40Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            gb.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $i_, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: ${timePerIteration} ms\")"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "\"using level 1 and level 2 BLAS\"",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T20:39:12Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable."
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "I prefer putting an actual description of the method here. Where is the original code? The reference impl I found doesn't have comments, so this wouldn't be helpful unless you have one with comments.",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T20:40:16Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            gb.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $i_, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: ${timePerIteration} ms\")\n+    }\n+    digitSentences.unpersist()\n+    vocabMapBroadcast.destroy()\n+    unigramTableBroadcast.destroy()\n+    sampleTableBroadcast.destroy()\n+\n+    new feature.Word2VecModel(vocabMap, syn0Global)\n+  }\n+\n+  /**\n+   * Similar to InitUnigramTable in the original code."
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "style: maps, filters, etc... should be `.map { case`",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T20:41:11Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            gb.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $i_, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: ${timePerIteration} ms\")\n+    }\n+    digitSentences.unpersist()\n+    vocabMapBroadcast.destroy()\n+    unigramTableBroadcast.destroy()\n+    sampleTableBroadcast.destroy()\n+\n+    new feature.Word2VecModel(vocabMap, syn0Global)\n+  }\n+\n+  /**\n+   * Similar to InitUnigramTable in the original code.\n+   */\n+  private def generateUnigramTable(normalizedWeights: Array[Double], tableSize: Int): Array[Int] = {\n+    val table = new Array[Int](tableSize)\n+    var index = 0\n+    var wordId = 0\n+    while (index < table.length) {\n+      table.update(index, wordId)\n+      if (index.toFloat / table.length >= normalizedWeights(wordId)) {\n+        wordId = math.min(normalizedWeights.length - 1, wordId + 1)\n+      }\n+      index += 1\n+    }\n+    table\n+  }\n+\n+  private def generateVocab[S <: Iterable[String]](\n+      input: RDD[S],\n+      minCount: Int,\n+      sample: Double,\n+      unigramTableSize: Int): Vocabulary = {\n+    val sc = input.context\n+\n+    val words = input.flatMap(x => x)\n+\n+    val sortedWordCounts = words.map(w => (w, 1L))\n+      .reduceByKey(_ + _)\n+      .filter{case (w, c) => c >= minCount}\n+      .collect()\n+      .sortWith{case ((w1, c1), (w2, c2)) => c1 > c2}\n+      .zipWithIndex\n+\n+    val totalWordCount = sortedWordCounts.map(_._1._2).sum\n+\n+    val vocabMap = sortedWordCounts.map{case ((w, c), i) =>"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "prefer underscores where variables are unused.",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T20:42:43Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            gb.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $i_, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: ${timePerIteration} ms\")\n+    }\n+    digitSentences.unpersist()\n+    vocabMapBroadcast.destroy()\n+    unigramTableBroadcast.destroy()\n+    sampleTableBroadcast.destroy()\n+\n+    new feature.Word2VecModel(vocabMap, syn0Global)\n+  }\n+\n+  /**\n+   * Similar to InitUnigramTable in the original code.\n+   */\n+  private def generateUnigramTable(normalizedWeights: Array[Double], tableSize: Int): Array[Int] = {\n+    val table = new Array[Int](tableSize)\n+    var index = 0\n+    var wordId = 0\n+    while (index < table.length) {\n+      table.update(index, wordId)\n+      if (index.toFloat / table.length >= normalizedWeights(wordId)) {\n+        wordId = math.min(normalizedWeights.length - 1, wordId + 1)\n+      }\n+      index += 1\n+    }\n+    table\n+  }\n+\n+  private def generateVocab[S <: Iterable[String]](\n+      input: RDD[S],\n+      minCount: Int,\n+      sample: Double,\n+      unigramTableSize: Int): Vocabulary = {\n+    val sc = input.context\n+\n+    val words = input.flatMap(x => x)\n+\n+    val sortedWordCounts = words.map(w => (w, 1L))\n+      .reduceByKey(_ + _)\n+      .filter{case (w, c) => c >= minCount}"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "add doc",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T20:43:41Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            gb.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $i_, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: ${timePerIteration} ms\")\n+    }\n+    digitSentences.unpersist()\n+    vocabMapBroadcast.destroy()\n+    unigramTableBroadcast.destroy()\n+    sampleTableBroadcast.destroy()\n+\n+    new feature.Word2VecModel(vocabMap, syn0Global)\n+  }\n+\n+  /**\n+   * Similar to InitUnigramTable in the original code.\n+   */\n+  private def generateUnigramTable(normalizedWeights: Array[Double], tableSize: Int): Array[Int] = {\n+    val table = new Array[Int](tableSize)\n+    var index = 0\n+    var wordId = 0\n+    while (index < table.length) {\n+      table.update(index, wordId)\n+      if (index.toFloat / table.length >= normalizedWeights(wordId)) {\n+        wordId = math.min(normalizedWeights.length - 1, wordId + 1)\n+      }\n+      index += 1\n+    }\n+    table\n+  }\n+\n+  private def generateVocab[S <: Iterable[String]]("
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "nit: `.map(_ / totalWeight)`",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T20:45:25Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            gb.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $i_, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: ${timePerIteration} ms\")\n+    }\n+    digitSentences.unpersist()\n+    vocabMapBroadcast.destroy()\n+    unigramTableBroadcast.destroy()\n+    sampleTableBroadcast.destroy()\n+\n+    new feature.Word2VecModel(vocabMap, syn0Global)\n+  }\n+\n+  /**\n+   * Similar to InitUnigramTable in the original code.\n+   */\n+  private def generateUnigramTable(normalizedWeights: Array[Double], tableSize: Int): Array[Int] = {\n+    val table = new Array[Int](tableSize)\n+    var index = 0\n+    var wordId = 0\n+    while (index < table.length) {\n+      table.update(index, wordId)\n+      if (index.toFloat / table.length >= normalizedWeights(wordId)) {\n+        wordId = math.min(normalizedWeights.length - 1, wordId + 1)\n+      }\n+      index += 1\n+    }\n+    table\n+  }\n+\n+  private def generateVocab[S <: Iterable[String]](\n+      input: RDD[S],\n+      minCount: Int,\n+      sample: Double,\n+      unigramTableSize: Int): Vocabulary = {\n+    val sc = input.context\n+\n+    val words = input.flatMap(x => x)\n+\n+    val sortedWordCounts = words.map(w => (w, 1L))\n+      .reduceByKey(_ + _)\n+      .filter{case (w, c) => c >= minCount}\n+      .collect()\n+      .sortWith{case ((w1, c1), (w2, c2)) => c1 > c2}\n+      .zipWithIndex\n+\n+    val totalWordCount = sortedWordCounts.map(_._1._2).sum\n+\n+    val vocabMap = sortedWordCounts.map{case ((w, c), i) =>\n+      w -> i\n+    }.toMap\n+\n+    val samplingTable = new Array[Float](vocabMap.size)\n+\n+    if (sample > Double.MinPositiveValue) {\n+      sortedWordCounts.foreach { case ((w, c), i) =>\n+        val samplingRatio = sample * totalWordCount / c\n+        samplingTable.update(i, (math.sqrt(samplingRatio) + samplingRatio).toFloat)\n+      }\n+    }\n+\n+    val weights = sortedWordCounts.map{ case((_, x), _) => scala.math.pow(x, power)}\n+    val totalWeight = weights.sum\n+\n+    val normalizedCumWeights = weights.scanLeft(0.0)(_ + _).tail.map(x => x / totalWeight)"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "use scaladoc format",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T20:48:54Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            gb.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $i_, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: ${timePerIteration} ms\")\n+    }\n+    digitSentences.unpersist()\n+    vocabMapBroadcast.destroy()\n+    unigramTableBroadcast.destroy()\n+    sampleTableBroadcast.destroy()\n+\n+    new feature.Word2VecModel(vocabMap, syn0Global)\n+  }\n+\n+  /**\n+   * Similar to InitUnigramTable in the original code.\n+   */\n+  private def generateUnigramTable(normalizedWeights: Array[Double], tableSize: Int): Array[Int] = {\n+    val table = new Array[Int](tableSize)\n+    var index = 0\n+    var wordId = 0\n+    while (index < table.length) {\n+      table.update(index, wordId)\n+      if (index.toFloat / table.length >= normalizedWeights(wordId)) {\n+        wordId = math.min(normalizedWeights.length - 1, wordId + 1)\n+      }\n+      index += 1\n+    }\n+    table\n+  }\n+\n+  private def generateVocab[S <: Iterable[String]](\n+      input: RDD[S],\n+      minCount: Int,\n+      sample: Double,\n+      unigramTableSize: Int): Vocabulary = {\n+    val sc = input.context\n+\n+    val words = input.flatMap(x => x)\n+\n+    val sortedWordCounts = words.map(w => (w, 1L))\n+      .reduceByKey(_ + _)\n+      .filter{case (w, c) => c >= minCount}\n+      .collect()\n+      .sortWith{case ((w1, c1), (w2, c2)) => c1 > c2}\n+      .zipWithIndex\n+\n+    val totalWordCount = sortedWordCounts.map(_._1._2).sum\n+\n+    val vocabMap = sortedWordCounts.map{case ((w, c), i) =>\n+      w -> i\n+    }.toMap\n+\n+    val samplingTable = new Array[Float](vocabMap.size)\n+\n+    if (sample > Double.MinPositiveValue) {\n+      sortedWordCounts.foreach { case ((w, c), i) =>\n+        val samplingRatio = sample * totalWordCount / c\n+        samplingTable.update(i, (math.sqrt(samplingRatio) + samplingRatio).toFloat)\n+      }\n+    }\n+\n+    val weights = sortedWordCounts.map{ case((_, x), _) => scala.math.pow(x, power)}\n+    val totalWeight = weights.sum\n+\n+    val normalizedCumWeights = weights.scanLeft(0.0)(_ + _).tail.map(x => x / totalWeight)\n+\n+    val unigramTable = generateUnigramTable(normalizedCumWeights, unigramTableSize)\n+\n+    Vocabulary(totalWordCount, vocabMap, unigramTable, samplingTable)\n+  }\n+\n+  private def zeroVector(v: Array[Float]): Unit = {\n+    var i = 0\n+    while(i < v.length) {\n+      v.update(i, 0.0f)\n+      i+= 1\n+    }\n+  }\n+\n+  private def generateContextWordPairs(\n+      sentence: Array[Int],\n+      window: Int,\n+      doSample: Boolean,\n+      samplingTable: Array[Float],\n+      random: XORShiftRandom): Iterator[(Array[Int], Int)] = {\n+    val reducedSentence = if (doSample) {\n+      sentence.filter(i => samplingTable(i) > random.nextFloat)\n+    } else {\n+      sentence\n+    }\n+    reducedSentence.iterator.zipWithIndex.map { case (word, i) =>\n+      val b = window - random.nextInt(window) // (window - a) in original code\n+      // pick b words around the current word index\n+      val start = math.max(0, i - b) // c in original code, floor ar 0\n+      val end = math.min(reducedSentence.length, i + b + 1) // cap at sentence length\n+      // make sure current word is not a part of the context\n+      val contextIds = reducedSentence.view.zipWithIndex.slice(start, end)\n+        .filter{case (_, pos) => pos != i}.map(_._1)\n+      (contextIds.toArray, word)\n+    }\n+  }\n+\n+  // This essentially helps translate from uniform distribution to a distribution\n+  // resembling uni-gram frequency distribution.\n+  private def generateNegativeSamples("
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "I prefer not to reference \"original code\" and certainly not without providing a reference somewhere, but I don't feel too strongly about it.",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T20:49:55Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            gb.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $i_, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: ${timePerIteration} ms\")\n+    }\n+    digitSentences.unpersist()\n+    vocabMapBroadcast.destroy()\n+    unigramTableBroadcast.destroy()\n+    sampleTableBroadcast.destroy()\n+\n+    new feature.Word2VecModel(vocabMap, syn0Global)\n+  }\n+\n+  /**\n+   * Similar to InitUnigramTable in the original code.\n+   */\n+  private def generateUnigramTable(normalizedWeights: Array[Double], tableSize: Int): Array[Int] = {\n+    val table = new Array[Int](tableSize)\n+    var index = 0\n+    var wordId = 0\n+    while (index < table.length) {\n+      table.update(index, wordId)\n+      if (index.toFloat / table.length >= normalizedWeights(wordId)) {\n+        wordId = math.min(normalizedWeights.length - 1, wordId + 1)\n+      }\n+      index += 1\n+    }\n+    table\n+  }\n+\n+  private def generateVocab[S <: Iterable[String]](\n+      input: RDD[S],\n+      minCount: Int,\n+      sample: Double,\n+      unigramTableSize: Int): Vocabulary = {\n+    val sc = input.context\n+\n+    val words = input.flatMap(x => x)\n+\n+    val sortedWordCounts = words.map(w => (w, 1L))\n+      .reduceByKey(_ + _)\n+      .filter{case (w, c) => c >= minCount}\n+      .collect()\n+      .sortWith{case ((w1, c1), (w2, c2)) => c1 > c2}\n+      .zipWithIndex\n+\n+    val totalWordCount = sortedWordCounts.map(_._1._2).sum\n+\n+    val vocabMap = sortedWordCounts.map{case ((w, c), i) =>\n+      w -> i\n+    }.toMap\n+\n+    val samplingTable = new Array[Float](vocabMap.size)\n+\n+    if (sample > Double.MinPositiveValue) {\n+      sortedWordCounts.foreach { case ((w, c), i) =>\n+        val samplingRatio = sample * totalWordCount / c\n+        samplingTable.update(i, (math.sqrt(samplingRatio) + samplingRatio).toFloat)\n+      }\n+    }\n+\n+    val weights = sortedWordCounts.map{ case((_, x), _) => scala.math.pow(x, power)}\n+    val totalWeight = weights.sum\n+\n+    val normalizedCumWeights = weights.scanLeft(0.0)(_ + _).tail.map(x => x / totalWeight)\n+\n+    val unigramTable = generateUnigramTable(normalizedCumWeights, unigramTableSize)\n+\n+    Vocabulary(totalWordCount, vocabMap, unigramTable, samplingTable)\n+  }\n+\n+  private def zeroVector(v: Array[Float]): Unit = {\n+    var i = 0\n+    while(i < v.length) {\n+      v.update(i, 0.0f)\n+      i+= 1\n+    }\n+  }\n+\n+  private def generateContextWordPairs(\n+      sentence: Array[Int],\n+      window: Int,\n+      doSample: Boolean,\n+      samplingTable: Array[Float],\n+      random: XORShiftRandom): Iterator[(Array[Int], Int)] = {\n+    val reducedSentence = if (doSample) {\n+      sentence.filter(i => samplingTable(i) > random.nextFloat)\n+    } else {\n+      sentence\n+    }\n+    reducedSentence.iterator.zipWithIndex.map { case (word, i) =>\n+      val b = window - random.nextInt(window) // (window - a) in original code\n+      // pick b words around the current word index\n+      val start = math.max(0, i - b) // c in original code, floor ar 0"
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "add doc",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T20:50:27Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            gb.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $i_, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: ${timePerIteration} ms\")\n+    }\n+    digitSentences.unpersist()\n+    vocabMapBroadcast.destroy()\n+    unigramTableBroadcast.destroy()\n+    sampleTableBroadcast.destroy()\n+\n+    new feature.Word2VecModel(vocabMap, syn0Global)\n+  }\n+\n+  /**\n+   * Similar to InitUnigramTable in the original code.\n+   */\n+  private def generateUnigramTable(normalizedWeights: Array[Double], tableSize: Int): Array[Int] = {\n+    val table = new Array[Int](tableSize)\n+    var index = 0\n+    var wordId = 0\n+    while (index < table.length) {\n+      table.update(index, wordId)\n+      if (index.toFloat / table.length >= normalizedWeights(wordId)) {\n+        wordId = math.min(normalizedWeights.length - 1, wordId + 1)\n+      }\n+      index += 1\n+    }\n+    table\n+  }\n+\n+  private def generateVocab[S <: Iterable[String]](\n+      input: RDD[S],\n+      minCount: Int,\n+      sample: Double,\n+      unigramTableSize: Int): Vocabulary = {\n+    val sc = input.context\n+\n+    val words = input.flatMap(x => x)\n+\n+    val sortedWordCounts = words.map(w => (w, 1L))\n+      .reduceByKey(_ + _)\n+      .filter{case (w, c) => c >= minCount}\n+      .collect()\n+      .sortWith{case ((w1, c1), (w2, c2)) => c1 > c2}\n+      .zipWithIndex\n+\n+    val totalWordCount = sortedWordCounts.map(_._1._2).sum\n+\n+    val vocabMap = sortedWordCounts.map{case ((w, c), i) =>\n+      w -> i\n+    }.toMap\n+\n+    val samplingTable = new Array[Float](vocabMap.size)\n+\n+    if (sample > Double.MinPositiveValue) {\n+      sortedWordCounts.foreach { case ((w, c), i) =>\n+        val samplingRatio = sample * totalWordCount / c\n+        samplingTable.update(i, (math.sqrt(samplingRatio) + samplingRatio).toFloat)\n+      }\n+    }\n+\n+    val weights = sortedWordCounts.map{ case((_, x), _) => scala.math.pow(x, power)}\n+    val totalWeight = weights.sum\n+\n+    val normalizedCumWeights = weights.scanLeft(0.0)(_ + _).tail.map(x => x / totalWeight)\n+\n+    val unigramTable = generateUnigramTable(normalizedCumWeights, unigramTableSize)\n+\n+    Vocabulary(totalWordCount, vocabMap, unigramTable, samplingTable)\n+  }\n+\n+  private def zeroVector(v: Array[Float]): Unit = {\n+    var i = 0\n+    while(i < v.length) {\n+      v.update(i, 0.0f)\n+      i+= 1\n+    }\n+  }\n+\n+  private def generateContextWordPairs("
  }],
  "prId": 17673
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "The unigram table by default is `100e6 * 4bytes = 400 MB` in size. The original code was meant to run on a single machine, but for Spark we are going to serialize this every iteration, which may be prohibitive. AFAICT you can just simulate the behavior needed here without actually duplicating word indices over a 100million length array. Just keep the normalized cumulative weights around and here, generate a random number between 0 and 1. Then, just do a binary search to figure out which word index it is. You only need to store O(vocabSize) numbers. The time complexity increases from O(numNonNegativeSamples) to O(numNonNegativeSamples * log2(vocabSize)), but the inner loop of the training is bound by O(numNonNegativeSamples * vectorSize), which should typically be much larger. Have you tested it at scale? Does the broadcast step take a long time? Also, what are typical vocabulary sizes? A vocab size of 1e6 is going to be about 4 MB so the space savings are pretty big.",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-05T21:35:36Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            gb.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $i_, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: ${timePerIteration} ms\")\n+    }\n+    digitSentences.unpersist()\n+    vocabMapBroadcast.destroy()\n+    unigramTableBroadcast.destroy()\n+    sampleTableBroadcast.destroy()\n+\n+    new feature.Word2VecModel(vocabMap, syn0Global)\n+  }\n+\n+  /**\n+   * Similar to InitUnigramTable in the original code.\n+   */\n+  private def generateUnigramTable(normalizedWeights: Array[Double], tableSize: Int): Array[Int] = {\n+    val table = new Array[Int](tableSize)\n+    var index = 0\n+    var wordId = 0\n+    while (index < table.length) {\n+      table.update(index, wordId)\n+      if (index.toFloat / table.length >= normalizedWeights(wordId)) {\n+        wordId = math.min(normalizedWeights.length - 1, wordId + 1)\n+      }\n+      index += 1\n+    }\n+    table\n+  }\n+\n+  private def generateVocab[S <: Iterable[String]](\n+      input: RDD[S],\n+      minCount: Int,\n+      sample: Double,\n+      unigramTableSize: Int): Vocabulary = {\n+    val sc = input.context\n+\n+    val words = input.flatMap(x => x)\n+\n+    val sortedWordCounts = words.map(w => (w, 1L))\n+      .reduceByKey(_ + _)\n+      .filter{case (w, c) => c >= minCount}\n+      .collect()\n+      .sortWith{case ((w1, c1), (w2, c2)) => c1 > c2}\n+      .zipWithIndex\n+\n+    val totalWordCount = sortedWordCounts.map(_._1._2).sum\n+\n+    val vocabMap = sortedWordCounts.map{case ((w, c), i) =>\n+      w -> i\n+    }.toMap\n+\n+    val samplingTable = new Array[Float](vocabMap.size)\n+\n+    if (sample > Double.MinPositiveValue) {\n+      sortedWordCounts.foreach { case ((w, c), i) =>\n+        val samplingRatio = sample * totalWordCount / c\n+        samplingTable.update(i, (math.sqrt(samplingRatio) + samplingRatio).toFloat)\n+      }\n+    }\n+\n+    val weights = sortedWordCounts.map{ case((_, x), _) => scala.math.pow(x, power)}\n+    val totalWeight = weights.sum\n+\n+    val normalizedCumWeights = weights.scanLeft(0.0)(_ + _).tail.map(x => x / totalWeight)\n+\n+    val unigramTable = generateUnigramTable(normalizedCumWeights, unigramTableSize)\n+\n+    Vocabulary(totalWordCount, vocabMap, unigramTable, samplingTable)\n+  }\n+\n+  private def zeroVector(v: Array[Float]): Unit = {\n+    var i = 0\n+    while(i < v.length) {\n+      v.update(i, 0.0f)\n+      i+= 1\n+    }\n+  }\n+\n+  private def generateContextWordPairs(\n+      sentence: Array[Int],\n+      window: Int,\n+      doSample: Boolean,\n+      samplingTable: Array[Float],\n+      random: XORShiftRandom): Iterator[(Array[Int], Int)] = {\n+    val reducedSentence = if (doSample) {\n+      sentence.filter(i => samplingTable(i) > random.nextFloat)\n+    } else {\n+      sentence\n+    }\n+    reducedSentence.iterator.zipWithIndex.map { case (word, i) =>\n+      val b = window - random.nextInt(window) // (window - a) in original code\n+      // pick b words around the current word index\n+      val start = math.max(0, i - b) // c in original code, floor ar 0\n+      val end = math.min(reducedSentence.length, i + b + 1) // cap at sentence length\n+      // make sure current word is not a part of the context\n+      val contextIds = reducedSentence.view.zipWithIndex.slice(start, end)\n+        .filter{case (_, pos) => pos != i}.map(_._1)\n+      (contextIds.toArray, word)\n+    }\n+  }\n+\n+  // This essentially helps translate from uniform distribution to a distribution\n+  // resembling uni-gram frequency distribution.\n+  private def generateNegativeSamples(\n+      random: XORShiftRandom,\n+      word: Int,\n+      unigramTable: Array[Int],\n+      numSamples: Int,\n+      arr: Array[Int]): Unit = {\n+    assert(numSamples + 1 == arr.length,\n+      s\"Input array should be large enough to hold ${numSamples} negative samples\")\n+    arr.update(0, word)\n+    var i = 1\n+    while (i <= numSamples) {\n+      val negSample = unigramTable(random.nextInt(unigramTable.length))"
  }, {
    "author": {
      "login": "shubhamchopra"
    },
    "body": "I agree with the analysis. I did initially implement it using the binary search method you suggested. Changing to this reduce that complexity to constant time, and I saw 5%-10% performance gain from it. I do agree about the space savings, I thought the space sacrifice was justified for any time savings we could get. Open to discussion/suggestions on this.\r\n\r\nTo answer your questions:\r\n1. I have run it at scale, on wikipedia dump and some other data sets.\r\n2. This broadcast doesn't take too long, and happens only once at startup. Note that the weights are also collected and broadcasted every iteration, and they are also similar in size (1e6 vocab x 100 dimensions x 4bytes ~ 400Mb). \r\n3. Typical vocab sizes can be 500k-1e6. As a comparison [GloVe Vectors](https://nlp.stanford.edu/projects/glove/) vocab sizes range from 400k for wikipedia to 2e6 for web-crawl and such.",
    "commit": "9090b967e03e43e3a709d9c2c94fe75de5b9a8e6",
    "createdAt": "2017-10-11T20:18:03Z",
    "diffHunk": "@@ -0,0 +1,344 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import com.github.fommil.netlib.BLAS.{getInstance => blas}\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.mllib.feature\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.util.random.XORShiftRandom\n+\n+object Word2VecCBOWSolver extends Logging {\n+  // learning rate is updated for every batch of size batchSize\n+  private val batchSize = 10000\n+\n+  // power to raise the unigram distribution with\n+  private val power = 0.75\n+\n+  private val MAX_EXP = 6\n+\n+  case class Vocabulary(\n+    totalWordCount: Long,\n+    vocabMap: Map[String, Int],\n+    unigramTable: Array[Int],\n+    samplingTable: Array[Float])\n+\n+  /**\n+   * This method implements Word2Vec Continuous Bag Of Words based implementation using\n+   * negative sampling optimization, using BLAS for vectorizing operations where applicable.\n+   * The algorithm is parallelized in the same way as the skip-gram based estimation.\n+   * We divide input data into N equally sized random partitions.\n+   * We then generate initial weights and broadcast them to the N partitions. This way\n+   * all the partitions start with the same initial weights. We then run N independent\n+   * estimations that each estimate a model on a partition. The weights learned\n+   * from each of the N models are averaged and rebroadcast the weights.\n+   * This process is repeated `maxIter` number of times.\n+   *\n+   * @param input A RDD of strings. Each string would be considered a sentence.\n+   * @return Estimated word2vec model\n+   */\n+  def fitCBOW[S <: Iterable[String]](\n+      word2Vec: Word2Vec,\n+      input: RDD[S]): feature.Word2VecModel = {\n+\n+    val negativeSamples = word2Vec.getNegativeSamples\n+    val sample = word2Vec.getSample\n+\n+    val Vocabulary(totalWordCount, vocabMap, uniTable, sampleTable) =\n+      generateVocab(input, word2Vec.getMinCount, sample, word2Vec.getUnigramTableSize)\n+    val vocabSize = vocabMap.size\n+\n+    assert(negativeSamples < vocabSize, s\"Vocab size ($vocabSize) cannot be smaller\" +\n+      s\" than negative samples($negativeSamples)\")\n+\n+    val seed = word2Vec.getSeed\n+    val initRandom = new XORShiftRandom(seed)\n+\n+    val vectorSize = word2Vec.getVectorSize\n+    val syn0Global = Array.fill(vocabSize * vectorSize)(initRandom.nextFloat - 0.5f)\n+    val syn1Global = Array.fill(vocabSize * vectorSize)(0.0f)\n+\n+    val sc = input.context\n+\n+    val vocabMapBroadcast = sc.broadcast(vocabMap)\n+    val unigramTableBroadcast = sc.broadcast(uniTable)\n+    val sampleTableBroadcast = sc.broadcast(sampleTable)\n+\n+    val windowSize = word2Vec.getWindowSize\n+    val maxSentenceLength = word2Vec.getMaxSentenceLength\n+    val numPartitions = word2Vec.getNumPartitions\n+\n+    val digitSentences = input.flatMap { sentence =>\n+      val wordIndexes = sentence.flatMap(vocabMapBroadcast.value.get)\n+      wordIndexes.grouped(maxSentenceLength).map(_.toArray)\n+    }.repartition(numPartitions).cache()\n+\n+    val learningRate = word2Vec.getStepSize\n+\n+    val wordsPerPartition = totalWordCount / numPartitions\n+\n+    logInfo(s\"VocabSize: ${vocabMap.size}, TotalWordCount: $totalWordCount\")\n+\n+    val maxIter = word2Vec.getMaxIter\n+    for {iteration <- 1 to maxIter} {\n+      logInfo(s\"Starting iteration: $iteration\")\n+      val iterationStartTime = System.nanoTime()\n+\n+      val syn0bc = sc.broadcast(syn0Global)\n+      val syn1bc = sc.broadcast(syn1Global)\n+\n+      val partialFits = digitSentences.mapPartitionsWithIndex { case (i_, iter) =>\n+        logInfo(s\"Iteration: $iteration, Partition: $i_\")\n+        val random = new XORShiftRandom(seed ^ ((i_ + 1) << 16) ^ ((-iteration - 1) << 8))\n+        val contextWordPairs = iter.flatMap { s =>\n+          val doSample = sample > Double.MinPositiveValue\n+          generateContextWordPairs(s, windowSize, doSample, sampleTableBroadcast.value, random)\n+        }\n+\n+        val groupedBatches = contextWordPairs.grouped(batchSize)\n+\n+        val negLabels = 1.0f +: Array.fill(negativeSamples)(0.0f)\n+        val syn0 = syn0bc.value\n+        val syn1 = syn1bc.value\n+        val unigramTable = unigramTableBroadcast.value\n+\n+        // initialize intermediate arrays\n+        val contextVec = new Array[Float](vectorSize)\n+        val l2Vectors = new Array[Float](vectorSize * (negativeSamples + 1))\n+        val gb = new Array[Float](negativeSamples + 1)\n+        val neu1e = new Array[Float](vectorSize)\n+        val wordIndices = new Array[Int](negativeSamples + 1)\n+\n+        val time = System.nanoTime\n+        var batchTime = System.nanoTime\n+        var idx = -1L\n+        for (batch <- groupedBatches) {\n+          idx = idx + 1\n+\n+          val wordRatio =\n+            idx.toFloat * batchSize /\n+              (maxIter * (wordsPerPartition.toFloat + 1)) + ((iteration - 1).toFloat / maxIter)\n+          val alpha = math.max(learningRate * 0.0001, learningRate * (1 - wordRatio)).toFloat\n+\n+          if(idx % 10 == 0 && idx > 0) {\n+            logInfo(s\"Partition: $i_, wordRatio = $wordRatio, alpha = $alpha\")\n+            val wordCount = batchSize * idx\n+            val timeTaken = (System.nanoTime - time) / 1e6\n+            val batchWordCount = 10 * batchSize\n+            val currentBatchTime = (System.nanoTime - batchTime) / 1e6\n+            batchTime = System.nanoTime\n+            logDebug(s\"Partition: $i_, Batch time: $currentBatchTime ms, batch speed: \" +\n+              s\"${batchWordCount / currentBatchTime * 1000} words/s\")\n+            logDebug(s\"Partition: $i_, Cumulative time: $timeTaken ms, cumulative speed: \" +\n+              s\"${wordCount / timeTaken * 1000} words/s\")\n+          }\n+\n+          val errors = for ((contextIds, word) <- batch) yield {\n+            // initialize vectors to 0\n+            zeroVector(contextVec)\n+            zeroVector(l2Vectors)\n+            zeroVector(gb)\n+            zeroVector(neu1e)\n+\n+            val scale = 1.0f / contextIds.length\n+\n+            // feed forward\n+            contextIds.foreach { c =>\n+              blas.saxpy(vectorSize, scale, syn0, c * vectorSize, 1, contextVec, 0, 1)\n+            }\n+\n+            generateNegativeSamples(random, word, unigramTable, negativeSamples, wordIndices)\n+\n+            Iterator.range(0, wordIndices.length).foreach { i =>\n+              Array.copy(syn1, vectorSize * wordIndices(i), l2Vectors, vectorSize * i, vectorSize)\n+            }\n+\n+            // propagating hidden to output in batch\n+            val rows = negativeSamples + 1\n+            val cols = vectorSize\n+            blas.sgemv(\"T\", cols, rows, 1.0f, l2Vectors, 0, cols, contextVec, 0, 1, 0.0f, gb, 0, 1)\n+\n+            Iterator.range(0, negativeSamples + 1).foreach { i =>\n+              if (gb(i) > -MAX_EXP && gb(i) < MAX_EXP) {\n+                val v = 1.0f / (1 + math.exp(-gb(i)).toFloat)\n+                // computing error gradient\n+                val err = (negLabels(i) - v) * alpha\n+                // update hidden -> output layer, syn1\n+                blas.saxpy(vectorSize, err, contextVec, 0, 1, syn1, wordIndices(i) * vectorSize, 1)\n+                // update for word vectors\n+                blas.saxpy(vectorSize, err, l2Vectors, i * vectorSize, 1, neu1e, 0, 1)\n+                gb.update(i, err)\n+              } else {\n+                gb.update(i, 0.0f)\n+              }\n+            }\n+\n+            // update input -> hidden layer, syn0\n+            contextIds.foreach { i =>\n+              blas.saxpy(vectorSize, 1.0f, neu1e, 0, 1, syn0, i * vectorSize, 1)\n+            }\n+            gb.map(math.abs).sum / alpha\n+          }\n+          logInfo(s\"Partition: $i_, Average Batch Error = ${errors.sum / batchSize}\")\n+        }\n+        Iterator.tabulate(vocabSize) { index =>\n+          (index, syn0.slice(index * vectorSize, (index + 1) * vectorSize))\n+        } ++ Iterator.tabulate(vocabSize) { index =>\n+          (vocabSize + index, syn1.slice(index * vectorSize, (index + 1) * vectorSize))\n+        }\n+      }\n+\n+      val aggedMatrices = partialFits.reduceByKey { case (v1, v2) =>\n+        blas.saxpy(vectorSize, 1.0f, v2, 1, v1, 1)\n+        v1\n+      }.collect()\n+\n+      val norm = 1.0f / numPartitions\n+      aggedMatrices.foreach {case (index, v) =>\n+        blas.sscal(v.length, norm, v, 0, 1)\n+        if (index < vocabSize) {\n+          Array.copy(v, 0, syn0Global, index * vectorSize, vectorSize)\n+        } else {\n+          Array.copy(v, 0, syn1Global, (index - vocabSize) * vectorSize, vectorSize)\n+        }\n+      }\n+\n+      syn0bc.destroy(false)\n+      syn1bc.destroy(false)\n+      val timePerIteration = (System.nanoTime() - iterationStartTime) / 1e6\n+      logInfo(s\"Total time taken per iteration: ${timePerIteration} ms\")\n+    }\n+    digitSentences.unpersist()\n+    vocabMapBroadcast.destroy()\n+    unigramTableBroadcast.destroy()\n+    sampleTableBroadcast.destroy()\n+\n+    new feature.Word2VecModel(vocabMap, syn0Global)\n+  }\n+\n+  /**\n+   * Similar to InitUnigramTable in the original code.\n+   */\n+  private def generateUnigramTable(normalizedWeights: Array[Double], tableSize: Int): Array[Int] = {\n+    val table = new Array[Int](tableSize)\n+    var index = 0\n+    var wordId = 0\n+    while (index < table.length) {\n+      table.update(index, wordId)\n+      if (index.toFloat / table.length >= normalizedWeights(wordId)) {\n+        wordId = math.min(normalizedWeights.length - 1, wordId + 1)\n+      }\n+      index += 1\n+    }\n+    table\n+  }\n+\n+  private def generateVocab[S <: Iterable[String]](\n+      input: RDD[S],\n+      minCount: Int,\n+      sample: Double,\n+      unigramTableSize: Int): Vocabulary = {\n+    val sc = input.context\n+\n+    val words = input.flatMap(x => x)\n+\n+    val sortedWordCounts = words.map(w => (w, 1L))\n+      .reduceByKey(_ + _)\n+      .filter{case (w, c) => c >= minCount}\n+      .collect()\n+      .sortWith{case ((w1, c1), (w2, c2)) => c1 > c2}\n+      .zipWithIndex\n+\n+    val totalWordCount = sortedWordCounts.map(_._1._2).sum\n+\n+    val vocabMap = sortedWordCounts.map{case ((w, c), i) =>\n+      w -> i\n+    }.toMap\n+\n+    val samplingTable = new Array[Float](vocabMap.size)\n+\n+    if (sample > Double.MinPositiveValue) {\n+      sortedWordCounts.foreach { case ((w, c), i) =>\n+        val samplingRatio = sample * totalWordCount / c\n+        samplingTable.update(i, (math.sqrt(samplingRatio) + samplingRatio).toFloat)\n+      }\n+    }\n+\n+    val weights = sortedWordCounts.map{ case((_, x), _) => scala.math.pow(x, power)}\n+    val totalWeight = weights.sum\n+\n+    val normalizedCumWeights = weights.scanLeft(0.0)(_ + _).tail.map(x => x / totalWeight)\n+\n+    val unigramTable = generateUnigramTable(normalizedCumWeights, unigramTableSize)\n+\n+    Vocabulary(totalWordCount, vocabMap, unigramTable, samplingTable)\n+  }\n+\n+  private def zeroVector(v: Array[Float]): Unit = {\n+    var i = 0\n+    while(i < v.length) {\n+      v.update(i, 0.0f)\n+      i+= 1\n+    }\n+  }\n+\n+  private def generateContextWordPairs(\n+      sentence: Array[Int],\n+      window: Int,\n+      doSample: Boolean,\n+      samplingTable: Array[Float],\n+      random: XORShiftRandom): Iterator[(Array[Int], Int)] = {\n+    val reducedSentence = if (doSample) {\n+      sentence.filter(i => samplingTable(i) > random.nextFloat)\n+    } else {\n+      sentence\n+    }\n+    reducedSentence.iterator.zipWithIndex.map { case (word, i) =>\n+      val b = window - random.nextInt(window) // (window - a) in original code\n+      // pick b words around the current word index\n+      val start = math.max(0, i - b) // c in original code, floor ar 0\n+      val end = math.min(reducedSentence.length, i + b + 1) // cap at sentence length\n+      // make sure current word is not a part of the context\n+      val contextIds = reducedSentence.view.zipWithIndex.slice(start, end)\n+        .filter{case (_, pos) => pos != i}.map(_._1)\n+      (contextIds.toArray, word)\n+    }\n+  }\n+\n+  // This essentially helps translate from uniform distribution to a distribution\n+  // resembling uni-gram frequency distribution.\n+  private def generateNegativeSamples(\n+      random: XORShiftRandom,\n+      word: Int,\n+      unigramTable: Array[Int],\n+      numSamples: Int,\n+      arr: Array[Int]): Unit = {\n+    assert(numSamples + 1 == arr.length,\n+      s\"Input array should be large enough to hold ${numSamples} negative samples\")\n+    arr.update(0, word)\n+    var i = 1\n+    while (i <= numSamples) {\n+      val negSample = unigramTable(random.nextInt(unigramTable.length))"
  }],
  "prId": 17673
}]