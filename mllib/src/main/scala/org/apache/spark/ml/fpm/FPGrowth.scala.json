[{
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "also need a `ParamValidator` here",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-01-05T05:52:30Z",
    "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param.{DoubleParam, ParamMap, Params}\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * the minimal support level of the frequent pattern\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\")"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "MLLib's `FPGrowth` have a param `numPartitions`, will it be included here?",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-01-05T05:55:09Z",
    "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param.{DoubleParam, ParamMap, Params}\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * the minimal support level of the frequent pattern\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\")\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+",
    "line": 57
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "I perfer use `setInputCol` and `inputCol` instead of this.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-01-05T05:59:08Z",
    "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param.{DoubleParam, ParamMap, Params}\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * the minimal support level of the frequent pattern\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\")\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)"
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "Thanks. Let's collect more feedback about it.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-01-19T07:25:18Z",
    "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param.{DoubleParam, ParamMap, Params}\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * the minimal support level of the frequent pattern\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\")\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "ditto, `setOutputCol`",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-01-05T06:00:04Z",
    "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param.{DoubleParam, ParamMap, Params}\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * the minimal support level of the frequent pattern\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\")\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)",
    "line": 140
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "`ParamValidator.inRange(0,1,...)`",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-01-05T06:02:54Z",
    "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param.{DoubleParam, ParamMap, Params}\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * the minimal support level of the frequent pattern\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\")\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[String](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    copyValues(new FPGrowthModel(uid, parentModel))\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param parentModel a model trained by spark.mllib.fpm.FPGrowth\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val parentModel: MLlibFPGrowthModel[_])\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\")"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "Is there some discussion about the behavior of `transform` here? It seems a new feature.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-01-05T06:14:25Z",
    "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param.{DoubleParam, ParamMap, Params}\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * the minimal support level of the frequent pattern\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\")\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[String](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    copyValues(new FPGrowthModel(uid, parentModel))\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param parentModel a model trained by spark.mllib.fpm.FPGrowth\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val parentModel: MLlibFPGrowthModel[_])\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\")\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {"
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "Indeed it's a new feature for making FPGrowthModel a Transformer. I've added it to the PR description to help draw more attention.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-01-19T07:27:40Z",
    "diffHunk": "@@ -0,0 +1,232 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param.{DoubleParam, ParamMap, Params}\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * the minimal support level of the frequent pattern\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\")\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[String](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    copyValues(new FPGrowthModel(uid, parentModel))\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param parentModel a model trained by spark.mllib.fpm.FPGrowth\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val parentModel: MLlibFPGrowthModel[_])\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\")\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "aray"
    },
    "body": "So if there is a rule {a,b} -> c and I pass this the set {a,b,c} it's going to \"predict\" c? Also, should we include the confidence number in the result?",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-01-20T21:47:32Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[String](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    copyValues(new FPGrowthModel(uid, parentModel))\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param parentModel a model trained by spark.mllib.fpm.FPGrowth\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val parentModel: MLlibFPGrowthModel[_])\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    val associationRules = getAssociationRules.rdd.map(r =>\n+      (r.getSeq[String](0), r.getSeq[String](1))\n+    ).collect()\n+\n+    // For each rule, examine the input items and summarize the consequents\n+    val predictUDF = udf((items: Seq[String]) => associationRules.flatMap( r =>\n+      if (r._1.forall(items.contains(_))) r._2 else Array.empty[String]"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "aray"
    },
    "body": "Should we guard against this being too large?",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-01-20T22:15:39Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[String](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    copyValues(new FPGrowthModel(uid, parentModel))\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param parentModel a model trained by spark.mllib.fpm.FPGrowth\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val parentModel: MLlibFPGrowthModel[_])\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    val associationRules = getAssociationRules.rdd.map(r =>\n+      (r.getSeq[String](0), r.getSeq[String](1))\n+    ).collect()"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "aray"
    },
    "body": "Why are we including `featuresCol` as a new column here?",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-01-20T22:24:42Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[String](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    copyValues(new FPGrowthModel(uid, parentModel))\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param parentModel a model trained by spark.mllib.fpm.FPGrowth\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val parentModel: MLlibFPGrowthModel[_])\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    val associationRules = getAssociationRules.rdd.map(r =>\n+      (r.getSeq[String](0), r.getSeq[String](1))\n+    ).collect()\n+\n+    // For each rule, examine the input items and summarize the consequents\n+    val predictUDF = udf((items: Seq[String]) => associationRules.flatMap( r =>\n+      if (r._1.forall(items.contains(_))) r._2 else Array.empty[String]\n+    ).distinct)\n+    dataset.withColumn($(predictionCol), predictUDF(col($(featuresCol))))"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "aray"
    },
    "body": "Are we worried about recomputing association rules every time we do a transform?",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-01-20T22:25:32Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[String](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    copyValues(new FPGrowthModel(uid, parentModel))\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param parentModel a model trained by spark.mllib.fpm.FPGrowth\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val parentModel: MLlibFPGrowthModel[_])\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    val associationRules = getAssociationRules.rdd.map(r =>"
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "Good point. Thanks @aray. I've tried to leverage lazy val to avoid unnecessary computation.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-01T06:53:10Z",
    "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[String](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    copyValues(new FPGrowthModel(uid, parentModel))\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param parentModel a model trained by spark.mllib.fpm.FPGrowth\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val parentModel: MLlibFPGrowthModel[_])\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    val associationRules = getAssociationRules.rdd.map(r =>"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Make this an expertParam",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-01T18:21:41Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group param"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Use lowercase name to match other algs: \"fpgrowth\"",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-01T18:21:42Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "override",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-01T18:21:43Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  def fit(dataset: Dataset[_]): FPGrowthModel = {"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "setParent",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-01T18:21:44Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[String](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    copyValues(new FPGrowthModel(uid, parentModel))"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "getFreqItems -> getFreqItemsets",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-01T18:21:47Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[String](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    copyValues(new FPGrowthModel(uid, parentModel))\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param parentModel a model trained by spark.mllib.fpm.FPGrowth\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val parentModel: MLlibFPGrowthModel[_])\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+   /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", with * \"antecedent\" and\n+   * \"consequent\" being Array[String] and the \"confidence\" being Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient private lazy val associationRulesModel: AssociationRulesModel = {\n+    val freqItems = getFreqItems\n+    val associationRules = new AssociationRules()\n+      .setMinConfidence($(minConfidence))\n+      .setItemsCol(\"items\")\n+      .setFreqCol(\"freq\")\n+    associationRules.fit(freqItems)\n+  }\n+\n+   /**\n+   * Get association rules fitted by AssociationRules using the minConfidence, in the format\n+   * of DataFrame(\"antecedent\", \"consequent\", \"confidence\")\n+   */\n+  @Since(\"2.2.0\")\n+  def getAssociationRules: DataFrame = {\n+     associationRulesModel.associationRules\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getFreqItems: DataFrame = {"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "setParent",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-01T18:21:49Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{FPGrowth => MLlibFPGrowth, FPGrowthModel => MLlibFPGrowthModel}\n+import org.apache.spark.sql.{DataFrame, _}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{ArrayType, StringType, StructType}\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new ArrayType(StringType, false))\n+    SchemaUtils.appendColumn(schema, $(predictionCol), new ArrayType(StringType, false))\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"FPGrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[String](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    copyValues(new FPGrowthModel(uid, parentModel))\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param parentModel a model trained by spark.mllib.fpm.FPGrowth\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val parentModel: MLlibFPGrowthModel[_])\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+   /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", with * \"antecedent\" and\n+   * \"consequent\" being Array[String] and the \"confidence\" being Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient private lazy val associationRulesModel: AssociationRulesModel = {\n+    val freqItems = getFreqItems\n+    val associationRules = new AssociationRules()\n+      .setMinConfidence($(minConfidence))\n+      .setItemsCol(\"items\")\n+      .setFreqCol(\"freq\")\n+    associationRules.fit(freqItems)\n+  }\n+\n+   /**\n+   * Get association rules fitted by AssociationRules using the minConfidence, in the format\n+   * of DataFrame(\"antecedent\", \"consequent\", \"confidence\")\n+   */\n+  @Since(\"2.2.0\")\n+  def getAssociationRules: DataFrame = {\n+     associationRulesModel.associationRules\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getFreqItems: DataFrame = {\n+    val sqlContext = SparkSession.builder().getOrCreate()\n+    import sqlContext.implicits._\n+    parentModel.freqItemsets.map(f => (f.items.map(_.toString), f.freq))\n+      .toDF(\"items\", \"freq\")\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    associationRulesModel.setItemsCol($(featuresCol)).transform(dataset)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowthModel = {\n+    val copied = new FPGrowthModel(uid, parentModel)\n+    copyValues(copied, extra)"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "style: indent with 2 spaces",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-17T07:23:04Z",
    "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+FPGrowth => MLlibFPGrowth}"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Since annotation",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-17T07:23:07Z",
    "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Document what it means when this is not set and that this must be >= 1.\r\nAlso say this is not set by default.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-17T07:23:10Z",
    "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "\"has not\" -> \"has no\"\r\nAlso, put this comment in the Scaladoc for \"val minConfidence\"",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-17T07:23:40Z",
    "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  minConfidence has not effect during fitting."
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Add Since annotation",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-17T07:23:44Z",
    "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  minConfidence has not effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Call transformSchema first to do schema validation.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-17T07:23:46Z",
    "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  minConfidence has not effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    genericFit(dataset)"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Since annotation",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-17T07:23:48Z",
    "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  minConfidence has not effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    val rows = parentModel.freqItemsets\n+      .map(f => (f.items, f.freq))\n+      .map(cols => Row(cols._1, cols._2))\n+\n+    val dt = dataset.schema($(featuresCol)).dataType\n+    val fields = Array(StructField(\"items\", dt, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false))\n+    val schema = StructType(fields)\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema).toDF(\"items\", \"freq\")\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Since annotation",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-17T07:23:50Z",
    "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  minConfidence has not effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    val rows = parentModel.freqItemsets\n+      .map(f => (f.items, f.freq))\n+      .map(cols => Row(cols._1, cols._2))\n+\n+    val dt = dataset.schema($(featuresCol)).dataType\n+    val fields = Array(StructField(\"items\", dt, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false))\n+    val schema = StructType(fields)\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema).toDF(\"items\", \"freq\")\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    val freqItemsets: DataFrame)"
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "how about make it private since we already have a getter for it.\r\n",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-20T02:22:00Z",
    "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  minConfidence has not effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    val rows = parentModel.freqItemsets\n+      .map(f => (f.items, f.freq))\n+      .map(cols => Row(cols._1, cols._2))\n+\n+    val dt = dataset.schema($(featuresCol)).dataType\n+    val fields = Array(StructField(\"items\", dt, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false))\n+    val schema = StructType(fields)\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema).toDF(\"items\", \"freq\")\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    val freqItemsets: DataFrame)"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Add Since annotation to object",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-17T07:23:54Z",
    "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  minConfidence has not effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    val rows = parentModel.freqItemsets\n+      .map(f => (f.items, f.freq))\n+      .map(cols => Row(cols._1, cols._2))\n+\n+    val dt = dataset.schema($(featuresCol)).dataType\n+    val fields = Array(StructField(\"items\", dt, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false))\n+    val schema = StructType(fields)\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema).toDF(\"items\", \"freq\")\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+   /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {\n+    val freqItems = getFreqItemsets\n+    AssocaitionRules.getAssocationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient val getFreqItemsets: DataFrame = freqItemsets\n+\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    genericTransform(dataset, getAssociationRules)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_], associationRules: DataFrame): DataFrame = {\n+    // use unique id to perform the join and aggregateByKey\n+    val itemsRDD = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .distinct().zipWithUniqueId().map(_.swap).cache()\n+    val rulesRDD = associationRules.rdd.map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val itemsWithConsequents = itemsRDD.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val itemSet = items.toSet\n+        val consequents = if (antecedent.forall(itemSet.contains(_))) consequent else Seq.empty\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])(\n+      (ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)\n+\n+    val mappingRDD = itemsRDD.join(itemsWithConsequents)\n+      .map { case (id, (items, consequent)) => (items, consequent) }\n+      .map (cols => Row(cols._1, cols._2))\n+    val dt = dataset.schema($(featuresCol)).dataType\n+    val fields = Array($(featuresCol), $(predictionCol))\n+      .map(fieldName => StructField(fieldName, dt, nullable = true))\n+    val schema = StructType(fields)\n+    val mapping = dataset.sparkSession.createDataFrame(mappingRDD, schema)\n+\n+    dataset.join(mapping, $(featuresCol))\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowthModel = {\n+    val copied = new FPGrowthModel(uid, freqItemsets)\n+    copyValues(copied, extra).setParent(this.parent)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def write: MLWriter = new FPGrowthModel.FPGrowthModelWriter(this)\n+}\n+\n+object FPGrowthModel extends MLReadable[FPGrowthModel] {"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "typo: AssocaitionRules -> AssociationRules",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-17T07:23:56Z",
    "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  minConfidence has not effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    val rows = parentModel.freqItemsets\n+      .map(f => (f.items, f.freq))\n+      .map(cols => Row(cols._1, cols._2))\n+\n+    val dt = dataset.schema($(featuresCol)).dataType\n+    val fields = Array(StructField(\"items\", dt, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false))\n+    val schema = StructType(fields)\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema).toDF(\"items\", \"freq\")\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+   /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {\n+    val freqItems = getFreqItemsets\n+    AssocaitionRules.getAssocationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient val getFreqItemsets: DataFrame = freqItemsets\n+\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    genericTransform(dataset, getAssociationRules)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_], associationRules: DataFrame): DataFrame = {\n+    // use unique id to perform the join and aggregateByKey\n+    val itemsRDD = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .distinct().zipWithUniqueId().map(_.swap).cache()\n+    val rulesRDD = associationRules.rdd.map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val itemsWithConsequents = itemsRDD.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val itemSet = items.toSet\n+        val consequents = if (antecedent.forall(itemSet.contains(_))) consequent else Seq.empty\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])(\n+      (ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)\n+\n+    val mappingRDD = itemsRDD.join(itemsWithConsequents)\n+      .map { case (id, (items, consequent)) => (items, consequent) }\n+      .map (cols => Row(cols._1, cols._2))\n+    val dt = dataset.schema($(featuresCol)).dataType\n+    val fields = Array($(featuresCol), $(predictionCol))\n+      .map(fieldName => StructField(fieldName, dt, nullable = true))\n+    val schema = StructType(fields)\n+    val mapping = dataset.sparkSession.createDataFrame(mappingRDD, schema)\n+\n+    dataset.join(mapping, $(featuresCol))\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowthModel = {\n+    val copied = new FPGrowthModel(uid, freqItemsets)\n+    copyValues(copied, extra).setParent(this.parent)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def write: MLWriter = new FPGrowthModel.FPGrowthModelWriter(this)\n+}\n+\n+object FPGrowthModel extends MLReadable[FPGrowthModel] {\n+  @Since(\"2.2.0\")\n+  override def read: MLReader[FPGrowthModel] = new FPGrowthModelReader\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowthModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FPGrowthModel]] */\n+  private[FPGrowthModel]\n+  class FPGrowthModelWriter(instance: FPGrowthModel) extends MLWriter {\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val dataPath = new Path(path, \"data\").toString\n+      instance.freqItemsets.write.save(dataPath)\n+    }\n+  }\n+\n+  private class FPGrowthModelReader extends MLReader[FPGrowthModel] {\n+\n+    /** Checked against metadata when loading model */\n+    private val className = classOf[FPGrowthModel].getName\n+\n+    override def load(path: String): FPGrowthModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val frequentItems = sparkSession.read.load(dataPath)\n+      val model = new FPGrowthModel(metadata.uid, frequentItems)\n+      DefaultParamsReader.getAndSetParams(model, metadata)\n+      model\n+    }\n+  }\n+}\n+\n+private[fpm] object AssocaitionRules {"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "can remove Since annotation since the object is private",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-17T07:23:59Z",
    "diffHunk": "@@ -0,0 +1,327 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions used by parallel FP-growth\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * minimal confidence for generating Association Rule\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  minConfidence has not effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(data)\n+    val rows = parentModel.freqItemsets\n+      .map(f => (f.items, f.freq))\n+      .map(cols => Row(cols._1, cols._2))\n+\n+    val dt = dataset.schema($(featuresCol)).dataType\n+    val fields = Array(StructField(\"items\", dt, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false))\n+    val schema = StructType(fields)\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema).toDF(\"items\", \"freq\")\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+   /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {\n+    val freqItems = getFreqItemsets\n+    AssocaitionRules.getAssocationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient val getFreqItemsets: DataFrame = freqItemsets\n+\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    genericTransform(dataset, getAssociationRules)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_], associationRules: DataFrame): DataFrame = {\n+    // use unique id to perform the join and aggregateByKey\n+    val itemsRDD = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .distinct().zipWithUniqueId().map(_.swap).cache()\n+    val rulesRDD = associationRules.rdd.map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val itemsWithConsequents = itemsRDD.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val itemSet = items.toSet\n+        val consequents = if (antecedent.forall(itemSet.contains(_))) consequent else Seq.empty\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])(\n+      (ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)\n+\n+    val mappingRDD = itemsRDD.join(itemsWithConsequents)\n+      .map { case (id, (items, consequent)) => (items, consequent) }\n+      .map (cols => Row(cols._1, cols._2))\n+    val dt = dataset.schema($(featuresCol)).dataType\n+    val fields = Array($(featuresCol), $(predictionCol))\n+      .map(fieldName => StructField(fieldName, dt, nullable = true))\n+    val schema = StructType(fields)\n+    val mapping = dataset.sparkSession.createDataFrame(mappingRDD, schema)\n+\n+    dataset.join(mapping, $(featuresCol))\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowthModel = {\n+    val copied = new FPGrowthModel(uid, freqItemsets)\n+    copyValues(copied, extra).setParent(this.parent)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def write: MLWriter = new FPGrowthModel.FPGrowthModelWriter(this)\n+}\n+\n+object FPGrowthModel extends MLReadable[FPGrowthModel] {\n+  @Since(\"2.2.0\")\n+  override def read: MLReader[FPGrowthModel] = new FPGrowthModelReader\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowthModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FPGrowthModel]] */\n+  private[FPGrowthModel]\n+  class FPGrowthModelWriter(instance: FPGrowthModel) extends MLWriter {\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val dataPath = new Path(path, \"data\").toString\n+      instance.freqItemsets.write.save(dataPath)\n+    }\n+  }\n+\n+  private class FPGrowthModelReader extends MLReader[FPGrowthModel] {\n+\n+    /** Checked against metadata when loading model */\n+    private val className = classOf[FPGrowthModel].getName\n+\n+    override def load(path: String): FPGrowthModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val frequentItems = sparkSession.read.load(dataPath)\n+      val model = new FPGrowthModel(metadata.uid, frequentItems)\n+      DefaultParamsReader.getAndSetParams(model, metadata)\n+      model\n+    }\n+  }\n+}\n+\n+private[fpm] object AssocaitionRules {\n+\n+  /**\n+   * Computes the association rules with confidence above minConfidence.\n+   * @param dataset DataFrame(\"items\", \"freq\") containing frequent itemset obtained from\n+   *                algorithms like [[FPGrowth]].\n+   * @param itemsCol column name for frequent itemsets\n+   * @param freqCol column name for frequent itemsets count\n+   * @param minConfidence minimum confidence for the result association rules\n+   * @return a DataFrame(\"antecedent\", \"consequent\", \"confidence\") containing the association\n+   *         rules.\n+   */\n+  @Since(\"2.2.0\")"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "fix style of function args",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T16:25:48Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {\n+    val freqItems = getFreqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient val getFreqItemsets: DataFrame = freqItemsets\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset, getAssociationRules)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_], associationRules: DataFrame): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap).cache()\n+    val rulesRDD = associationRules.rdd.map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)\n+     .map { case (index, cons) => (index, cons.distinct) }\n+\n+    val rowAndConsequents = dataset.toDF().rdd.zipWithIndex().map(_.swap)\n+      .join(indexToConsequents).sortByKey(ascending = true, dataset.rdd.getNumPartitions)\n+      .map(_._2).map(t => Row.merge(t._1, Row(t._2)))\n+    val mergedSchema = dataset.schema.add(StructField($(predictionCol),\n+      dataset.schema($(featuresCol)).dataType, dataset.schema($(featuresCol)).nullable))\n+    dataset.sparkSession.createDataFrame(rowAndConsequents, mergedSchema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowthModel = {\n+    val copied = new FPGrowthModel(uid, freqItemsets)\n+    copyValues(copied, extra).setParent(this.parent)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def write: MLWriter = new FPGrowthModel.FPGrowthModelWriter(this)\n+}\n+\n+@Since(\"2.2.0\")\n+object FPGrowthModel extends MLReadable[FPGrowthModel] {\n+  @Since(\"2.2.0\")\n+  override def read: MLReader[FPGrowthModel] = new FPGrowthModelReader\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowthModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FPGrowthModel]] */\n+  private[FPGrowthModel]\n+  class FPGrowthModelWriter(instance: FPGrowthModel) extends MLWriter {\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val dataPath = new Path(path, \"data\").toString\n+      instance.freqItemsets.write.save(dataPath)\n+    }\n+  }\n+\n+  private class FPGrowthModelReader extends MLReader[FPGrowthModel] {\n+\n+    /** Checked against metadata when loading model */\n+    private val className = classOf[FPGrowthModel].getName\n+\n+    override def load(path: String): FPGrowthModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val frequentItems = sparkSession.read.load(dataPath)\n+      val model = new FPGrowthModel(metadata.uid, frequentItems)\n+      DefaultParamsReader.getAndSetParams(model, metadata)\n+      model\n+    }\n+  }\n+}\n+\n+private[fpm] object AssociationRules {\n+\n+  /**\n+   * Computes the association rules with confidence above minConfidence.\n+   * @param dataset DataFrame(\"items\", \"freq\") containing frequent itemset obtained from\n+   *                algorithms like [[FPGrowth]].\n+   * @param itemsCol column name for frequent itemsets\n+   * @param freqCol column name for frequent itemsets count\n+   * @param minConfidence minimum confidence for the result association rules\n+   * @return a DataFrame(\"antecedent\", \"consequent\", \"confidence\") containing the association\n+   *         rules.\n+   */\n+  def getAssociationRulesFromFP[T: ClassTag](dataset: Dataset[_],"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Could you please go ahead and copy the relevant text and links from the Scaladoc string for mllib.fpm.FPGrowth?",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T18:24:19Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "\"of the frequent pattern\" -> \"of a frequent pattern\"",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T18:24:21Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\","
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Don't state default value in built-in Param doc",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T18:24:23Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\","
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Remove this line; it's already in the minConfidence doc",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T18:24:25Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting."
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Ditto: Just make freqItemsets public (and transient)",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T18:24:27Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {\n+    val freqItems = getFreqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient val getFreqItemsets: DataFrame = freqItemsets"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "rename getAssociationRules -> associationRules since it's a lazy val",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T18:24:35Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "Also, these may not be String values",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T23:08:17Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Just have this method call getAssociationRules.  No need to pass it as an arg.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T22:46:29Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {\n+    val freqItems = getFreqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient val getFreqItemsets: DataFrame = freqItemsets\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset, getAssociationRules)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_], associationRules: DataFrame): DataFrame = {"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Use a select() here to guarantee the order of antecedent, consequence",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T22:50:12Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {\n+    val freqItems = getFreqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient val getFreqItemsets: DataFrame = freqItemsets\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset, getAssociationRules)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_], associationRules: DataFrame): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap).cache()\n+    val rulesRDD = associationRules.rdd.map(r => (r.getSeq[T](0), r.getSeq[T](1)))"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "This needs more explanation so the user knows precisely what is being done.\r\n1. Warning that this uses a Cartesian join and may explode for large datasets.\r\n2. Explanation of how predictions are made.  There are several options here, and I'm actually not sure myself about how rules are most commonly applied.  Things I'm wondering: Do users need confidences attached to predictions?  How should we merge multiple predictions for a single itemset (row)?  Should we use an approximate algorithm to avoid the Cartesian join?\r\n\r\nI'll think more about this.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T23:02:24Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {\n+    val freqItems = getFreqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient val getFreqItemsets: DataFrame = freqItemsets\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction."
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "Also state new column's output schema",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T23:09:58Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {\n+    val freqItems = getFreqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient val getFreqItemsets: DataFrame = freqItemsets\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction."
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "Originally I thought we can collect all rules and then use broadcast to avoid Cartesian join. But this applies only to smaller set of association rules. Perhaps we can add a if ... else... here.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-23T06:19:37Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {\n+    val freqItems = getFreqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient val getFreqItemsets: DataFrame = freqItemsets\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction."
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "newline",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T23:02:45Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {\n+    val freqItems = getFreqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient val getFreqItemsets: DataFrame = freqItemsets\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset, getAssociationRules)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_], associationRules: DataFrame): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap).cache()\n+    val rulesRDD = associationRules.rdd.map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)\n+     .map { case (index, cons) => (index, cons.distinct) }\n+\n+    val rowAndConsequents = dataset.toDF().rdd.zipWithIndex().map(_.swap)\n+      .join(indexToConsequents).sortByKey(ascending = true, dataset.rdd.getNumPartitions)\n+      .map(_._2).map(t => Row.merge(t._1, Row(t._2)))\n+    val mergedSchema = dataset.schema.add(StructField($(predictionCol),\n+      dataset.schema($(featuresCol)).dataType, dataset.schema($(featuresCol)).nullable))\n+    dataset.sparkSession.createDataFrame(rowAndConsequents, mergedSchema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowthModel = {\n+    val copied = new FPGrowthModel(uid, freqItemsets)\n+    copyValues(copied, extra).setParent(this.parent)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def write: MLWriter = new FPGrowthModel.FPGrowthModelWriter(this)\n+}\n+\n+@Since(\"2.2.0\")\n+object FPGrowthModel extends MLReadable[FPGrowthModel] {\n+  @Since(\"2.2.0\")"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "remove this doc line",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T23:03:47Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {\n+    val freqItems = getFreqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient val getFreqItemsets: DataFrame = freqItemsets\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset, getAssociationRules)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_], associationRules: DataFrame): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap).cache()\n+    val rulesRDD = associationRules.rdd.map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)\n+     .map { case (index, cons) => (index, cons.distinct) }\n+\n+    val rowAndConsequents = dataset.toDF().rdd.zipWithIndex().map(_.swap)\n+      .join(indexToConsequents).sortByKey(ascending = true, dataset.rdd.getNumPartitions)\n+      .map(_._2).map(t => Row.merge(t._1, Row(t._2)))\n+    val mergedSchema = dataset.schema.add(StructField($(predictionCol),\n+      dataset.schema($(featuresCol)).dataType, dataset.schema($(featuresCol)).nullable))\n+    dataset.sparkSession.createDataFrame(rowAndConsequents, mergedSchema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowthModel = {\n+    val copied = new FPGrowthModel(uid, freqItemsets)\n+    copyValues(copied, extra).setParent(this.parent)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def write: MLWriter = new FPGrowthModel.FPGrowthModelWriter(this)\n+}\n+\n+@Since(\"2.2.0\")\n+object FPGrowthModel extends MLReadable[FPGrowthModel] {\n+  @Since(\"2.2.0\")\n+  override def read: MLReader[FPGrowthModel] = new FPGrowthModelReader\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowthModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FPGrowthModel]] */\n+  private[FPGrowthModel]\n+  class FPGrowthModelWriter(instance: FPGrowthModel) extends MLWriter {\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "specify parquet explicitly",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T23:04:26Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {\n+    val freqItems = getFreqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient val getFreqItemsets: DataFrame = freqItemsets\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset, getAssociationRules)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_], associationRules: DataFrame): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap).cache()\n+    val rulesRDD = associationRules.rdd.map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)\n+     .map { case (index, cons) => (index, cons.distinct) }\n+\n+    val rowAndConsequents = dataset.toDF().rdd.zipWithIndex().map(_.swap)\n+      .join(indexToConsequents).sortByKey(ascending = true, dataset.rdd.getNumPartitions)\n+      .map(_._2).map(t => Row.merge(t._1, Row(t._2)))\n+    val mergedSchema = dataset.schema.add(StructField($(predictionCol),\n+      dataset.schema($(featuresCol)).dataType, dataset.schema($(featuresCol)).nullable))\n+    dataset.sparkSession.createDataFrame(rowAndConsequents, mergedSchema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowthModel = {\n+    val copied = new FPGrowthModel(uid, freqItemsets)\n+    copyValues(copied, extra).setParent(this.parent)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def write: MLWriter = new FPGrowthModel.FPGrowthModelWriter(this)\n+}\n+\n+@Since(\"2.2.0\")\n+object FPGrowthModel extends MLReadable[FPGrowthModel] {\n+  @Since(\"2.2.0\")\n+  override def read: MLReader[FPGrowthModel] = new FPGrowthModelReader\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowthModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FPGrowthModel]] */\n+  private[FPGrowthModel]\n+  class FPGrowthModelWriter(instance: FPGrowthModel) extends MLWriter {\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val dataPath = new Path(path, \"data\").toString\n+      instance.freqItemsets.write.save(dataPath)"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "specify parquet",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T23:05:09Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {\n+    val freqItems = getFreqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient val getFreqItemsets: DataFrame = freqItemsets\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset, getAssociationRules)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_], associationRules: DataFrame): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap).cache()\n+    val rulesRDD = associationRules.rdd.map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)\n+     .map { case (index, cons) => (index, cons.distinct) }\n+\n+    val rowAndConsequents = dataset.toDF().rdd.zipWithIndex().map(_.swap)\n+      .join(indexToConsequents).sortByKey(ascending = true, dataset.rdd.getNumPartitions)\n+      .map(_._2).map(t => Row.merge(t._1, Row(t._2)))\n+    val mergedSchema = dataset.schema.add(StructField($(predictionCol),\n+      dataset.schema($(featuresCol)).dataType, dataset.schema($(featuresCol)).nullable))\n+    dataset.sparkSession.createDataFrame(rowAndConsequents, mergedSchema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowthModel = {\n+    val copied = new FPGrowthModel(uid, freqItemsets)\n+    copyValues(copied, extra).setParent(this.parent)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def write: MLWriter = new FPGrowthModel.FPGrowthModelWriter(this)\n+}\n+\n+@Since(\"2.2.0\")\n+object FPGrowthModel extends MLReadable[FPGrowthModel] {\n+  @Since(\"2.2.0\")\n+  override def read: MLReader[FPGrowthModel] = new FPGrowthModelReader\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowthModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FPGrowthModel]] */\n+  private[FPGrowthModel]\n+  class FPGrowthModelWriter(instance: FPGrowthModel) extends MLWriter {\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val dataPath = new Path(path, \"data\").toString\n+      instance.freqItemsets.write.save(dataPath)\n+    }\n+  }\n+\n+  private class FPGrowthModelReader extends MLReader[FPGrowthModel] {\n+\n+    /** Checked against metadata when loading model */\n+    private val className = classOf[FPGrowthModel].getName\n+\n+    override def load(path: String): FPGrowthModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val frequentItems = sparkSession.read.load(dataPath)"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "No need to specify default arg values.  If we make this public, we will need to avoid default args to make it Java friendly.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T23:06:36Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    private val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[String] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val getAssociationRules: DataFrame = {\n+    val freqItems = getFreqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * Get frequent items fitted by FPGrowth, in the format of DataFrame(\"items\", \"freq\")\n+   */\n+  @Since(\"2.2.0\")\n+  @transient val getFreqItemsets: DataFrame = freqItemsets\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset, getAssociationRules)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_], associationRules: DataFrame): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap).cache()\n+    val rulesRDD = associationRules.rdd.map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)\n+     .map { case (index, cons) => (index, cons.distinct) }\n+\n+    val rowAndConsequents = dataset.toDF().rdd.zipWithIndex().map(_.swap)\n+      .join(indexToConsequents).sortByKey(ascending = true, dataset.rdd.getNumPartitions)\n+      .map(_._2).map(t => Row.merge(t._1, Row(t._2)))\n+    val mergedSchema = dataset.schema.add(StructField($(predictionCol),\n+      dataset.schema($(featuresCol)).dataType, dataset.schema($(featuresCol)).nullable))\n+    dataset.sparkSession.createDataFrame(rowAndConsequents, mergedSchema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowthModel = {\n+    val copied = new FPGrowthModel(uid, freqItemsets)\n+    copyValues(copied, extra).setParent(this.parent)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def write: MLWriter = new FPGrowthModel.FPGrowthModelWriter(this)\n+}\n+\n+@Since(\"2.2.0\")\n+object FPGrowthModel extends MLReadable[FPGrowthModel] {\n+  @Since(\"2.2.0\")\n+  override def read: MLReader[FPGrowthModel] = new FPGrowthModelReader\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowthModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FPGrowthModel]] */\n+  private[FPGrowthModel]\n+  class FPGrowthModelWriter(instance: FPGrowthModel) extends MLWriter {\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val dataPath = new Path(path, \"data\").toString\n+      instance.freqItemsets.write.save(dataPath)\n+    }\n+  }\n+\n+  private class FPGrowthModelReader extends MLReader[FPGrowthModel] {\n+\n+    /** Checked against metadata when loading model */\n+    private val className = classOf[FPGrowthModel].getName\n+\n+    override def load(path: String): FPGrowthModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val frequentItems = sparkSession.read.load(dataPath)\n+      val model = new FPGrowthModel(metadata.uid, frequentItems)\n+      DefaultParamsReader.getAndSetParams(model, metadata)\n+      model\n+    }\n+  }\n+}\n+\n+private[fpm] object AssociationRules {\n+\n+  /**\n+   * Computes the association rules with confidence above minConfidence.\n+   * @param dataset DataFrame(\"items\", \"freq\") containing frequent itemset obtained from\n+   *                algorithms like [[FPGrowth]].\n+   * @param itemsCol column name for frequent itemsets\n+   * @param freqCol column name for frequent itemsets count\n+   * @param minConfidence minimum confidence for the result association rules\n+   * @return a DataFrame(\"antecedent\", \"consequent\", \"confidence\") containing the association\n+   *         rules.\n+   */\n+  def getAssociationRulesFromFP[T: ClassTag](dataset: Dataset[_],\n+        itemsCol: String = \"items\","
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Specify schema (Seq, Long)",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T23:07:54Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\", \"freq\")"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Specify numPartitions",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-22T23:11:32Z",
    "diffHunk": "@@ -0,0 +1,341 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of the frequent pattern (Default: 0.3)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule (Default: 0.8)\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets.\n+ *\n+ * @see [[http://dx.doi.org/10.1145/1454008.1454027 Li et al., PFP: Parallel FP-Growth for Query\n+ *  Recommendation]]\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam\n+   *  Note that minConfidence has no effect during fitting.\n+   */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val parentModel = new MLlibFPGrowth().setMinSupport($(minSupport)).run(items)"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "How about using an OpenHashSet here to avoid collecting duplicates during aggregation?",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-23T19:08:55Z",
    "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of a frequent pattern\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets. The algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-Growth for Query\n+ * Recommendation</a>. PFP distributes computation in such a way that each worker executes an\n+ * independent group of mining tasks. The FP-Growth algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/335191.335372\">Han et al., Mining frequent patterns without\n+ * candidate generation</a>.\n+ *\n+ * @see <a href=\"http://en.wikipedia.org/wiki/Association_rule_learning\">\n+ * Association rule learning (Wikipedia)</a>\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val mllibFP = new MLlibFPGrowth().setMinSupport($(minSupport))\n+    if (isSet(numPartitions)) {\n+      mllibFP.setNumPartitions($(numPartitions))\n+    }\n+    val parentModel = mllibFP.run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\"[Seq], \"freq\"[Long])\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    @transient val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[T] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val associationRules: DataFrame = {\n+    val freqItems = freqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction. The prediction column has the same data type as the\n+   * input column. (Array[T])\n+   * Note that internally it uses Cartesian join and may exhaust memory for large datasets.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_]): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap)\n+    val rulesRDD = associationRules.select(\"antecedent\", \"consequent\").rdd\n+      .map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)"
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "I tried OpenHashSet and it's about %15 slower than ArrayBuffer. \r\n\r\n.aggregateByKey(new OpenHashSet[T])((set, seq) => {\r\n      seq.foreach(t => set.add(t))\r\n      set\r\n    } , (set1, set2) => set1.union(set2))",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-23T22:52:15Z",
    "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of a frequent pattern\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets. The algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-Growth for Query\n+ * Recommendation</a>. PFP distributes computation in such a way that each worker executes an\n+ * independent group of mining tasks. The FP-Growth algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/335191.335372\">Han et al., Mining frequent patterns without\n+ * candidate generation</a>.\n+ *\n+ * @see <a href=\"http://en.wikipedia.org/wiki/Association_rule_learning\">\n+ * Association rule learning (Wikipedia)</a>\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val mllibFP = new MLlibFPGrowth().setMinSupport($(minSupport))\n+    if (isSet(numPartitions)) {\n+      mllibFP.setNumPartitions($(numPartitions))\n+    }\n+    val parentModel = mllibFP.run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\"[Seq], \"freq\"[Long])\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    @transient val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[T] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val associationRules: DataFrame = {\n+    val freqItems = freqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction. The prediction column has the same data type as the\n+   * input column. (Array[T])\n+   * Note that internally it uses Cartesian join and may exhaust memory for large datasets.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_]): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap)\n+    val rulesRDD = associationRules.select(\"antecedent\", \"consequent\").rdd\n+      .map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "Huh, that's surprising to me.  Maybe it depends on how many duplicates are introduced.  Let's leave it as is then.\r\n\r\nJust curious: What dataset did you test it on?",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-23T23:25:53Z",
    "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of a frequent pattern\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets. The algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-Growth for Query\n+ * Recommendation</a>. PFP distributes computation in such a way that each worker executes an\n+ * independent group of mining tasks. The FP-Growth algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/335191.335372\">Han et al., Mining frequent patterns without\n+ * candidate generation</a>.\n+ *\n+ * @see <a href=\"http://en.wikipedia.org/wiki/Association_rule_learning\">\n+ * Association rule learning (Wikipedia)</a>\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val mllibFP = new MLlibFPGrowth().setMinSupport($(minSupport))\n+    if (isSet(numPartitions)) {\n+      mllibFP.setNumPartitions($(numPartitions))\n+    }\n+    val parentModel = mllibFP.run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\"[Seq], \"freq\"[Long])\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    @transient val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[T] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val associationRules: DataFrame = {\n+    val freqItems = freqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction. The prediction column has the same data type as the\n+   * input column. (Array[T])\n+   * Note that internally it uses Cartesian join and may exhaust memory for large datasets.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_]): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap)\n+    val rulesRDD = associationRules.select(\"antecedent\", \"consequent\").rdd\n+      .map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)"
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "random number... The duplicate number should be small.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-24T00:34:50Z",
    "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of a frequent pattern\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets. The algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-Growth for Query\n+ * Recommendation</a>. PFP distributes computation in such a way that each worker executes an\n+ * independent group of mining tasks. The FP-Growth algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/335191.335372\">Han et al., Mining frequent patterns without\n+ * candidate generation</a>.\n+ *\n+ * @see <a href=\"http://en.wikipedia.org/wiki/Association_rule_learning\">\n+ * Association rule learning (Wikipedia)</a>\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val mllibFP = new MLlibFPGrowth().setMinSupport($(minSupport))\n+    if (isSet(numPartitions)) {\n+      mllibFP.setNumPartitions($(numPartitions))\n+    }\n+    val parentModel = mllibFP.run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\"[Seq], \"freq\"[Long])\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    @transient val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[T] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val associationRules: DataFrame = {\n+    val freqItems = freqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction. The prediction column has the same data type as the\n+   * input column. (Array[T])\n+   * Note that internally it uses Cartesian join and may exhaust memory for large datasets.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_]): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap)\n+    val rulesRDD = associationRules.select(\"antecedent\", \"consequent\").rdd\n+      .map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "No need to sortByKey",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-23T19:09:46Z",
    "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of a frequent pattern\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets. The algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-Growth for Query\n+ * Recommendation</a>. PFP distributes computation in such a way that each worker executes an\n+ * independent group of mining tasks. The FP-Growth algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/335191.335372\">Han et al., Mining frequent patterns without\n+ * candidate generation</a>.\n+ *\n+ * @see <a href=\"http://en.wikipedia.org/wiki/Association_rule_learning\">\n+ * Association rule learning (Wikipedia)</a>\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val mllibFP = new MLlibFPGrowth().setMinSupport($(minSupport))\n+    if (isSet(numPartitions)) {\n+      mllibFP.setNumPartitions($(numPartitions))\n+    }\n+    val parentModel = mllibFP.run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\"[Seq], \"freq\"[Long])\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    @transient val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[T] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val associationRules: DataFrame = {\n+    val freqItems = freqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction. The prediction column has the same data type as the\n+   * input column. (Array[T])\n+   * Note that internally it uses Cartesian join and may exhaust memory for large datasets.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_]): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap)\n+    val rulesRDD = associationRules.select(\"antecedent\", \"consequent\").rdd\n+      .map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)\n+     .map { case (index, cons) => (index, cons.distinct) }\n+\n+    val rowAndConsequents = dataset.toDF().rdd.zipWithIndex().map(_.swap)\n+      .join(indexToConsequents).sortByKey(ascending = true, dataset.rdd.getNumPartitions)"
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "Shall we try to keep the original order of the input dataset? The time cost is about 5% of the total transform time.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-23T22:53:58Z",
    "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of a frequent pattern\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets. The algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-Growth for Query\n+ * Recommendation</a>. PFP distributes computation in such a way that each worker executes an\n+ * independent group of mining tasks. The FP-Growth algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/335191.335372\">Han et al., Mining frequent patterns without\n+ * candidate generation</a>.\n+ *\n+ * @see <a href=\"http://en.wikipedia.org/wiki/Association_rule_learning\">\n+ * Association rule learning (Wikipedia)</a>\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val mllibFP = new MLlibFPGrowth().setMinSupport($(minSupport))\n+    if (isSet(numPartitions)) {\n+      mllibFP.setNumPartitions($(numPartitions))\n+    }\n+    val parentModel = mllibFP.run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\"[Seq], \"freq\"[Long])\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    @transient val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[T] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val associationRules: DataFrame = {\n+    val freqItems = freqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction. The prediction column has the same data type as the\n+   * input column. (Array[T])\n+   * Note that internally it uses Cartesian join and may exhaust memory for large datasets.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_]): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap)\n+    val rulesRDD = associationRules.select(\"antecedent\", \"consequent\").rdd\n+      .map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)\n+     .map { case (index, cons) => (index, cons.distinct) }\n+\n+    val rowAndConsequents = dataset.toDF().rdd.zipWithIndex().map(_.swap)\n+      .join(indexToConsequents).sortByKey(ascending = true, dataset.rdd.getNumPartitions)"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "DataFrames don't really have a row ordering, so we don't need to maintain one.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-23T23:26:46Z",
    "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of a frequent pattern\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets. The algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-Growth for Query\n+ * Recommendation</a>. PFP distributes computation in such a way that each worker executes an\n+ * independent group of mining tasks. The FP-Growth algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/335191.335372\">Han et al., Mining frequent patterns without\n+ * candidate generation</a>.\n+ *\n+ * @see <a href=\"http://en.wikipedia.org/wiki/Association_rule_learning\">\n+ * Association rule learning (Wikipedia)</a>\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val mllibFP = new MLlibFPGrowth().setMinSupport($(minSupport))\n+    if (isSet(numPartitions)) {\n+      mllibFP.setNumPartitions($(numPartitions))\n+    }\n+    val parentModel = mllibFP.run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\"[Seq], \"freq\"[Long])\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    @transient val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[T] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val associationRules: DataFrame = {\n+    val freqItems = freqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction. The prediction column has the same data type as the\n+   * input column. (Array[T])\n+   * Note that internally it uses Cartesian join and may exhaust memory for large datasets.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_]): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap)\n+    val rulesRDD = associationRules.select(\"antecedent\", \"consequent\").rdd\n+      .map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)\n+     .map { case (index, cons) => (index, cons.distinct) }\n+\n+    val rowAndConsequents = dataset.toDF().rdd.zipWithIndex().map(_.swap)\n+      .join(indexToConsequents).sortByKey(ascending = true, dataset.rdd.getNumPartitions)"
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "If that's the case, does zipWithIndex() guarantees the same result for multiple calls? \r\nAnd if can ignore the ordering, perhaps I have a faster way to write the transform.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-24T00:43:46Z",
    "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of a frequent pattern\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets. The algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-Growth for Query\n+ * Recommendation</a>. PFP distributes computation in such a way that each worker executes an\n+ * independent group of mining tasks. The FP-Growth algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/335191.335372\">Han et al., Mining frequent patterns without\n+ * candidate generation</a>.\n+ *\n+ * @see <a href=\"http://en.wikipedia.org/wiki/Association_rule_learning\">\n+ * Association rule learning (Wikipedia)</a>\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val mllibFP = new MLlibFPGrowth().setMinSupport($(minSupport))\n+    if (isSet(numPartitions)) {\n+      mllibFP.setNumPartitions($(numPartitions))\n+    }\n+    val parentModel = mllibFP.run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\"[Seq], \"freq\"[Long])\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    @transient val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[T] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val associationRules: DataFrame = {\n+    val freqItems = freqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction. The prediction column has the same data type as the\n+   * input column. (Array[T])\n+   * Note that internally it uses Cartesian join and may exhaust memory for large datasets.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_]): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap)\n+    val rulesRDD = associationRules.select(\"antecedent\", \"consequent\").rdd\n+      .map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)\n+     .map { case (index, cons) => (index, cons.distinct) }\n+\n+    val rowAndConsequents = dataset.toDF().rdd.zipWithIndex().map(_.swap)\n+      .join(indexToConsequents).sortByKey(ascending = true, dataset.rdd.getNumPartitions)"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "I think zipWithIndex guarantees the order if the data have not been shuffled between load/creation and the zipWithIndex.  AFAIK, shuffling can reorder rows in a partition arbitrarily.\r\n\r\nWe can definitely ignore the ordering for DataFrames.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-24T00:51:06Z",
    "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of a frequent pattern\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets. The algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-Growth for Query\n+ * Recommendation</a>. PFP distributes computation in such a way that each worker executes an\n+ * independent group of mining tasks. The FP-Growth algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/335191.335372\">Han et al., Mining frequent patterns without\n+ * candidate generation</a>.\n+ *\n+ * @see <a href=\"http://en.wikipedia.org/wiki/Association_rule_learning\">\n+ * Association rule learning (Wikipedia)</a>\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val mllibFP = new MLlibFPGrowth().setMinSupport($(minSupport))\n+    if (isSet(numPartitions)) {\n+      mllibFP.setNumPartitions($(numPartitions))\n+    }\n+    val parentModel = mllibFP.run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\"[Seq], \"freq\"[Long])\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    @transient val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[T] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val associationRules: DataFrame = {\n+    val freqItems = freqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction. The prediction column has the same data type as the\n+   * input column. (Array[T])\n+   * Note that internally it uses Cartesian join and may exhaust memory for large datasets.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_]): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap)\n+    val rulesRDD = associationRules.select(\"antecedent\", \"consequent\").rdd\n+      .map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)\n+     .map { case (index, cons) => (index, cons.distinct) }\n+\n+    val rowAndConsequents = dataset.toDF().rdd.zipWithIndex().map(_.swap)\n+      .join(indexToConsequents).sortByKey(ascending = true, dataset.rdd.getNumPartitions)"
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "Checked again and the current implementation is as quick. I will just remove the sort.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-24T01:18:40Z",
    "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of a frequent pattern\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets. The algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-Growth for Query\n+ * Recommendation</a>. PFP distributes computation in such a way that each worker executes an\n+ * independent group of mining tasks. The FP-Growth algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/335191.335372\">Han et al., Mining frequent patterns without\n+ * candidate generation</a>.\n+ *\n+ * @see <a href=\"http://en.wikipedia.org/wiki/Association_rule_learning\">\n+ * Association rule learning (Wikipedia)</a>\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val mllibFP = new MLlibFPGrowth().setMinSupport($(minSupport))\n+    if (isSet(numPartitions)) {\n+      mllibFP.setNumPartitions($(numPartitions))\n+    }\n+    val parentModel = mllibFP.run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\"[Seq], \"freq\"[Long])\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    @transient val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[T] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val associationRules: DataFrame = {\n+    val freqItems = freqItemsets\n+    AssociationRules.getAssociationRulesFromFP(freqItems, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction. The prediction column has the same data type as the\n+   * input column. (Array[T])\n+   * Note that internally it uses Cartesian join and may exhaust memory for large datasets.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset)\n+  }\n+\n+  private def genericTransform[T](dataset: Dataset[_]): DataFrame = {\n+    // use index to perform the join and aggregateByKey, and keep the original order after join.\n+    val indexToItems = dataset.select($(featuresCol)).rdd.map(r => r.getSeq[T](0))\n+      .zipWithIndex().map(_.swap)\n+    val rulesRDD = associationRules.select(\"antecedent\", \"consequent\").rdd\n+      .map(r => (r.getSeq[T](0), r.getSeq[T](1)))\n+\n+    val indexToConsequents = indexToItems.cartesian(rulesRDD).map {\n+      case ((id, items), (antecedent, consequent)) =>\n+        val consequents = if (items != null) {\n+          val itemSet = items.toSet\n+          if (antecedent.forall(itemSet.contains)) {\n+            consequent.filterNot(itemSet.contains)\n+          } else {\n+            Seq.empty\n+          }\n+        } else {\n+          Seq.empty\n+        }\n+        (id, consequents)\n+    }.aggregateByKey(new ArrayBuffer[T])((ar, seq) => ar ++= seq, (ar, seq) => ar ++= seq)\n+     .map { case (index, cons) => (index, cons.distinct) }\n+\n+    val rowAndConsequents = dataset.toDF().rdd.zipWithIndex().map(_.swap)\n+      .join(indexToConsequents).sortByKey(ascending = true, dataset.rdd.getNumPartitions)"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Here or elsewhere, comment that null featuresCol values are ignored during fit() and are treated as empty sets during transform().",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-23T19:09:53Z",
    "diffHunk": "@@ -0,0 +1,346 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of a frequent pattern\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets. The algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-Growth for Query\n+ * Recommendation</a>. PFP distributes computation in such a way that each worker executes an\n+ * independent group of mining tasks. The FP-Growth algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/335191.335372\">Han et al., Mining frequent patterns without\n+ * candidate generation</a>.\n+ *"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "No need for this temp val",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-24T22:56:22Z",
    "diffHunk": "@@ -0,0 +1,347 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of a frequent pattern\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets. The algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-Growth for Query\n+ * Recommendation</a>. PFP distributes computation in such a way that each worker executes an\n+ * independent group of mining tasks. The FP-Growth algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/335191.335372\">Han et al., Mining frequent patterns without\n+ * candidate generation</a>. Note null values in the feature column are ignored during fit().\n+ *\n+ * @see <a href=\"http://en.wikipedia.org/wiki/Association_rule_learning\">\n+ * Association rule learning (Wikipedia)</a>\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val mllibFP = new MLlibFPGrowth().setMinSupport($(minSupport))\n+    if (isSet(numPartitions)) {\n+      mllibFP.setNumPartitions($(numPartitions))\n+    }\n+    val parentModel = mllibFP.run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\"[Seq], \"freq\"[Long])\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    @transient val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[T] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val associationRules: DataFrame = {\n+    val freqItems = freqItemsets"
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "No need to check items != null here since you put that above.",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-25T02:48:45Z",
    "diffHunk": "@@ -0,0 +1,339 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of a frequent pattern\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets. The algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-Growth for Query\n+ * Recommendation</a>. PFP distributes computation in such a way that each worker executes an\n+ * independent group of mining tasks. The FP-Growth algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/335191.335372\">Han et al., Mining frequent patterns without\n+ * candidate generation</a>. Note null values in the feature column are ignored during fit().\n+ *\n+ * @see <a href=\"http://en.wikipedia.org/wiki/Association_rule_learning\">\n+ * Association rule learning (Wikipedia)</a>\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val mllibFP = new MLlibFPGrowth().setMinSupport($(minSupport))\n+    if (isSet(numPartitions)) {\n+      mllibFP.setNumPartitions($(numPartitions))\n+    }\n+    val parentModel = mllibFP.run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\"[Seq], \"freq\"[Long])\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    @transient val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[T] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val associationRules: DataFrame = {\n+    AssociationRules.getAssociationRulesFromFP(freqItemsets, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction. The prediction column has the same data type as the\n+   * input column(Array[T]) and will not contain existing items in the input column. The null\n+   * values in the feature columns are treated as empty sets.\n+   * WARNING: internally it collects association rules to the driver and uses broadcast for\n+   * efficiency. This may bring pressure to driver memory for large set of association rules.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset)\n+  }\n+\n+  private def genericTransform(dataset: Dataset[_]): DataFrame = {\n+    val rules: Array[(Seq[Any], Seq[Any])] = associationRules.select(\"antecedent\", \"consequent\")\n+      .rdd.map(r => (r.getSeq(0), r.getSeq(1)))\n+      .collect().asInstanceOf[Array[(Seq[Any], Seq[Any])]]\n+    val brRules = dataset.sparkSession.sparkContext.broadcast(rules)\n+\n+    val dt = dataset.schema($(featuresCol)).dataType\n+    // For each rule, examine the input items and summarize the consequents\n+    val predictUDF = udf((items: Seq[_]) => {\n+      if (items != null) {\n+        val itemset = items.toSet\n+        brRules.value.flatMap(rule =>\n+          if (items != null && rule._1.forall(item => itemset.contains(item))) {",
    "line": 244
  }],
  "prId": 15415
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "style:\r\n```\r\n      brRules.value.flatMap { rule =>\r\n        ...\r\n      }\r\n```",
    "commit": "9940c4716daf47c6678fdd45abba8afa71a3e53a",
    "createdAt": "2017-02-25T02:49:30Z",
    "diffHunk": "@@ -0,0 +1,339 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.fpm\n+\n+import scala.collection.mutable.ArrayBuffer\n+import scala.reflect.ClassTag\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasFeaturesCol, HasPredictionCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.fpm.{AssociationRules => MLlibAssociationRules,\n+  FPGrowth => MLlibFPGrowth}\n+import org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Common params for FPGrowth and FPGrowthModel\n+ */\n+private[fpm] trait FPGrowthParams extends Params with HasFeaturesCol with HasPredictionCol {\n+\n+  /**\n+   * Minimal support level of the frequent pattern. [0.0, 1.0]. Any pattern that appears\n+   * more than (minSupport * size-of-the-dataset) times will be output\n+   * Default: 0.3\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minSupport: DoubleParam = new DoubleParam(this, \"minSupport\",\n+    \"the minimal support level of a frequent pattern\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minSupport -> 0.3)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinSupport: Double = $(minSupport)\n+\n+  /**\n+   * Number of partitions (>=1) used by parallel FP-growth. By default the param is not set, and\n+   * partition number of the input dataset is used.\n+   * @group expertParam\n+   */\n+  @Since(\"2.2.0\")\n+  val numPartitions: IntParam = new IntParam(this, \"numPartitions\",\n+    \"Number of partitions used by parallel FP-growth\", ParamValidators.gtEq[Int](1))\n+\n+  /** @group expertGetParam */\n+  @Since(\"2.2.0\")\n+  def getNumPartitions: Int = $(numPartitions)\n+\n+  /**\n+   * Minimal confidence for generating Association Rule.\n+   * Note that minConfidence has no effect during fitting.\n+   * Default: 0.8\n+   * @group param\n+   */\n+  @Since(\"2.2.0\")\n+  val minConfidence: DoubleParam = new DoubleParam(this, \"minConfidence\",\n+    \"minimal confidence for generating Association Rule\",\n+    ParamValidators.inRange(0.0, 1.0))\n+  setDefault(minConfidence -> 0.8)\n+\n+  /** @group getParam */\n+  @Since(\"2.2.0\")\n+  def getMinConfidence: Double = $(minConfidence)\n+\n+  /**\n+   * Validates and transforms the input schema.\n+   * @param schema input schema\n+   * @return output schema\n+   */\n+  @Since(\"2.2.0\")\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    val inputType = schema($(featuresCol)).dataType\n+    require(inputType.isInstanceOf[ArrayType],\n+      s\"The input column must be ArrayType, but got $inputType.\")\n+    SchemaUtils.appendColumn(schema, $(predictionCol), schema($(featuresCol)).dataType)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * A parallel FP-growth algorithm to mine frequent itemsets. The algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/1454008.1454027\">Li et al., PFP: Parallel FP-Growth for Query\n+ * Recommendation</a>. PFP distributes computation in such a way that each worker executes an\n+ * independent group of mining tasks. The FP-Growth algorithm is described in\n+ * <a href=\"http://dx.doi.org/10.1145/335191.335372\">Han et al., Mining frequent patterns without\n+ * candidate generation</a>. Note null values in the feature column are ignored during fit().\n+ *\n+ * @see <a href=\"http://en.wikipedia.org/wiki/Association_rule_learning\">\n+ * Association rule learning (Wikipedia)</a>\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowth @Since(\"2.2.0\") (\n+    @Since(\"2.2.0\") override val uid: String)\n+  extends Estimator[FPGrowthModel] with FPGrowthParams with DefaultParamsWritable {\n+\n+  @Since(\"2.2.0\")\n+  def this() = this(Identifiable.randomUID(\"fpgrowth\"))\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinSupport(value: Double): this.type = set(minSupport, value)\n+\n+  /** @group expertSetParam */\n+  @Since(\"2.2.0\")\n+  def setNumPartitions(value: Int): this.type = set(numPartitions, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  @Since(\"2.2.0\")\n+  override def fit(dataset: Dataset[_]): FPGrowthModel = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericFit(dataset)\n+  }\n+\n+  private def genericFit[T: ClassTag](dataset: Dataset[_]): FPGrowthModel = {\n+    val data = dataset.select($(featuresCol))\n+    val items = data.where(col($(featuresCol)).isNotNull).rdd.map(r => r.getSeq[T](0).toArray)\n+    val mllibFP = new MLlibFPGrowth().setMinSupport($(minSupport))\n+    if (isSet(numPartitions)) {\n+      mllibFP.setNumPartitions($(numPartitions))\n+    }\n+    val parentModel = mllibFP.run(items)\n+    val rows = parentModel.freqItemsets.map(f => Row(f.items, f.freq))\n+\n+    val schema = StructType(Seq(\n+      StructField(\"items\", dataset.schema($(featuresCol)).dataType, nullable = false),\n+      StructField(\"freq\", LongType, nullable = false)))\n+    val frequentItems = dataset.sparkSession.createDataFrame(rows, schema)\n+    copyValues(new FPGrowthModel(uid, frequentItems)).setParent(this)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  @Since(\"2.2.0\")\n+  override def copy(extra: ParamMap): FPGrowth = defaultCopy(extra)\n+}\n+\n+\n+@Since(\"2.2.0\")\n+object FPGrowth extends DefaultParamsReadable[FPGrowth] {\n+\n+  @Since(\"2.2.0\")\n+  override def load(path: String): FPGrowth = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model fitted by FPGrowth.\n+ *\n+ * @param freqItemsets frequent items in the format of DataFrame(\"items\"[Seq], \"freq\"[Long])\n+ */\n+@Since(\"2.2.0\")\n+@Experimental\n+class FPGrowthModel private[ml] (\n+    @Since(\"2.2.0\") override val uid: String,\n+    @transient val freqItemsets: DataFrame)\n+  extends Model[FPGrowthModel] with FPGrowthParams with MLWritable {\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setMinConfidence(value: Double): this.type = set(minConfidence, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  @Since(\"2.2.0\")\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Get association rules fitted by AssociationRules using the minConfidence. Returns a dataframe\n+   * with three fields, \"antecedent\", \"consequent\" and \"confidence\", where \"antecedent\" and\n+   * \"consequent\" are Array[T] and \"confidence\" is Double.\n+   */\n+  @Since(\"2.2.0\")\n+  @transient lazy val associationRules: DataFrame = {\n+    AssociationRules.getAssociationRulesFromFP(freqItemsets, \"items\", \"freq\", $(minConfidence))\n+  }\n+\n+  /**\n+   * The transform method first generates the association rules according to the frequent itemsets.\n+   * Then for each association rule, it will examine the input items against antecedents and\n+   * summarize the consequents as prediction. The prediction column has the same data type as the\n+   * input column(Array[T]) and will not contain existing items in the input column. The null\n+   * values in the feature columns are treated as empty sets.\n+   * WARNING: internally it collects association rules to the driver and uses broadcast for\n+   * efficiency. This may bring pressure to driver memory for large set of association rules.\n+   */\n+  @Since(\"2.2.0\")\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    genericTransform(dataset)\n+  }\n+\n+  private def genericTransform(dataset: Dataset[_]): DataFrame = {\n+    val rules: Array[(Seq[Any], Seq[Any])] = associationRules.select(\"antecedent\", \"consequent\")\n+      .rdd.map(r => (r.getSeq(0), r.getSeq(1)))\n+      .collect().asInstanceOf[Array[(Seq[Any], Seq[Any])]]\n+    val brRules = dataset.sparkSession.sparkContext.broadcast(rules)\n+\n+    val dt = dataset.schema($(featuresCol)).dataType\n+    // For each rule, examine the input items and summarize the consequents\n+    val predictUDF = udf((items: Seq[_]) => {\n+      if (items != null) {\n+        val itemset = items.toSet\n+        brRules.value.flatMap(rule =>",
    "line": 243
  }],
  "prId": 15415
}]