[{
  "comments": [{
    "author": {
      "login": "rotationsymmetry"
    },
    "body": "Please provide documentation for public API. Similar for other getters and setters if not inherited. \n",
    "commit": "aa37878c50ef6e7722a615298240ba6e61ea083c",
    "createdAt": "2015-09-09T16:32:21Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, DiffFunction, LBFGS => BreezeLBFGS}\n+\n+import org.apache.spark.{SparkException, Logging}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.ml.{Model, Estimator}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util.{SchemaUtils, Identifiable}\n+import org.apache.spark.mllib.linalg.{DenseVector, Vector, Vectors, VectorUDT}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Row, DataFrame}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{DoubleType, StructType}\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Accelerated Failure Time regression.\n+ */\n+private[regression] trait AFTRegressionParams extends Params\n+  with HasFeaturesCol with HasLabelCol with HasPredictionCol with HasMaxIter\n+  with HasTol with HasFitIntercept {\n+\n+  /**\n+   * Param for censored column name.\n+   * @group param\n+   */\n+  final val censorCol: Param[String] = new Param[String](this, \"censorCol\", \"censored column name\")\n+\n+  /** @group getParam */\n+  final def getCensorCol: String = $(censorCol)\n+\n+  /**\n+   * Param for quantile column name.\n+   * @group param\n+   */\n+  final val quantileCol: Param[String] = new Param[String](this,\n+    \"quantileCol\", \"quantile column name\")\n+\n+  /** @group getParam */\n+  final def getQuantileCol: String = $(quantileCol)\n+\n+  /**\n+   * Validates and transforms the input schema with the provided param map.\n+   * @param schema input schema\n+   * @param fitting whether this is in fitting or prediction\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(\n+      schema: StructType,\n+      fitting: Boolean): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new VectorUDT)\n+    if (fitting) {\n+      SchemaUtils.checkColumnType(schema, $(censorCol), DoubleType)\n+      SchemaUtils.checkColumnType(schema, $(labelCol), DoubleType)\n+    } else {\n+      SchemaUtils.checkColumnType(schema, $(quantileCol), new VectorUDT)\n+    }\n+    SchemaUtils.appendColumn(schema, $(predictionCol), DoubleType)\n+  }\n+}\n+\n+@Experimental\n+class AFTRegression(override val uid: String)\n+  extends Estimator[AFTRegressionModel] with AFTRegressionParams with Logging {\n+\n+  def this() = this(Identifiable.randomUID(\"aftReg\"))\n+\n+  /** @group setParam */"
  }],
  "prId": 8611
}, {
  "comments": [{
    "author": {
      "login": "rotationsymmetry"
    },
    "body": "set `intercept` to be zero if `fitIntercept` is `false`.\n",
    "commit": "aa37878c50ef6e7722a615298240ba6e61ea083c",
    "createdAt": "2015-09-10T07:15:28Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, DiffFunction, LBFGS => BreezeLBFGS}\n+\n+import org.apache.spark.{SparkException, Logging}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.ml.{Model, Estimator}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util.{SchemaUtils, Identifiable}\n+import org.apache.spark.mllib.linalg.{DenseVector, Vector, Vectors, VectorUDT}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Row, DataFrame}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{DoubleType, StructType}\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Accelerated Failure Time regression.\n+ */\n+private[regression] trait AFTRegressionParams extends Params\n+  with HasFeaturesCol with HasLabelCol with HasPredictionCol with HasMaxIter\n+  with HasTol with HasFitIntercept {\n+\n+  /**\n+   * Param for censored column name.\n+   * @group param\n+   */\n+  final val censorCol: Param[String] = new Param[String](this, \"censorCol\", \"censored column name\")\n+\n+  /** @group getParam */\n+  final def getCensorCol: String = $(censorCol)\n+\n+  /**\n+   * Param for quantile column name.\n+   * @group param\n+   */\n+  final val quantileCol: Param[String] = new Param[String](this,\n+    \"quantileCol\", \"quantile column name\")\n+\n+  /** @group getParam */\n+  final def getQuantileCol: String = $(quantileCol)\n+\n+  /**\n+   * Validates and transforms the input schema with the provided param map.\n+   * @param schema input schema\n+   * @param fitting whether this is in fitting or prediction\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(\n+      schema: StructType,\n+      fitting: Boolean): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new VectorUDT)\n+    if (fitting) {\n+      SchemaUtils.checkColumnType(schema, $(censorCol), DoubleType)\n+      SchemaUtils.checkColumnType(schema, $(labelCol), DoubleType)\n+    } else {\n+      SchemaUtils.checkColumnType(schema, $(quantileCol), new VectorUDT)\n+    }\n+    SchemaUtils.appendColumn(schema, $(predictionCol), DoubleType)\n+  }\n+}\n+\n+@Experimental\n+class AFTRegression(override val uid: String)\n+  extends Estimator[AFTRegressionModel] with AFTRegressionParams with Logging {\n+\n+  def this() = this(Identifiable.randomUID(\"aftReg\"))\n+\n+  /** @group setParam */\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  def setLabelCol(value: String): this.type = set(labelCol, value)\n+\n+  /** @group setParam */\n+  def setCensorCol(value: String): this.type = set(censorCol, value)\n+  setDefault(censorCol -> \"censored\")\n+\n+  /** @group setParam */\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Set if we should fit the intercept\n+   * Default is true.\n+   * @group setParam\n+   */\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   * @group setParam\n+   */\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   * @group setParam\n+   */\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Extracts (feature, label, censored) from input dataset.\n+   */\n+  protected[ml] def extractCensoredLabeledPoints(\n+      dataset: DataFrame): RDD[(Vector, Double, Double)] = {\n+    dataset.select($(featuresCol), $(labelCol), $(censorCol))\n+      .map { case Row(feature: Vector, label: Double, censored: Double) =>\n+      (feature, label, censored)\n+    }\n+  }\n+\n+  override def fit(dataset: DataFrame): AFTRegressionModel = {\n+    validateAndTransformSchema(dataset.schema, fitting = true)\n+    val instances = extractCensoredLabeledPoints(dataset)\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val costFun = new AFTCostFun(instances, $(fitIntercept))\n+    val optimizer = new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+\n+    val numFeatures = dataset.select($(featuresCol)).take(1)(0).getAs[Vector](0).size\n+    /*\n+       The weights vector has three parts:\n+       the first element: Double, log(sigma), the log of scale parameter\n+       the second element: Double, intercept of the beta parameter\n+       the third to the end elements: Doubles, weights vector of the beta parameter\n+     */\n+    val initialWeights = new DenseVector(Array.fill(numFeatures + 2)(1.0))\n+\n+    val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+      initialWeights.toBreeze.toDenseVector)\n+\n+    val weights = {\n+      val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+      var state: optimizer.State = null\n+      while (states.hasNext) {\n+        state = states.next()\n+        // println(state.fVals.mkString(\",\"))\n+        arrayBuilder += state.adjustedValue\n+      }\n+      if (state == null) {\n+        val msg = s\"${optimizer.getClass.getName} failed.\"\n+        throw new SparkException(msg)\n+      }\n+\n+      val rawWeights = state.x.toArray.clone()\n+      rawWeights\n+    }\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val realWeights = Vectors.dense(weights.slice(2, weights.length))\n+    val intercept = weights(1)\n+    val scale = math.exp(weights(0))\n+    val model = new AFTRegressionModel(uid, realWeights, intercept, scale)"
  }],
  "prId": 8611
}, {
  "comments": [{
    "author": {
      "login": "rotationsymmetry"
    },
    "body": "revise depending on how you decide to set up the `initalWeights`. See comment above.\n",
    "commit": "aa37878c50ef6e7722a615298240ba6e61ea083c",
    "createdAt": "2015-09-10T07:17:27Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, DiffFunction, LBFGS => BreezeLBFGS}\n+\n+import org.apache.spark.{SparkException, Logging}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.ml.{Model, Estimator}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util.{SchemaUtils, Identifiable}\n+import org.apache.spark.mllib.linalg.{DenseVector, Vector, Vectors, VectorUDT}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Row, DataFrame}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{DoubleType, StructType}\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Accelerated Failure Time regression.\n+ */\n+private[regression] trait AFTRegressionParams extends Params\n+  with HasFeaturesCol with HasLabelCol with HasPredictionCol with HasMaxIter\n+  with HasTol with HasFitIntercept {\n+\n+  /**\n+   * Param for censored column name.\n+   * @group param\n+   */\n+  final val censorCol: Param[String] = new Param[String](this, \"censorCol\", \"censored column name\")\n+\n+  /** @group getParam */\n+  final def getCensorCol: String = $(censorCol)\n+\n+  /**\n+   * Param for quantile column name.\n+   * @group param\n+   */\n+  final val quantileCol: Param[String] = new Param[String](this,\n+    \"quantileCol\", \"quantile column name\")\n+\n+  /** @group getParam */\n+  final def getQuantileCol: String = $(quantileCol)\n+\n+  /**\n+   * Validates and transforms the input schema with the provided param map.\n+   * @param schema input schema\n+   * @param fitting whether this is in fitting or prediction\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(\n+      schema: StructType,\n+      fitting: Boolean): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new VectorUDT)\n+    if (fitting) {\n+      SchemaUtils.checkColumnType(schema, $(censorCol), DoubleType)\n+      SchemaUtils.checkColumnType(schema, $(labelCol), DoubleType)\n+    } else {\n+      SchemaUtils.checkColumnType(schema, $(quantileCol), new VectorUDT)\n+    }\n+    SchemaUtils.appendColumn(schema, $(predictionCol), DoubleType)\n+  }\n+}\n+\n+@Experimental\n+class AFTRegression(override val uid: String)\n+  extends Estimator[AFTRegressionModel] with AFTRegressionParams with Logging {\n+\n+  def this() = this(Identifiable.randomUID(\"aftReg\"))\n+\n+  /** @group setParam */\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  def setLabelCol(value: String): this.type = set(labelCol, value)\n+\n+  /** @group setParam */\n+  def setCensorCol(value: String): this.type = set(censorCol, value)\n+  setDefault(censorCol -> \"censored\")\n+\n+  /** @group setParam */\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Set if we should fit the intercept\n+   * Default is true.\n+   * @group setParam\n+   */\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   * @group setParam\n+   */\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   * @group setParam\n+   */\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Extracts (feature, label, censored) from input dataset.\n+   */\n+  protected[ml] def extractCensoredLabeledPoints(\n+      dataset: DataFrame): RDD[(Vector, Double, Double)] = {\n+    dataset.select($(featuresCol), $(labelCol), $(censorCol))\n+      .map { case Row(feature: Vector, label: Double, censored: Double) =>\n+      (feature, label, censored)\n+    }\n+  }\n+\n+  override def fit(dataset: DataFrame): AFTRegressionModel = {\n+    validateAndTransformSchema(dataset.schema, fitting = true)\n+    val instances = extractCensoredLabeledPoints(dataset)\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val costFun = new AFTCostFun(instances, $(fitIntercept))\n+    val optimizer = new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+\n+    val numFeatures = dataset.select($(featuresCol)).take(1)(0).getAs[Vector](0).size\n+    /*\n+       The weights vector has three parts:\n+       the first element: Double, log(sigma), the log of scale parameter\n+       the second element: Double, intercept of the beta parameter\n+       the third to the end elements: Doubles, weights vector of the beta parameter\n+     */\n+    val initialWeights = new DenseVector(Array.fill(numFeatures + 2)(1.0))\n+\n+    val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+      initialWeights.toBreeze.toDenseVector)\n+\n+    val weights = {\n+      val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+      var state: optimizer.State = null\n+      while (states.hasNext) {\n+        state = states.next()\n+        // println(state.fVals.mkString(\",\"))\n+        arrayBuilder += state.adjustedValue\n+      }\n+      if (state == null) {\n+        val msg = s\"${optimizer.getClass.getName} failed.\"\n+        throw new SparkException(msg)\n+      }\n+\n+      val rawWeights = state.x.toArray.clone()\n+      rawWeights\n+    }\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val realWeights = Vectors.dense(weights.slice(2, weights.length))\n+    val intercept = weights(1)\n+    val scale = math.exp(weights(0))\n+    val model = new AFTRegressionModel(uid, realWeights, intercept, scale)\n+    copyValues(model.setParent(this))\n+  }\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = true)\n+  }\n+\n+  override def copy(extra: ParamMap): AFTRegression = defaultCopy(extra)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[AFTRegression]].\n+ */\n+@Experimental\n+class AFTRegressionModel private[ml] (\n+    override val uid: String,\n+    val weights: Vector,\n+    val intercept: Double,\n+    val scale: Double)\n+  extends Model[AFTRegressionModel] with AFTRegressionParams {\n+\n+  /** @group setParam */\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  def setQuantileCol(value: String): this.type = set(quantileCol, value)\n+  setDefault(quantileCol -> \"quantile\")\n+\n+  /** @group setParam */\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  def predict(features: Vector, quantile: Vector): Vector = {\n+    // scale parameter of the Weibull distribution\n+    val lambda = math.exp(weights.toBreeze.dot(features.toBreeze) + intercept)\n+    // shape parameter of the Weibull distribution\n+    val k = 1 / scale\n+    val array = quantile.toArray.map { q => lambda * math.exp(math.log(-math.log(1-q)) / k) }\n+    Vectors.dense(array)\n+  }\n+\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    transformSchema(dataset.schema)\n+    val predictUDF = udf { (features: Vector, quantile: Vector) => predict(features, quantile) }\n+    dataset.withColumn($(predictionCol), predictUDF(col($(featuresCol)), col($(quantileCol))))\n+  }\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false)\n+  }\n+\n+  override def copy(extra: ParamMap): AFTRegressionModel = {\n+    copyValues(new AFTRegressionModel(uid, weights, intercept, scale), extra).setParent(parent)\n+  }\n+}\n+\n+private class AFTAggregator(weights: BDV[Double], fitIntercept: Boolean)\n+  extends Serializable {\n+\n+  private val beta = weights.slice(1, weights.length)\n+  private val sigma = math.exp(weights(0))\n+\n+  private var totalCnt: Long = 0L\n+  private var lossSum = 0.0\n+  private var gradientBetaSum = BDV.zeros[Double](beta.length)\n+  private var gradientLogSigmaSum = 0.0\n+\n+  def count: Long = totalCnt\n+  def loss: Double = if (totalCnt == 0) 1.0 else lossSum / totalCnt\n+  // Here we optimize loss function over beta and log(sigma)\n+  def gradient: BDV[Double] = BDV.vertcat(BDV(Array(gradientLogSigmaSum / totalCnt.toDouble)),\n+    gradientBetaSum/totalCnt.toDouble)\n+\n+  def add(data: (Vector, Double, Double)): this.type = {\n+\n+    val xi = if (fitIntercept) {"
  }],
  "prId": 8611
}, {
  "comments": [{
    "author": {
      "login": "rotationsymmetry"
    },
    "body": "here intercept (2nd element) is initialized to 1 regardless of `fitIntercept`. If `fitIntercept` is false, the 2nd element will stay as 1 after the LBFGS optimization. This value will be carry to the `AFTRegressionModel`. Then `predict` and `transform` will be incorrect. \n\nHow about we include intercept in `initialWeights` _only_ when `fitIntercept` is `true`?\n\nBtw, shall we set the initial values to be zero since we are working in the log scale. \n",
    "commit": "aa37878c50ef6e7722a615298240ba6e61ea083c",
    "createdAt": "2015-09-10T07:29:20Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, DiffFunction, LBFGS => BreezeLBFGS}\n+\n+import org.apache.spark.{SparkException, Logging}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.ml.{Model, Estimator}\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util.{SchemaUtils, Identifiable}\n+import org.apache.spark.mllib.linalg.{DenseVector, Vector, Vectors, VectorUDT}\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Row, DataFrame}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{DoubleType, StructType}\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Accelerated Failure Time regression.\n+ */\n+private[regression] trait AFTRegressionParams extends Params\n+  with HasFeaturesCol with HasLabelCol with HasPredictionCol with HasMaxIter\n+  with HasTol with HasFitIntercept {\n+\n+  /**\n+   * Param for censored column name.\n+   * @group param\n+   */\n+  final val censorCol: Param[String] = new Param[String](this, \"censorCol\", \"censored column name\")\n+\n+  /** @group getParam */\n+  final def getCensorCol: String = $(censorCol)\n+\n+  /**\n+   * Param for quantile column name.\n+   * @group param\n+   */\n+  final val quantileCol: Param[String] = new Param[String](this,\n+    \"quantileCol\", \"quantile column name\")\n+\n+  /** @group getParam */\n+  final def getQuantileCol: String = $(quantileCol)\n+\n+  /**\n+   * Validates and transforms the input schema with the provided param map.\n+   * @param schema input schema\n+   * @param fitting whether this is in fitting or prediction\n+   * @return output schema\n+   */\n+  protected def validateAndTransformSchema(\n+      schema: StructType,\n+      fitting: Boolean): StructType = {\n+    SchemaUtils.checkColumnType(schema, $(featuresCol), new VectorUDT)\n+    if (fitting) {\n+      SchemaUtils.checkColumnType(schema, $(censorCol), DoubleType)\n+      SchemaUtils.checkColumnType(schema, $(labelCol), DoubleType)\n+    } else {\n+      SchemaUtils.checkColumnType(schema, $(quantileCol), new VectorUDT)\n+    }\n+    SchemaUtils.appendColumn(schema, $(predictionCol), DoubleType)\n+  }\n+}\n+\n+@Experimental\n+class AFTRegression(override val uid: String)\n+  extends Estimator[AFTRegressionModel] with AFTRegressionParams with Logging {\n+\n+  def this() = this(Identifiable.randomUID(\"aftReg\"))\n+\n+  /** @group setParam */\n+  def setFeaturesCol(value: String): this.type = set(featuresCol, value)\n+\n+  /** @group setParam */\n+  def setLabelCol(value: String): this.type = set(labelCol, value)\n+\n+  /** @group setParam */\n+  def setCensorCol(value: String): this.type = set(censorCol, value)\n+  setDefault(censorCol -> \"censored\")\n+\n+  /** @group setParam */\n+  def setPredictionCol(value: String): this.type = set(predictionCol, value)\n+\n+  /**\n+   * Set if we should fit the intercept\n+   * Default is true.\n+   * @group setParam\n+   */\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   * @group setParam\n+   */\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   * @group setParam\n+   */\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Extracts (feature, label, censored) from input dataset.\n+   */\n+  protected[ml] def extractCensoredLabeledPoints(\n+      dataset: DataFrame): RDD[(Vector, Double, Double)] = {\n+    dataset.select($(featuresCol), $(labelCol), $(censorCol))\n+      .map { case Row(feature: Vector, label: Double, censored: Double) =>\n+      (feature, label, censored)\n+    }\n+  }\n+\n+  override def fit(dataset: DataFrame): AFTRegressionModel = {\n+    validateAndTransformSchema(dataset.schema, fitting = true)\n+    val instances = extractCensoredLabeledPoints(dataset)\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val costFun = new AFTCostFun(instances, $(fitIntercept))\n+    val optimizer = new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+\n+    val numFeatures = dataset.select($(featuresCol)).take(1)(0).getAs[Vector](0).size\n+    /*\n+       The weights vector has three parts:\n+       the first element: Double, log(sigma), the log of scale parameter\n+       the second element: Double, intercept of the beta parameter\n+       the third to the end elements: Doubles, weights vector of the beta parameter\n+     */\n+    val initialWeights = new DenseVector(Array.fill(numFeatures + 2)(1.0))"
  }],
  "prId": 8611
}]