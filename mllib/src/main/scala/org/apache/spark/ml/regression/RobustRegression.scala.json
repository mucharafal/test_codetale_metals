[{
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "indentation\n",
    "commit": "a79855a72aa9f567996f1c406951ce0aad51aa33",
    "createdAt": "2015-07-30T23:56:56Z",
    "diffHunk": "@@ -0,0 +1,609 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV, norm => brzNorm}\n+import breeze.optimize.{CachedDiffFunction, DiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+\n+import org.apache.spark.{Logging, SparkException}\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.ml.PredictorParams\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util.Identifiable\n+import org.apache.spark.mllib.evaluation.RegressionMetrics\n+import org.apache.spark.mllib.linalg.{Vector, Vectors}\n+import org.apache.spark.mllib.linalg.BLAS._\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions.{col, udf}\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.util.StatCounter\n+import scala.math.pow\n+\n+/**\n+ * Params for Robust regression.\n+ */\n+private[regression] trait RobustRegressionParams extends PredictorParams\n+    with HasRegParam with HasElasticNetParam with HasMaxIter with HasTol\n+    with HasFitIntercept\n+\n+/**\n+ * :: Experimental ::\n+ * Robust regression.\n+ *\n+ * The learning objective is to minimize the squared error, with regularization.\n+ * The specific squared error loss function used is:\n+ *   L = 1/2n ||A weights - y||^2^\n+ *\n+ * This support multiple types of regularization:\n+ *  - none (a.k.a. ordinary least squares)\n+ *  - L2 (ridge regression)\n+ *  - L1 (Lasso)\n+ *  - L2 + L1 (elastic net)\n+ */\n+@Experimental\n+class RobustRegression(override val uid: String)\n+  extends Regressor[Vector, RobustRegression, RobustRegressionModel]\n+  with RobustRegressionParams with Logging {\n+\n+  def this() = this(Identifiable.randomUID(\"linReg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   * @group setParam\n+   */\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set if we should fit the intercept\n+   * Default is true.\n+   * @group setParam\n+   */\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   * @group setParam\n+   */\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   * @group setParam\n+   */\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   * @group setParam\n+   */\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  override protected def train(dataset: DataFrame): RobustRegressionModel = {\n+    // Extract columns from data.  If dataset is persisted, do not persist instances.\n+    val instances = extractLabeledPoints(dataset).map {\n+      case LabeledPoint(label: Double, features: Vector) => (label, features)\n+    }\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val (summarizer, statCounter) = instances.treeAggregate(\n+      (new MultivariateOnlineSummarizer, new StatCounter))(\n+        seqOp = (c, v) => (c, v) match {\n+          case ((summarizer: MultivariateOnlineSummarizer, statCounter: StatCounter),\n+          (label: Double, features: Vector)) =>\n+            (summarizer.add(features), statCounter.merge(label))\n+      },\n+        combOp = (c1, c2) => (c1, c2) match {\n+          case ((summarizer1: MultivariateOnlineSummarizer, statCounter1: StatCounter),\n+          (summarizer2: MultivariateOnlineSummarizer, statCounter2: StatCounter)) =>\n+            (summarizer1.merge(summarizer2), statCounter1.merge(statCounter2))\n+      })\n+\n+    val numFeatures = summarizer.mean.size\n+    val yMean = statCounter.mean\n+    val yStd = math.sqrt(statCounter.variance)\n+\n+    // If the yStd is zero, then the intercept is yMean with zero weights;\n+    // as a result, training is not needed.\n+    if (yStd == 0.0) {\n+      logWarning(s\"The standard deviation of the label is zero, so the weights will be zeros \" +\n+        s\"and the intercept will be the mean of the label; as a result, training is not needed.\")\n+      if (handlePersistence) instances.unpersist()\n+      val weights = Vectors.sparse(numFeatures, Seq())\n+      val intercept = yMean\n+\n+      val model = new RobustRegressionModel(uid, weights, intercept)\n+      val trainingSummary = new RobustRegressionTrainingSummary(\n+        model.transform(dataset).select($(predictionCol), $(labelCol)),\n+        $(predictionCol),\n+        $(labelCol),\n+        Array(0D))\n+      return copyValues(model.setSummary(trainingSummary))\n+    }\n+\n+    val featuresMean = summarizer.mean.toArray\n+    val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+    // Since we implicitly do the feature scaling when we compute the cost function\n+    // to improve the convergence, the effective regParam will be changed.\n+    val effectiveRegParam = $(regParam) / yStd\n+    val effectiveL1RegParam = $(elasticNetParam) * effectiveRegParam\n+    val effectiveL2RegParam = (1.0 - $(elasticNetParam)) * effectiveRegParam\n+\n+    val costFun = new HuberCostFun(instances, yStd, yMean, $(fitIntercept),\n+      featuresStd, featuresMean, effectiveL2RegParam)\n+\n+    val optimizer = if ($(elasticNetParam) == 0.0 || effectiveRegParam == 0.0) {\n+      new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+    } else {\n+      new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, effectiveL1RegParam, $(tol))\n+    }\n+\n+    val initialWeights = Vectors.zeros(numFeatures)\n+    val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+      initialWeights.toBreeze.toDenseVector)\n+\n+    val (weights, objectiveHistory) = {\n+      /*\n+         Note that in Robust Regression, the objective history (loss + regularization) returned\n+         from optimizer is computed in the scaled space given by the following formula.\n+         {{{\n+         L = 1/2n||\\sum_i w_i(x_i - \\bar{x_i}) / \\hat{x_i} - (y - \\bar{y}) / \\hat{y}||^2 + regTerms\n+         }}}\n+       */\n+      val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+      var state: optimizer.State = null\n+      while (states.hasNext) {\n+        state = states.next()\n+        arrayBuilder += state.adjustedValue\n+      }\n+      if (state == null) {\n+        val msg = s\"${optimizer.getClass.getName} failed.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      /*\n+         The weights are trained in the scaled space; we're converting them back to\n+         the original space.\n+       */\n+      val rawWeights = state.x.toArray.clone()\n+      var i = 0\n+      val len = rawWeights.length\n+      while (i < len) {\n+        rawWeights(i) *= { if (featuresStd(i) != 0.0) yStd / featuresStd(i) else 0.0 }\n+        i += 1\n+      }\n+\n+      (Vectors.dense(rawWeights).compressed, arrayBuilder.result())\n+    }\n+\n+    /*\n+       The intercept in R's GLMNET is computed using closed form after the coefficients are\n+       converged. See the following discussion for detail.\n+       http://stats.stackexchange.com/questions/13617/how-is-the-intercept-computed-in-glmnet\n+     */\n+    val intercept = if ($(fitIntercept)) yMean - dot(weights, Vectors.dense(featuresMean)) else 0.0\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(new RobustRegressionModel(uid, weights, intercept))\n+    val trainingSummary = new RobustRegressionTrainingSummary(\n+      model.transform(dataset).select($(predictionCol), $(labelCol)),\n+      $(predictionCol),\n+      $(labelCol),\n+      objectiveHistory)\n+    model.setSummary(trainingSummary)\n+  }\n+\n+  override def copy(extra: ParamMap): RobustRegression = defaultCopy(extra)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[RobustRegression]].\n+ */\n+@Experimental\n+class RobustRegressionModel private[ml] (\n+    override val uid: String,\n+    val weights: Vector,\n+    val intercept: Double)\n+  extends RegressionModel[Vector, RobustRegressionModel]\n+  with RobustRegressionParams {\n+\n+  private var trainingSummary: Option[RobustRegressionTrainingSummary] = None\n+\n+  /**\n+   * Gets summary (e.g. residuals, mse, r-squared ) of model on training set. An exception is\n+   * thrown if `trainingSummary == None`.\n+   */\n+  def summary: RobustRegressionTrainingSummary = trainingSummary match {\n+    case Some(summ) => summ\n+    case None =>\n+      throw new SparkException(\n+        \"No training summary available for this RobustRegressionModel\",\n+        new NullPointerException())\n+  }\n+\n+  private[regression] def setSummary(summary: RobustRegressionTrainingSummary): this.type = {\n+    this.trainingSummary = Some(summary)\n+    this\n+  }\n+\n+  /** Indicates whether a training summary exists for this model instance. */\n+  def hasSummary: Boolean = trainingSummary.isDefined\n+\n+  /**\n+   * Evaluates the model on a testset.\n+   * @param dataset Test dataset to evaluate model on.\n+   */\n+  // TODO: decide on a good name before exposing to public API\n+  private[regression] def evaluate(dataset: DataFrame): RobustRegressionSummary = {\n+    val t = udf { features: Vector => predict(features) }\n+    val predictionAndObservations = dataset\n+      .select(col($(labelCol)), t(col($(featuresCol))).as($(predictionCol)))\n+\n+    new RobustRegressionSummary(predictionAndObservations, $(predictionCol), $(labelCol))\n+  }\n+\n+  override protected def predict(features: Vector): Double = {\n+    dot(features, weights) + intercept\n+  }\n+\n+  override def copy(extra: ParamMap): RobustRegressionModel = {\n+    val newModel = copyValues(new RobustRegressionModel(uid, weights, intercept))\n+    if (trainingSummary.isDefined) newModel.setSummary(trainingSummary.get)\n+    newModel\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Robust regression training results.\n+ * @param predictions predictions outputted by the model's `transform` method.\n+ * @param objectiveHistory objective function (scaled loss + regularization) at each iteration.\n+ */\n+@Experimental\n+class RobustRegressionTrainingSummary private[regression] (\n+    predictions: DataFrame,\n+    predictionCol: String,\n+    labelCol: String,\n+    val objectiveHistory: Array[Double])\n+  extends RobustRegressionSummary(predictions, predictionCol, labelCol) {\n+\n+  /** Number of training iterations until termination */\n+  val totalIterations = objectiveHistory.length\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Robust regression results evaluated on a dataset.\n+ * @param predictions predictions outputted by the model's `transform` method.\n+ */\n+@Experimental\n+class RobustRegressionSummary private[regression] (\n+    @transient val predictions: DataFrame,\n+    val predictionCol: String,\n+    val labelCol: String) extends Serializable {\n+\n+  @transient private val metrics = new RegressionMetrics(\n+    predictions\n+      .select(predictionCol, labelCol)\n+      .map { case Row(pred: Double, label: Double) => (pred, label) } )\n+\n+  /**\n+   * Returns the explained variance regression score.\n+   * explainedVariance = 1 - variance(y - \\hat{y}) / variance(y)\n+   * Reference: [[http://en.wikipedia.org/wiki/Explained_variation]]\n+   */\n+  val explainedVariance: Double = metrics.explainedVariance\n+\n+  /**\n+   * Returns the mean absolute error, which is a risk function corresponding to the\n+   * expected value of the absolute error loss or l1-norm loss.\n+   */\n+  val meanAbsoluteError: Double = metrics.meanAbsoluteError\n+\n+  /**\n+   * Returns the mean squared error, which is a risk function corresponding to the\n+   * expected value of the squared error loss or quadratic loss.\n+   */\n+  val meanSquaredError: Double = metrics.meanSquaredError\n+\n+  /**\n+   * Returns the root mean squared error, which is defined as the square root of\n+   * the mean squared error.\n+   */\n+  val rootMeanSquaredError: Double = metrics.rootMeanSquaredError\n+\n+  /**\n+   * Returns R^2^, the coefficient of determination.\n+   * Reference: [[http://en.wikipedia.org/wiki/Coefficient_of_determination]]\n+   */\n+  val r2: Double = metrics.r2\n+\n+  /** Residuals (label - predicted value) */\n+  @transient lazy val residuals: DataFrame = {\n+    val t = udf { (pred: Double, label: Double) => label - pred }\n+    predictions.select(t(col(predictionCol), col(labelCol)).as(\"residuals\"))\n+  }\n+\n+}\n+\n+/**\n+ * HuberAggregator computes the gradient and loss for a Huber loss function,\n+ * as used in Robust regression for samples in sparse or dense vector in a online fashion.\n+ *\n+ * Two HuberAggregator can be merged together to have a summary of loss and gradient of\n+ * the corresponding joint dataset.\n+ *\n+ * For improving the convergence rate during the optimization process, and also preventing against\n+ * features with very large variances exerting an overly large influence during model training,\n+ * package like R's GLMNET performs the scaling to unit variance and removing the mean to reduce\n+ * the condition number, and then trains the model in scaled space but returns the weights in\n+ * the original scale. See page 9 in http://cran.r-project.org/web/packages/glmnet/glmnet.pdf\n+ *\n+ * However, we don't want to apply the `StandardScaler` on the training dataset, and then cache\n+ * the standardized dataset since it will create a lot of overhead. As a result, we perform the\n+ * scaling implicitly when we compute the objective function. The following is the mathematical\n+ * derivation.\n+ *\n+ * Note that we don't deal with intercept by adding bias here, because the intercept\n+ * can be computed using closed form after the coefficients are converged.\n+ * See this discussion for detail.\n+ * http://stats.stackexchange.com/questions/13617/how-is-the-intercept-computed-in-glmnet\n+ *\n+ * When training with intercept enabled,\n+ * The objective function in the scaled space is given by\n+ * {{{\n+ * L = 1/2n ||\\sum_i w_i(x_i - \\bar{x_i}) / \\hat{x_i} - (y - \\bar{y}) / \\hat{y}||^2,\n+ * }}}\n+ * where \\bar{x_i} is the mean of x_i, \\hat{x_i} is the standard deviation of x_i,\n+ * \\bar{y} is the mean of label, and \\hat{y} is the standard deviation of label.\n+ *\n+ * If we fitting the intercept disabled (that is forced through 0.0),\n+ * we can use the same equation except we set \\bar{y} and \\bar{x_i} to 0 instead\n+ * of the respective means.\n+ *\n+ * This can be rewritten as\n+ * {{{\n+ * L = 1/2n ||\\sum_i (w_i/\\hat{x_i})x_i - \\sum_i (w_i/\\hat{x_i})\\bar{x_i} - y / \\hat{y}\n+ *     + \\bar{y} / \\hat{y}||^2\n+ *   = 1/2n ||\\sum_i w_i^\\prime x_i - y / \\hat{y} + offset||^2 = 1/2n diff^2\n+ * }}}\n+ * where w_i^\\prime^ is the effective weights defined by w_i/\\hat{x_i}, offset is\n+ * {{{\n+ * - \\sum_i (w_i/\\hat{x_i})\\bar{x_i} + \\bar{y} / \\hat{y}.\n+ * }}}, and diff is\n+ * {{{\n+ * \\sum_i w_i^\\prime x_i - y / \\hat{y} + offset\n+ * }}}\n+ *\n+ *\n+ * Note that the effective weights and offset don't depend on training dataset,\n+ * so they can be precomputed.\n+ *\n+ * Now, the first derivative of the objective function in scaled space is\n+ * {{{\n+ * \\frac{\\partial L}{\\partial\\w_i} = diff/N (x_i - \\bar{x_i}) / \\hat{x_i}\n+ * }}}\n+ * However, ($x_i - \\bar{x_i}$) will densify the computation, so it's not\n+ * an ideal formula when the training dataset is sparse format.\n+ *\n+ * This can be addressed by adding the dense \\bar{x_i} / \\har{x_i} terms\n+ * in the end by keeping the sum of diff. The first derivative of total\n+ * objective function from all the samples is\n+ * {{{\n+ * \\frac{\\partial L}{\\partial\\w_i} =\n+ *     1/N \\sum_j diff_j (x_{ij} - \\bar{x_i}) / \\hat{x_i}\n+ *   = 1/N ((\\sum_j diff_j x_{ij} / \\hat{x_i}) - diffSum \\bar{x_i}) / \\hat{x_i})\n+ *   = 1/N ((\\sum_j diff_j x_{ij} / \\hat{x_i}) + correction_i)\n+ * }}},\n+ * where correction_i = - diffSum \\bar{x_i}) / \\hat{x_i}\n+ *\n+ * A simple math can show that diffSum is actually zero, so we don't even\n+ * need to add the correction terms in the end. From the definition of diff,\n+ * {{{\n+ * diffSum = \\sum_j (\\sum_i w_i(x_{ij} - \\bar{x_i}) / \\hat{x_i} - (y_j - \\bar{y}) / \\hat{y})\n+ *         = N * (\\sum_i w_i(\\bar{x_i} - \\bar{x_i}) / \\hat{x_i} - (\\bar{y_j} - \\bar{y}) / \\hat{y})\n+ *         = 0\n+ * }}}\n+ *\n+ * As a result, the first derivative of the total objective function only depends on\n+ * the training dataset, which can be easily computed in distributed fashion, and is\n+ * sparse format friendly.\n+ * {{{\n+ * \\frac{\\partial L}{\\partial\\w_i} = 1/N ((\\sum_j diff_j x_{ij} / \\hat{x_i})\n+ * }}},\n+ *\n+ * @param weights The weights/coefficients corresponding to the features.\n+ * @param labelStd The standard deviation value of the label.\n+ * @param labelMean The mean value of the label.\n+ * @param featuresStd The standard deviation values of the features.\n+ * @param featuresMean The mean values of the features.\n+ */\n+private class HuberAggregator(\n+    weights: Vector,\n+    labelStd: Double,\n+    labelMean: Double,\n+    fitIntercept: Boolean,\n+    featuresStd: Array[Double],\n+    featuresMean: Array[Double]) extends Serializable {\n+\n+  private var totalCnt: Long = 0L\n+  private var lossSum = 0.0\n+\n+  private val (effectiveWeightsArray: Array[Double], offset: Double, dim: Int) = {\n+    val weightsArray = weights.toArray.clone()\n+    var sum = 0.0\n+    var i = 0\n+    val len = weightsArray.length\n+    while (i < len) {\n+      if (featuresStd(i) != 0.0) {\n+        weightsArray(i) /=  featuresStd(i)\n+        sum += weightsArray(i) * featuresMean(i)\n+      } else {\n+        weightsArray(i) = 0.0\n+      }\n+      i += 1\n+    }\n+    (weightsArray, if (fitIntercept) labelMean / labelStd - sum else 0.0, weightsArray.length)\n+  }\n+\n+  private val effectiveWeightsVector = Vectors.dense(effectiveWeightsArray)\n+\n+  private val gradientSumArray = Array.ofDim[Double](dim)\n+\n+  /**\n+   * Add a new training data to this HuberAggregator, and update the loss and gradient\n+   * of the objective function.\n+   *\n+   * @param label The label for this data point.\n+   * @param data The features for one data point in dense/sparse vector format to be added\n+   *             into this aggregator.\n+   * @return This HuberAggregator object.\n+   */\n+  def add(label: Double, data: Vector): this.type = {\n+    require(dim == data.size, s\"Dimensions mismatch when adding new sample.\" +\n+      s\" Expecting $dim but got ${data.size}.\")\n+\n+    val diff = dot(data, effectiveWeightsVector) - label / labelStd + offset\n+\n+    if (diff != 0) {\n+      val localGradientSumArray = gradientSumArray\n+      data.foreachActive { (index, value) =>\n+        if (featuresStd(index) != 0.0 && value != 0.0) {\n+          localGradientSumArray(index) += diff * value / featuresStd(index)\n+        }\n+      }\n+      lossSum += diff * diff / 2.0\n+    }\n+\n+    totalCnt += 1\n+    this\n+  }\n+\n+  /**\n+   * Merge another HuberAggregator, and update the loss and gradient\n+   * of the objective function.\n+   * (Note that it's in place merging; as a result, `this` object will be modified.)\n+   *\n+   * @param other The other HuberAggregator to be merged.\n+   * @return This HuberAggregator object.\n+   */\n+  def merge(other: HuberAggregator): this.type = {\n+    require(dim == other.dim, s\"Dimensions mismatch when merging with another \" +\n+      s\"HuberAggregator. Expecting $dim but got ${other.dim}.\")\n+\n+    if (other.totalCnt != 0) {\n+      totalCnt += other.totalCnt\n+      lossSum += other.lossSum\n+\n+      var i = 0\n+      val localThisGradientSumArray = this.gradientSumArray\n+      val localOtherGradientSumArray = other.gradientSumArray\n+      while (i < dim) {\n+        localThisGradientSumArray(i) += localOtherGradientSumArray(i)\n+        i += 1\n+      }\n+    }\n+    this\n+  }\n+\n+  def count: Long = totalCnt\n+\n+  def loss: Double = lossSum / totalCnt\n+\n+  def gradient: Vector = {\n+    val result = Vectors.dense(gradientSumArray.clone())\n+    scal(1.0 / totalCnt, result)\n+    result\n+  }\n+}\n+\n+/**\n+ * HuberCostFun implements Breeze's DiffFunction[T] for Huber cost as used in Robust regression.\n+ * The Huber M-estimator corresponds to a probability distribution for the errors which is normal\n+ * in the centre but like a double exponential distribution in the tails (Hogg 1979: 109).\n+ * L = 1/2 ||A weights-y||^2 if |A weights-y| <= k\n+ * L = k |A weights-y| - 1/2 K^2 if |A weights-y| > k\n+ * where k = 1.345 which produce 95% efficiency when the errors are normal and\n+ * substantial resistance to outliers otherwise.\n+ * See also the documentation for the precise formulation.\n+ * It's used in Breeze's convex optimization routines.\n+ */\n+private class HuberCostFun(\n+                            data: RDD[(Double, Vector)],"
  }],
  "prId": 7722
}]