[{
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "This is here because, when we subtract off the max margin below, we will end up doing `Double.PositiveInfinity - Double.PositiveInfinity` which equals `NaN` in scala. Alternatively, we could just use `Double.MaxValue` instead.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-12T19:35:27Z",
    "diffHunk": "@@ -0,0 +1,651 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                   instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                    c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+        (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+        val standardizedInstances = instances.map { case Instance(label, weight, features) =>\n+          val f = features match {\n+            case DenseVector(vs) =>\n+              val values = vs.clone()\n+              val size = values.length\n+              var i = 0\n+              while (i < size) {\n+                values(i) *= (if (featuresStd(i) != 0.0) 1.0 / featuresStd(i) else 0.0)\n+                i += 1\n+              }\n+              Vectors.dense(values)\n+            case SparseVector(size, indices, vs) =>\n+              val values = vs.clone()\n+              val nnz = values.length\n+              var i = 0\n+              while (i < nnz) {\n+                values(i) *= (if (featuresStd(indices(i)) != 0.0) {\n+                  1.0 / featuresStd(indices(i))\n+                } else {\n+                  0.0\n+                })\n+                i += 1\n+              }\n+              Vectors.sparse(size, indices, values)\n+            case v => throw new IllegalArgumentException(\"Do not support vector type \" + v.getClass)\n+          }\n+          Instance(label, weight, f)\n+        }\n+        if (handlePersistence) standardizedInstances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val costFun = new LogisticCostFun(standardizedInstances, numClasses, $(fitIntercept),\n+          $(standardization), featuresStd, regParamL2, multinomial = true, standardize = false)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+        if (handlePersistence) standardizedInstances.unpersist()\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = state.x.toArray.clone()\n+        val coefArray = Array.ofDim[Double](numFeatures * numClasses)\n+        val interceptArray = Array.ofDim[Double](if (getFitIntercept) numClasses else 0)\n+        (0 until numClasses).foreach { k =>\n+          var i = 0\n+          while (i < numFeatures) {\n+            val rawValue = rawCoefficients(k * numFeaturesPlusIntercept + i)\n+            val unscaledCoef =\n+              rawValue * { if (featuresStd(i) != 0.0) 1.0 / featuresStd(i) else 0.0 }\n+            coefArray(k * numFeatures + i) = unscaledCoef\n+            coefSum += unscaledCoef\n+            i += 1\n+          }\n+          if (getFitIntercept) {\n+            val intercept = rawCoefficients(k * numFeaturesPlusIntercept + numFeatures)\n+            interceptArray(k) = intercept\n+            interceptSum += intercept\n+          }\n+        }\n+\n+        val _coefficients = {\n+          /*\n+            When no regularization is applied, the coefficients lack identifiability because\n+            we do not use a pivot class. We can add any constant value to the coefficients and\n+            get the same likelihood. So here, we choose the mean centered coefficients for\n+            reproducibility. This method follows the approach in glmnet, described here:\n+\n+            Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+              Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+           */\n+          if ($(regParam) == 0) {\n+            val coefficientMean = coefSum / (numClasses * numFeatures)\n+            var i = 0\n+            while (i < coefArray.length) {\n+              coefArray(i) -= coefficientMean\n+              i += 1\n+            }\n+          }\n+          new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true)\n+        }\n+\n+        val _intercepts = if (getFitIntercept) {\n+          /*\n+            The intercepts are never regularized, so we always center the mean.\n+          */\n+          val interceptMean = interceptSum / numClasses\n+          var k = 0\n+          while (k < interceptArray.length) {\n+            interceptArray(k) -= interceptMean\n+            k += 1\n+          }\n+          Vectors.dense(interceptArray)\n+        } else {\n+          Vectors.sparse(numClasses, Seq())\n+        }\n+\n+        (_coefficients, _intercepts, arrayBuilder.result())\n+      }\n+    }\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {",
    "line": 515
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "I couldn't find much (any) documentation on this subject, so I'd appreciate feedback on the correctness of this approach. I did not _thoroughly_ test this, but I don't recall seeing better performance using this initialization over zeros.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-15T21:58:53Z",
    "diffHunk": "@@ -0,0 +1,626 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                   instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                    c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+        (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,",
    "line": 288
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "The idea is the same as regularization path. I think you can test it by having a large L1 regularization with intercept on, all the coefficients will be zeros, and the intercepts will represent the prior distribution. If I remember correctly, the BLOR has this test. Thanks.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-16T00:20:25Z",
    "diffHunk": "@@ -0,0 +1,626 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                   instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                    c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+        (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,",
    "line": 288
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "I checked that the intercepts that the model converges to using high L1 penalty are the same as the initial intercepts computed here. The tests for this would require having the option to set the initial model in MLOR, but I left that out of this PR. I can either update this PR to accept initial model and add some new tests, or we can leave that as a TODO and add it in after this gets merged. Thoughts?\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-16T05:12:35Z",
    "diffHunk": "@@ -0,0 +1,626 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                   instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                    c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+        (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,",
    "line": 288
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Cool. The test I have is just testing that with high L1 penalty, the algorithm will be converged within one step, and also the resulting coefficients are the one expected computed theoretically.  We can have the test that with different initial model, and then converges into the expected one later. I remember there is a PR which allows users to specify the initial coefficient. \n\nAlso, the current implementation uses one level tree aggregation, and it has been seen that with high dimensional problems, even BLOR can cause OOM in driver in the final reduce when pulling all the dense coefficients. I'll suggest that we allow users to customize the tree aggregation level as param for larger problem. Let's open another JIRA to discuss the detail about this. Thanks.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-16T06:01:07Z",
    "diffHunk": "@@ -0,0 +1,626 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                   instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                    c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+        (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,",
    "line": 288
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "I updated the follow up items in the PR description to include the intercept prior test and initial model, as well as the tree aggregation configuration. I'll create JIRAs for those items when this is closer to complete.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-16T16:01:09Z",
    "diffHunk": "@@ -0,0 +1,626 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                   instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                    c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+        (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,",
    "line": 288
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "For tree aggregation configuration, this will be a starter task. Maybe we can have someone taking a look at this in parallel. (I got couple people asking me about this while they are running high dimensional problems, and the workaround they had now is reducing the # of partitions.) Thanks.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-16T17:32:10Z",
    "diffHunk": "@@ -0,0 +1,626 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                   instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                    c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+        (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,",
    "line": 288
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "[SPARK-17090](https://issues.apache.org/jira/browse/SPARK-17090)\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-16T17:48:17Z",
    "diffHunk": "@@ -0,0 +1,626 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                   instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                    c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+        (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,",
    "line": 288
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "I changed this code so that it could potentially handle sparse data structures in the future.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-17T20:50:27Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                   instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                    c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+        (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {"
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Since this code doesn't support sparse for now, maybe for readability, the following will be easier?\n\n``` scala\nval rawCoefficients = state.x.toArray\nval intercepts: Array[Double] = if ($(fitIntercept)) {\n  Array.tabulate(numClasses) { i =>\n    val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n    val intercept = rawCoefficients(coefIndex)\n  )\n} else {\n  Array[Double]()\n}\nval coefficientMatrix = Matrix.denseMatrix(numClasses, numFeatures, Array.tabulate(numClasses * numFeatures) { i =>\n  // flatIndex will loop though rawCoefficients, and skip the intercept terms.\n  val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n  val featureIndex = i % numFeatures\n  if (featuresStd(featureIndex) != 0.0) {\n    rawCoefficients(flatIndex) / featuresStd(featureIndex)\n   } else {\n     0.0\n   }\n}, isTransposed = true)\n```\n\nThis can be another JIRA. For `Vector`, we support `compressed` api when the sparse format saves more space. `Matrix` should support it, and compress the model into sparse matrix when it's needed.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:15:04Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                   instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                    c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+        (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "I refactored this to fit to your suggestion. We can discuss the compressed as a follow up.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:22:32Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                   instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                    c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+        (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "multinomial logistic (softmax) regression\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-17T22:43:08Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression."
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T00:09:34Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression."
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "ditto \n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-17T22:49:07Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression."
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "also, in `LogisticRegression.scala` update the message.\n\n``` scala\n /**\n  * Logistic regression.\n  * Currently, this class only supports binary classification.  It will support multiclass\n  * in the future.\n  */\n @Since(\"1.2.0\")\n class LogisticRegression @Since(\"1.2.0\") (\n```\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-17T22:51:14Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression."
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T00:09:28Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression."
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "just styling. seems in most of the place, we don't have empty line between `def setElasticNetParam` and `setDefault`\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-17T22:55:02Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)",
    "line": 105
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T00:09:18Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)",
    "line": 105
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "ditto\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-17T22:55:21Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T00:09:10Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "Historically, this was added for being called from `mllib` to properly handle caching. Since we will not update `mllib` anymore, let's avoid doing this, and just use the style in LinearRegression. (Remove the `handlePersistence` in the function).\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-17T23:00:46Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T00:09:04Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "I know it's painful. The default indentation when you copy & paste from Intellji is different from spark style. The following is the one from LOR. \n\nThere are so many copy & paste from LOR. Even a significant amount of them should be shared between LiR. Let's create a JIRA to refactor those shared code later. \n\n``` scala\n    val (summarizer, labelSummarizer) = {\n      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n        instance: Instance) =>\n          (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n\n      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n\n      instances.treeAggregate(\n        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n    }\n```\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-17T23:04:52Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                   instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                    c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+        (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Actually, I'm wondering if at some point the LogisticRegression can wrap the multinomial implementation. They produce different results, as we've discussed, when using BLOR with regularization, so I'm not sure how we can get around that. But I definitely agree, there is a lot of shared/copied code, and that was one of my concerns in creating a separate multinomial estimator class.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-17T23:27:18Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                   instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                    c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+        (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Fixed indentation.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T00:08:56Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                   instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                    c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+        (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }"
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "I'm thinking maybe we can have `setMultinomial` default to `false` in `LogisticRegression`, and have most of the code there. Then `MultinomialLogisticRegression` will force to have  `setMultinomial` true, and just wrap `LogisticRegression`. Thus, we will only have one implementation. Let's discuss this more in separate JIRA.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T00:46:01Z",
    "diffHunk": "@@ -0,0 +1,622 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                   instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+                    c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+        (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "`instances.persist(StorageLevel.MEMORY_AND_DISK)` is the one cached; as a result, you will touch the source twice which is not ideal. Why do you need to use `MetadataUtils.getNumClasses`? I think we just just do `val numClasses = labelSummarizer.numClasses` or `val numClasses = histogram.length` which should return the same value.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T01:24:46Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }",
    "line": 205
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Also, please move it into the same order like LOR so it's easier to code review or refactor them together later. \n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T01:26:19Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }",
    "line": 205
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "The reason I use `MetadataUtils.getNumClasses` is that @jkbradley requested that we first check the dataframe metadata for number of classes on the JIRA ticket. This is the convention taken in random forest. It's possible to miss one of the class labels in cross validation and then you might end up with models that have differently shaped coefficient matrices. That is one risk I can think of off the top of my head.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T05:40:42Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }",
    "line": 205
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Get you. I think it's okay for MLOR since if there is unseen label, we will never predict that one. As long as the dimension of features is not changed, it's still fine even the # of classes is changed. \n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T06:10:41Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }",
    "line": 205
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "You can wind up with different length `rawPrediction` and `probability` vectors for the output too, though. I suppose this could be a problem when evaluating models using the probability vectors. I don't have a great sense of whether this can be a problem in practice very often, but since it was explicitly discussed on the JIRA I wanted to make sure to at least address it. It doesn't seem like too much overhead to leave this, as we can still fall back to the histogram. Thoughts? I'll change it if we decide it isn't necessary.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T15:22:18Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }",
    "line": 205
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "This comes down to how we handle metadata I think. The problem here could be that a training data only has label 3 and label 4, but in the current design, we treat it as 5 classes problem resulting a coefficient matrix with dimensions `5 * featureSizePlusIntercept`. This will not only introduce negative infinity as intercepts in label 0, 1, and 2, but also we need to do smoothing in computing the initial coefficients. This will make the entire optimization less stable as well since classes 0, 1, and 2 are no-seen with negative infinity intercepts. \n\nNow I strongly think we either have a better metadata handling to map each class from 0 to K -1 such that all the labels have its own samples, or just fail the training like R. Users need to make sure that all the labels from 0 to K - 1 have at least one sample, and they have the responsibility of making sure all the labels in testing or validation should be in training. \n\nHowever, for cross validation, there is chance that training data doesn't contain certain label when sampling is introduced. Thus, I think the first approach can be more desirable, which knows the real numClasses from metadata, and only trains for those labels in the training resulting smaller coefficient matrix. In prediction, we just put probability as zeros for those label not in training. Thus, we don't need to do smoothing and the training will be more stable without having negative infinity as intercepts.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T23:28:50Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }",
    "line": 205
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "One thing is that we always output the prediction as a vector, so for the above class, seems to be hard to ignore label 0 to 2. Need to keep the metadata in something like Map structure even for prediction.  \n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T23:34:01Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }",
    "line": 205
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "I think it's reasonable to expect users to zero index the labels using `StringIndexer` or similar. Perhaps we can clearly communicate, in the docs or elsewhere, that the labels should be zero indexed by using `StringIndexer`? Bookkeeping metadata, but training only on seen classes, and then injecting zero probability at prediction time seems like a complex solution that caters to an edge case. I like keeping the metadata for the cross-validation case. \n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-19T01:09:55Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }",
    "line": 205
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Let's have a JIRA discussing about this. I personally don't prefer smoothing on the initialization and having negative infinity as intercepts which may cause stability issue.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-19T05:10:12Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }",
    "line": 205
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Ok, I'll create a JIRA for it.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-19T05:20:23Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }",
    "line": 205
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "Minor, seems that we often name it as `isConstantLabel`\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T02:19:12Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1"
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "I like using `isConstantLabel`, maybe you can also do so in `LogisticRegression.scala` in this PR so the if else condition can be consolidated. \n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T02:29:02Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done. You can check LOR now to see if it's what you had in mind. I consolidated one of the if statements.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:26:07Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "For consistency, either in `Matrcies.sparse`, you do `Array.empty[Int]`,  and `Array.empty[Double]` or in `Vectors.sparse`, you do `Array()`.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T02:23:48Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])"
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Add a document that when `isConstantLabel == true`, the last class in the histogram will be the only label shown in the training dataset, and this is not obvious when reading the code.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T02:25:07Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "I think the following also applies.\n\n``` scala\n        if (!$(fitIntercept) && (0 until numFeatures).exists { i =>\n          featuresStd(i) == 0.0 && featuresMean(i) != 0.0 }) {\n          logWarning(\"Fitting LogisticRegressionModel without intercept on dataset with \" +\n            \"constant nonzero column, Spark MLlib outputs zero coefficients for constant \" +\n            \"nonzero columns. This behavior is the same as R glmnet but different from LIBSVM.\")\n        }\n```\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T02:42:53Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)",
    "line": 234
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:24:20Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)",
    "line": 234
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "Not in this PR. There are people trying to use their optimizer, and since we don't allow it as plugin, many users end up copy & paste the whole files. Let's create a JIRA and design the optimizer plugin. That will be useful when we want to remove the breeze dependency in the future. \n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T02:45:31Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))",
    "line": 251
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "What happen when `numClasses == 1`? We need at least two sets of coefficient to make softmax work. I think in this case, we need to  `val initialCoefficientsWithIntercept = Vectors.zeros(2 * numFeaturesPlusIntercept)`\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T02:51:37Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)",
    "line": 284
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "I guess the cleaner way to handle this is not here. We could handle this earlier like\n\n``` scala\nval histogram: Array[Double] = {\n  if (labelSummarizer.histogram.length >= 2) histogram \n  else if(labelSummarizer.histogram.length == 1) Array[Double](histogram(0), 0.0)\n  else ...\n}\nval numClasses = histogram.length\n```\n\nthis will guarantee that `numClasses >= 2`. \n\nHowever, this will change the logic of `($(fitIntercept) && labelIsConstant) == true` when the label 0 is constant. You need to handle that.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T03:02:03Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)",
    "line": 284
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "BTW, this is a edge case, and it's great to have a test on this.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T03:02:50Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)",
    "line": 284
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "![image](https://cloud.githubusercontent.com/assets/7275795/17765099/8e38fda2-64d8-11e6-8411-b71d57b89c03.png)\nAbove results show how the current case is handled. It completes in a single iteration. I'm not sure what is the best way. R seems to fail when you provide a constant label. If we're sticking to the convention that for we return `K` sets of coefficients for `K` classes, then returning a single set of coefficients seems ok here. Regardless, I can add a test to check this.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T07:18:39Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)",
    "line": 284
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "This comes down to how we handle metadata as we discussed above. \n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T23:32:06Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)",
    "line": 284
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "I added a test for this case. We can expand on the behavior in the future, or now, if we wish.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-19T03:02:34Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)",
    "line": 284
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "Is the solution the same that minimizes the L1 penalty?\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T03:06:25Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,"
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Since we're not pivoting, for `K+1` classes problem, `count_0` should not be in formula. \n\n``` latex\n             {{{\n               P(0) = \\exp(b_0) / (\\sum_{k=0}^K \\exp(b_k))\n               ...\n               P(K) = \\exp(b_K) / (\\sum_{k=0}^K \\exp(b_k))\n             }}}\n```\n\nwe have a freedom to chose the phase `lambda` since the solution isn't identifiable, so\n\n``` latex\n             {{{\n               b_k = \\log{count_k + 1} + \\lambda    for all k in {0, K}  \n               ( +1 is for smoothing for the classes that zero count so this is applied even histogram(i) == 0)\n             }}}\n```\n\nwe can chose `\\lambda  = - \\mean{b_k}` as you suggest to minimize L1 and L2 loss together. \n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T03:31:25Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "I modified the note and checked that the new computation (which is simpler) produces the same result.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:23:56Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Also, this does not minimize the L1 penalty. Intercepts are never regularized so there is no correctness reason to choose one or the other. We simply choose to minimize L2 penalty because we have to choose something. It lines up with R.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:37:20Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "add a new line here.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T03:32:51Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "add a new line here.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T03:37:10Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:21:43Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "Updating this in the following big block is not really functional, and may cause error easily. Once the `coefficientMatrix` and `interceptVector` are constructed, we can compute those then. This is not bottleneck, so clarity is more important.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:18:36Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "It's true. done.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:22:58Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "This can not be a sparse vector once the `interceptVector` is centered. \n\n``` scala\nval interceptVector = if (intercepts.length > 1) {\n  val interceptMean = intercepts.sum / intercepts.length\n  for (i = 0 until i < intercepts.length) {\n    intercepts(i) -= interceptMean\n  }\n} else {\n  Vectors.sparse(numClasses, Seq())\n}\n```\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:28:26Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Brain lapse. done.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:21:55Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "wow, do they substrate the same value for all the different feature? my intuition will be substrating the mean of coef for each feature. Anyway, as long as we get the unique solution, and follow glmnet, it's great.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:32:02Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Yes, you can show that\n\n`argmin_c \\sum_{i,j} (\\beta_{i,j}  c)^2` is solved for `c = 1 / N \\sum_{i,j} \\beta_{i,j}`\n\nIt's a somewhat arbitrary choice since the coefficients aren't identifiable and the user has not specified an L2 penalty. From the paper\n\n```\nNot all the parameters in our model are regularized. The intercepts β0 are not, and with our\npenalty modifiers γj (Section 2.6) others need not be as well. For these parameters we use\nmean centering\n```\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T05:32:06Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "val m = margins(features).toArray\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:37:07Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "made a reference to the array after I call argmax\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:21:36Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "not use toArray here which is expensive.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:37:21Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:21:13Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "extra space after numClasses\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:38:43Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "Since this is tight loop, and will have significant impact on runtime scoring performance, please use `while` loop.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:39:24Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:20:49Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "update `m = math.exp(m(i))` so you don't have to compute the `exp` again which is very expensive.  \n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:40:55Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:21:17Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "m.size == numClasses. don't use m.size\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:41:21Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:20:35Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "ditto on calling `.values` and `exp`\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:42:03Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:20:31Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "this is fairly slow. maybe \n\n``` scala\nval probabilities = scores(features).toArray\nvar i = 0\nwhile ( i < numClasses) {\n  if (thresholds(i) == 0.0) probabilities(i) =  Double.PositiveInfinity else probabilities(i) / = t\n\n}\n```\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:49:07Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "I changed this up to do everything in a single pass. It's a bit more verbose. I can use the argmax function instead if that's clearer.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:20:24Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "have a copy of `dv.values` reference to avoid dereference in every loop. not use for loop.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:54:40Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:18:50Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "ditto\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:54:47Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0\n+            } else {\n+              dv.values(j) = 0.0"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:18:55Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0\n+            } else {\n+              dv.values(j) = 0.0"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "ditto. `exp` can be precomputed.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:55:13Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0\n+            } else {\n+              dv.values(j) = 0.0\n+            }\n+          }\n+        } else {\n+          val sum = {\n+            var temp = 0.0\n+            if (maxMargin > 0) {\n+              // adjust margins for overflow\n+              for (j <- 0 until numClasses) {\n+                dv.values(j) -= maxMargin\n+                temp += math.exp(dv.values(j))"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:19:00Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0\n+            } else {\n+              dv.values(j) = 0.0\n+            }\n+          }\n+        } else {\n+          val sum = {\n+            var temp = 0.0\n+            if (maxMargin > 0) {\n+              // adjust margins for overflow\n+              for (j <- 0 until numClasses) {\n+                dv.values(j) -= maxMargin\n+                temp += math.exp(dv.values(j))"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "intercepts\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T04:58:32Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0\n+            } else {\n+              dv.values(j) = 0.0\n+            }\n+          }\n+        } else {\n+          val sum = {\n+            var temp = 0.0\n+            if (maxMargin > 0) {\n+              // adjust margins for overflow\n+              for (j <- 0 until numClasses) {\n+                dv.values(j) -= maxMargin\n+                temp += math.exp(dv.values(j))\n+              }\n+            } else {\n+              for (j <- 0 until numClasses) {\n+                temp += math.exp(dv.values(j))\n+              }\n+            }\n+            temp\n+          }\n+\n+          // update in place\n+          var i = 0\n+          while (i < size) {\n+            dv.values(i) = math.exp(dv.values(i)) / sum\n+            i += 1\n+          }\n+        }\n+        dv\n+      case sv: SparseVector =>\n+        throw new RuntimeException(\"Unexpected error in MultinomialLogisticRegressionModel:\" +\n+          \" raw2probabilitiesInPlace encountered SparseVector\")\n+    }\n+  }\n+\n+  override protected def predictRaw(features: Vector): Vector = margins(features)\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegressionModel = {\n+    val newModel =\n+      copyValues(\n+        new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses), extra)\n+    newModel.setParent(parent)\n+  }\n+\n+  /**\n+   * Returns a [[org.apache.spark.ml.util.MLWriter]] instance for this ML instance.\n+   *\n+   * This does not save the [[parent]] currently.\n+   */\n+  @Since(\"2.1.0\")\n+  override def write: MLWriter =\n+    new MultinomialLogisticRegressionModel.MultinomialLogisticRegressionModelWriter(this)\n+}\n+\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegressionModel extends MLReadable[MultinomialLogisticRegressionModel] {\n+\n+  @Since(\"2.1.0\")\n+  override def read: MLReader[MultinomialLogisticRegressionModel] =\n+    new MultinomialLogisticRegressionModelReader\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegressionModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[MultinomialLogisticRegressionModel]] */\n+  private[MultinomialLogisticRegressionModel]\n+  class MultinomialLogisticRegressionModelWriter(instance: MultinomialLogisticRegressionModel)\n+    extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numClasses: Int,\n+        numFeatures: Int,\n+        intercept: Vector,"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:19:10Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0\n+            } else {\n+              dv.values(j) = 0.0\n+            }\n+          }\n+        } else {\n+          val sum = {\n+            var temp = 0.0\n+            if (maxMargin > 0) {\n+              // adjust margins for overflow\n+              for (j <- 0 until numClasses) {\n+                dv.values(j) -= maxMargin\n+                temp += math.exp(dv.values(j))\n+              }\n+            } else {\n+              for (j <- 0 until numClasses) {\n+                temp += math.exp(dv.values(j))\n+              }\n+            }\n+            temp\n+          }\n+\n+          // update in place\n+          var i = 0\n+          while (i < size) {\n+            dv.values(i) = math.exp(dv.values(i)) / sum\n+            i += 1\n+          }\n+        }\n+        dv\n+      case sv: SparseVector =>\n+        throw new RuntimeException(\"Unexpected error in MultinomialLogisticRegressionModel:\" +\n+          \" raw2probabilitiesInPlace encountered SparseVector\")\n+    }\n+  }\n+\n+  override protected def predictRaw(features: Vector): Vector = margins(features)\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegressionModel = {\n+    val newModel =\n+      copyValues(\n+        new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses), extra)\n+    newModel.setParent(parent)\n+  }\n+\n+  /**\n+   * Returns a [[org.apache.spark.ml.util.MLWriter]] instance for this ML instance.\n+   *\n+   * This does not save the [[parent]] currently.\n+   */\n+  @Since(\"2.1.0\")\n+  override def write: MLWriter =\n+    new MultinomialLogisticRegressionModel.MultinomialLogisticRegressionModelWriter(this)\n+}\n+\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegressionModel extends MLReadable[MultinomialLogisticRegressionModel] {\n+\n+  @Since(\"2.1.0\")\n+  override def read: MLReader[MultinomialLogisticRegressionModel] =\n+    new MultinomialLogisticRegressionModelReader\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegressionModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[MultinomialLogisticRegressionModel]] */\n+  private[MultinomialLogisticRegressionModel]\n+  class MultinomialLogisticRegressionModelWriter(instance: MultinomialLogisticRegressionModel)\n+    extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numClasses: Int,\n+        numFeatures: Int,\n+        intercept: Vector,"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "intercepts\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T05:00:25Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0\n+            } else {\n+              dv.values(j) = 0.0\n+            }\n+          }\n+        } else {\n+          val sum = {\n+            var temp = 0.0\n+            if (maxMargin > 0) {\n+              // adjust margins for overflow\n+              for (j <- 0 until numClasses) {\n+                dv.values(j) -= maxMargin\n+                temp += math.exp(dv.values(j))\n+              }\n+            } else {\n+              for (j <- 0 until numClasses) {\n+                temp += math.exp(dv.values(j))\n+              }\n+            }\n+            temp\n+          }\n+\n+          // update in place\n+          var i = 0\n+          while (i < size) {\n+            dv.values(i) = math.exp(dv.values(i)) / sum\n+            i += 1\n+          }\n+        }\n+        dv\n+      case sv: SparseVector =>\n+        throw new RuntimeException(\"Unexpected error in MultinomialLogisticRegressionModel:\" +\n+          \" raw2probabilitiesInPlace encountered SparseVector\")\n+    }\n+  }\n+\n+  override protected def predictRaw(features: Vector): Vector = margins(features)\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegressionModel = {\n+    val newModel =\n+      copyValues(\n+        new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses), extra)\n+    newModel.setParent(parent)\n+  }\n+\n+  /**\n+   * Returns a [[org.apache.spark.ml.util.MLWriter]] instance for this ML instance.\n+   *\n+   * This does not save the [[parent]] currently.\n+   */\n+  @Since(\"2.1.0\")\n+  override def write: MLWriter =\n+    new MultinomialLogisticRegressionModel.MultinomialLogisticRegressionModelWriter(this)\n+}\n+\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegressionModel extends MLReadable[MultinomialLogisticRegressionModel] {\n+\n+  @Since(\"2.1.0\")\n+  override def read: MLReader[MultinomialLogisticRegressionModel] =\n+    new MultinomialLogisticRegressionModelReader\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegressionModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[MultinomialLogisticRegressionModel]] */\n+  private[MultinomialLogisticRegressionModel]\n+  class MultinomialLogisticRegressionModelWriter(instance: MultinomialLogisticRegressionModel)\n+    extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numClasses: Int,\n+        numFeatures: Int,\n+        intercept: Vector,\n+        coefficients: Matrix)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      // Save model data: numClasses, numFeatures, intercept, coefficients\n+      val data = Data(instance.numClasses, instance.numFeatures, instance.intercepts,\n+        instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sqlContext.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class MultinomialLogisticRegressionModelReader\n+    extends MLReader[MultinomialLogisticRegressionModel] {\n+\n+    /** Checked against metadata when loading model */\n+    private val className = classOf[MultinomialLogisticRegressionModel].getName\n+\n+    override def load(path: String): MultinomialLogisticRegressionModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sqlContext.read.format(\"parquet\").load(dataPath)\n+        .select(\"numClasses\", \"numFeatures\", \"intercept\", \"coefficients\").head()"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:19:16Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0\n+            } else {\n+              dv.values(j) = 0.0\n+            }\n+          }\n+        } else {\n+          val sum = {\n+            var temp = 0.0\n+            if (maxMargin > 0) {\n+              // adjust margins for overflow\n+              for (j <- 0 until numClasses) {\n+                dv.values(j) -= maxMargin\n+                temp += math.exp(dv.values(j))\n+              }\n+            } else {\n+              for (j <- 0 until numClasses) {\n+                temp += math.exp(dv.values(j))\n+              }\n+            }\n+            temp\n+          }\n+\n+          // update in place\n+          var i = 0\n+          while (i < size) {\n+            dv.values(i) = math.exp(dv.values(i)) / sum\n+            i += 1\n+          }\n+        }\n+        dv\n+      case sv: SparseVector =>\n+        throw new RuntimeException(\"Unexpected error in MultinomialLogisticRegressionModel:\" +\n+          \" raw2probabilitiesInPlace encountered SparseVector\")\n+    }\n+  }\n+\n+  override protected def predictRaw(features: Vector): Vector = margins(features)\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegressionModel = {\n+    val newModel =\n+      copyValues(\n+        new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses), extra)\n+    newModel.setParent(parent)\n+  }\n+\n+  /**\n+   * Returns a [[org.apache.spark.ml.util.MLWriter]] instance for this ML instance.\n+   *\n+   * This does not save the [[parent]] currently.\n+   */\n+  @Since(\"2.1.0\")\n+  override def write: MLWriter =\n+    new MultinomialLogisticRegressionModel.MultinomialLogisticRegressionModelWriter(this)\n+}\n+\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegressionModel extends MLReadable[MultinomialLogisticRegressionModel] {\n+\n+  @Since(\"2.1.0\")\n+  override def read: MLReader[MultinomialLogisticRegressionModel] =\n+    new MultinomialLogisticRegressionModelReader\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegressionModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[MultinomialLogisticRegressionModel]] */\n+  private[MultinomialLogisticRegressionModel]\n+  class MultinomialLogisticRegressionModelWriter(instance: MultinomialLogisticRegressionModel)\n+    extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numClasses: Int,\n+        numFeatures: Int,\n+        intercept: Vector,\n+        coefficients: Matrix)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      // Save model data: numClasses, numFeatures, intercept, coefficients\n+      val data = Data(instance.numClasses, instance.numFeatures, instance.intercepts,\n+        instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sqlContext.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class MultinomialLogisticRegressionModelReader\n+    extends MLReader[MultinomialLogisticRegressionModel] {\n+\n+    /** Checked against metadata when loading model */\n+    private val className = classOf[MultinomialLogisticRegressionModel].getName\n+\n+    override def load(path: String): MultinomialLogisticRegressionModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sqlContext.read.format(\"parquet\").load(dataPath)\n+        .select(\"numClasses\", \"numFeatures\", \"intercept\", \"coefficients\").head()"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "`data.getAs[Int](data.fieldIndex(\"numClasses\"))`\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T05:01:24Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0\n+            } else {\n+              dv.values(j) = 0.0\n+            }\n+          }\n+        } else {\n+          val sum = {\n+            var temp = 0.0\n+            if (maxMargin > 0) {\n+              // adjust margins for overflow\n+              for (j <- 0 until numClasses) {\n+                dv.values(j) -= maxMargin\n+                temp += math.exp(dv.values(j))\n+              }\n+            } else {\n+              for (j <- 0 until numClasses) {\n+                temp += math.exp(dv.values(j))\n+              }\n+            }\n+            temp\n+          }\n+\n+          // update in place\n+          var i = 0\n+          while (i < size) {\n+            dv.values(i) = math.exp(dv.values(i)) / sum\n+            i += 1\n+          }\n+        }\n+        dv\n+      case sv: SparseVector =>\n+        throw new RuntimeException(\"Unexpected error in MultinomialLogisticRegressionModel:\" +\n+          \" raw2probabilitiesInPlace encountered SparseVector\")\n+    }\n+  }\n+\n+  override protected def predictRaw(features: Vector): Vector = margins(features)\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegressionModel = {\n+    val newModel =\n+      copyValues(\n+        new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses), extra)\n+    newModel.setParent(parent)\n+  }\n+\n+  /**\n+   * Returns a [[org.apache.spark.ml.util.MLWriter]] instance for this ML instance.\n+   *\n+   * This does not save the [[parent]] currently.\n+   */\n+  @Since(\"2.1.0\")\n+  override def write: MLWriter =\n+    new MultinomialLogisticRegressionModel.MultinomialLogisticRegressionModelWriter(this)\n+}\n+\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegressionModel extends MLReadable[MultinomialLogisticRegressionModel] {\n+\n+  @Since(\"2.1.0\")\n+  override def read: MLReader[MultinomialLogisticRegressionModel] =\n+    new MultinomialLogisticRegressionModelReader\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegressionModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[MultinomialLogisticRegressionModel]] */\n+  private[MultinomialLogisticRegressionModel]\n+  class MultinomialLogisticRegressionModelWriter(instance: MultinomialLogisticRegressionModel)\n+    extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numClasses: Int,\n+        numFeatures: Int,\n+        intercept: Vector,\n+        coefficients: Matrix)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      // Save model data: numClasses, numFeatures, intercept, coefficients\n+      val data = Data(instance.numClasses, instance.numFeatures, instance.intercepts,\n+        instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sqlContext.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class MultinomialLogisticRegressionModelReader\n+    extends MLReader[MultinomialLogisticRegressionModel] {\n+\n+    /** Checked against metadata when loading model */\n+    private val className = classOf[MultinomialLogisticRegressionModel].getName\n+\n+    override def load(path: String): MultinomialLogisticRegressionModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sqlContext.read.format(\"parquet\").load(dataPath)\n+        .select(\"numClasses\", \"numFeatures\", \"intercept\", \"coefficients\").head()\n+      val numClasses = data.getInt(0)"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:19:23Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0\n+            } else {\n+              dv.values(j) = 0.0\n+            }\n+          }\n+        } else {\n+          val sum = {\n+            var temp = 0.0\n+            if (maxMargin > 0) {\n+              // adjust margins for overflow\n+              for (j <- 0 until numClasses) {\n+                dv.values(j) -= maxMargin\n+                temp += math.exp(dv.values(j))\n+              }\n+            } else {\n+              for (j <- 0 until numClasses) {\n+                temp += math.exp(dv.values(j))\n+              }\n+            }\n+            temp\n+          }\n+\n+          // update in place\n+          var i = 0\n+          while (i < size) {\n+            dv.values(i) = math.exp(dv.values(i)) / sum\n+            i += 1\n+          }\n+        }\n+        dv\n+      case sv: SparseVector =>\n+        throw new RuntimeException(\"Unexpected error in MultinomialLogisticRegressionModel:\" +\n+          \" raw2probabilitiesInPlace encountered SparseVector\")\n+    }\n+  }\n+\n+  override protected def predictRaw(features: Vector): Vector = margins(features)\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegressionModel = {\n+    val newModel =\n+      copyValues(\n+        new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses), extra)\n+    newModel.setParent(parent)\n+  }\n+\n+  /**\n+   * Returns a [[org.apache.spark.ml.util.MLWriter]] instance for this ML instance.\n+   *\n+   * This does not save the [[parent]] currently.\n+   */\n+  @Since(\"2.1.0\")\n+  override def write: MLWriter =\n+    new MultinomialLogisticRegressionModel.MultinomialLogisticRegressionModelWriter(this)\n+}\n+\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegressionModel extends MLReadable[MultinomialLogisticRegressionModel] {\n+\n+  @Since(\"2.1.0\")\n+  override def read: MLReader[MultinomialLogisticRegressionModel] =\n+    new MultinomialLogisticRegressionModelReader\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegressionModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[MultinomialLogisticRegressionModel]] */\n+  private[MultinomialLogisticRegressionModel]\n+  class MultinomialLogisticRegressionModelWriter(instance: MultinomialLogisticRegressionModel)\n+    extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numClasses: Int,\n+        numFeatures: Int,\n+        intercept: Vector,\n+        coefficients: Matrix)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      // Save model data: numClasses, numFeatures, intercept, coefficients\n+      val data = Data(instance.numClasses, instance.numFeatures, instance.intercepts,\n+        instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sqlContext.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class MultinomialLogisticRegressionModelReader\n+    extends MLReader[MultinomialLogisticRegressionModel] {\n+\n+    /** Checked against metadata when loading model */\n+    private val className = classOf[MultinomialLogisticRegressionModel].getName\n+\n+    override def load(path: String): MultinomialLogisticRegressionModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sqlContext.read.format(\"parquet\").load(dataPath)\n+        .select(\"numClasses\", \"numFeatures\", \"intercept\", \"coefficients\").head()\n+      val numClasses = data.getInt(0)"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "use fieldIndex\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T05:01:45Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0\n+            } else {\n+              dv.values(j) = 0.0\n+            }\n+          }\n+        } else {\n+          val sum = {\n+            var temp = 0.0\n+            if (maxMargin > 0) {\n+              // adjust margins for overflow\n+              for (j <- 0 until numClasses) {\n+                dv.values(j) -= maxMargin\n+                temp += math.exp(dv.values(j))\n+              }\n+            } else {\n+              for (j <- 0 until numClasses) {\n+                temp += math.exp(dv.values(j))\n+              }\n+            }\n+            temp\n+          }\n+\n+          // update in place\n+          var i = 0\n+          while (i < size) {\n+            dv.values(i) = math.exp(dv.values(i)) / sum\n+            i += 1\n+          }\n+        }\n+        dv\n+      case sv: SparseVector =>\n+        throw new RuntimeException(\"Unexpected error in MultinomialLogisticRegressionModel:\" +\n+          \" raw2probabilitiesInPlace encountered SparseVector\")\n+    }\n+  }\n+\n+  override protected def predictRaw(features: Vector): Vector = margins(features)\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegressionModel = {\n+    val newModel =\n+      copyValues(\n+        new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses), extra)\n+    newModel.setParent(parent)\n+  }\n+\n+  /**\n+   * Returns a [[org.apache.spark.ml.util.MLWriter]] instance for this ML instance.\n+   *\n+   * This does not save the [[parent]] currently.\n+   */\n+  @Since(\"2.1.0\")\n+  override def write: MLWriter =\n+    new MultinomialLogisticRegressionModel.MultinomialLogisticRegressionModelWriter(this)\n+}\n+\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegressionModel extends MLReadable[MultinomialLogisticRegressionModel] {\n+\n+  @Since(\"2.1.0\")\n+  override def read: MLReader[MultinomialLogisticRegressionModel] =\n+    new MultinomialLogisticRegressionModelReader\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegressionModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[MultinomialLogisticRegressionModel]] */\n+  private[MultinomialLogisticRegressionModel]\n+  class MultinomialLogisticRegressionModelWriter(instance: MultinomialLogisticRegressionModel)\n+    extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numClasses: Int,\n+        numFeatures: Int,\n+        intercept: Vector,\n+        coefficients: Matrix)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      // Save model data: numClasses, numFeatures, intercept, coefficients\n+      val data = Data(instance.numClasses, instance.numFeatures, instance.intercepts,\n+        instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sqlContext.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class MultinomialLogisticRegressionModelReader\n+    extends MLReader[MultinomialLogisticRegressionModel] {\n+\n+    /** Checked against metadata when loading model */\n+    private val className = classOf[MultinomialLogisticRegressionModel].getName\n+\n+    override def load(path: String): MultinomialLogisticRegressionModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sqlContext.read.format(\"parquet\").load(dataPath)\n+        .select(\"numClasses\", \"numFeatures\", \"intercept\", \"coefficients\").head()\n+      val numClasses = data.getInt(0)\n+      val intercepts = data.getAs[Vector](2)"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:19:28Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0\n+            } else {\n+              dv.values(j) = 0.0\n+            }\n+          }\n+        } else {\n+          val sum = {\n+            var temp = 0.0\n+            if (maxMargin > 0) {\n+              // adjust margins for overflow\n+              for (j <- 0 until numClasses) {\n+                dv.values(j) -= maxMargin\n+                temp += math.exp(dv.values(j))\n+              }\n+            } else {\n+              for (j <- 0 until numClasses) {\n+                temp += math.exp(dv.values(j))\n+              }\n+            }\n+            temp\n+          }\n+\n+          // update in place\n+          var i = 0\n+          while (i < size) {\n+            dv.values(i) = math.exp(dv.values(i)) / sum\n+            i += 1\n+          }\n+        }\n+        dv\n+      case sv: SparseVector =>\n+        throw new RuntimeException(\"Unexpected error in MultinomialLogisticRegressionModel:\" +\n+          \" raw2probabilitiesInPlace encountered SparseVector\")\n+    }\n+  }\n+\n+  override protected def predictRaw(features: Vector): Vector = margins(features)\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegressionModel = {\n+    val newModel =\n+      copyValues(\n+        new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses), extra)\n+    newModel.setParent(parent)\n+  }\n+\n+  /**\n+   * Returns a [[org.apache.spark.ml.util.MLWriter]] instance for this ML instance.\n+   *\n+   * This does not save the [[parent]] currently.\n+   */\n+  @Since(\"2.1.0\")\n+  override def write: MLWriter =\n+    new MultinomialLogisticRegressionModel.MultinomialLogisticRegressionModelWriter(this)\n+}\n+\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegressionModel extends MLReadable[MultinomialLogisticRegressionModel] {\n+\n+  @Since(\"2.1.0\")\n+  override def read: MLReader[MultinomialLogisticRegressionModel] =\n+    new MultinomialLogisticRegressionModelReader\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegressionModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[MultinomialLogisticRegressionModel]] */\n+  private[MultinomialLogisticRegressionModel]\n+  class MultinomialLogisticRegressionModelWriter(instance: MultinomialLogisticRegressionModel)\n+    extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numClasses: Int,\n+        numFeatures: Int,\n+        intercept: Vector,\n+        coefficients: Matrix)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      // Save model data: numClasses, numFeatures, intercept, coefficients\n+      val data = Data(instance.numClasses, instance.numFeatures, instance.intercepts,\n+        instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sqlContext.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class MultinomialLogisticRegressionModelReader\n+    extends MLReader[MultinomialLogisticRegressionModel] {\n+\n+    /** Checked against metadata when loading model */\n+    private val className = classOf[MultinomialLogisticRegressionModel].getName\n+\n+    override def load(path: String): MultinomialLogisticRegressionModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sqlContext.read.format(\"parquet\").load(dataPath)\n+        .select(\"numClasses\", \"numFeatures\", \"intercept\", \"coefficients\").head()\n+      val numClasses = data.getInt(0)\n+      val intercepts = data.getAs[Vector](2)"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "ditto\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T05:01:55Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0\n+            } else {\n+              dv.values(j) = 0.0\n+            }\n+          }\n+        } else {\n+          val sum = {\n+            var temp = 0.0\n+            if (maxMargin > 0) {\n+              // adjust margins for overflow\n+              for (j <- 0 until numClasses) {\n+                dv.values(j) -= maxMargin\n+                temp += math.exp(dv.values(j))\n+              }\n+            } else {\n+              for (j <- 0 until numClasses) {\n+                temp += math.exp(dv.values(j))\n+              }\n+            }\n+            temp\n+          }\n+\n+          // update in place\n+          var i = 0\n+          while (i < size) {\n+            dv.values(i) = math.exp(dv.values(i)) / sum\n+            i += 1\n+          }\n+        }\n+        dv\n+      case sv: SparseVector =>\n+        throw new RuntimeException(\"Unexpected error in MultinomialLogisticRegressionModel:\" +\n+          \" raw2probabilitiesInPlace encountered SparseVector\")\n+    }\n+  }\n+\n+  override protected def predictRaw(features: Vector): Vector = margins(features)\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegressionModel = {\n+    val newModel =\n+      copyValues(\n+        new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses), extra)\n+    newModel.setParent(parent)\n+  }\n+\n+  /**\n+   * Returns a [[org.apache.spark.ml.util.MLWriter]] instance for this ML instance.\n+   *\n+   * This does not save the [[parent]] currently.\n+   */\n+  @Since(\"2.1.0\")\n+  override def write: MLWriter =\n+    new MultinomialLogisticRegressionModel.MultinomialLogisticRegressionModelWriter(this)\n+}\n+\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegressionModel extends MLReadable[MultinomialLogisticRegressionModel] {\n+\n+  @Since(\"2.1.0\")\n+  override def read: MLReader[MultinomialLogisticRegressionModel] =\n+    new MultinomialLogisticRegressionModelReader\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegressionModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[MultinomialLogisticRegressionModel]] */\n+  private[MultinomialLogisticRegressionModel]\n+  class MultinomialLogisticRegressionModelWriter(instance: MultinomialLogisticRegressionModel)\n+    extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numClasses: Int,\n+        numFeatures: Int,\n+        intercept: Vector,\n+        coefficients: Matrix)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      // Save model data: numClasses, numFeatures, intercept, coefficients\n+      val data = Data(instance.numClasses, instance.numFeatures, instance.intercepts,\n+        instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sqlContext.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class MultinomialLogisticRegressionModelReader\n+    extends MLReader[MultinomialLogisticRegressionModel] {\n+\n+    /** Checked against metadata when loading model */\n+    private val className = classOf[MultinomialLogisticRegressionModel].getName\n+\n+    override def load(path: String): MultinomialLogisticRegressionModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sqlContext.read.format(\"parquet\").load(dataPath)\n+        .select(\"numClasses\", \"numFeatures\", \"intercept\", \"coefficients\").head()\n+      val numClasses = data.getInt(0)\n+      val intercepts = data.getAs[Vector](2)\n+      val coefficients = data.getAs[Matrix](3)"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-18T17:19:37Z",
    "diffHunk": "@@ -0,0 +1,611 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val labelIsConstant = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && labelIsConstant) {\n+        // we want to produce a model that will always predict the constant label\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0), Array(), Array()),\n+          Vectors.sparse(numClasses, Seq((numClasses - 1, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && labelIsConstant) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(0) = \\exp(b_0) / (\\sum_{k=1}^K \\exp(b_k))\n+               ...\n+               P(K) = \\exp(b_K) / (\\sum_{k=1}^K \\exp(b_k))\n+             }}}\n+             The solution to this is not identifiable, so choose the solution with minimum\n+             L2 penalty (i.e. subtract the mean). Hence,\n+             {{{\n+               b_k = \\log{count_k / count_0}\n+               b_k' = b_k - \\frac{1}{K} \\sum b_k\n+             }}}\n+           */\n+          val referenceCoef = histogram.indices.map { i =>\n+            if (histogram(i) > 0) {\n+              math.log(histogram(i) / (histogram(0) + 1)) // add 1 for smoothing\n+            } else {\n+              0.0\n+            }\n+          }\n+          val referenceMean = referenceCoef.sum / referenceCoef.length\n+          histogram.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              referenceCoef(i) - referenceMean\n+          }\n+        }\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        var interceptSum = 0.0\n+        var coefSum = 0.0\n+        val rawCoefficients = Vectors.fromBreeze(state.x)\n+        val (coefMatrix, interceptVector) = rawCoefficients match {\n+          case dv: DenseVector =>\n+            val coefArray = Array.tabulate(numClasses * numFeatures) { i =>\n+              val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+              val featureIndex = i % numFeatures\n+              val unscaledCoef = if (featuresStd(featureIndex) != 0.0) {\n+                dv(flatIndex) / featuresStd(featureIndex)\n+              } else {\n+                0.0\n+              }\n+              coefSum += unscaledCoef\n+              unscaledCoef\n+            }\n+            val interceptVector = if ($(fitIntercept)) {\n+              Vectors.dense(Array.tabulate(numClasses) { i =>\n+                val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+                val intercept = dv(coefIndex)\n+                interceptSum += intercept\n+                intercept\n+              })\n+            } else {\n+              Vectors.sparse(numClasses, Seq())\n+            }\n+            (new DenseMatrix(numClasses, numFeatures, coefArray, isTransposed = true),\n+              interceptVector)\n+          case sv: SparseVector =>\n+            throw new IllegalArgumentException(\"SparseVector is not supported for coefficients\")\n+        }\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefSum / (numClasses * numFeatures)\n+          coefMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptMean = interceptSum / numClasses\n+        interceptVector match {\n+          case dv: DenseVector => (0 until dv.size).foreach { i => dv.toArray(i) -= interceptMean }\n+          case sv: SparseVector =>\n+            (0 until sv.numNonzeros).foreach { i => sv.values(i) -= interceptMean }\n+        }\n+        (coefMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features).toDense\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      if (maxMargin > 0) {\n+        for (i <- 0 until numClasses) {\n+          m.toArray(i) -= maxMargin\n+          temp += math.exp(m(i))\n+        }\n+      } else {\n+        for (i <- 0 until numClasses ) {\n+          temp += math.exp(m(i))\n+        }\n+      }\n+      temp\n+    }\n+\n+    var i = 0\n+    while (i < m.size) {\n+      m.values(i) = math.exp(m.values(i)) / sum\n+      i += 1\n+    }\n+    m\n+  }\n+\n+  /**\n+   * Predict label for the given feature vector.\n+   * The behavior of this can be adjusted using [[thresholds]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    if (isDefined(thresholds)) {\n+      val thresholds: Array[Double] = getThresholds\n+      val scaledProbability: Array[Double] =\n+        scores(features).toArray.zip(thresholds).map { case (p, t) =>\n+          if (t == 0.0) Double.PositiveInfinity else p / t\n+        }\n+      Vectors.dense(scaledProbability).argmax\n+    } else {\n+      scores(features).argmax\n+    }\n+  }\n+\n+  override protected def raw2probabilityInPlace(rawPrediction: Vector): Vector = {\n+    rawPrediction match {\n+      case dv: DenseVector =>\n+        val size = dv.size\n+\n+        // get the maximum margin\n+        val maxMarginIndex = rawPrediction.argmax\n+        val maxMargin = rawPrediction(maxMarginIndex)\n+\n+        if (maxMargin == Double.PositiveInfinity) {\n+          for (j <- 0 until size) {\n+            if (j == maxMarginIndex) {\n+              dv.values(j) = 1.0\n+            } else {\n+              dv.values(j) = 0.0\n+            }\n+          }\n+        } else {\n+          val sum = {\n+            var temp = 0.0\n+            if (maxMargin > 0) {\n+              // adjust margins for overflow\n+              for (j <- 0 until numClasses) {\n+                dv.values(j) -= maxMargin\n+                temp += math.exp(dv.values(j))\n+              }\n+            } else {\n+              for (j <- 0 until numClasses) {\n+                temp += math.exp(dv.values(j))\n+              }\n+            }\n+            temp\n+          }\n+\n+          // update in place\n+          var i = 0\n+          while (i < size) {\n+            dv.values(i) = math.exp(dv.values(i)) / sum\n+            i += 1\n+          }\n+        }\n+        dv\n+      case sv: SparseVector =>\n+        throw new RuntimeException(\"Unexpected error in MultinomialLogisticRegressionModel:\" +\n+          \" raw2probabilitiesInPlace encountered SparseVector\")\n+    }\n+  }\n+\n+  override protected def predictRaw(features: Vector): Vector = margins(features)\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegressionModel = {\n+    val newModel =\n+      copyValues(\n+        new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses), extra)\n+    newModel.setParent(parent)\n+  }\n+\n+  /**\n+   * Returns a [[org.apache.spark.ml.util.MLWriter]] instance for this ML instance.\n+   *\n+   * This does not save the [[parent]] currently.\n+   */\n+  @Since(\"2.1.0\")\n+  override def write: MLWriter =\n+    new MultinomialLogisticRegressionModel.MultinomialLogisticRegressionModelWriter(this)\n+}\n+\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegressionModel extends MLReadable[MultinomialLogisticRegressionModel] {\n+\n+  @Since(\"2.1.0\")\n+  override def read: MLReader[MultinomialLogisticRegressionModel] =\n+    new MultinomialLogisticRegressionModelReader\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegressionModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[MultinomialLogisticRegressionModel]] */\n+  private[MultinomialLogisticRegressionModel]\n+  class MultinomialLogisticRegressionModelWriter(instance: MultinomialLogisticRegressionModel)\n+    extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numClasses: Int,\n+        numFeatures: Int,\n+        intercept: Vector,\n+        coefficients: Matrix)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      // Save model data: numClasses, numFeatures, intercept, coefficients\n+      val data = Data(instance.numClasses, instance.numFeatures, instance.intercepts,\n+        instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sqlContext.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class MultinomialLogisticRegressionModelReader\n+    extends MLReader[MultinomialLogisticRegressionModel] {\n+\n+    /** Checked against metadata when loading model */\n+    private val className = classOf[MultinomialLogisticRegressionModel].getName\n+\n+    override def load(path: String): MultinomialLogisticRegressionModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sqlContext.read.format(\"parquet\").load(dataPath)\n+        .select(\"numClasses\", \"numFeatures\", \"intercept\", \"coefficients\").head()\n+      val numClasses = data.getInt(0)\n+      val intercepts = data.getAs[Vector](2)\n+      val coefficients = data.getAs[Matrix](3)"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "Minor: this deviation doesn't show why you can chose `\\lambda` freely given `\\log(count) + \\log(Z)` should be already determined by the problem. Since the `\\lambda` is in the `\\exp`, we call it phase.\n\n``` latex\n             {{{\n                P(1) = \\exp(b_1) / Z\n                ...\n                P(K) = \\exp(b_K) / Z\n                where Z = \\sum_{k=1}{K} \\exp(b_k)\n\n               Since this problem doesn't have unique solution, one of the solution that satisfy the above equation will be\n\n               \\exp(b_k) = count_k * exp(\\lambda)\n               hence\n               \\b_k = \\log(count_k) + \\lambda\n             }}}\n```\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-19T00:52:07Z",
    "diffHunk": "@@ -0,0 +1,619 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val isConstantLabel = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && isConstantLabel) {\n+        // we want to produce a model that will always predict the constant label so all the\n+        // coefficients will be zero, and the constant label class intercept will be +inf\n+        val constantLabelIndex = Vectors.dense(histogram).argmax\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0),\n+          Array.empty[Int], Array.empty[Double]),\n+          Vectors.sparse(numClasses, Seq((constantLabelIndex, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && isConstantLabel) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+        val featuresMean = summarizer.mean.toArray\n+        if (!$(fitIntercept) && (0 until numFeatures).exists { i =>\n+          featuresStd(i) == 0.0 && featuresMean(i) != 0.0 }) {\n+          logWarning(\"Fitting MultinomialLogisticRegressionModel without intercept on dataset \" +\n+            \"with constant nonzero column, Spark MLlib outputs zero coefficients for constant \" +\n+            \"nonzero columns. This behavior is the same as R glmnet but different from LIBSVM.\")\n+        }\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(1) = \\exp(b_1) / Z\n+               ...\n+               P(K) = \\exp(b_K) / Z\n+             }}}\n+             Where Z is a normalizing constant. Hence,\n+             {{{\n+              b_k = \\log(P(k)) + \\log(Z)\n+                  = \\log(count_k) - \\log(count) + \\log(Z)"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Updated.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-19T03:02:04Z",
    "diffHunk": "@@ -0,0 +1,619 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val isConstantLabel = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && isConstantLabel) {\n+        // we want to produce a model that will always predict the constant label so all the\n+        // coefficients will be zero, and the constant label class intercept will be +inf\n+        val constantLabelIndex = Vectors.dense(histogram).argmax\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0),\n+          Array.empty[Int], Array.empty[Double]),\n+          Vectors.sparse(numClasses, Seq((constantLabelIndex, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && isConstantLabel) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+        val featuresMean = summarizer.mean.toArray\n+        if (!$(fitIntercept) && (0 until numFeatures).exists { i =>\n+          featuresStd(i) == 0.0 && featuresMean(i) != 0.0 }) {\n+          logWarning(\"Fitting MultinomialLogisticRegressionModel without intercept on dataset \" +\n+            \"with constant nonzero column, Spark MLlib outputs zero coefficients for constant \" +\n+            \"nonzero columns. This behavior is the same as R glmnet but different from LIBSVM.\")\n+        }\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(1) = \\exp(b_1) / Z\n+               ...\n+               P(K) = \\exp(b_K) / Z\n+             }}}\n+             Where Z is a normalizing constant. Hence,\n+             {{{\n+              b_k = \\log(P(k)) + \\log(Z)\n+                  = \\log(count_k) - \\log(count) + \\log(Z)"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "Now, I think we should remove those label that `c == 0.0`, so we don't have negative infinity in intercepts or smoothing.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-19T00:54:32Z",
    "diffHunk": "@@ -0,0 +1,619 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val isConstantLabel = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && isConstantLabel) {\n+        // we want to produce a model that will always predict the constant label so all the\n+        // coefficients will be zero, and the constant label class intercept will be +inf\n+        val constantLabelIndex = Vectors.dense(histogram).argmax\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0),\n+          Array.empty[Int], Array.empty[Double]),\n+          Vectors.sparse(numClasses, Seq((constantLabelIndex, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && isConstantLabel) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+        val featuresMean = summarizer.mean.toArray\n+        if (!$(fitIntercept) && (0 until numFeatures).exists { i =>\n+          featuresStd(i) == 0.0 && featuresMean(i) != 0.0 }) {\n+          logWarning(\"Fitting MultinomialLogisticRegressionModel without intercept on dataset \" +\n+            \"with constant nonzero column, Spark MLlib outputs zero coefficients for constant \" +\n+            \"nonzero columns. This behavior is the same as R glmnet but different from LIBSVM.\")\n+        }\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(1) = \\exp(b_1) / Z\n+               ...\n+               P(K) = \\exp(b_K) / Z\n+             }}}\n+             Where Z is a normalizing constant. Hence,\n+             {{{\n+              b_k = \\log(P(k)) + \\log(Z)\n+                  = \\log(count_k) - \\log(count) + \\log(Z)\n+                  = \\log(count_k) + \\lambda\n+             }}}\n+             The solution to this is not identifiable, so choose the phase \\lambda such that the\n+             mean is centered. This yields\n+             {{{\n+               b_k = \\log(count_k)\n+               b_k' = b_k - \\mean(b_k)\n+             }}}\n+           */\n+          val rawIntercepts = histogram.map(c => math.log(c + 1)) // add 1 for smoothing",
    "line": 310
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Let's deal with it in the followup PR.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-19T00:59:31Z",
    "diffHunk": "@@ -0,0 +1,619 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val isConstantLabel = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && isConstantLabel) {\n+        // we want to produce a model that will always predict the constant label so all the\n+        // coefficients will be zero, and the constant label class intercept will be +inf\n+        val constantLabelIndex = Vectors.dense(histogram).argmax\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0),\n+          Array.empty[Int], Array.empty[Double]),\n+          Vectors.sparse(numClasses, Seq((constantLabelIndex, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && isConstantLabel) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+        val featuresMean = summarizer.mean.toArray\n+        if (!$(fitIntercept) && (0 until numFeatures).exists { i =>\n+          featuresStd(i) == 0.0 && featuresMean(i) != 0.0 }) {\n+          logWarning(\"Fitting MultinomialLogisticRegressionModel without intercept on dataset \" +\n+            \"with constant nonzero column, Spark MLlib outputs zero coefficients for constant \" +\n+            \"nonzero columns. This behavior is the same as R glmnet but different from LIBSVM.\")\n+        }\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(1) = \\exp(b_1) / Z\n+               ...\n+               P(K) = \\exp(b_K) / Z\n+             }}}\n+             Where Z is a normalizing constant. Hence,\n+             {{{\n+              b_k = \\log(P(k)) + \\log(Z)\n+                  = \\log(count_k) - \\log(count) + \\log(Z)\n+                  = \\log(count_k) + \\lambda\n+             }}}\n+             The solution to this is not identifiable, so choose the phase \\lambda such that the\n+             mean is centered. This yields\n+             {{{\n+               b_k = \\log(count_k)\n+               b_k' = b_k - \\mean(b_k)\n+             }}}\n+           */\n+          val rawIntercepts = histogram.map(c => math.log(c + 1)) // add 1 for smoothing",
    "line": 310
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "dbtsai"
    },
    "body": "`val maxMargin = marginArray(maxMarginIndex)`\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-19T01:31:38Z",
    "diffHunk": "@@ -0,0 +1,619 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val isConstantLabel = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && isConstantLabel) {\n+        // we want to produce a model that will always predict the constant label so all the\n+        // coefficients will be zero, and the constant label class intercept will be +inf\n+        val constantLabelIndex = Vectors.dense(histogram).argmax\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0),\n+          Array.empty[Int], Array.empty[Double]),\n+          Vectors.sparse(numClasses, Seq((constantLabelIndex, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && isConstantLabel) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+        val featuresMean = summarizer.mean.toArray\n+        if (!$(fitIntercept) && (0 until numFeatures).exists { i =>\n+          featuresStd(i) == 0.0 && featuresMean(i) != 0.0 }) {\n+          logWarning(\"Fitting MultinomialLogisticRegressionModel without intercept on dataset \" +\n+            \"with constant nonzero column, Spark MLlib outputs zero coefficients for constant \" +\n+            \"nonzero columns. This behavior is the same as R glmnet but different from LIBSVM.\")\n+        }\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(1) = \\exp(b_1) / Z\n+               ...\n+               P(K) = \\exp(b_K) / Z\n+             }}}\n+             Where Z is a normalizing constant. Hence,\n+             {{{\n+              b_k = \\log(P(k)) + \\log(Z)\n+                  = \\log(count_k) - \\log(count) + \\log(Z)\n+                  = \\log(count_k) + \\lambda\n+             }}}\n+             The solution to this is not identifiable, so choose the phase \\lambda such that the\n+             mean is centered. This yields\n+             {{{\n+               b_k = \\log(count_k)\n+               b_k' = b_k - \\mean(b_k)\n+             }}}\n+           */\n+          val rawIntercepts = histogram.map(c => math.log(c + 1)) // add 1 for smoothing\n+          val rawMean = rawIntercepts.sum / rawIntercepts.length\n+          rawIntercepts.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              rawIntercepts(i) - rawMean\n+          }\n+        }\n+\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        val rawCoefficients = state.x.toArray\n+        val interceptsArray: Array[Double] = if ($(fitIntercept)) {\n+          Array.tabulate(numClasses) { i =>\n+            val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+            rawCoefficients(coefIndex)\n+          }\n+        } else {\n+          Array[Double]()\n+        }\n+\n+        val coefficientArray: Array[Double] = Array.tabulate(numClasses * numFeatures) { i =>\n+          // flatIndex will loop though rawCoefficients, and skip the intercept terms.\n+          val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+          val featureIndex = i % numFeatures\n+          if (featuresStd(featureIndex) != 0.0) {\n+            rawCoefficients(flatIndex) / featuresStd(featureIndex)\n+          } else {\n+            0.0\n+          }\n+        }\n+        val coefficientMatrix =\n+          new DenseMatrix(numClasses, numFeatures, coefficientArray, isTransposed = true)\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefficientMatrix.values.sum / (numClasses * numFeatures)\n+          coefficientMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptVector = if (interceptsArray.nonEmpty) {\n+          val interceptMean = interceptsArray.sum / numClasses\n+          interceptsArray.indices.foreach { i => interceptsArray(i) -= interceptMean }\n+          Vectors.dense(interceptsArray)\n+        } else {\n+          Vectors.sparse(numClasses, Seq())\n+        }\n+\n+        (coefficientMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features)\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+    val marginArray = m.toArray"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "done\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-19T03:01:51Z",
    "diffHunk": "@@ -0,0 +1,619 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val isConstantLabel = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && isConstantLabel) {\n+        // we want to produce a model that will always predict the constant label so all the\n+        // coefficients will be zero, and the constant label class intercept will be +inf\n+        val constantLabelIndex = Vectors.dense(histogram).argmax\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0),\n+          Array.empty[Int], Array.empty[Double]),\n+          Vectors.sparse(numClasses, Seq((constantLabelIndex, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && isConstantLabel) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+        val featuresMean = summarizer.mean.toArray\n+        if (!$(fitIntercept) && (0 until numFeatures).exists { i =>\n+          featuresStd(i) == 0.0 && featuresMean(i) != 0.0 }) {\n+          logWarning(\"Fitting MultinomialLogisticRegressionModel without intercept on dataset \" +\n+            \"with constant nonzero column, Spark MLlib outputs zero coefficients for constant \" +\n+            \"nonzero columns. This behavior is the same as R glmnet but different from LIBSVM.\")\n+        }\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(1) = \\exp(b_1) / Z\n+               ...\n+               P(K) = \\exp(b_K) / Z\n+             }}}\n+             Where Z is a normalizing constant. Hence,\n+             {{{\n+              b_k = \\log(P(k)) + \\log(Z)\n+                  = \\log(count_k) - \\log(count) + \\log(Z)\n+                  = \\log(count_k) + \\lambda\n+             }}}\n+             The solution to this is not identifiable, so choose the phase \\lambda such that the\n+             mean is centered. This yields\n+             {{{\n+               b_k = \\log(count_k)\n+               b_k' = b_k - \\mean(b_k)\n+             }}}\n+           */\n+          val rawIntercepts = histogram.map(c => math.log(c + 1)) // add 1 for smoothing\n+          val rawMean = rawIntercepts.sum / rawIntercepts.length\n+          rawIntercepts.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              rawIntercepts(i) - rawMean\n+          }\n+        }\n+\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        val rawCoefficients = state.x.toArray\n+        val interceptsArray: Array[Double] = if ($(fitIntercept)) {\n+          Array.tabulate(numClasses) { i =>\n+            val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+            rawCoefficients(coefIndex)\n+          }\n+        } else {\n+          Array[Double]()\n+        }\n+\n+        val coefficientArray: Array[Double] = Array.tabulate(numClasses * numFeatures) { i =>\n+          // flatIndex will loop though rawCoefficients, and skip the intercept terms.\n+          val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+          val featureIndex = i % numFeatures\n+          if (featuresStd(featureIndex) != 0.0) {\n+            rawCoefficients(flatIndex) / featuresStd(featureIndex)\n+          } else {\n+            0.0\n+          }\n+        }\n+        val coefficientMatrix =\n+          new DenseMatrix(numClasses, numFeatures, coefficientArray, isTransposed = true)\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefficientMatrix.values.sum / (numClasses * numFeatures)\n+          coefficientMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptVector = if (interceptsArray.nonEmpty) {\n+          val interceptMean = interceptsArray.sum / numClasses\n+          interceptsArray.indices.foreach { i => interceptsArray(i) -= interceptMean }\n+          Vectors.dense(interceptsArray)\n+        } else {\n+          Vectors.sparse(numClasses, Seq())\n+        }\n+\n+        (coefficientMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features)\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+    val marginArray = m.toArray"
  }],
  "prId": 13796
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "Not sure if the extra conditional will really slow this down. We could factor it out, but the code is uglier that way. Would have two separate while loops.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-19T01:36:57Z",
    "diffHunk": "@@ -0,0 +1,619 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val isConstantLabel = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && isConstantLabel) {\n+        // we want to produce a model that will always predict the constant label so all the\n+        // coefficients will be zero, and the constant label class intercept will be +inf\n+        val constantLabelIndex = Vectors.dense(histogram).argmax\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0),\n+          Array.empty[Int], Array.empty[Double]),\n+          Vectors.sparse(numClasses, Seq((constantLabelIndex, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && isConstantLabel) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+        val featuresMean = summarizer.mean.toArray\n+        if (!$(fitIntercept) && (0 until numFeatures).exists { i =>\n+          featuresStd(i) == 0.0 && featuresMean(i) != 0.0 }) {\n+          logWarning(\"Fitting MultinomialLogisticRegressionModel without intercept on dataset \" +\n+            \"with constant nonzero column, Spark MLlib outputs zero coefficients for constant \" +\n+            \"nonzero columns. This behavior is the same as R glmnet but different from LIBSVM.\")\n+        }\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(1) = \\exp(b_1) / Z\n+               ...\n+               P(K) = \\exp(b_K) / Z\n+             }}}\n+             Where Z is a normalizing constant. Hence,\n+             {{{\n+              b_k = \\log(P(k)) + \\log(Z)\n+                  = \\log(count_k) - \\log(count) + \\log(Z)\n+                  = \\log(count_k) + \\lambda\n+             }}}\n+             The solution to this is not identifiable, so choose the phase \\lambda such that the\n+             mean is centered. This yields\n+             {{{\n+               b_k = \\log(count_k)\n+               b_k' = b_k - \\mean(b_k)\n+             }}}\n+           */\n+          val rawIntercepts = histogram.map(c => math.log(c + 1)) // add 1 for smoothing\n+          val rawMean = rawIntercepts.sum / rawIntercepts.length\n+          rawIntercepts.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              rawIntercepts(i) - rawMean\n+          }\n+        }\n+\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        val rawCoefficients = state.x.toArray\n+        val interceptsArray: Array[Double] = if ($(fitIntercept)) {\n+          Array.tabulate(numClasses) { i =>\n+            val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+            rawCoefficients(coefIndex)\n+          }\n+        } else {\n+          Array[Double]()\n+        }\n+\n+        val coefficientArray: Array[Double] = Array.tabulate(numClasses * numFeatures) { i =>\n+          // flatIndex will loop though rawCoefficients, and skip the intercept terms.\n+          val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+          val featureIndex = i % numFeatures\n+          if (featuresStd(featureIndex) != 0.0) {\n+            rawCoefficients(flatIndex) / featuresStd(featureIndex)\n+          } else {\n+            0.0\n+          }\n+        }\n+        val coefficientMatrix =\n+          new DenseMatrix(numClasses, numFeatures, coefficientArray, isTransposed = true)\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefficientMatrix.values.sum / (numClasses * numFeatures)\n+          coefficientMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptVector = if (interceptsArray.nonEmpty) {\n+          val interceptMean = interceptsArray.sum / numClasses\n+          interceptsArray.indices.foreach { i => interceptsArray(i) -= interceptMean }\n+          Vectors.dense(interceptsArray)\n+        } else {\n+          Vectors.sparse(numClasses, Seq())\n+        }\n+\n+        (coefficientMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features)\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+    val marginArray = m.toArray\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      var k = 0\n+      while (k < numClasses) {\n+        marginArray(k) = if (maxMargin > 0) {",
    "line": 459
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "I was thinking the same question two 5 mins ago. :) In the end, I think `\\exp` is more expensive than `if`. Let's keep as it for now.\n",
    "commit": "fc2aa95dc89cae21d9d66d47598ddb37b787202b",
    "createdAt": "2016-08-19T01:45:58Z",
    "diffHunk": "@@ -0,0 +1,619 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import scala.collection.mutable\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, LBFGS => BreezeLBFGS, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for multinomial logistic (softmax) regression.\n+ */\n+private[classification] trait MultinomialLogisticRegressionParams\n+  extends ProbabilisticClassifierParams with HasRegParam with HasElasticNetParam with HasMaxIter\n+    with HasFitIntercept with HasTol with HasStandardization with HasWeightCol {\n+\n+  /**\n+   * Set thresholds in multiclass (or binary) classification to adjust the probability of\n+   * predicting each class. Array must have length equal to the number of classes, with values >= 0.\n+   * The class with largest value p/t is predicted, where p is the original probability of that\n+   * class and t is the class' threshold.\n+   *\n+   * @group setParam\n+   */\n+  def setThresholds(value: Array[Double]): this.type = {\n+    set(thresholds, value)\n+  }\n+\n+  /**\n+   * Get thresholds for binary or multiclass classification.\n+   *\n+   * @group getParam\n+   */\n+  override def getThresholds: Array[Double] = {\n+    $(thresholds)\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Multinomial Logistic (softmax) regression.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegression @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends ProbabilisticClassifier[Vector,\n+    MultinomialLogisticRegression, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"mlogreg\"))\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the ElasticNet mixing parameter.\n+   * For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.\n+   * For 0 < alpha < 1, the penalty is a combination of L1 and L2.\n+   * Default is 0.0 which is an L2 penalty.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setElasticNetParam(value: Double): this.type = set(elasticNetParam, value)\n+  setDefault(elasticNetParam -> 0.0)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Whether to fit an intercept term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setFitIntercept(value: Boolean): this.type = set(fitIntercept, value)\n+  setDefault(fitIntercept -> true)\n+\n+  /**\n+   * Whether to standardize the training features before fitting the model.\n+   * The coefficients of models will be always returned on the original scale,\n+   * so it will be transparent for users. Note that with/without standardization,\n+   * the models should always converge to the same solution when no regularization\n+   * is applied. In R's GLMNET package, the default behavior is true as well.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setStandardization(value: Boolean): this.type = set(standardization, value)\n+  setDefault(standardization -> true)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): MultinomialLogisticRegressionModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, elasticNetParam, standardization, thresholds,\n+      maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+       instance: Instance) =>\n+        (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer)(seqOp, combOp)\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficients, intercepts, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in {0 to ${numClasses - 1} \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val isConstantLabel = histogram.count(_ != 0) == 1\n+\n+      if ($(fitIntercept) && isConstantLabel) {\n+        // we want to produce a model that will always predict the constant label so all the\n+        // coefficients will be zero, and the constant label class intercept will be +inf\n+        val constantLabelIndex = Vectors.dense(histogram).argmax\n+        (Matrices.sparse(numClasses, numFeatures, Array.fill(numFeatures + 1)(0),\n+          Array.empty[Int], Array.empty[Double]),\n+          Vectors.sparse(numClasses, Seq((constantLabelIndex, Double.PositiveInfinity))),\n+          Array.empty[Double])\n+      } else {\n+        if (!$(fitIntercept) && isConstantLabel) {\n+          logWarning(s\"All labels belong to a single class and fitIntercept=false. It's\" +\n+            s\"a dangerous ground, so the algorithm may not converge.\")\n+        }\n+\n+        val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+        val featuresMean = summarizer.mean.toArray\n+        if (!$(fitIntercept) && (0 until numFeatures).exists { i =>\n+          featuresStd(i) == 0.0 && featuresMean(i) != 0.0 }) {\n+          logWarning(\"Fitting MultinomialLogisticRegressionModel without intercept on dataset \" +\n+            \"with constant nonzero column, Spark MLlib outputs zero coefficients for constant \" +\n+            \"nonzero columns. This behavior is the same as R glmnet but different from LIBSVM.\")\n+        }\n+\n+        val regParamL1 = $(elasticNetParam) * $(regParam)\n+        val regParamL2 = (1.0 - $(elasticNetParam)) * $(regParam)\n+\n+        val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+        val costFun = new LogisticCostFun(instances, numClasses, $(fitIntercept),\n+          $(standardization), bcFeaturesStd, regParamL2, multinomial = true)\n+\n+        val optimizer = if ($(elasticNetParam) == 0.0 || $(regParam) == 0.0) {\n+          new BreezeLBFGS[BDV[Double]]($(maxIter), 10, $(tol))\n+        } else {\n+          val standardizationParam = $(standardization)\n+          def regParamL1Fun = (index: Int) => {\n+            // Remove the L1 penalization on the intercept\n+            val isIntercept = $(fitIntercept) && ((index + 1) % numFeaturesPlusIntercept == 0)\n+            if (isIntercept) {\n+              0.0\n+            } else {\n+              if (standardizationParam) {\n+                regParamL1\n+              } else {\n+                val featureIndex = if ($(fitIntercept)) {\n+                  index % numFeaturesPlusIntercept\n+                } else {\n+                  index % numFeatures\n+                }\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                if (featuresStd(featureIndex) != 0.0) {\n+                  regParamL1 / featuresStd(featureIndex)\n+                } else {\n+                  0.0\n+                }\n+              }\n+            }\n+          }\n+          new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+        }\n+\n+        val initialCoefficientsWithIntercept = Vectors.zeros(numClasses * numFeaturesPlusIntercept)\n+\n+        if ($(fitIntercept)) {\n+          /*\n+             For multinomial logistic regression, when we initialize the coefficients as zeros,\n+             it will converge faster if we initialize the intercepts such that\n+             it follows the distribution of the labels.\n+             {{{\n+               P(1) = \\exp(b_1) / Z\n+               ...\n+               P(K) = \\exp(b_K) / Z\n+             }}}\n+             Where Z is a normalizing constant. Hence,\n+             {{{\n+              b_k = \\log(P(k)) + \\log(Z)\n+                  = \\log(count_k) - \\log(count) + \\log(Z)\n+                  = \\log(count_k) + \\lambda\n+             }}}\n+             The solution to this is not identifiable, so choose the phase \\lambda such that the\n+             mean is centered. This yields\n+             {{{\n+               b_k = \\log(count_k)\n+               b_k' = b_k - \\mean(b_k)\n+             }}}\n+           */\n+          val rawIntercepts = histogram.map(c => math.log(c + 1)) // add 1 for smoothing\n+          val rawMean = rawIntercepts.sum / rawIntercepts.length\n+          rawIntercepts.indices.foreach { i =>\n+            initialCoefficientsWithIntercept.toArray(i * numFeaturesPlusIntercept + numFeatures) =\n+              rawIntercepts(i) - rawMean\n+          }\n+        }\n+\n+        val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+          initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+        /*\n+           Note that in Multinomial Logistic Regression, the objective history\n+           (loss + regularization) is log-likelihood which is invariant under feature\n+           standardization. As a result, the objective history from optimizer is the same as the\n+           one in the original space.\n+         */\n+        val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+        var state: optimizer.State = null\n+        while (states.hasNext) {\n+          state = states.next()\n+          arrayBuilder += state.adjustedValue\n+        }\n+\n+        if (state == null) {\n+          val msg = s\"${optimizer.getClass.getName} failed.\"\n+          logError(msg)\n+          throw new SparkException(msg)\n+        }\n+        bcFeaturesStd.destroy(blocking = false)\n+\n+        /*\n+           The coefficients are trained in the scaled space; we're converting them back to\n+           the original space.\n+           Note that the intercept in scaled space and original space is the same;\n+           as a result, no scaling is needed.\n+         */\n+        val rawCoefficients = state.x.toArray\n+        val interceptsArray: Array[Double] = if ($(fitIntercept)) {\n+          Array.tabulate(numClasses) { i =>\n+            val coefIndex = (i + 1) * numFeaturesPlusIntercept - 1\n+            rawCoefficients(coefIndex)\n+          }\n+        } else {\n+          Array[Double]()\n+        }\n+\n+        val coefficientArray: Array[Double] = Array.tabulate(numClasses * numFeatures) { i =>\n+          // flatIndex will loop though rawCoefficients, and skip the intercept terms.\n+          val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+          val featureIndex = i % numFeatures\n+          if (featuresStd(featureIndex) != 0.0) {\n+            rawCoefficients(flatIndex) / featuresStd(featureIndex)\n+          } else {\n+            0.0\n+          }\n+        }\n+        val coefficientMatrix =\n+          new DenseMatrix(numClasses, numFeatures, coefficientArray, isTransposed = true)\n+\n+        /*\n+          When no regularization is applied, the coefficients lack identifiability because\n+          we do not use a pivot class. We can add any constant value to the coefficients and\n+          get the same likelihood. So here, we choose the mean centered coefficients for\n+          reproducibility. This method follows the approach in glmnet, described here:\n+\n+          Friedman, et al. \"Regularization Paths for Generalized Linear Models via\n+            Coordinate Descent,\" https://core.ac.uk/download/files/153/6287975.pdf\n+         */\n+        if ($(regParam) == 0.0) {\n+          val coefficientMean = coefficientMatrix.values.sum / (numClasses * numFeatures)\n+          coefficientMatrix.update(_ - coefficientMean)\n+        }\n+        /*\n+          The intercepts are never regularized, so we always center the mean.\n+         */\n+        val interceptVector = if (interceptsArray.nonEmpty) {\n+          val interceptMean = interceptsArray.sum / numClasses\n+          interceptsArray.indices.foreach { i => interceptsArray(i) -= interceptMean }\n+          Vectors.dense(interceptsArray)\n+        } else {\n+          Vectors.sparse(numClasses, Seq())\n+        }\n+\n+        (coefficientMatrix, interceptVector, arrayBuilder.result())\n+      }\n+    }\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(\n+      new MultinomialLogisticRegressionModel(uid, coefficients, intercepts, numClasses))\n+    instr.logSuccess(model)\n+    model\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): MultinomialLogisticRegression = defaultCopy(extra)\n+}\n+\n+@Since(\"2.1.0\")\n+object MultinomialLogisticRegression extends DefaultParamsReadable[MultinomialLogisticRegression] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): MultinomialLogisticRegression = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * Model produced by [[MultinomialLogisticRegression]].\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class MultinomialLogisticRegressionModel private[spark] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val coefficients: Matrix,\n+    @Since(\"2.1.0\") val intercepts: Vector,\n+    @Since(\"2.1.0\") val numClasses: Int)\n+  extends ProbabilisticClassificationModel[Vector, MultinomialLogisticRegressionModel]\n+    with MultinomialLogisticRegressionParams with MLWritable {\n+\n+  @Since(\"2.1.0\")\n+  override def setThresholds(value: Array[Double]): this.type = super.setThresholds(value)\n+\n+  @Since(\"2.1.0\")\n+  override def getThresholds: Array[Double] = super.getThresholds\n+\n+  @Since(\"2.1.0\")\n+  override val numFeatures: Int = coefficients.numCols\n+\n+  /** Margin (rawPrediction) for each class label. */\n+  private val margins: Vector => Vector = (features) => {\n+    val m = intercepts.toDense.copy\n+    BLAS.gemv(1.0, coefficients, features, 1.0, m)\n+    m\n+  }\n+\n+  /** Score (probability) for each class label. */\n+  private val scores: Vector => Vector = (features) => {\n+    val m = margins(features)\n+    val maxMarginIndex = m.argmax\n+    val maxMargin = m(maxMarginIndex)\n+    val marginArray = m.toArray\n+\n+    // adjust margins for overflow\n+    val sum = {\n+      var temp = 0.0\n+      var k = 0\n+      while (k < numClasses) {\n+        marginArray(k) = if (maxMargin > 0) {",
    "line": 459
  }],
  "prId": 13796
}]