[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "What about `clusterIds.filter(c != pointClusterId).map(averageDistanceToCluster).min` (except that it needs to deal with the case that `filter` returns no elements",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-01-25T19:13:45Z",
    "diffHunk": "@@ -111,6 +129,53 @@ object ClusteringEvaluator\n }\n \n \n+private[evaluation] class Silhouette {\n+\n+  /**\n+   * It computes the Silhouette coefficient for a point.\n+   */\n+  def pointSilhouetteCoefficient(\n+      clusterIds: Set[Double],\n+      pointClusterId: Double,\n+      pointClusterNumOfPoints: Long,\n+      averageDistanceToCluster: (Double) => Double): Double = {\n+    // Here we compute the average dissimilarity of the current point to any cluster of which the\n+    // point is not a member.\n+    // The cluster with the lowest average dissimilarity - i.e. the nearest cluster to the current\n+    // point - s said to be the \"neighboring cluster\".\n+    var neighboringClusterDissimilarity = Double.MaxValue\n+    clusterIds.foreach {"
  }, {
    "author": {
      "login": "mgaido91"
    },
    "body": "we don't need to deal with that case, because it is asserted that `clusterIds.size > 1`",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-01-26T08:55:04Z",
    "diffHunk": "@@ -111,6 +129,53 @@ object ClusteringEvaluator\n }\n \n \n+private[evaluation] class Silhouette {\n+\n+  /**\n+   * It computes the Silhouette coefficient for a point.\n+   */\n+  def pointSilhouetteCoefficient(\n+      clusterIds: Set[Double],\n+      pointClusterId: Double,\n+      pointClusterNumOfPoints: Long,\n+      averageDistanceToCluster: (Double) => Double): Double = {\n+    // Here we compute the average dissimilarity of the current point to any cluster of which the\n+    // point is not a member.\n+    // The cluster with the lowest average dissimilarity - i.e. the nearest cluster to the current\n+    // point - s said to be the \"neighboring cluster\".\n+    var neighboringClusterDissimilarity = Double.MaxValue\n+    clusterIds.foreach {"
  }],
  "prId": 20396
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "should this be abstract?",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-01-25T19:14:46Z",
    "diffHunk": "@@ -111,6 +129,53 @@ object ClusteringEvaluator\n }\n \n \n+private[evaluation] class Silhouette {"
  }],
  "prId": 20396
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Does this work as `.as[(Double,Vector)]` before `.rdd`?",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-01-25T19:16:20Z",
    "diffHunk": "@@ -421,13 +460,220 @@ private[evaluation] object SquaredEuclideanSilhouette {\n       computeSilhouetteCoefficient(bClustersStatsMap, _: Vector, _: Double, _: Double)\n     }\n \n-    val silhouetteScore = dfWithSquaredNorm\n-      .select(avg(\n-        computeSilhouetteCoefficientUDF(\n-          col(featuresCol), col(predictionCol).cast(DoubleType), col(\"squaredNorm\"))\n-      ))\n-      .collect()(0)\n-      .getDouble(0)\n+    val silhouetteScore = overallScore(dfWithSquaredNorm,\n+      computeSilhouetteCoefficientUDF(col(featuresCol), col(predictionCol).cast(DoubleType),\n+        col(\"squaredNorm\")))\n+\n+    bClustersStatsMap.destroy()\n+\n+    silhouetteScore\n+  }\n+}\n+\n+\n+/**\n+ * The algorithm which is implemented in this object, instead, is an efficient and parallel\n+ * implementation of the Silhouette using the cosine distance measure. The cosine distance\n+ * measure is defined as `1 - s` where `s` is the cosine similarity between two points.\n+ *\n+ * The total distance of the point `X` to the points `$C_{i}$` belonging to the cluster `$\\Gamma$`\n+ * is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\sum\\limits_{i=1}^N d(X, C_{i} ) =\n+ *   \\sum\\limits_{i=1}^N \\Big( 1 - \\frac{\\sum\\limits_{j=1}^D x_{j}c_{ij} }{ \\|X\\|\\|C_{i}\\|} \\Big)\n+ *   = \\sum\\limits_{i=1}^N 1 - \\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|}\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|}\n+ *   = N - \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|} \\Big( \\sum\\limits_{i=1}^N\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|} \\Big)\n+ *   $$\n+ * </blockquote>\n+ *\n+ * where `$x_{j}$` is the `j`-th dimension of the point `X` and `$c_{ij}$` is the `j`-th dimension\n+ * of the `i`-th point in cluster `$\\Gamma$`.\n+ *\n+ * Then, we can define the vector:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\xi_{X} : \\xi_{X i} = \\frac{x_{i}}{\\|X\\|}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed for each point and the vector\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\Omega_{\\Gamma} : \\Omega_{\\Gamma i} = \\sum\\limits_{j=1}^N \\xi_{C_{j}i}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed too for each cluster `$\\Gamma$` by its points `$C_{i}$`.\n+ *\n+ * With these definitions, the numerator becomes:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   N - \\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * Thus the average distance of a point `X` to the points of the cluster `$\\Gamma$` is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   1 - \\frac{\\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}}{N}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * In the implementation, the precomputed values for the clusters are distributed among the worker\n+ * nodes via broadcasted variables, because we can assume that the clusters are limited in number.\n+ *\n+ * The main strengths of this algorithm are the low computational complexity and the intrinsic\n+ * parallelism. The precomputed information for each point and for each cluster can be computed\n+ * with a computational complexity which is `O(N/W)`, where `N` is the number of points in the\n+ * dataset and `W` is the number of worker nodes. After that, every point can be analyzed\n+ * independently from the others.\n+ *\n+ * For every point we need to compute the average distance to all the clusters. Since the formula\n+ * above requires `O(D)` operations, this phase has a computational complexity which is\n+ * `O(C*D*N/W)` where `C` is the number of clusters (which we assume quite low), `D` is the number\n+ * of dimensions, `N` is the number of points in the dataset and `W` is the number of worker\n+ * nodes.\n+ */\n+private[evaluation] object CosineSilhouette extends Silhouette {\n+\n+  private[this] var kryoRegistrationPerformed: Boolean = false\n+\n+  private[this] val normalizedFeaturesColName = \"normalizedFeatures\"\n+\n+  /**\n+   * This method registers the class\n+   * [[org.apache.spark.ml.evaluation.CosineSilhouette.ClusterStats]]\n+   * for kryo serialization.\n+   *\n+   * @param sc `SparkContext` to be used\n+   */\n+  def registerKryoClasses(sc: SparkContext): Unit = {\n+    if (!kryoRegistrationPerformed) {\n+      sc.getConf.registerKryoClasses(\n+        Array(\n+          classOf[CosineSilhouette.ClusterStats]\n+        )\n+      )\n+      kryoRegistrationPerformed = true\n+    }\n+  }\n+\n+  case class ClusterStats(normalizedFeatureSum: Vector, numOfPoints: Long)\n+\n+  /**\n+   * The method takes the input dataset and computes the aggregated values\n+   * about a cluster which are needed by the algorithm.\n+   *\n+   * @param df The DataFrame which contains the input data\n+   * @param predictionCol The name of the column which contains the predicted cluster id\n+   *                      for the point.\n+   * @return A [[scala.collection.immutable.Map]] which associates each cluster id to a\n+   *         [[ClusterStats]] object (which contains the precomputed values `N` and\n+   *         `$\\Omega_{\\Gamma}$`).\n+   */\n+  def computeClusterStats(df: DataFrame, predictionCol: String): Map[Double, ClusterStats] = {\n+    val numFeatures = df.select(col(normalizedFeaturesColName)).first().getAs[Vector](0).size\n+    val clustersStatsRDD = df.select(\n+      col(predictionCol).cast(DoubleType), col(normalizedFeaturesColName))\n+      .rdd\n+      .map { row => (row.getDouble(0), row.getAs[Vector](1)) }",
    "line": 332
  }, {
    "author": {
      "login": "mgaido91"
    },
    "body": "it requires importing `implicits._`, do you think it is worth to be changed?",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-01-26T08:56:51Z",
    "diffHunk": "@@ -421,13 +460,220 @@ private[evaluation] object SquaredEuclideanSilhouette {\n       computeSilhouetteCoefficient(bClustersStatsMap, _: Vector, _: Double, _: Double)\n     }\n \n-    val silhouetteScore = dfWithSquaredNorm\n-      .select(avg(\n-        computeSilhouetteCoefficientUDF(\n-          col(featuresCol), col(predictionCol).cast(DoubleType), col(\"squaredNorm\"))\n-      ))\n-      .collect()(0)\n-      .getDouble(0)\n+    val silhouetteScore = overallScore(dfWithSquaredNorm,\n+      computeSilhouetteCoefficientUDF(col(featuresCol), col(predictionCol).cast(DoubleType),\n+        col(\"squaredNorm\")))\n+\n+    bClustersStatsMap.destroy()\n+\n+    silhouetteScore\n+  }\n+}\n+\n+\n+/**\n+ * The algorithm which is implemented in this object, instead, is an efficient and parallel\n+ * implementation of the Silhouette using the cosine distance measure. The cosine distance\n+ * measure is defined as `1 - s` where `s` is the cosine similarity between two points.\n+ *\n+ * The total distance of the point `X` to the points `$C_{i}$` belonging to the cluster `$\\Gamma$`\n+ * is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\sum\\limits_{i=1}^N d(X, C_{i} ) =\n+ *   \\sum\\limits_{i=1}^N \\Big( 1 - \\frac{\\sum\\limits_{j=1}^D x_{j}c_{ij} }{ \\|X\\|\\|C_{i}\\|} \\Big)\n+ *   = \\sum\\limits_{i=1}^N 1 - \\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|}\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|}\n+ *   = N - \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|} \\Big( \\sum\\limits_{i=1}^N\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|} \\Big)\n+ *   $$\n+ * </blockquote>\n+ *\n+ * where `$x_{j}$` is the `j`-th dimension of the point `X` and `$c_{ij}$` is the `j`-th dimension\n+ * of the `i`-th point in cluster `$\\Gamma$`.\n+ *\n+ * Then, we can define the vector:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\xi_{X} : \\xi_{X i} = \\frac{x_{i}}{\\|X\\|}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed for each point and the vector\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\Omega_{\\Gamma} : \\Omega_{\\Gamma i} = \\sum\\limits_{j=1}^N \\xi_{C_{j}i}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed too for each cluster `$\\Gamma$` by its points `$C_{i}$`.\n+ *\n+ * With these definitions, the numerator becomes:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   N - \\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * Thus the average distance of a point `X` to the points of the cluster `$\\Gamma$` is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   1 - \\frac{\\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}}{N}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * In the implementation, the precomputed values for the clusters are distributed among the worker\n+ * nodes via broadcasted variables, because we can assume that the clusters are limited in number.\n+ *\n+ * The main strengths of this algorithm are the low computational complexity and the intrinsic\n+ * parallelism. The precomputed information for each point and for each cluster can be computed\n+ * with a computational complexity which is `O(N/W)`, where `N` is the number of points in the\n+ * dataset and `W` is the number of worker nodes. After that, every point can be analyzed\n+ * independently from the others.\n+ *\n+ * For every point we need to compute the average distance to all the clusters. Since the formula\n+ * above requires `O(D)` operations, this phase has a computational complexity which is\n+ * `O(C*D*N/W)` where `C` is the number of clusters (which we assume quite low), `D` is the number\n+ * of dimensions, `N` is the number of points in the dataset and `W` is the number of worker\n+ * nodes.\n+ */\n+private[evaluation] object CosineSilhouette extends Silhouette {\n+\n+  private[this] var kryoRegistrationPerformed: Boolean = false\n+\n+  private[this] val normalizedFeaturesColName = \"normalizedFeatures\"\n+\n+  /**\n+   * This method registers the class\n+   * [[org.apache.spark.ml.evaluation.CosineSilhouette.ClusterStats]]\n+   * for kryo serialization.\n+   *\n+   * @param sc `SparkContext` to be used\n+   */\n+  def registerKryoClasses(sc: SparkContext): Unit = {\n+    if (!kryoRegistrationPerformed) {\n+      sc.getConf.registerKryoClasses(\n+        Array(\n+          classOf[CosineSilhouette.ClusterStats]\n+        )\n+      )\n+      kryoRegistrationPerformed = true\n+    }\n+  }\n+\n+  case class ClusterStats(normalizedFeatureSum: Vector, numOfPoints: Long)\n+\n+  /**\n+   * The method takes the input dataset and computes the aggregated values\n+   * about a cluster which are needed by the algorithm.\n+   *\n+   * @param df The DataFrame which contains the input data\n+   * @param predictionCol The name of the column which contains the predicted cluster id\n+   *                      for the point.\n+   * @return A [[scala.collection.immutable.Map]] which associates each cluster id to a\n+   *         [[ClusterStats]] object (which contains the precomputed values `N` and\n+   *         `$\\Omega_{\\Gamma}$`).\n+   */\n+  def computeClusterStats(df: DataFrame, predictionCol: String): Map[Double, ClusterStats] = {\n+    val numFeatures = df.select(col(normalizedFeaturesColName)).first().getAs[Vector](0).size\n+    val clustersStatsRDD = df.select(\n+      col(predictionCol).cast(DoubleType), col(normalizedFeaturesColName))\n+      .rdd\n+      .map { row => (row.getDouble(0), row.getAs[Vector](1)) }",
    "line": 332
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "I'm neutral on it. I don't think importing implicits is a big deal either way.",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-01-26T13:33:13Z",
    "diffHunk": "@@ -421,13 +460,220 @@ private[evaluation] object SquaredEuclideanSilhouette {\n       computeSilhouetteCoefficient(bClustersStatsMap, _: Vector, _: Double, _: Double)\n     }\n \n-    val silhouetteScore = dfWithSquaredNorm\n-      .select(avg(\n-        computeSilhouetteCoefficientUDF(\n-          col(featuresCol), col(predictionCol).cast(DoubleType), col(\"squaredNorm\"))\n-      ))\n-      .collect()(0)\n-      .getDouble(0)\n+    val silhouetteScore = overallScore(dfWithSquaredNorm,\n+      computeSilhouetteCoefficientUDF(col(featuresCol), col(predictionCol).cast(DoubleType),\n+        col(\"squaredNorm\")))\n+\n+    bClustersStatsMap.destroy()\n+\n+    silhouetteScore\n+  }\n+}\n+\n+\n+/**\n+ * The algorithm which is implemented in this object, instead, is an efficient and parallel\n+ * implementation of the Silhouette using the cosine distance measure. The cosine distance\n+ * measure is defined as `1 - s` where `s` is the cosine similarity between two points.\n+ *\n+ * The total distance of the point `X` to the points `$C_{i}$` belonging to the cluster `$\\Gamma$`\n+ * is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\sum\\limits_{i=1}^N d(X, C_{i} ) =\n+ *   \\sum\\limits_{i=1}^N \\Big( 1 - \\frac{\\sum\\limits_{j=1}^D x_{j}c_{ij} }{ \\|X\\|\\|C_{i}\\|} \\Big)\n+ *   = \\sum\\limits_{i=1}^N 1 - \\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|}\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|}\n+ *   = N - \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|} \\Big( \\sum\\limits_{i=1}^N\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|} \\Big)\n+ *   $$\n+ * </blockquote>\n+ *\n+ * where `$x_{j}$` is the `j`-th dimension of the point `X` and `$c_{ij}$` is the `j`-th dimension\n+ * of the `i`-th point in cluster `$\\Gamma$`.\n+ *\n+ * Then, we can define the vector:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\xi_{X} : \\xi_{X i} = \\frac{x_{i}}{\\|X\\|}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed for each point and the vector\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\Omega_{\\Gamma} : \\Omega_{\\Gamma i} = \\sum\\limits_{j=1}^N \\xi_{C_{j}i}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed too for each cluster `$\\Gamma$` by its points `$C_{i}$`.\n+ *\n+ * With these definitions, the numerator becomes:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   N - \\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * Thus the average distance of a point `X` to the points of the cluster `$\\Gamma$` is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   1 - \\frac{\\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}}{N}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * In the implementation, the precomputed values for the clusters are distributed among the worker\n+ * nodes via broadcasted variables, because we can assume that the clusters are limited in number.\n+ *\n+ * The main strengths of this algorithm are the low computational complexity and the intrinsic\n+ * parallelism. The precomputed information for each point and for each cluster can be computed\n+ * with a computational complexity which is `O(N/W)`, where `N` is the number of points in the\n+ * dataset and `W` is the number of worker nodes. After that, every point can be analyzed\n+ * independently from the others.\n+ *\n+ * For every point we need to compute the average distance to all the clusters. Since the formula\n+ * above requires `O(D)` operations, this phase has a computational complexity which is\n+ * `O(C*D*N/W)` where `C` is the number of clusters (which we assume quite low), `D` is the number\n+ * of dimensions, `N` is the number of points in the dataset and `W` is the number of worker\n+ * nodes.\n+ */\n+private[evaluation] object CosineSilhouette extends Silhouette {\n+\n+  private[this] var kryoRegistrationPerformed: Boolean = false\n+\n+  private[this] val normalizedFeaturesColName = \"normalizedFeatures\"\n+\n+  /**\n+   * This method registers the class\n+   * [[org.apache.spark.ml.evaluation.CosineSilhouette.ClusterStats]]\n+   * for kryo serialization.\n+   *\n+   * @param sc `SparkContext` to be used\n+   */\n+  def registerKryoClasses(sc: SparkContext): Unit = {\n+    if (!kryoRegistrationPerformed) {\n+      sc.getConf.registerKryoClasses(\n+        Array(\n+          classOf[CosineSilhouette.ClusterStats]\n+        )\n+      )\n+      kryoRegistrationPerformed = true\n+    }\n+  }\n+\n+  case class ClusterStats(normalizedFeatureSum: Vector, numOfPoints: Long)\n+\n+  /**\n+   * The method takes the input dataset and computes the aggregated values\n+   * about a cluster which are needed by the algorithm.\n+   *\n+   * @param df The DataFrame which contains the input data\n+   * @param predictionCol The name of the column which contains the predicted cluster id\n+   *                      for the point.\n+   * @return A [[scala.collection.immutable.Map]] which associates each cluster id to a\n+   *         [[ClusterStats]] object (which contains the precomputed values `N` and\n+   *         `$\\Omega_{\\Gamma}$`).\n+   */\n+  def computeClusterStats(df: DataFrame, predictionCol: String): Map[Double, ClusterStats] = {\n+    val numFeatures = df.select(col(normalizedFeaturesColName)).first().getAs[Vector](0).size\n+    val clustersStatsRDD = df.select(\n+      col(predictionCol).cast(DoubleType), col(normalizedFeaturesColName))\n+      .rdd\n+      .map { row => (row.getDouble(0), row.getAs[Vector](1)) }",
    "line": 332
  }],
  "prId": 20396
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "What about `mapValues` on the RDD before collecting or does that not work?",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-01-25T19:17:16Z",
    "diffHunk": "@@ -421,13 +460,220 @@ private[evaluation] object SquaredEuclideanSilhouette {\n       computeSilhouetteCoefficient(bClustersStatsMap, _: Vector, _: Double, _: Double)\n     }\n \n-    val silhouetteScore = dfWithSquaredNorm\n-      .select(avg(\n-        computeSilhouetteCoefficientUDF(\n-          col(featuresCol), col(predictionCol).cast(DoubleType), col(\"squaredNorm\"))\n-      ))\n-      .collect()(0)\n-      .getDouble(0)\n+    val silhouetteScore = overallScore(dfWithSquaredNorm,\n+      computeSilhouetteCoefficientUDF(col(featuresCol), col(predictionCol).cast(DoubleType),\n+        col(\"squaredNorm\")))\n+\n+    bClustersStatsMap.destroy()\n+\n+    silhouetteScore\n+  }\n+}\n+\n+\n+/**\n+ * The algorithm which is implemented in this object, instead, is an efficient and parallel\n+ * implementation of the Silhouette using the cosine distance measure. The cosine distance\n+ * measure is defined as `1 - s` where `s` is the cosine similarity between two points.\n+ *\n+ * The total distance of the point `X` to the points `$C_{i}$` belonging to the cluster `$\\Gamma$`\n+ * is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\sum\\limits_{i=1}^N d(X, C_{i} ) =\n+ *   \\sum\\limits_{i=1}^N \\Big( 1 - \\frac{\\sum\\limits_{j=1}^D x_{j}c_{ij} }{ \\|X\\|\\|C_{i}\\|} \\Big)\n+ *   = \\sum\\limits_{i=1}^N 1 - \\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|}\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|}\n+ *   = N - \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|} \\Big( \\sum\\limits_{i=1}^N\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|} \\Big)\n+ *   $$\n+ * </blockquote>\n+ *\n+ * where `$x_{j}$` is the `j`-th dimension of the point `X` and `$c_{ij}$` is the `j`-th dimension\n+ * of the `i`-th point in cluster `$\\Gamma$`.\n+ *\n+ * Then, we can define the vector:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\xi_{X} : \\xi_{X i} = \\frac{x_{i}}{\\|X\\|}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed for each point and the vector\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\Omega_{\\Gamma} : \\Omega_{\\Gamma i} = \\sum\\limits_{j=1}^N \\xi_{C_{j}i}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed too for each cluster `$\\Gamma$` by its points `$C_{i}$`.\n+ *\n+ * With these definitions, the numerator becomes:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   N - \\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * Thus the average distance of a point `X` to the points of the cluster `$\\Gamma$` is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   1 - \\frac{\\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}}{N}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * In the implementation, the precomputed values for the clusters are distributed among the worker\n+ * nodes via broadcasted variables, because we can assume that the clusters are limited in number.\n+ *\n+ * The main strengths of this algorithm are the low computational complexity and the intrinsic\n+ * parallelism. The precomputed information for each point and for each cluster can be computed\n+ * with a computational complexity which is `O(N/W)`, where `N` is the number of points in the\n+ * dataset and `W` is the number of worker nodes. After that, every point can be analyzed\n+ * independently from the others.\n+ *\n+ * For every point we need to compute the average distance to all the clusters. Since the formula\n+ * above requires `O(D)` operations, this phase has a computational complexity which is\n+ * `O(C*D*N/W)` where `C` is the number of clusters (which we assume quite low), `D` is the number\n+ * of dimensions, `N` is the number of points in the dataset and `W` is the number of worker\n+ * nodes.\n+ */\n+private[evaluation] object CosineSilhouette extends Silhouette {\n+\n+  private[this] var kryoRegistrationPerformed: Boolean = false\n+\n+  private[this] val normalizedFeaturesColName = \"normalizedFeatures\"\n+\n+  /**\n+   * This method registers the class\n+   * [[org.apache.spark.ml.evaluation.CosineSilhouette.ClusterStats]]\n+   * for kryo serialization.\n+   *\n+   * @param sc `SparkContext` to be used\n+   */\n+  def registerKryoClasses(sc: SparkContext): Unit = {\n+    if (!kryoRegistrationPerformed) {\n+      sc.getConf.registerKryoClasses(\n+        Array(\n+          classOf[CosineSilhouette.ClusterStats]\n+        )\n+      )\n+      kryoRegistrationPerformed = true\n+    }\n+  }\n+\n+  case class ClusterStats(normalizedFeatureSum: Vector, numOfPoints: Long)\n+\n+  /**\n+   * The method takes the input dataset and computes the aggregated values\n+   * about a cluster which are needed by the algorithm.\n+   *\n+   * @param df The DataFrame which contains the input data\n+   * @param predictionCol The name of the column which contains the predicted cluster id\n+   *                      for the point.\n+   * @return A [[scala.collection.immutable.Map]] which associates each cluster id to a\n+   *         [[ClusterStats]] object (which contains the precomputed values `N` and\n+   *         `$\\Omega_{\\Gamma}$`).\n+   */\n+  def computeClusterStats(df: DataFrame, predictionCol: String): Map[Double, ClusterStats] = {\n+    val numFeatures = df.select(col(normalizedFeaturesColName)).first().getAs[Vector](0).size\n+    val clustersStatsRDD = df.select(\n+      col(predictionCol).cast(DoubleType), col(normalizedFeaturesColName))\n+      .rdd\n+      .map { row => (row.getDouble(0), row.getAs[Vector](1)) }\n+      .aggregateByKey[(DenseVector, Long)]((Vectors.zeros(numFeatures).toDense, 0L))(\n+      seqOp = {\n+        case ((normalizedFeaturesSum: DenseVector, numOfPoints: Long), (normalizedFeatures)) =>\n+          BLAS.axpy(1.0, normalizedFeatures, normalizedFeaturesSum)\n+          (normalizedFeaturesSum, numOfPoints + 1)\n+      },\n+      combOp = {\n+        case ((normalizedFeaturesSum1, numOfPoints1), (normalizedFeaturesSum2, numOfPoints2)) =>\n+          BLAS.axpy(1.0, normalizedFeaturesSum2, normalizedFeaturesSum1)\n+          (normalizedFeaturesSum1, numOfPoints1 + numOfPoints2)\n+      }\n+    )\n+\n+    clustersStatsRDD\n+      .collectAsMap()\n+      .mapValues {"
  }, {
    "author": {
      "login": "mgaido91"
    },
    "body": "that would work too, but it is quite pointless up to me, since here we are dealing with the clusters and they are assumed not to be many... usually they are << 100....",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-01-26T08:59:06Z",
    "diffHunk": "@@ -421,13 +460,220 @@ private[evaluation] object SquaredEuclideanSilhouette {\n       computeSilhouetteCoefficient(bClustersStatsMap, _: Vector, _: Double, _: Double)\n     }\n \n-    val silhouetteScore = dfWithSquaredNorm\n-      .select(avg(\n-        computeSilhouetteCoefficientUDF(\n-          col(featuresCol), col(predictionCol).cast(DoubleType), col(\"squaredNorm\"))\n-      ))\n-      .collect()(0)\n-      .getDouble(0)\n+    val silhouetteScore = overallScore(dfWithSquaredNorm,\n+      computeSilhouetteCoefficientUDF(col(featuresCol), col(predictionCol).cast(DoubleType),\n+        col(\"squaredNorm\")))\n+\n+    bClustersStatsMap.destroy()\n+\n+    silhouetteScore\n+  }\n+}\n+\n+\n+/**\n+ * The algorithm which is implemented in this object, instead, is an efficient and parallel\n+ * implementation of the Silhouette using the cosine distance measure. The cosine distance\n+ * measure is defined as `1 - s` where `s` is the cosine similarity between two points.\n+ *\n+ * The total distance of the point `X` to the points `$C_{i}$` belonging to the cluster `$\\Gamma$`\n+ * is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\sum\\limits_{i=1}^N d(X, C_{i} ) =\n+ *   \\sum\\limits_{i=1}^N \\Big( 1 - \\frac{\\sum\\limits_{j=1}^D x_{j}c_{ij} }{ \\|X\\|\\|C_{i}\\|} \\Big)\n+ *   = \\sum\\limits_{i=1}^N 1 - \\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|}\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|}\n+ *   = N - \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|} \\Big( \\sum\\limits_{i=1}^N\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|} \\Big)\n+ *   $$\n+ * </blockquote>\n+ *\n+ * where `$x_{j}$` is the `j`-th dimension of the point `X` and `$c_{ij}$` is the `j`-th dimension\n+ * of the `i`-th point in cluster `$\\Gamma$`.\n+ *\n+ * Then, we can define the vector:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\xi_{X} : \\xi_{X i} = \\frac{x_{i}}{\\|X\\|}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed for each point and the vector\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\Omega_{\\Gamma} : \\Omega_{\\Gamma i} = \\sum\\limits_{j=1}^N \\xi_{C_{j}i}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed too for each cluster `$\\Gamma$` by its points `$C_{i}$`.\n+ *\n+ * With these definitions, the numerator becomes:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   N - \\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * Thus the average distance of a point `X` to the points of the cluster `$\\Gamma$` is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   1 - \\frac{\\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}}{N}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * In the implementation, the precomputed values for the clusters are distributed among the worker\n+ * nodes via broadcasted variables, because we can assume that the clusters are limited in number.\n+ *\n+ * The main strengths of this algorithm are the low computational complexity and the intrinsic\n+ * parallelism. The precomputed information for each point and for each cluster can be computed\n+ * with a computational complexity which is `O(N/W)`, where `N` is the number of points in the\n+ * dataset and `W` is the number of worker nodes. After that, every point can be analyzed\n+ * independently from the others.\n+ *\n+ * For every point we need to compute the average distance to all the clusters. Since the formula\n+ * above requires `O(D)` operations, this phase has a computational complexity which is\n+ * `O(C*D*N/W)` where `C` is the number of clusters (which we assume quite low), `D` is the number\n+ * of dimensions, `N` is the number of points in the dataset and `W` is the number of worker\n+ * nodes.\n+ */\n+private[evaluation] object CosineSilhouette extends Silhouette {\n+\n+  private[this] var kryoRegistrationPerformed: Boolean = false\n+\n+  private[this] val normalizedFeaturesColName = \"normalizedFeatures\"\n+\n+  /**\n+   * This method registers the class\n+   * [[org.apache.spark.ml.evaluation.CosineSilhouette.ClusterStats]]\n+   * for kryo serialization.\n+   *\n+   * @param sc `SparkContext` to be used\n+   */\n+  def registerKryoClasses(sc: SparkContext): Unit = {\n+    if (!kryoRegistrationPerformed) {\n+      sc.getConf.registerKryoClasses(\n+        Array(\n+          classOf[CosineSilhouette.ClusterStats]\n+        )\n+      )\n+      kryoRegistrationPerformed = true\n+    }\n+  }\n+\n+  case class ClusterStats(normalizedFeatureSum: Vector, numOfPoints: Long)\n+\n+  /**\n+   * The method takes the input dataset and computes the aggregated values\n+   * about a cluster which are needed by the algorithm.\n+   *\n+   * @param df The DataFrame which contains the input data\n+   * @param predictionCol The name of the column which contains the predicted cluster id\n+   *                      for the point.\n+   * @return A [[scala.collection.immutable.Map]] which associates each cluster id to a\n+   *         [[ClusterStats]] object (which contains the precomputed values `N` and\n+   *         `$\\Omega_{\\Gamma}$`).\n+   */\n+  def computeClusterStats(df: DataFrame, predictionCol: String): Map[Double, ClusterStats] = {\n+    val numFeatures = df.select(col(normalizedFeaturesColName)).first().getAs[Vector](0).size\n+    val clustersStatsRDD = df.select(\n+      col(predictionCol).cast(DoubleType), col(normalizedFeaturesColName))\n+      .rdd\n+      .map { row => (row.getDouble(0), row.getAs[Vector](1)) }\n+      .aggregateByKey[(DenseVector, Long)]((Vectors.zeros(numFeatures).toDense, 0L))(\n+      seqOp = {\n+        case ((normalizedFeaturesSum: DenseVector, numOfPoints: Long), (normalizedFeatures)) =>\n+          BLAS.axpy(1.0, normalizedFeatures, normalizedFeaturesSum)\n+          (normalizedFeaturesSum, numOfPoints + 1)\n+      },\n+      combOp = {\n+        case ((normalizedFeaturesSum1, numOfPoints1), (normalizedFeaturesSum2, numOfPoints2)) =>\n+          BLAS.axpy(1.0, normalizedFeaturesSum2, normalizedFeaturesSum1)\n+          (normalizedFeaturesSum1, numOfPoints1 + numOfPoints2)\n+      }\n+    )\n+\n+    clustersStatsRDD\n+      .collectAsMap()\n+      .mapValues {"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "OK sure, it's not worth another distributed stage.",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-01-26T13:35:41Z",
    "diffHunk": "@@ -421,13 +460,220 @@ private[evaluation] object SquaredEuclideanSilhouette {\n       computeSilhouetteCoefficient(bClustersStatsMap, _: Vector, _: Double, _: Double)\n     }\n \n-    val silhouetteScore = dfWithSquaredNorm\n-      .select(avg(\n-        computeSilhouetteCoefficientUDF(\n-          col(featuresCol), col(predictionCol).cast(DoubleType), col(\"squaredNorm\"))\n-      ))\n-      .collect()(0)\n-      .getDouble(0)\n+    val silhouetteScore = overallScore(dfWithSquaredNorm,\n+      computeSilhouetteCoefficientUDF(col(featuresCol), col(predictionCol).cast(DoubleType),\n+        col(\"squaredNorm\")))\n+\n+    bClustersStatsMap.destroy()\n+\n+    silhouetteScore\n+  }\n+}\n+\n+\n+/**\n+ * The algorithm which is implemented in this object, instead, is an efficient and parallel\n+ * implementation of the Silhouette using the cosine distance measure. The cosine distance\n+ * measure is defined as `1 - s` where `s` is the cosine similarity between two points.\n+ *\n+ * The total distance of the point `X` to the points `$C_{i}$` belonging to the cluster `$\\Gamma$`\n+ * is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\sum\\limits_{i=1}^N d(X, C_{i} ) =\n+ *   \\sum\\limits_{i=1}^N \\Big( 1 - \\frac{\\sum\\limits_{j=1}^D x_{j}c_{ij} }{ \\|X\\|\\|C_{i}\\|} \\Big)\n+ *   = \\sum\\limits_{i=1}^N 1 - \\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|}\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|}\n+ *   = N - \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|} \\Big( \\sum\\limits_{i=1}^N\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|} \\Big)\n+ *   $$\n+ * </blockquote>\n+ *\n+ * where `$x_{j}$` is the `j`-th dimension of the point `X` and `$c_{ij}$` is the `j`-th dimension\n+ * of the `i`-th point in cluster `$\\Gamma$`.\n+ *\n+ * Then, we can define the vector:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\xi_{X} : \\xi_{X i} = \\frac{x_{i}}{\\|X\\|}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed for each point and the vector\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\Omega_{\\Gamma} : \\Omega_{\\Gamma i} = \\sum\\limits_{j=1}^N \\xi_{C_{j}i}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed too for each cluster `$\\Gamma$` by its points `$C_{i}$`.\n+ *\n+ * With these definitions, the numerator becomes:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   N - \\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * Thus the average distance of a point `X` to the points of the cluster `$\\Gamma$` is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   1 - \\frac{\\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}}{N}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * In the implementation, the precomputed values for the clusters are distributed among the worker\n+ * nodes via broadcasted variables, because we can assume that the clusters are limited in number.\n+ *\n+ * The main strengths of this algorithm are the low computational complexity and the intrinsic\n+ * parallelism. The precomputed information for each point and for each cluster can be computed\n+ * with a computational complexity which is `O(N/W)`, where `N` is the number of points in the\n+ * dataset and `W` is the number of worker nodes. After that, every point can be analyzed\n+ * independently from the others.\n+ *\n+ * For every point we need to compute the average distance to all the clusters. Since the formula\n+ * above requires `O(D)` operations, this phase has a computational complexity which is\n+ * `O(C*D*N/W)` where `C` is the number of clusters (which we assume quite low), `D` is the number\n+ * of dimensions, `N` is the number of points in the dataset and `W` is the number of worker\n+ * nodes.\n+ */\n+private[evaluation] object CosineSilhouette extends Silhouette {\n+\n+  private[this] var kryoRegistrationPerformed: Boolean = false\n+\n+  private[this] val normalizedFeaturesColName = \"normalizedFeatures\"\n+\n+  /**\n+   * This method registers the class\n+   * [[org.apache.spark.ml.evaluation.CosineSilhouette.ClusterStats]]\n+   * for kryo serialization.\n+   *\n+   * @param sc `SparkContext` to be used\n+   */\n+  def registerKryoClasses(sc: SparkContext): Unit = {\n+    if (!kryoRegistrationPerformed) {\n+      sc.getConf.registerKryoClasses(\n+        Array(\n+          classOf[CosineSilhouette.ClusterStats]\n+        )\n+      )\n+      kryoRegistrationPerformed = true\n+    }\n+  }\n+\n+  case class ClusterStats(normalizedFeatureSum: Vector, numOfPoints: Long)\n+\n+  /**\n+   * The method takes the input dataset and computes the aggregated values\n+   * about a cluster which are needed by the algorithm.\n+   *\n+   * @param df The DataFrame which contains the input data\n+   * @param predictionCol The name of the column which contains the predicted cluster id\n+   *                      for the point.\n+   * @return A [[scala.collection.immutable.Map]] which associates each cluster id to a\n+   *         [[ClusterStats]] object (which contains the precomputed values `N` and\n+   *         `$\\Omega_{\\Gamma}$`).\n+   */\n+  def computeClusterStats(df: DataFrame, predictionCol: String): Map[Double, ClusterStats] = {\n+    val numFeatures = df.select(col(normalizedFeaturesColName)).first().getAs[Vector](0).size\n+    val clustersStatsRDD = df.select(\n+      col(predictionCol).cast(DoubleType), col(normalizedFeaturesColName))\n+      .rdd\n+      .map { row => (row.getDouble(0), row.getAs[Vector](1)) }\n+      .aggregateByKey[(DenseVector, Long)]((Vectors.zeros(numFeatures).toDense, 0L))(\n+      seqOp = {\n+        case ((normalizedFeaturesSum: DenseVector, numOfPoints: Long), (normalizedFeatures)) =>\n+          BLAS.axpy(1.0, normalizedFeatures, normalizedFeaturesSum)\n+          (normalizedFeaturesSum, numOfPoints + 1)\n+      },\n+      combOp = {\n+        case ((normalizedFeaturesSum1, numOfPoints1), (normalizedFeaturesSum2, numOfPoints2)) =>\n+          BLAS.axpy(1.0, normalizedFeaturesSum2, normalizedFeaturesSum1)\n+          (normalizedFeaturesSum1, numOfPoints1 + numOfPoints2)\n+      }\n+    )\n+\n+    clustersStatsRDD\n+      .collectAsMap()\n+      .mapValues {"
  }],
  "prId": 20396
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "You don't need to change this, but it occurs to me that on lots of the parameters that take discrete values, the error message could reference the same array of values the validator uses, to make sure they're always consistent.",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-01-26T13:30:26Z",
    "diffHunk": "@@ -84,18 +81,39 @@ class ClusteringEvaluator @Since(\"2.3.0\") (@Since(\"2.3.0\") override val uid: Str\n   @Since(\"2.3.0\")\n   def setMetricName(value: String): this.type = set(metricName, value)\n \n-  setDefault(metricName -> \"silhouette\")\n+  /**\n+   * param for distance measure to be used in evaluation\n+   * (supports `\"squaredEuclidean\"` (default), `\"cosine\"`)\n+   * @group param\n+   */\n+  @Since(\"2.4.0\")\n+  val distanceMeasure: Param[String] = {\n+    val allowedParams = ParamValidators.inArray(Array(\"squaredEuclidean\", \"cosine\"))"
  }],
  "prId": 20396
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "This duplicates a method in ClusteringEvaluator right? I wonder if this can happen just once. It's OK if it registers a bunch of classes, not all of which will be used. ",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-01-26T13:35:18Z",
    "diffHunk": "@@ -421,13 +453,220 @@ private[evaluation] object SquaredEuclideanSilhouette {\n       computeSilhouetteCoefficient(bClustersStatsMap, _: Vector, _: Double, _: Double)\n     }\n \n-    val silhouetteScore = dfWithSquaredNorm\n-      .select(avg(\n-        computeSilhouetteCoefficientUDF(\n-          col(featuresCol), col(predictionCol).cast(DoubleType), col(\"squaredNorm\"))\n-      ))\n-      .collect()(0)\n-      .getDouble(0)\n+    val silhouetteScore = overallScore(dfWithSquaredNorm,\n+      computeSilhouetteCoefficientUDF(col(featuresCol), col(predictionCol).cast(DoubleType),\n+        col(\"squaredNorm\")))\n+\n+    bClustersStatsMap.destroy()\n+\n+    silhouetteScore\n+  }\n+}\n+\n+\n+/**\n+ * The algorithm which is implemented in this object, instead, is an efficient and parallel\n+ * implementation of the Silhouette using the cosine distance measure. The cosine distance\n+ * measure is defined as `1 - s` where `s` is the cosine similarity between two points.\n+ *\n+ * The total distance of the point `X` to the points `$C_{i}$` belonging to the cluster `$\\Gamma$`\n+ * is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\sum\\limits_{i=1}^N d(X, C_{i} ) =\n+ *   \\sum\\limits_{i=1}^N \\Big( 1 - \\frac{\\sum\\limits_{j=1}^D x_{j}c_{ij} }{ \\|X\\|\\|C_{i}\\|} \\Big)\n+ *   = \\sum\\limits_{i=1}^N 1 - \\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|}\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|}\n+ *   = N - \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|} \\Big( \\sum\\limits_{i=1}^N\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|} \\Big)\n+ *   $$\n+ * </blockquote>\n+ *\n+ * where `$x_{j}$` is the `j`-th dimension of the point `X` and `$c_{ij}$` is the `j`-th dimension\n+ * of the `i`-th point in cluster `$\\Gamma$`.\n+ *\n+ * Then, we can define the vector:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\xi_{X} : \\xi_{X i} = \\frac{x_{i}}{\\|X\\|}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed for each point and the vector\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\Omega_{\\Gamma} : \\Omega_{\\Gamma i} = \\sum\\limits_{j=1}^N \\xi_{C_{j}i}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed too for each cluster `$\\Gamma$` by its points `$C_{i}$`.\n+ *\n+ * With these definitions, the numerator becomes:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   N - \\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * Thus the average distance of a point `X` to the points of the cluster `$\\Gamma$` is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   1 - \\frac{\\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}}{N}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * In the implementation, the precomputed values for the clusters are distributed among the worker\n+ * nodes via broadcasted variables, because we can assume that the clusters are limited in number.\n+ *\n+ * The main strengths of this algorithm are the low computational complexity and the intrinsic\n+ * parallelism. The precomputed information for each point and for each cluster can be computed\n+ * with a computational complexity which is `O(N/W)`, where `N` is the number of points in the\n+ * dataset and `W` is the number of worker nodes. After that, every point can be analyzed\n+ * independently from the others.\n+ *\n+ * For every point we need to compute the average distance to all the clusters. Since the formula\n+ * above requires `O(D)` operations, this phase has a computational complexity which is\n+ * `O(C*D*N/W)` where `C` is the number of clusters (which we assume quite low), `D` is the number\n+ * of dimensions, `N` is the number of points in the dataset and `W` is the number of worker\n+ * nodes.\n+ */\n+private[evaluation] object CosineSilhouette extends Silhouette {\n+\n+  private[this] var kryoRegistrationPerformed: Boolean = false\n+\n+  private[this] val normalizedFeaturesColName = \"normalizedFeatures\"\n+\n+  /**\n+   * This method registers the class\n+   * [[org.apache.spark.ml.evaluation.CosineSilhouette.ClusterStats]]\n+   * for kryo serialization.\n+   *\n+   * @param sc `SparkContext` to be used\n+   */\n+  def registerKryoClasses(sc: SparkContext): Unit = {"
  }, {
    "author": {
      "login": "mgaido91"
    },
    "body": "sorry, I am not sure I can get what you mean. Which method is this duplicating? The registration happens once since there is a flag for it...",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-01-26T13:55:46Z",
    "diffHunk": "@@ -421,13 +453,220 @@ private[evaluation] object SquaredEuclideanSilhouette {\n       computeSilhouetteCoefficient(bClustersStatsMap, _: Vector, _: Double, _: Double)\n     }\n \n-    val silhouetteScore = dfWithSquaredNorm\n-      .select(avg(\n-        computeSilhouetteCoefficientUDF(\n-          col(featuresCol), col(predictionCol).cast(DoubleType), col(\"squaredNorm\"))\n-      ))\n-      .collect()(0)\n-      .getDouble(0)\n+    val silhouetteScore = overallScore(dfWithSquaredNorm,\n+      computeSilhouetteCoefficientUDF(col(featuresCol), col(predictionCol).cast(DoubleType),\n+        col(\"squaredNorm\")))\n+\n+    bClustersStatsMap.destroy()\n+\n+    silhouetteScore\n+  }\n+}\n+\n+\n+/**\n+ * The algorithm which is implemented in this object, instead, is an efficient and parallel\n+ * implementation of the Silhouette using the cosine distance measure. The cosine distance\n+ * measure is defined as `1 - s` where `s` is the cosine similarity between two points.\n+ *\n+ * The total distance of the point `X` to the points `$C_{i}$` belonging to the cluster `$\\Gamma$`\n+ * is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\sum\\limits_{i=1}^N d(X, C_{i} ) =\n+ *   \\sum\\limits_{i=1}^N \\Big( 1 - \\frac{\\sum\\limits_{j=1}^D x_{j}c_{ij} }{ \\|X\\|\\|C_{i}\\|} \\Big)\n+ *   = \\sum\\limits_{i=1}^N 1 - \\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|}\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|}\n+ *   = N - \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|} \\Big( \\sum\\limits_{i=1}^N\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|} \\Big)\n+ *   $$\n+ * </blockquote>\n+ *\n+ * where `$x_{j}$` is the `j`-th dimension of the point `X` and `$c_{ij}$` is the `j`-th dimension\n+ * of the `i`-th point in cluster `$\\Gamma$`.\n+ *\n+ * Then, we can define the vector:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\xi_{X} : \\xi_{X i} = \\frac{x_{i}}{\\|X\\|}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed for each point and the vector\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\Omega_{\\Gamma} : \\Omega_{\\Gamma i} = \\sum\\limits_{j=1}^N \\xi_{C_{j}i}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed too for each cluster `$\\Gamma$` by its points `$C_{i}$`.\n+ *\n+ * With these definitions, the numerator becomes:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   N - \\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * Thus the average distance of a point `X` to the points of the cluster `$\\Gamma$` is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   1 - \\frac{\\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}}{N}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * In the implementation, the precomputed values for the clusters are distributed among the worker\n+ * nodes via broadcasted variables, because we can assume that the clusters are limited in number.\n+ *\n+ * The main strengths of this algorithm are the low computational complexity and the intrinsic\n+ * parallelism. The precomputed information for each point and for each cluster can be computed\n+ * with a computational complexity which is `O(N/W)`, where `N` is the number of points in the\n+ * dataset and `W` is the number of worker nodes. After that, every point can be analyzed\n+ * independently from the others.\n+ *\n+ * For every point we need to compute the average distance to all the clusters. Since the formula\n+ * above requires `O(D)` operations, this phase has a computational complexity which is\n+ * `O(C*D*N/W)` where `C` is the number of clusters (which we assume quite low), `D` is the number\n+ * of dimensions, `N` is the number of points in the dataset and `W` is the number of worker\n+ * nodes.\n+ */\n+private[evaluation] object CosineSilhouette extends Silhouette {\n+\n+  private[this] var kryoRegistrationPerformed: Boolean = false\n+\n+  private[this] val normalizedFeaturesColName = \"normalizedFeatures\"\n+\n+  /**\n+   * This method registers the class\n+   * [[org.apache.spark.ml.evaluation.CosineSilhouette.ClusterStats]]\n+   * for kryo serialization.\n+   *\n+   * @param sc `SparkContext` to be used\n+   */\n+  def registerKryoClasses(sc: SparkContext): Unit = {"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "There are two methods like this in each implementation of the Silhouette class. I was wondering if there can be just one place where all relevant classes are registered. It may not be possible without, say, making the superclass refer to its subclasses, and that may not be worth it just to make a single registerKryoClasses method.",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-01-28T15:49:22Z",
    "diffHunk": "@@ -421,13 +453,220 @@ private[evaluation] object SquaredEuclideanSilhouette {\n       computeSilhouetteCoefficient(bClustersStatsMap, _: Vector, _: Double, _: Double)\n     }\n \n-    val silhouetteScore = dfWithSquaredNorm\n-      .select(avg(\n-        computeSilhouetteCoefficientUDF(\n-          col(featuresCol), col(predictionCol).cast(DoubleType), col(\"squaredNorm\"))\n-      ))\n-      .collect()(0)\n-      .getDouble(0)\n+    val silhouetteScore = overallScore(dfWithSquaredNorm,\n+      computeSilhouetteCoefficientUDF(col(featuresCol), col(predictionCol).cast(DoubleType),\n+        col(\"squaredNorm\")))\n+\n+    bClustersStatsMap.destroy()\n+\n+    silhouetteScore\n+  }\n+}\n+\n+\n+/**\n+ * The algorithm which is implemented in this object, instead, is an efficient and parallel\n+ * implementation of the Silhouette using the cosine distance measure. The cosine distance\n+ * measure is defined as `1 - s` where `s` is the cosine similarity between two points.\n+ *\n+ * The total distance of the point `X` to the points `$C_{i}$` belonging to the cluster `$\\Gamma$`\n+ * is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\sum\\limits_{i=1}^N d(X, C_{i} ) =\n+ *   \\sum\\limits_{i=1}^N \\Big( 1 - \\frac{\\sum\\limits_{j=1}^D x_{j}c_{ij} }{ \\|X\\|\\|C_{i}\\|} \\Big)\n+ *   = \\sum\\limits_{i=1}^N 1 - \\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|}\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|}\n+ *   = N - \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|} \\Big( \\sum\\limits_{i=1}^N\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|} \\Big)\n+ *   $$\n+ * </blockquote>\n+ *\n+ * where `$x_{j}$` is the `j`-th dimension of the point `X` and `$c_{ij}$` is the `j`-th dimension\n+ * of the `i`-th point in cluster `$\\Gamma$`.\n+ *\n+ * Then, we can define the vector:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\xi_{X} : \\xi_{X i} = \\frac{x_{i}}{\\|X\\|}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed for each point and the vector\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\Omega_{\\Gamma} : \\Omega_{\\Gamma i} = \\sum\\limits_{j=1}^N \\xi_{C_{j}i}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed too for each cluster `$\\Gamma$` by its points `$C_{i}$`.\n+ *\n+ * With these definitions, the numerator becomes:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   N - \\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * Thus the average distance of a point `X` to the points of the cluster `$\\Gamma$` is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   1 - \\frac{\\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}}{N}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * In the implementation, the precomputed values for the clusters are distributed among the worker\n+ * nodes via broadcasted variables, because we can assume that the clusters are limited in number.\n+ *\n+ * The main strengths of this algorithm are the low computational complexity and the intrinsic\n+ * parallelism. The precomputed information for each point and for each cluster can be computed\n+ * with a computational complexity which is `O(N/W)`, where `N` is the number of points in the\n+ * dataset and `W` is the number of worker nodes. After that, every point can be analyzed\n+ * independently from the others.\n+ *\n+ * For every point we need to compute the average distance to all the clusters. Since the formula\n+ * above requires `O(D)` operations, this phase has a computational complexity which is\n+ * `O(C*D*N/W)` where `C` is the number of clusters (which we assume quite low), `D` is the number\n+ * of dimensions, `N` is the number of points in the dataset and `W` is the number of worker\n+ * nodes.\n+ */\n+private[evaluation] object CosineSilhouette extends Silhouette {\n+\n+  private[this] var kryoRegistrationPerformed: Boolean = false\n+\n+  private[this] val normalizedFeaturesColName = \"normalizedFeatures\"\n+\n+  /**\n+   * This method registers the class\n+   * [[org.apache.spark.ml.evaluation.CosineSilhouette.ClusterStats]]\n+   * for kryo serialization.\n+   *\n+   * @param sc `SparkContext` to be used\n+   */\n+  def registerKryoClasses(sc: SparkContext): Unit = {"
  }],
  "prId": 20396
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Is this just expressing ...\r\n\r\n```\r\nif (currentClusterDissimilarity < neighboringClusterDissimilarity) {\r\n  ...\r\n} else if (currentClusterDissimilarity > neighboringClusterDissimilarity) {\r\n\r\n} else {\r\n  ...\r\n}\r\n```\r\n\r\nThat seems more straightforward if that's all it is, to my eyes. This has postfix notation, signum, match statement",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-01-26T13:38:38Z",
    "diffHunk": "@@ -111,6 +129,46 @@ object ClusteringEvaluator\n }\n \n \n+private[evaluation] abstract class Silhouette {\n+\n+  /**\n+   * It computes the Silhouette coefficient for a point.\n+   */\n+  def pointSilhouetteCoefficient(\n+      clusterIds: Set[Double],\n+      pointClusterId: Double,\n+      pointClusterNumOfPoints: Long,\n+      averageDistanceToCluster: (Double) => Double): Double = {\n+    // Here we compute the average dissimilarity of the current point to any cluster of which the\n+    // point is not a member.\n+    // The cluster with the lowest average dissimilarity - i.e. the nearest cluster to the current\n+    // point - s said to be the \"neighboring cluster\".\n+    val otherClusterIds = clusterIds.filter(_ != pointClusterId)\n+    val neighboringClusterDissimilarity = otherClusterIds.map(averageDistanceToCluster).min\n+\n+    // adjustment for excluding the node itself from the computation of the average dissimilarity\n+    val currentClusterDissimilarity = if (pointClusterNumOfPoints == 1) {\n+      0\n+    } else {\n+      averageDistanceToCluster(pointClusterId) * pointClusterNumOfPoints /\n+        (pointClusterNumOfPoints - 1)\n+    }\n+\n+    (currentClusterDissimilarity compare neighboringClusterDissimilarity).signum match {"
  }],
  "prId": 20396
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Back on this -- how about just using a Tuple2 of Vector, Long? no new class to register. ",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-02-12T15:05:38Z",
    "diffHunk": "@@ -421,13 +456,220 @@ private[evaluation] object SquaredEuclideanSilhouette {\n       computeSilhouetteCoefficient(bClustersStatsMap, _: Vector, _: Double, _: Double)\n     }\n \n-    val silhouetteScore = dfWithSquaredNorm\n-      .select(avg(\n-        computeSilhouetteCoefficientUDF(\n-          col(featuresCol), col(predictionCol).cast(DoubleType), col(\"squaredNorm\"))\n-      ))\n-      .collect()(0)\n-      .getDouble(0)\n+    val silhouetteScore = overallScore(dfWithSquaredNorm,\n+      computeSilhouetteCoefficientUDF(col(featuresCol), col(predictionCol).cast(DoubleType),\n+        col(\"squaredNorm\")))\n+\n+    bClustersStatsMap.destroy()\n+\n+    silhouetteScore\n+  }\n+}\n+\n+\n+/**\n+ * The algorithm which is implemented in this object, instead, is an efficient and parallel\n+ * implementation of the Silhouette using the cosine distance measure. The cosine distance\n+ * measure is defined as `1 - s` where `s` is the cosine similarity between two points.\n+ *\n+ * The total distance of the point `X` to the points `$C_{i}$` belonging to the cluster `$\\Gamma$`\n+ * is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\sum\\limits_{i=1}^N d(X, C_{i} ) =\n+ *   \\sum\\limits_{i=1}^N \\Big( 1 - \\frac{\\sum\\limits_{j=1}^D x_{j}c_{ij} }{ \\|X\\|\\|C_{i}\\|} \\Big)\n+ *   = \\sum\\limits_{i=1}^N 1 - \\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|}\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|}\n+ *   = N - \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|} \\Big( \\sum\\limits_{i=1}^N\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|} \\Big)\n+ *   $$\n+ * </blockquote>\n+ *\n+ * where `$x_{j}$` is the `j`-th dimension of the point `X` and `$c_{ij}$` is the `j`-th dimension\n+ * of the `i`-th point in cluster `$\\Gamma$`.\n+ *\n+ * Then, we can define the vector:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\xi_{X} : \\xi_{X i} = \\frac{x_{i}}{\\|X\\|}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed for each point and the vector\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\Omega_{\\Gamma} : \\Omega_{\\Gamma i} = \\sum\\limits_{j=1}^N \\xi_{C_{j}i}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed too for each cluster `$\\Gamma$` by its points `$C_{i}$`.\n+ *\n+ * With these definitions, the numerator becomes:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   N - \\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * Thus the average distance of a point `X` to the points of the cluster `$\\Gamma$` is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   1 - \\frac{\\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}}{N}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * In the implementation, the precomputed values for the clusters are distributed among the worker\n+ * nodes via broadcasted variables, because we can assume that the clusters are limited in number.\n+ *\n+ * The main strengths of this algorithm are the low computational complexity and the intrinsic\n+ * parallelism. The precomputed information for each point and for each cluster can be computed\n+ * with a computational complexity which is `O(N/W)`, where `N` is the number of points in the\n+ * dataset and `W` is the number of worker nodes. After that, every point can be analyzed\n+ * independently from the others.\n+ *\n+ * For every point we need to compute the average distance to all the clusters. Since the formula\n+ * above requires `O(D)` operations, this phase has a computational complexity which is\n+ * `O(C*D*N/W)` where `C` is the number of clusters (which we assume quite low), `D` is the number\n+ * of dimensions, `N` is the number of points in the dataset and `W` is the number of worker\n+ * nodes.\n+ */\n+private[evaluation] object CosineSilhouette extends Silhouette {\n+\n+  private[this] var kryoRegistrationPerformed: Boolean = false\n+\n+  private[this] val normalizedFeaturesColName = \"normalizedFeatures\"\n+\n+  /**\n+   * This method registers the class\n+   * [[org.apache.spark.ml.evaluation.CosineSilhouette.ClusterStats]]\n+   * for kryo serialization.\n+   *\n+   * @param sc `SparkContext` to be used\n+   */\n+  def registerKryoClasses(sc: SparkContext): Unit = {\n+    if (!kryoRegistrationPerformed) {\n+      sc.getConf.registerKryoClasses(\n+        Array(\n+          classOf[CosineSilhouette.ClusterStats]\n+        )\n+      )\n+      kryoRegistrationPerformed = true\n+    }\n+  }\n+\n+  case class ClusterStats(normalizedFeatureSum: Vector, numOfPoints: Long)"
  }, {
    "author": {
      "login": "mgaido91"
    },
    "body": "I just thought it was clearer. Do you think it is better to use a Tuple2 for this and Tuple3 for `SquaredEuclideanSilhouette`?",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-02-12T15:35:50Z",
    "diffHunk": "@@ -421,13 +456,220 @@ private[evaluation] object SquaredEuclideanSilhouette {\n       computeSilhouetteCoefficient(bClustersStatsMap, _: Vector, _: Double, _: Double)\n     }\n \n-    val silhouetteScore = dfWithSquaredNorm\n-      .select(avg(\n-        computeSilhouetteCoefficientUDF(\n-          col(featuresCol), col(predictionCol).cast(DoubleType), col(\"squaredNorm\"))\n-      ))\n-      .collect()(0)\n-      .getDouble(0)\n+    val silhouetteScore = overallScore(dfWithSquaredNorm,\n+      computeSilhouetteCoefficientUDF(col(featuresCol), col(predictionCol).cast(DoubleType),\n+        col(\"squaredNorm\")))\n+\n+    bClustersStatsMap.destroy()\n+\n+    silhouetteScore\n+  }\n+}\n+\n+\n+/**\n+ * The algorithm which is implemented in this object, instead, is an efficient and parallel\n+ * implementation of the Silhouette using the cosine distance measure. The cosine distance\n+ * measure is defined as `1 - s` where `s` is the cosine similarity between two points.\n+ *\n+ * The total distance of the point `X` to the points `$C_{i}$` belonging to the cluster `$\\Gamma$`\n+ * is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\sum\\limits_{i=1}^N d(X, C_{i} ) =\n+ *   \\sum\\limits_{i=1}^N \\Big( 1 - \\frac{\\sum\\limits_{j=1}^D x_{j}c_{ij} }{ \\|X\\|\\|C_{i}\\|} \\Big)\n+ *   = \\sum\\limits_{i=1}^N 1 - \\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|}\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|}\n+ *   = N - \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|} \\Big( \\sum\\limits_{i=1}^N\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|} \\Big)\n+ *   $$\n+ * </blockquote>\n+ *\n+ * where `$x_{j}$` is the `j`-th dimension of the point `X` and `$c_{ij}$` is the `j`-th dimension\n+ * of the `i`-th point in cluster `$\\Gamma$`.\n+ *\n+ * Then, we can define the vector:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\xi_{X} : \\xi_{X i} = \\frac{x_{i}}{\\|X\\|}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed for each point and the vector\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\Omega_{\\Gamma} : \\Omega_{\\Gamma i} = \\sum\\limits_{j=1}^N \\xi_{C_{j}i}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed too for each cluster `$\\Gamma$` by its points `$C_{i}$`.\n+ *\n+ * With these definitions, the numerator becomes:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   N - \\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * Thus the average distance of a point `X` to the points of the cluster `$\\Gamma$` is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   1 - \\frac{\\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}}{N}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * In the implementation, the precomputed values for the clusters are distributed among the worker\n+ * nodes via broadcasted variables, because we can assume that the clusters are limited in number.\n+ *\n+ * The main strengths of this algorithm are the low computational complexity and the intrinsic\n+ * parallelism. The precomputed information for each point and for each cluster can be computed\n+ * with a computational complexity which is `O(N/W)`, where `N` is the number of points in the\n+ * dataset and `W` is the number of worker nodes. After that, every point can be analyzed\n+ * independently from the others.\n+ *\n+ * For every point we need to compute the average distance to all the clusters. Since the formula\n+ * above requires `O(D)` operations, this phase has a computational complexity which is\n+ * `O(C*D*N/W)` where `C` is the number of clusters (which we assume quite low), `D` is the number\n+ * of dimensions, `N` is the number of points in the dataset and `W` is the number of worker\n+ * nodes.\n+ */\n+private[evaluation] object CosineSilhouette extends Silhouette {\n+\n+  private[this] var kryoRegistrationPerformed: Boolean = false\n+\n+  private[this] val normalizedFeaturesColName = \"normalizedFeatures\"\n+\n+  /**\n+   * This method registers the class\n+   * [[org.apache.spark.ml.evaluation.CosineSilhouette.ClusterStats]]\n+   * for kryo serialization.\n+   *\n+   * @param sc `SparkContext` to be used\n+   */\n+  def registerKryoClasses(sc: SparkContext): Unit = {\n+    if (!kryoRegistrationPerformed) {\n+      sc.getConf.registerKryoClasses(\n+        Array(\n+          classOf[CosineSilhouette.ClusterStats]\n+        )\n+      )\n+      kryoRegistrationPerformed = true\n+    }\n+  }\n+\n+  case class ClusterStats(normalizedFeatureSum: Vector, numOfPoints: Long)"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "I think it avoids another class to deal with registering, which is the real cost. A one-off 2-field class is about as clear in the case of a pair. For a Tuple3, not as clear. Up to your taste",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-02-13T12:18:22Z",
    "diffHunk": "@@ -421,13 +456,220 @@ private[evaluation] object SquaredEuclideanSilhouette {\n       computeSilhouetteCoefficient(bClustersStatsMap, _: Vector, _: Double, _: Double)\n     }\n \n-    val silhouetteScore = dfWithSquaredNorm\n-      .select(avg(\n-        computeSilhouetteCoefficientUDF(\n-          col(featuresCol), col(predictionCol).cast(DoubleType), col(\"squaredNorm\"))\n-      ))\n-      .collect()(0)\n-      .getDouble(0)\n+    val silhouetteScore = overallScore(dfWithSquaredNorm,\n+      computeSilhouetteCoefficientUDF(col(featuresCol), col(predictionCol).cast(DoubleType),\n+        col(\"squaredNorm\")))\n+\n+    bClustersStatsMap.destroy()\n+\n+    silhouetteScore\n+  }\n+}\n+\n+\n+/**\n+ * The algorithm which is implemented in this object, instead, is an efficient and parallel\n+ * implementation of the Silhouette using the cosine distance measure. The cosine distance\n+ * measure is defined as `1 - s` where `s` is the cosine similarity between two points.\n+ *\n+ * The total distance of the point `X` to the points `$C_{i}$` belonging to the cluster `$\\Gamma$`\n+ * is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\sum\\limits_{i=1}^N d(X, C_{i} ) =\n+ *   \\sum\\limits_{i=1}^N \\Big( 1 - \\frac{\\sum\\limits_{j=1}^D x_{j}c_{ij} }{ \\|X\\|\\|C_{i}\\|} \\Big)\n+ *   = \\sum\\limits_{i=1}^N 1 - \\sum\\limits_{i=1}^N \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|}\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|}\n+ *   = N - \\sum\\limits_{j=1}^D \\frac{x_{j}}{\\|X\\|} \\Big( \\sum\\limits_{i=1}^N\n+ *   \\frac{c_{ij}}{\\|C_{i}\\|} \\Big)\n+ *   $$\n+ * </blockquote>\n+ *\n+ * where `$x_{j}$` is the `j`-th dimension of the point `X` and `$c_{ij}$` is the `j`-th dimension\n+ * of the `i`-th point in cluster `$\\Gamma$`.\n+ *\n+ * Then, we can define the vector:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\xi_{X} : \\xi_{X i} = \\frac{x_{i}}{\\|X\\|}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed for each point and the vector\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   \\Omega_{\\Gamma} : \\Omega_{\\Gamma i} = \\sum\\limits_{j=1}^N \\xi_{C_{j}i}, i = 1, ..., D\n+ *   $$\n+ * </blockquote>\n+ *\n+ * which can be precomputed too for each cluster `$\\Gamma$` by its points `$C_{i}$`.\n+ *\n+ * With these definitions, the numerator becomes:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   N - \\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * Thus the average distance of a point `X` to the points of the cluster `$\\Gamma$` is:\n+ *\n+ * <blockquote>\n+ *   $$\n+ *   1 - \\frac{\\sum\\limits_{j=1}^D \\xi_{X j} \\Omega_{\\Gamma j}}{N}\n+ *   $$\n+ * </blockquote>\n+ *\n+ * In the implementation, the precomputed values for the clusters are distributed among the worker\n+ * nodes via broadcasted variables, because we can assume that the clusters are limited in number.\n+ *\n+ * The main strengths of this algorithm are the low computational complexity and the intrinsic\n+ * parallelism. The precomputed information for each point and for each cluster can be computed\n+ * with a computational complexity which is `O(N/W)`, where `N` is the number of points in the\n+ * dataset and `W` is the number of worker nodes. After that, every point can be analyzed\n+ * independently from the others.\n+ *\n+ * For every point we need to compute the average distance to all the clusters. Since the formula\n+ * above requires `O(D)` operations, this phase has a computational complexity which is\n+ * `O(C*D*N/W)` where `C` is the number of clusters (which we assume quite low), `D` is the number\n+ * of dimensions, `N` is the number of points in the dataset and `W` is the number of worker\n+ * nodes.\n+ */\n+private[evaluation] object CosineSilhouette extends Silhouette {\n+\n+  private[this] var kryoRegistrationPerformed: Boolean = false\n+\n+  private[this] val normalizedFeaturesColName = \"normalizedFeatures\"\n+\n+  /**\n+   * This method registers the class\n+   * [[org.apache.spark.ml.evaluation.CosineSilhouette.ClusterStats]]\n+   * for kryo serialization.\n+   *\n+   * @param sc `SparkContext` to be used\n+   */\n+  def registerKryoClasses(sc: SparkContext): Unit = {\n+    if (!kryoRegistrationPerformed) {\n+      sc.getConf.registerKryoClasses(\n+        Array(\n+          classOf[CosineSilhouette.ClusterStats]\n+        )\n+      )\n+      kryoRegistrationPerformed = true\n+    }\n+  }\n+\n+  case class ClusterStats(normalizedFeatureSum: Vector, numOfPoints: Long)"
  }],
  "prId": 20396
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "From skimming this I wasnt clear why you can only implement it this way for cosine similarity?",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-02-12T15:08:44Z",
    "diffHunk": "@@ -421,13 +456,220 @@ private[evaluation] object SquaredEuclideanSilhouette {\n       computeSilhouetteCoefficient(bClustersStatsMap, _: Vector, _: Double, _: Double)\n     }\n \n-    val silhouetteScore = dfWithSquaredNorm\n-      .select(avg(\n-        computeSilhouetteCoefficientUDF(\n-          col(featuresCol), col(predictionCol).cast(DoubleType), col(\"squaredNorm\"))\n-      ))\n-      .collect()(0)\n-      .getDouble(0)\n+    val silhouetteScore = overallScore(dfWithSquaredNorm,\n+      computeSilhouetteCoefficientUDF(col(featuresCol), col(predictionCol).cast(DoubleType),\n+        col(\"squaredNorm\")))\n+\n+    bClustersStatsMap.destroy()\n+\n+    silhouetteScore\n+  }\n+}\n+\n+\n+/**\n+ * The algorithm which is implemented in this object, instead, is an efficient and parallel",
    "line": 243
  }, {
    "author": {
      "login": "mgaido91"
    },
    "body": "there are 2 reason of my sentence, let me know if it is not clear:\r\n 1. the whole algorithm (all the math steps described) assumes that we are using the cosine distance; for a different distance measure, the algorithm is not valid, even though you can use the same approach to do something similar (as it is done for the squared Euclidean, above);\r\n 2. for some distance measure it is not possible to use this approach - ie. aggregate the score of the points - because math doesn't allow that. For instance, you can't use this approach with the Euclidean distance, since the sqrt prevents any possible aggregation.",
    "commit": "ab34243ec72bd5f0ad05ae6b531d351b2c461d5d",
    "createdAt": "2018-02-12T15:33:08Z",
    "diffHunk": "@@ -421,13 +456,220 @@ private[evaluation] object SquaredEuclideanSilhouette {\n       computeSilhouetteCoefficient(bClustersStatsMap, _: Vector, _: Double, _: Double)\n     }\n \n-    val silhouetteScore = dfWithSquaredNorm\n-      .select(avg(\n-        computeSilhouetteCoefficientUDF(\n-          col(featuresCol), col(predictionCol).cast(DoubleType), col(\"squaredNorm\"))\n-      ))\n-      .collect()(0)\n-      .getDouble(0)\n+    val silhouetteScore = overallScore(dfWithSquaredNorm,\n+      computeSilhouetteCoefficientUDF(col(featuresCol), col(predictionCol).cast(DoubleType),\n+        col(\"squaredNorm\")))\n+\n+    bClustersStatsMap.destroy()\n+\n+    silhouetteScore\n+  }\n+}\n+\n+\n+/**\n+ * The algorithm which is implemented in this object, instead, is an efficient and parallel",
    "line": 243
  }],
  "prId": 20396
}]