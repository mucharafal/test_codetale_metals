[{
  "comments": [{
    "author": {
      "login": "imatiach-msft"
    },
    "body": "can you use Double instead of java.lang.Double?  It should be the scala Double type.",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2017-03-02T15:24:29Z",
    "diffHunk": "@@ -105,20 +106,24 @@ final class Bucketizer @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String\n     transformSchema(dataset.schema)\n     val (filteredDataset, keepInvalid) = {\n       if (getHandleInvalid == Bucketizer.SKIP_INVALID) {\n-        // \"skip\" NaN option is set, will filter out NaN values in the dataset\n+        // \"skip\" NaN/NULL option is set, will filter out NaN/NULL values in the dataset\n         (dataset.na.drop().toDF(), false)\n       } else {\n         (dataset.toDF(), getHandleInvalid == Bucketizer.KEEP_INVALID)\n       }\n     }\n \n-    val bucketizer: UserDefinedFunction = udf { (feature: Double) =>\n-      Bucketizer.binarySearchForBuckets($(splits), feature, keepInvalid)\n+    val bucketizer: UserDefinedFunction = udf { (row: Row) =>\n+      Bucketizer.binarySearchForBuckets(\n+        $(splits),\n+        row.getAs[java.lang.Double]($(inputCol)),"
  }, {
    "author": {
      "login": "crackcell"
    },
    "body": "Hi, Scala's Double will convert null to zero. Say:\r\n\r\n> scala> val a: Double = null.asInstanceOf[Double]\r\n> a: Double = 0.0\r\n\r\nSo I use Java's Double instead to hold NULLs.",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2017-03-02T15:45:42Z",
    "diffHunk": "@@ -105,20 +106,24 @@ final class Bucketizer @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String\n     transformSchema(dataset.schema)\n     val (filteredDataset, keepInvalid) = {\n       if (getHandleInvalid == Bucketizer.SKIP_INVALID) {\n-        // \"skip\" NaN option is set, will filter out NaN values in the dataset\n+        // \"skip\" NaN/NULL option is set, will filter out NaN/NULL values in the dataset\n         (dataset.na.drop().toDF(), false)\n       } else {\n         (dataset.toDF(), getHandleInvalid == Bucketizer.KEEP_INVALID)\n       }\n     }\n \n-    val bucketizer: UserDefinedFunction = udf { (feature: Double) =>\n-      Bucketizer.binarySearchForBuckets($(splits), feature, keepInvalid)\n+    val bucketizer: UserDefinedFunction = udf { (row: Row) =>\n+      Bucketizer.binarySearchForBuckets(\n+        $(splits),\n+        row.getAs[java.lang.Double]($(inputCol)),"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "Ideally we should use `row.getDouble(index)` and `row.isNullAt(index)` together to get values for primitive types, but technically `Row` is just a `Array[Object]`, so there is no performance penalty by using `java.lang.Double`.(this may change in the future, if possible we should prefer `isNullAt` and `getDouble`)",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2017-03-03T19:26:05Z",
    "diffHunk": "@@ -105,20 +106,24 @@ final class Bucketizer @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String\n     transformSchema(dataset.schema)\n     val (filteredDataset, keepInvalid) = {\n       if (getHandleInvalid == Bucketizer.SKIP_INVALID) {\n-        // \"skip\" NaN option is set, will filter out NaN values in the dataset\n+        // \"skip\" NaN/NULL option is set, will filter out NaN/NULL values in the dataset\n         (dataset.na.drop().toDF(), false)\n       } else {\n         (dataset.toDF(), getHandleInvalid == Bucketizer.KEEP_INVALID)\n       }\n     }\n \n-    val bucketizer: UserDefinedFunction = udf { (feature: Double) =>\n-      Bucketizer.binarySearchForBuckets($(splits), feature, keepInvalid)\n+    val bucketizer: UserDefinedFunction = udf { (row: Row) =>\n+      Bucketizer.binarySearchForBuckets(\n+        $(splits),\n+        row.getAs[java.lang.Double]($(inputCol)),"
  }],
  "prId": 17123
}, {
  "comments": [{
    "author": {
      "login": "imatiach-msft"
    },
    "body": "could you please add some tests to validate that NULL values can now be handled in addition to NaN values by the bucketizer?",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2017-03-02T15:25:15Z",
    "diffHunk": "@@ -171,23 +176,23 @@ object Bucketizer extends DefaultParamsReadable[Bucketizer] {\n    * Binary searching in several buckets to place each data point.\n    * @param splits array of split points\n    * @param feature data point\n-   * @param keepInvalid NaN flag.\n-   *                    Set \"true\" to make an extra bucket for NaN values;\n-   *                    Set \"false\" to report an error for NaN values\n+   * @param keepInvalid NaN/NULL flag.\n+   *                    Set \"true\" to make an extra bucket for NaN/NULL values;\n+   *                    Set \"false\" to report an error for NaN/NULL values\n    * @return bucket for each data point\n    * @throws SparkException if a feature is < splits.head or > splits.last\n    */\n \n   private[feature] def binarySearchForBuckets(\n       splits: Array[Double],\n-      feature: Double,\n+      feature: java.lang.Double,\n       keepInvalid: Boolean): Double = {\n-    if (feature.isNaN) {\n+    if (feature == null || feature.isNaN) {\n       if (keepInvalid) {\n         splits.length - 1\n       } else {\n-        throw new SparkException(\"Bucketizer encountered NaN value. To handle or skip NaNs,\" +\n-          \" try setting Bucketizer.handleInvalid.\")\n+        throw new SparkException(\"Bucketizer encountered NaN/NULL values. \" +\n+          \"To handle or skip NaNs/NULLs, try setting Bucketizer.handleInvalid.\")\n       }\n     } else if (feature == splits.last) {"
  }, {
    "author": {
      "login": "crackcell"
    },
    "body": "My fault! I'll do it now!",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2017-03-02T15:46:38Z",
    "diffHunk": "@@ -171,23 +176,23 @@ object Bucketizer extends DefaultParamsReadable[Bucketizer] {\n    * Binary searching in several buckets to place each data point.\n    * @param splits array of split points\n    * @param feature data point\n-   * @param keepInvalid NaN flag.\n-   *                    Set \"true\" to make an extra bucket for NaN values;\n-   *                    Set \"false\" to report an error for NaN values\n+   * @param keepInvalid NaN/NULL flag.\n+   *                    Set \"true\" to make an extra bucket for NaN/NULL values;\n+   *                    Set \"false\" to report an error for NaN/NULL values\n    * @return bucket for each data point\n    * @throws SparkException if a feature is < splits.head or > splits.last\n    */\n \n   private[feature] def binarySearchForBuckets(\n       splits: Array[Double],\n-      feature: Double,\n+      feature: java.lang.Double,\n       keepInvalid: Boolean): Double = {\n-    if (feature.isNaN) {\n+    if (feature == null || feature.isNaN) {\n       if (keepInvalid) {\n         splits.length - 1\n       } else {\n-        throw new SparkException(\"Bucketizer encountered NaN value. To handle or skip NaNs,\" +\n-          \" try setting Bucketizer.handleInvalid.\")\n+        throw new SparkException(\"Bucketizer encountered NaN/NULL values. \" +\n+          \"To handle or skip NaNs/NULLs, try setting Bucketizer.handleInvalid.\")\n       }\n     } else if (feature == splits.last) {"
  }],
  "prId": 17123
}, {
  "comments": [{
    "author": {
      "login": "imatiach-msft"
    },
    "body": "Double here as well",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2017-03-02T15:25:28Z",
    "diffHunk": "@@ -171,23 +176,23 @@ object Bucketizer extends DefaultParamsReadable[Bucketizer] {\n    * Binary searching in several buckets to place each data point.\n    * @param splits array of split points\n    * @param feature data point\n-   * @param keepInvalid NaN flag.\n-   *                    Set \"true\" to make an extra bucket for NaN values;\n-   *                    Set \"false\" to report an error for NaN values\n+   * @param keepInvalid NaN/NULL flag.\n+   *                    Set \"true\" to make an extra bucket for NaN/NULL values;\n+   *                    Set \"false\" to report an error for NaN/NULL values\n    * @return bucket for each data point\n    * @throws SparkException if a feature is < splits.head or > splits.last\n    */\n \n   private[feature] def binarySearchForBuckets(\n       splits: Array[Double],\n-      feature: Double,\n+      feature: java.lang.Double,"
  }, {
    "author": {
      "login": "WeichenXu123"
    },
    "body": "Also change to `Option[Double]` here.",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2018-01-19T18:57:33Z",
    "diffHunk": "@@ -171,23 +176,23 @@ object Bucketizer extends DefaultParamsReadable[Bucketizer] {\n    * Binary searching in several buckets to place each data point.\n    * @param splits array of split points\n    * @param feature data point\n-   * @param keepInvalid NaN flag.\n-   *                    Set \"true\" to make an extra bucket for NaN values;\n-   *                    Set \"false\" to report an error for NaN values\n+   * @param keepInvalid NaN/NULL flag.\n+   *                    Set \"true\" to make an extra bucket for NaN/NULL values;\n+   *                    Set \"false\" to report an error for NaN/NULL values\n    * @return bucket for each data point\n    * @throws SparkException if a feature is < splits.head or > splits.last\n    */\n \n   private[feature] def binarySearchForBuckets(\n       splits: Array[Double],\n-      feature: Double,\n+      feature: java.lang.Double,"
  }],
  "prId": 17123
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "actually, can we just use `java.lang.Double` as the type for `feature`? Then we don't need to change https://github.com/apache/spark/pull/17123/files#diff-37f2c93b88c73b91cdc9e40fc8c45fc5R121",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2017-03-06T08:29:44Z",
    "diffHunk": "@@ -105,20 +106,21 @@ final class Bucketizer @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String\n     transformSchema(dataset.schema)\n     val (filteredDataset, keepInvalid) = {\n       if (getHandleInvalid == Bucketizer.SKIP_INVALID) {\n-        // \"skip\" NaN option is set, will filter out NaN values in the dataset\n+        // \"skip\" NaN/NULL option is set, will filter out NaN/NULL values in the dataset\n         (dataset.na.drop().toDF(), false)\n       } else {\n         (dataset.toDF(), getHandleInvalid == Bucketizer.KEEP_INVALID)\n       }\n     }\n \n-    val bucketizer: UserDefinedFunction = udf { (feature: Double) =>"
  }, {
    "author": {
      "login": "crackcell"
    },
    "body": "Use both Java and Scala types seems less graceful. Instead, is it better a way to pass a `Row` to  `bucketizer()` and then check NULLs with `isNullAt()` and `getDouble()` ?",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2017-03-06T15:08:55Z",
    "diffHunk": "@@ -105,20 +106,21 @@ final class Bucketizer @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String\n     transformSchema(dataset.schema)\n     val (filteredDataset, keepInvalid) = {\n       if (getHandleInvalid == Bucketizer.SKIP_INVALID) {\n-        // \"skip\" NaN option is set, will filter out NaN values in the dataset\n+        // \"skip\" NaN/NULL option is set, will filter out NaN/NULL values in the dataset\n         (dataset.na.drop().toDF(), false)\n       } else {\n         (dataset.toDF(), getHandleInvalid == Bucketizer.KEEP_INVALID)\n       }\n     }\n \n-    val bucketizer: UserDefinedFunction = udf { (feature: Double) =>"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "see the document of `ScalaUDF`, if you don't like mixing java and scala types, you can use `Option[Double]`",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2017-03-06T21:18:04Z",
    "diffHunk": "@@ -105,20 +106,21 @@ final class Bucketizer @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String\n     transformSchema(dataset.schema)\n     val (filteredDataset, keepInvalid) = {\n       if (getHandleInvalid == Bucketizer.SKIP_INVALID) {\n-        // \"skip\" NaN option is set, will filter out NaN values in the dataset\n+        // \"skip\" NaN/NULL option is set, will filter out NaN/NULL values in the dataset\n         (dataset.na.drop().toDF(), false)\n       } else {\n         (dataset.toDF(), getHandleInvalid == Bucketizer.KEEP_INVALID)\n       }\n     }\n \n-    val bucketizer: UserDefinedFunction = udf { (feature: Double) =>"
  }, {
    "author": {
      "login": "crackcell"
    },
    "body": "Thanks a lot. `Option[Double]` is much better. :-)",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2017-03-07T01:56:04Z",
    "diffHunk": "@@ -105,20 +106,21 @@ final class Bucketizer @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String\n     transformSchema(dataset.schema)\n     val (filteredDataset, keepInvalid) = {\n       if (getHandleInvalid == Bucketizer.SKIP_INVALID) {\n-        // \"skip\" NaN option is set, will filter out NaN values in the dataset\n+        // \"skip\" NaN/NULL option is set, will filter out NaN/NULL values in the dataset\n         (dataset.na.drop().toDF(), false)\n       } else {\n         (dataset.toDF(), getHandleInvalid == Bucketizer.KEEP_INVALID)\n       }\n     }\n \n-    val bucketizer: UserDefinedFunction = udf { (feature: Double) =>"
  }, {
    "author": {
      "login": "WeichenXu123"
    },
    "body": "As @cloud-fan suggested, `Option[Double]` is better. :-)",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2018-01-19T18:57:15Z",
    "diffHunk": "@@ -105,20 +106,21 @@ final class Bucketizer @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String\n     transformSchema(dataset.schema)\n     val (filteredDataset, keepInvalid) = {\n       if (getHandleInvalid == Bucketizer.SKIP_INVALID) {\n-        // \"skip\" NaN option is set, will filter out NaN values in the dataset\n+        // \"skip\" NaN/NULL option is set, will filter out NaN/NULL values in the dataset\n         (dataset.na.drop().toDF(), false)\n       } else {\n         (dataset.toDF(), getHandleInvalid == Bucketizer.KEEP_INVALID)\n       }\n     }\n \n-    val bucketizer: UserDefinedFunction = udf { (feature: Double) =>"
  }],
  "prId": 17123
}, {
  "comments": [{
    "author": {
      "login": "imatiach-msft"
    },
    "body": "I believe you should try to avoid using a udf on a row because the serialization costs will be more expensive... hmm how could we make this perform well and handle nulls?  Does it work with Option[Double] instead of Row?",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2017-03-16T05:04:13Z",
    "diffHunk": "@@ -105,20 +106,21 @@ final class Bucketizer @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String\n     transformSchema(dataset.schema)\n     val (filteredDataset, keepInvalid) = {\n       if (getHandleInvalid == Bucketizer.SKIP_INVALID) {\n-        // \"skip\" NaN option is set, will filter out NaN values in the dataset\n+        // \"skip\" NaN/NULL option is set, will filter out NaN/NULL values in the dataset\n         (dataset.na.drop().toDF(), false)\n       } else {\n         (dataset.toDF(), getHandleInvalid == Bucketizer.KEEP_INVALID)\n       }\n     }\n \n-    val bucketizer: UserDefinedFunction = udf { (feature: Double) =>\n+    val bucketizer: UserDefinedFunction = udf { (row: Row) =>"
  }, {
    "author": {
      "login": "crackcell"
    },
    "body": "Thanks for pointing out the performace problem. Maybe my original code will work better to use java.lang.Double instead of scala's Double to hold NULLs.",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2017-03-16T08:44:54Z",
    "diffHunk": "@@ -105,20 +106,21 @@ final class Bucketizer @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String\n     transformSchema(dataset.schema)\n     val (filteredDataset, keepInvalid) = {\n       if (getHandleInvalid == Bucketizer.SKIP_INVALID) {\n-        // \"skip\" NaN option is set, will filter out NaN values in the dataset\n+        // \"skip\" NaN/NULL option is set, will filter out NaN/NULL values in the dataset\n         (dataset.na.drop().toDF(), false)\n       } else {\n         (dataset.toDF(), getHandleInvalid == Bucketizer.KEEP_INVALID)\n       }\n     }\n \n-    val bucketizer: UserDefinedFunction = udf { (feature: Double) =>\n+    val bucketizer: UserDefinedFunction = udf { (row: Row) =>"
  }],
  "prId": 17123
}, {
  "comments": [{
    "author": {
      "login": "imatiach-msft"
    },
    "body": "I think you can equivalently write this as:\r\nif (feature.isEmpty) { ....",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2017-03-16T05:07:32Z",
    "diffHunk": "@@ -171,34 +173,34 @@ object Bucketizer extends DefaultParamsReadable[Bucketizer] {\n    * Binary searching in several buckets to place each data point.\n    * @param splits array of split points\n    * @param feature data point\n-   * @param keepInvalid NaN flag.\n-   *                    Set \"true\" to make an extra bucket for NaN values;\n-   *                    Set \"false\" to report an error for NaN values\n+   * @param keepInvalid NaN/NULL flag.\n+   *                    Set \"true\" to make an extra bucket for NaN/NULL values;\n+   *                    Set \"false\" to report an error for NaN/NULL values\n    * @return bucket for each data point\n    * @throws SparkException if a feature is < splits.head or > splits.last\n    */\n \n   private[feature] def binarySearchForBuckets(\n       splits: Array[Double],\n-      feature: Double,\n+      feature: Option[Double],\n       keepInvalid: Boolean): Double = {\n-    if (feature.isNaN) {\n+    if (feature.getOrElse(Double.NaN).isNaN) {"
  }],
  "prId": 17123
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "This sounds like a behavior change, we should add an item in migration guide of ML docs.",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2018-01-22T08:52:12Z",
    "diffHunk": "@@ -53,7 +53,8 @@ final class Bucketizer @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String\n    * Values at -inf, inf must be explicitly provided to cover all Double values;\n    * otherwise, values outside the splits specified will be treated as errors.\n    *\n-   * See also [[handleInvalid]], which can optionally create an additional bucket for NaN values.\n+   * See also [[handleInvalid]], which can optionally create an additional bucket for NaN/NULL",
    "line": 5
  }, {
    "author": {
      "login": "crackcell"
    },
    "body": "@viirya done.",
    "commit": "063327c21a3d807a5cadcfc959b6ca7d0415a272",
    "createdAt": "2018-01-22T09:43:00Z",
    "diffHunk": "@@ -53,7 +53,8 @@ final class Bucketizer @Since(\"1.4.0\") (@Since(\"1.4.0\") override val uid: String\n    * Values at -inf, inf must be explicitly provided to cover all Double values;\n    * otherwise, values outside the splits specified will be treated as errors.\n    *\n-   * See also [[handleInvalid]], which can optionally create an additional bucket for NaN values.\n+   * See also [[handleInvalid]], which can optionally create an additional bucket for NaN/NULL",
    "line": 5
  }],
  "prId": 17123
}]