[{
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "What about `SVMClassifier`? we can also train regression model with SVM.\n",
    "commit": "bbcb7cbcd17badced9936dfe0c8df24ae9059ceb",
    "createdAt": "2016-09-29T13:32:34Z",
    "diffHunk": "@@ -0,0 +1,527 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, DiffFunction, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+\n+/** Params for SVM. */\n+private[ml] trait SVMParams extends PredictorParams with HasRegParam with HasMaxIter\n+  with HasFitIntercept with HasTol with HasStandardization with HasWeightCol with HasThreshold\n+  with HasAggregationDepth {\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * SVM with Hinge and OWLQN.\n+ * Currently only Binary classification is supported.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class SVM @Since(\"2.1.0\") ("
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "I changed it to LinearSVC, so we can have other SVM Classifier in the future.",
    "commit": "bbcb7cbcd17badced9936dfe0c8df24ae9059ceb",
    "createdAt": "2016-12-15T01:22:01Z",
    "diffHunk": "@@ -0,0 +1,527 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, DiffFunction, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+\n+/** Params for SVM. */\n+private[ml] trait SVMParams extends PredictorParams with HasRegParam with HasMaxIter\n+  with HasFitIntercept with HasTol with HasStandardization with HasWeightCol with HasThreshold\n+  with HasAggregationDepth {\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * SVM with Hinge and OWLQN.\n+ * Currently only Binary classification is supported.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class SVM @Since(\"2.1.0\") ("
  }],
  "prId": 15211
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "Under the framework `Classifier` or `ProbabilisticClassifier`?\n",
    "commit": "bbcb7cbcd17badced9936dfe0c8df24ae9059ceb",
    "createdAt": "2016-09-29T13:33:22Z",
    "diffHunk": "@@ -0,0 +1,527 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, DiffFunction, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+\n+/** Params for SVM. */\n+private[ml] trait SVMParams extends PredictorParams with HasRegParam with HasMaxIter\n+  with HasFitIntercept with HasTol with HasStandardization with HasWeightCol with HasThreshold\n+  with HasAggregationDepth {\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * SVM with Hinge and OWLQN.\n+ * Currently only Binary classification is supported.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class SVM @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends Predictor[Vector, SVM, SVMModel]"
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "Thanks I changed it to Classifier. AFAIK, SVM raw result may not be used to indicate the probability.",
    "commit": "bbcb7cbcd17badced9936dfe0c8df24ae9059ceb",
    "createdAt": "2016-12-15T01:23:27Z",
    "diffHunk": "@@ -0,0 +1,527 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, DiffFunction, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+\n+/** Params for SVM. */\n+private[ml] trait SVMParams extends PredictorParams with HasRegParam with HasMaxIter\n+  with HasFitIntercept with HasTol with HasStandardization with HasWeightCol with HasThreshold\n+  with HasAggregationDepth {\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * SVM with Hinge and OWLQN.\n+ * Currently only Binary classification is supported.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class SVM @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends Predictor[Vector, SVM, SVMModel]"
  }],
  "prId": 15211
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "`labelCol` is now not need to be casted to `DoubleType`, because it is casted in `Predictor.fit()`\nsee https://github.com/apache/spark/pull/15414\n",
    "commit": "bbcb7cbcd17badced9936dfe0c8df24ae9059ceb",
    "createdAt": "2016-11-04T07:08:50Z",
    "diffHunk": "@@ -0,0 +1,527 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, DiffFunction, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+\n+/** Params for SVM. */\n+private[ml] trait SVMParams extends PredictorParams with HasRegParam with HasMaxIter\n+  with HasFitIntercept with HasTol with HasStandardization with HasWeightCol with HasThreshold\n+  with HasAggregationDepth {\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * SVM with Hinge and OWLQN.\n+ * Currently only Binary classification is supported.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class SVM @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends Predictor[Vector, SVM, SVMModel]\n+  with SVMParams with DefaultParamsWritable {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"svm\"))\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-4.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): SVM = defaultCopy(extra)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  /**\n+   * Train a model using the given dataset and parameters.\n+   * Developers can implement this instead of [[fit()]] to avoid dealing with schema validation\n+   * and copying parameters into the model.\n+   *\n+   * @param dataset Training dataset\n+   * @return Fitted model\n+   */\n+  override protected def train(dataset: Dataset[_]): SVMModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {"
  }, {
    "author": {
      "login": "hhbyyh"
    },
    "body": "Yes, thanks.",
    "commit": "bbcb7cbcd17badced9936dfe0c8df24ae9059ceb",
    "createdAt": "2016-12-15T18:09:22Z",
    "diffHunk": "@@ -0,0 +1,527 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, DiffFunction, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+\n+/** Params for SVM. */\n+private[ml] trait SVMParams extends PredictorParams with HasRegParam with HasMaxIter\n+  with HasFitIntercept with HasTol with HasStandardization with HasWeightCol with HasThreshold\n+  with HasAggregationDepth {\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * SVM with Hinge and OWLQN.\n+ * Currently only Binary classification is supported.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class SVM @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends Predictor[Vector, SVM, SVMModel]\n+  with SVMParams with DefaultParamsWritable {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"svm\"))\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-4.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): SVM = defaultCopy(extra)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  /**\n+   * Train a model using the given dataset and parameters.\n+   * Developers can implement this instead of [[fit()]] to avoid dealing with schema validation\n+   * and copying parameters into the model.\n+   *\n+   * @param dataset Training dataset\n+   * @return Fitted model\n+   */\n+  override protected def train(dataset: Dataset[_]): SVMModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {"
  }],
  "prId": 15211
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "if we rename this to `SVMClassifier`, the uid should be `svc`\n",
    "commit": "bbcb7cbcd17badced9936dfe0c8df24ae9059ceb",
    "createdAt": "2016-11-04T07:09:49Z",
    "diffHunk": "@@ -0,0 +1,527 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, DiffFunction, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+\n+/** Params for SVM. */\n+private[ml] trait SVMParams extends PredictorParams with HasRegParam with HasMaxIter\n+  with HasFitIntercept with HasTol with HasStandardization with HasWeightCol with HasThreshold\n+  with HasAggregationDepth {\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * SVM with Hinge and OWLQN.\n+ * Currently only Binary classification is supported.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class SVM @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends Predictor[Vector, SVM, SVMModel]\n+  with SVMParams with DefaultParamsWritable {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"svm\"))"
  }],
  "prId": 15211
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "To keep in line with other algos, `labelCol`, `weightCol`, `featuresCol` should be added here\n",
    "commit": "bbcb7cbcd17badced9936dfe0c8df24ae9059ceb",
    "createdAt": "2016-11-04T07:14:09Z",
    "diffHunk": "@@ -0,0 +1,527 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, DiffFunction, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+\n+/** Params for SVM. */\n+private[ml] trait SVMParams extends PredictorParams with HasRegParam with HasMaxIter\n+  with HasFitIntercept with HasTol with HasStandardization with HasWeightCol with HasThreshold\n+  with HasAggregationDepth {\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * SVM with Hinge and OWLQN.\n+ * Currently only Binary classification is supported.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class SVM @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends Predictor[Vector, SVM, SVMModel]\n+  with SVMParams with DefaultParamsWritable {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"svm\"))\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-4.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): SVM = defaultCopy(extra)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  /**\n+   * Train a model using the given dataset and parameters.\n+   * Developers can implement this instead of [[fit()]] to avoid dealing with schema validation\n+   * and copying parameters into the model.\n+   *\n+   * @param dataset Training dataset\n+   * @return Fitted model\n+   */\n+  override protected def train(dataset: Dataset[_]): SVMModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, standardization, threshold, maxIter, tol, fitIntercept)"
  }],
  "prId": 15211
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "`require(numClasses == 2, \"...\")` ?\n",
    "commit": "bbcb7cbcd17badced9936dfe0c8df24ae9059ceb",
    "createdAt": "2016-11-04T07:17:16Z",
    "diffHunk": "@@ -0,0 +1,527 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, DiffFunction, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+\n+/** Params for SVM. */\n+private[ml] trait SVMParams extends PredictorParams with HasRegParam with HasMaxIter\n+  with HasFitIntercept with HasTol with HasStandardization with HasWeightCol with HasThreshold\n+  with HasAggregationDepth {\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * SVM with Hinge and OWLQN.\n+ * Currently only Binary classification is supported.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class SVM @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends Predictor[Vector, SVM, SVMModel]\n+  with SVMParams with DefaultParamsWritable {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"svm\"))\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-4.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): SVM = defaultCopy(extra)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  /**\n+   * Train a model using the given dataset and parameters.\n+   * Developers can implement this instead of [[fit()]] to avoid dealing with schema validation\n+   * and copying parameters into the model.\n+   *\n+   * @param dataset Training dataset\n+   * @return Fitted model\n+   */\n+  override protected def train(dataset: Dataset[_]): SVMModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, standardization, threshold, maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        instance: Instance) =>\n+          (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer\n+      )(seqOp, combOp, $(aggregationDepth))\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}.\")\n+        n\n+      case None => histogram.length\n+    }\n+"
  }],
  "prId": 15211
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "what about moving checking of `numClasses` outside of `add()`? \n",
    "commit": "bbcb7cbcd17badced9936dfe0c8df24ae9059ceb",
    "createdAt": "2016-11-04T07:27:32Z",
    "diffHunk": "@@ -0,0 +1,527 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.classification\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.{CachedDiffFunction, DiffFunction, OWLQN => BreezeOWLQN}\n+import org.apache.hadoop.fs.Path\n+import scala.collection.mutable\n+\n+import org.apache.spark.SparkException\n+import org.apache.spark.annotation.{Experimental, Since}\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.stat.MultivariateOnlineSummarizer\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.{col, lit}\n+import org.apache.spark.sql.types.DoubleType\n+\n+/** Params for SVM. */\n+private[ml] trait SVMParams extends PredictorParams with HasRegParam with HasMaxIter\n+  with HasFitIntercept with HasTol with HasStandardization with HasWeightCol with HasThreshold\n+  with HasAggregationDepth {\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * SVM with Hinge and OWLQN.\n+ * Currently only Binary classification is supported.\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class SVM @Since(\"2.1.0\") (\n+    @Since(\"2.1.0\") override val uid: String)\n+  extends Predictor[Vector, SVM, SVMModel]\n+  with SVMParams with DefaultParamsWritable {\n+\n+  @Since(\"2.1.0\")\n+  def this() = this(Identifiable.randomUID(\"svm\"))\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Smaller value will lead to higher accuracy with the cost of more iterations.\n+   * Default is 1E-4.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): SVM = defaultCopy(extra)\n+\n+  /**\n+   * Sets the value of param [[weightCol]].\n+   * If this is not set or empty, we treat all instance weights as 1.0.\n+   * Default is not set, so all instances have weight one.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"2.1.0\")\n+  def setWeightCol(value: String): this.type = set(weightCol, value)\n+\n+  /**\n+   * Train a model using the given dataset and parameters.\n+   * Developers can implement this instead of [[fit()]] to avoid dealing with schema validation\n+   * and copying parameters into the model.\n+   *\n+   * @param dataset Training dataset\n+   * @return Fitted model\n+   */\n+  override protected def train(dataset: Dataset[_]): SVMModel = {\n+    val w = if (!isDefined(weightCol) || $(weightCol).isEmpty) lit(1.0) else col($(weightCol))\n+    val instances: RDD[Instance] =\n+      dataset.select(col($(labelCol)).cast(DoubleType), w, col($(featuresCol))).rdd.map {\n+        case Row(label: Double, weight: Double, features: Vector) =>\n+          Instance(label, weight, features)\n+      }\n+\n+    val instr = Instrumentation.create(this, instances)\n+    instr.logParams(regParam, standardization, threshold, maxIter, tol, fitIntercept)\n+\n+    val (summarizer, labelSummarizer) = {\n+      val seqOp = (c: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        instance: Instance) =>\n+          (c._1.add(instance.features, instance.weight), c._2.add(instance.label, instance.weight))\n+\n+      val combOp = (c1: (MultivariateOnlineSummarizer, MultiClassSummarizer),\n+        c2: (MultivariateOnlineSummarizer, MultiClassSummarizer)) =>\n+          (c1._1.merge(c2._1), c1._2.merge(c2._2))\n+\n+      instances.treeAggregate(\n+        new MultivariateOnlineSummarizer, new MultiClassSummarizer\n+      )(seqOp, combOp, $(aggregationDepth))\n+    }\n+\n+    val histogram = labelSummarizer.histogram\n+    val numInvalid = labelSummarizer.countInvalid\n+    val numFeatures = summarizer.mean.size\n+    val numFeaturesPlusIntercept = if (getFitIntercept) numFeatures + 1 else numFeatures\n+\n+    val numClasses = MetadataUtils.getNumClasses(dataset.schema($(labelCol))) match {\n+      case Some(n: Int) =>\n+        require(n >= histogram.length, s\"Specified number of classes $n was \" +\n+          s\"less than the number of unique labels ${histogram.length}.\")\n+        n\n+      case None => histogram.length\n+    }\n+\n+    instr.logNumClasses(numClasses)\n+    instr.logNumFeatures(numFeatures)\n+\n+    val (coefficientMatrix, interceptVector, objectiveHistory) = {\n+      if (numInvalid != 0) {\n+        val msg = s\"Classification labels should be in [0 to ${numClasses - 1}]. \" +\n+          s\"Found $numInvalid invalid labels.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      val featuresStd = summarizer.variance.toArray.map(math.sqrt)\n+      val regParamL2 = $(regParam)\n+      val bcFeaturesStd = instances.context.broadcast(featuresStd)\n+      val costFun = new SVMCostFun(instances, numClasses, $(fitIntercept),\n+        $(standardization), bcFeaturesStd, regParamL2, false,\n+        $(aggregationDepth))\n+\n+      def regParamL1Fun = (index: Int) => 0D\n+      val optimizer = new BreezeOWLQN[Int, BDV[Double]]($(maxIter), 10, regParamL1Fun, $(tol))\n+\n+      val initialCoefficientsWithIntercept = Vectors.zeros(numFeaturesPlusIntercept)\n+\n+      initialCoefficientsWithIntercept.toArray(numFeatures) = math.log(\n+        histogram(1) / histogram(0))\n+\n+      val states = optimizer.iterations(new CachedDiffFunction(costFun),\n+        initialCoefficientsWithIntercept.asBreeze.toDenseVector)\n+\n+      val arrayBuilder = mutable.ArrayBuilder.make[Double]\n+      var state: optimizer.State = null\n+      while (states.hasNext) {\n+        state = states.next()\n+        arrayBuilder += state.adjustedValue\n+      }\n+\n+      bcFeaturesStd.destroy(blocking = false)\n+      if (state == null) {\n+        val msg = s\"${optimizer.getClass.getName} failed.\"\n+        logError(msg)\n+        throw new SparkException(msg)\n+      }\n+\n+      /*\n+         The coefficients are trained in the scaled space; we're converting them back to\n+         the original space.\n+         Note that the intercept in scaled space and original space is the same;\n+         as a result, no scaling is needed.\n+       */\n+      val rawCoefficients = state.x.toArray.clone()\n+      val coefficientArray = Array.tabulate(numFeatures) { i =>\n+        // flatIndex will loop though rawCoefficients, and skip the intercept terms.\n+        val flatIndex = if ($(fitIntercept)) i + i / numFeatures else i\n+        val featureIndex = i % numFeatures\n+        if (featuresStd(featureIndex) != 0.0) {\n+          rawCoefficients(flatIndex) / featuresStd(featureIndex)\n+        } else {\n+          0.0\n+        }\n+      }\n+\n+      val intercept = rawCoefficients(numFeaturesPlusIntercept - 1)\n+      (Vectors.dense(coefficientArray), intercept, arrayBuilder.result())\n+    }\n+\n+    val model = copyValues(new SVMModel(uid, coefficientMatrix, interceptVector))\n+    instr.logSuccess(model)\n+    model\n+  }\n+}\n+\n+@Since(\"2.1.0\")\n+object SVM extends DefaultParamsReadable[SVM] {\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): SVM = super.load(path)\n+}\n+\n+/**\n+ * :: Experimental ::\n+ * SVM Model trained by [[SVM]]\n+ */\n+@Since(\"2.1.0\")\n+@Experimental\n+class SVMModel private[ml] (\n+    @Since(\"2.1.0\") override val uid: String,\n+    @Since(\"2.1.0\") val weights: Vector,\n+    @Since(\"2.1.0\") val intercept: Double)\n+  extends PredictionModel[Vector, SVMModel]\n+  with SVMParams with MLWritable {\n+\n+  /**\n+   * Predict label for the given features.\n+   * This internal method is used to implement [[transform()]] and output [[predictionCol]].\n+   */\n+  override protected def predict(features: Vector): Double = {\n+    val margin = features.asBreeze.dot(weights.asBreeze) + intercept\n+    if (margin > $(threshold)) 1.0 else 0.0\n+  }\n+\n+  @Since(\"2.1.0\")\n+  override def copy(extra: ParamMap): SVMModel = {\n+    copyValues(new SVMModel(uid, weights, intercept), extra)\n+  }\n+\n+  /**\n+   * Returns a [[org.apache.spark.ml.util.MLWriter]] instance for this ML instance.\n+   */\n+  @Since(\"2.1.0\")\n+  override def write: MLWriter = new SVMModel.SVMModelWriter(this)\n+\n+}\n+\n+\n+@Since(\"2.1.0\")\n+object SVMModel extends MLReadable[SVMModel] {\n+\n+  @Since(\"2.1.0\")\n+  override def read: MLReader[SVMModel] = new SVMModelReader\n+\n+  @Since(\"2.1.0\")\n+  override def load(path: String): SVMModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[SVMModel]] */\n+  private[SVMModel]\n+  class SVMModelWriter(instance: SVMModel)\n+    extends MLWriter with Logging {\n+\n+    private case class Data(\n+        coefficients: Vector,\n+        intercept: Double)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      // Save metadata and Params\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      // Save model data: numClasses, numFeatures, intercept, coefficients\n+      val data = Data(instance.weights, instance.intercept)\n+      val dataPath = new Path(path, \"data\").toString\n+      sparkSession.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class SVMModelReader extends MLReader[SVMModel] {\n+\n+    /** Checked against metadata when loading model */\n+    private val className = classOf[SVMModel].getName\n+\n+    override def load(path: String): SVMModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sparkSession.read.format(\"parquet\").load(dataPath)\n+      val Row(coefficients: Vector, intercept: Double) =\n+        data.select(\"coefficients\", \"intercept\").head()\n+      val model = new SVMModel(metadata.uid, coefficients, intercept)\n+      DefaultParamsReader.getAndSetParams(model, metadata)\n+      model\n+    }\n+  }\n+}\n+\n+\n+/**\n+ * SVMCostFun implements Breeze's DiffFunction[T] for hinge loss function\n+ */\n+private class SVMCostFun(\n+    instances: RDD[Instance],\n+    numClasses: Int,\n+    fitIntercept: Boolean,\n+    standardization: Boolean,\n+    bcFeaturesStd: Broadcast[Array[Double]],\n+    regParamL2: Double,\n+    multinomial: Boolean,\n+    aggregationDepth: Int) extends DiffFunction[BDV[Double]] {\n+\n+  override def calculate(coefficients: BDV[Double]): (Double, BDV[Double]) = {\n+    val coeffs = Vectors.fromBreeze(coefficients)\n+    val bcCoeffs = instances.context.broadcast(coeffs)\n+    val featuresStd = bcFeaturesStd.value\n+    val numFeatures = featuresStd.length\n+\n+    val svmAggregator = {\n+      val seqOp = (c: SVMAggregator, instance: Instance) => c.add(instance)\n+      val combOp = (c1: SVMAggregator, c2: SVMAggregator) => c1.merge(c2)\n+\n+      instances.treeAggregate(\n+        new SVMAggregator(bcCoeffs, bcFeaturesStd, numClasses, fitIntercept, false)\n+      )(seqOp, combOp, aggregationDepth)\n+    }\n+\n+    val totalGradientArray = svmAggregator.gradient.toArray\n+    // regVal is the sum of coefficients squares excluding intercept for L2 regularization.\n+    val regVal = if (regParamL2 == 0.0) {\n+      0.0\n+    } else {\n+      var sum = 0.0\n+      coeffs.foreachActive { case (index, value) =>\n+        // We do not apply regularization to the intercepts\n+        val isIntercept = fitIntercept && ((index + 1) % (numFeatures + 1) == 0)\n+        if (!isIntercept) {\n+          // The following code will compute the loss of the regularization; also\n+          // the gradient of the regularization, and add back to totalGradientArray.\n+          sum += {\n+            if (standardization) {\n+              totalGradientArray(index) += regParamL2 * value\n+              value * value\n+            } else {\n+              val featureIndex = if (fitIntercept) {\n+                index % (numFeatures + 1)\n+              } else {\n+                index % numFeatures\n+              }\n+              if (featuresStd(featureIndex) != 0.0) {\n+                // If `standardization` is false, we still standardize the data\n+                // to improve the rate of convergence; as a result, we have to\n+                // perform this reverse standardization by penalizing each component\n+                // differently to get effectively the same objective function when\n+                // the training dataset is not standardized.\n+                val temp = value / (featuresStd(featureIndex) * featuresStd(featureIndex))\n+                totalGradientArray(index) += regParamL2 * temp\n+                value * temp\n+              } else {\n+                0.0\n+              }\n+            }\n+          }\n+        }\n+      }\n+      0.5 * regParamL2 * sum\n+    }\n+    bcCoeffs.destroy(blocking = false)\n+\n+    (svmAggregator.loss + regVal, new BDV(totalGradientArray))\n+  }\n+}\n+\n+\n+/**\n+ * SVMAggregator computes the gradient and loss for hinge loss function, as used\n+ * in binary classification for instances in sparse or dense vector in a online fashion.\n+ *\n+ * Two SVMAggregator can be merged together to have a summary of loss and gradient of\n+ * the corresponding joint dataset.\n+ *\n+ * @param bcCoefficients The coefficients corresponding to the features.\n+ * @param fitIntercept Whether to fit an intercept term.\n+ * @param bcFeaturesStd The standard deviation values of the features.\n+ */\n+private class SVMAggregator(\n+    bcCoefficients: Broadcast[Vector],\n+    bcFeaturesStd: Broadcast[Array[Double]],\n+    numClasses: Int,\n+    fitIntercept: Boolean,\n+    multinomial: Boolean) extends Serializable {\n+\n+  private val numFeatures = bcFeaturesStd.value.length\n+  private val numFeaturesPlusIntercept = if (fitIntercept) numFeatures + 1 else numFeatures\n+  private val coefficients = bcCoefficients.value\n+  private val featuresStd = bcFeaturesStd.value\n+  private var weightSum = 0.0\n+  private var lossSum = 0.0\n+\n+  private val coefficientsArray = coefficients match {\n+    case dv: DenseVector => dv.values\n+    case _ =>\n+      throw new IllegalArgumentException(\n+        s\"coefficients only supports dense vector but got type ${coefficients.getClass}.\")\n+  }\n+\n+  private val dim = if (fitIntercept) coefficientsArray.length - 1 else coefficientsArray.length\n+\n+  private val gradientSumArray = Array.ofDim[Double](coefficientsArray.length)\n+\n+  /**\n+   * Add a new training instance to this SVMAggregator, and update the loss and gradient\n+   * of the objective function.\n+   *\n+   * @param instance The instance of data point to be added.\n+   * @return This SVMAggregator object.\n+   */\n+  def add(instance: Instance): this.type = {\n+    instance match { case Instance(label, weight, features) =>\n+      require(dim == features.size, s\"Dimensions mismatch when adding new instance.\" +\n+        s\" Expecting $dim but got ${features.size}.\")\n+      require(weight >= 0.0, s\"instance weight, $weight has to be >= 0.0\")\n+\n+      if (weight == 0.0) return this\n+\n+      val localCoefficientsArray = coefficientsArray\n+      val localGradientSumArray = gradientSumArray\n+\n+      numClasses match {"
  }],
  "prId": 15211
}]