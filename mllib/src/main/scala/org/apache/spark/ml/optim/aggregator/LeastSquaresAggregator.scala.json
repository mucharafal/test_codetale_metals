[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Add a space before 'deviation' or at the end of the previous line",
    "commit": "29052d3dbc97ad548128c13533de47d5488b4196",
    "createdAt": "2017-05-25T12:32:58Z",
    "diffHunk": "@@ -0,0 +1,224 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+\n+/**\n+ * LeastSquaresAggregator computes the gradient and loss for a Least-squared loss function,\n+ * as used in linear regression for samples in sparse or dense vector in an online fashion.\n+ *\n+ * Two LeastSquaresAggregator can be merged together to have a summary of loss and gradient of\n+ * the corresponding joint dataset.\n+ *\n+ * For improving the convergence rate during the optimization process, and also preventing against\n+ * features with very large variances exerting an overly large influence during model training,\n+ * package like R's GLMNET performs the scaling to unit variance and removing the mean to reduce\n+ * the condition number, and then trains the model in scaled space but returns the coefficients in\n+ * the original scale. See page 9 in http://cran.r-project.org/web/packages/glmnet/glmnet.pdf\n+ *\n+ * However, we don't want to apply the `StandardScaler` on the training dataset, and then cache\n+ * the standardized dataset since it will create a lot of overhead. As a result, we perform the\n+ * scaling implicitly when we compute the objective function. The following is the mathematical\n+ * derivation.\n+ *\n+ * Note that we don't deal with intercept by adding bias here, because the intercept\n+ * can be computed using closed form after the coefficients are converged.\n+ * See this discussion for detail.\n+ * http://stats.stackexchange.com/questions/13617/how-is-the-intercept-computed-in-glmnet\n+ *\n+ * When training with intercept enabled,\n+ * The objective function in the scaled space is given by\n+ *\n+ * <blockquote>\n+ *    $$\n+ *    L = 1/2n ||\\sum_i w_i(x_i - \\bar{x_i}) / \\hat{x_i} - (y - \\bar{y}) / \\hat{y}||^2,\n+ *    $$\n+ * </blockquote>\n+ *\n+ * where $\\bar{x_i}$ is the mean of $x_i$, $\\hat{x_i}$ is the standard deviation of $x_i$,\n+ * $\\bar{y}$ is the mean of label, and $\\hat{y}$ is the standard deviation of label.\n+ *\n+ * If we fitting the intercept disabled (that is forced through 0.0),\n+ * we can use the same equation except we set $\\bar{y}$ and $\\bar{x_i}$ to 0 instead\n+ * of the respective means.\n+ *\n+ * This can be rewritten as\n+ *\n+ * <blockquote>\n+ *    $$\n+ *    \\begin{align}\n+ *     L &= 1/2n ||\\sum_i (w_i/\\hat{x_i})x_i - \\sum_i (w_i/\\hat{x_i})\\bar{x_i} - y / \\hat{y}\n+ *          + \\bar{y} / \\hat{y}||^2 \\\\\n+ *       &= 1/2n ||\\sum_i w_i^\\prime x_i - y / \\hat{y} + offset||^2 = 1/2n diff^2\n+ *    \\end{align}\n+ *    $$\n+ * </blockquote>\n+ *\n+ * where $w_i^\\prime$ is the effective coefficients defined by $w_i/\\hat{x_i}$, offset is\n+ *\n+ * <blockquote>\n+ *    $$\n+ *    - \\sum_i (w_i/\\hat{x_i})\\bar{x_i} + \\bar{y} / \\hat{y}.\n+ *    $$\n+ * </blockquote>\n+ *\n+ * and diff is\n+ *\n+ * <blockquote>\n+ *    $$\n+ *    \\sum_i w_i^\\prime x_i - y / \\hat{y} + offset\n+ *    $$\n+ * </blockquote>\n+ *\n+ * Note that the effective coefficients and offset don't depend on training dataset,\n+ * so they can be precomputed.\n+ *\n+ * Now, the first derivative of the objective function in scaled space is\n+ *\n+ * <blockquote>\n+ *    $$\n+ *    \\frac{\\partial L}{\\partial w_i} = diff/N (x_i - \\bar{x_i}) / \\hat{x_i}\n+ *    $$\n+ * </blockquote>\n+ *\n+ * However, $(x_i - \\bar{x_i})$ will densify the computation, so it's not\n+ * an ideal formula when the training dataset is sparse format.\n+ *\n+ * This can be addressed by adding the dense $\\bar{x_i} / \\hat{x_i}$ terms\n+ * in the end by keeping the sum of diff. The first derivative of total\n+ * objective function from all the samples is\n+ *\n+ *\n+ * <blockquote>\n+ *    $$\n+ *    \\begin{align}\n+ *       \\frac{\\partial L}{\\partial w_i} &=\n+ *           1/N \\sum_j diff_j (x_{ij} - \\bar{x_i}) / \\hat{x_i} \\\\\n+ *         &= 1/N ((\\sum_j diff_j x_{ij} / \\hat{x_i}) - diffSum \\bar{x_i} / \\hat{x_i}) \\\\\n+ *         &= 1/N ((\\sum_j diff_j x_{ij} / \\hat{x_i}) + correction_i)\n+ *    \\end{align}\n+ *    $$\n+ * </blockquote>\n+ *\n+ * where $correction_i = - diffSum \\bar{x_i} / \\hat{x_i}$\n+ *\n+ * A simple math can show that diffSum is actually zero, so we don't even\n+ * need to add the correction terms in the end. From the definition of diff,\n+ *\n+ * <blockquote>\n+ *    $$\n+ *    \\begin{align}\n+ *       diffSum &= \\sum_j (\\sum_i w_i(x_{ij} - \\bar{x_i})\n+ *                    / \\hat{x_i} - (y_j - \\bar{y}) / \\hat{y}) \\\\\n+ *         &= N * (\\sum_i w_i(\\bar{x_i} - \\bar{x_i}) / \\hat{x_i} - (\\bar{y} - \\bar{y}) / \\hat{y}) \\\\\n+ *         &= 0\n+ *    \\end{align}\n+ *    $$\n+ * </blockquote>\n+ *\n+ * As a result, the first derivative of the total objective function only depends on\n+ * the training dataset, which can be easily computed in distributed fashion, and is\n+ * sparse format friendly.\n+ *\n+ * <blockquote>\n+ *    $$\n+ *    \\frac{\\partial L}{\\partial w_i} = 1/N ((\\sum_j diff_j x_{ij} / \\hat{x_i})\n+ *    $$\n+ * </blockquote>\n+ *\n+ * @note The constructor is curried, since the cost function will repeatedly create new versions\n+ *       of this class for different coefficient vectors.\n+ *\n+ * @param labelStd The standard deviation value of the label.\n+ * @param labelMean The mean value of the label.\n+ * @param fitIntercept Whether to fit an intercept term.\n+ * @param bcFeaturesStd The broadcast standard deviation values of the features.\n+ * @param bcFeaturesMean The broadcast mean values of the features.\n+ * @param bcCoefficients The broadcast coefficients corresponding to the features.\n+ */\n+private[ml] class LeastSquaresAggregator(\n+    labelStd: Double,\n+    labelMean: Double,\n+    fitIntercept: Boolean,\n+    bcFeaturesStd: Broadcast[Array[Double]],\n+    bcFeaturesMean: Broadcast[Array[Double]])(bcCoefficients: Broadcast[Vector])\n+  extends DifferentiableLossAggregator[Instance, LeastSquaresAggregator] {\n+  require(labelStd > 0.0, s\"${this.getClass.getName} requires the label standard\" +\n+    s\"deviation to be positive.\")"
  }],
  "prId": 17094
}]