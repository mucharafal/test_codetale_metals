[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Is there any paper you can link to to explain the implementation? or, just a few paragraphs about what the implementation does?",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-20T22:25:05Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines"
  }, {
    "author": {
      "login": "mob-ai"
    },
    "body": "> Is there any paper you can link to to explain the implementation? or, just a few paragraphs about what the implementation does?\r\n\r\nFM paper: S. Rendle, “Factorization machines,” in Proceedings of IEEE International Conference on Data Mining (ICDM), pp. 995–1000, 2010.\r\nhttps://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-25T02:43:07Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Yes, I mean, this should be in the docs along with a brief explanation of what it does ... enough that an informed reader understands how the parameters you expose map to the algorithm in the paper (which is probably straightforward)",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-25T13:11:28Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "These doc comments don't need to be long, but I think it could explain a little bit more. I think this text is OK if the 'dimension' in question is clear from an explanation in the class doc, or a paper.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-20T22:25:46Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factorization\")"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "This could be clarified a bit: strength of regularization? does this correspond to some parameter in a paper, some 'alpha'? etc . You can add a validator to ensure it's not negative, too. See some other DoubleParam for an example.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-20T22:38:43Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factorization\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"regularization for L2\")"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Likewise, minibatch of what? and can be validated to be nonnegative/positive?",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-20T22:39:03Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factorization\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"regularization for L2\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\","
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "I wouldn't add this; we don't elsewhere. Just use logging levels appropriately.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-20T22:39:51Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factorization\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"regularization for L2\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"mini-batch fraction\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+\n+  /**\n+   * Param for whether to print information per iteration step\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val verbose: BooleanParam = new BooleanParam(this, \"verbose\","
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: space before brace",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-20T22:41:01Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factorization\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"regularization for L2\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"mini-batch fraction\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+\n+  /**\n+   * Param for whether to print information per iteration step\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val verbose: BooleanParam = new BooleanParam(this, \"verbose\",\n+    \"whether to print information per iteration step\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getVerbose: Boolean = $(verbose)\n+\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  /**\n+   * Set the verbose.\n+   * Factorization Machines may produce a gradient explosion, so output some log use logInfo.\n+   * logInfo will output\n+   *  - iter_num: current iterate epoch\n+   *  - iter_step_size: step size(aka learning rate)\n+   *  - weights_norm2: weight's L2 norm\n+   *  - cur_tol: current tolerance in GradientDescent\n+   *  - grad_norm1: gradient's L1 norm\n+   *  - first_weight: first parameter in weights\n+   *  - first_gradient: first parameter's gradient\n+   * Default is false.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setVerbose(value: Boolean): this.type = set(verbose, value)\n+  setDefault(verbose -> false)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++\n+        (if ($(fitBias)) Array.fill(1)(0.0) else Array.empty[Double]))\n+\n+    val data = instances.map{ case OldLabeledPoint(label, features) => (label, features) }"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: this looks like a 'sum' not squared sum.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-20T22:42:14Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factorization\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"regularization for L2\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"mini-batch fraction\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+\n+  /**\n+   * Param for whether to print information per iteration step\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val verbose: BooleanParam = new BooleanParam(this, \"verbose\",\n+    \"whether to print information per iteration step\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getVerbose: Boolean = $(verbose)\n+\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  /**\n+   * Set the verbose.\n+   * Factorization Machines may produce a gradient explosion, so output some log use logInfo.\n+   * logInfo will output\n+   *  - iter_num: current iterate epoch\n+   *  - iter_step_size: step size(aka learning rate)\n+   *  - weights_norm2: weight's L2 norm\n+   *  - cur_tol: current tolerance in GradientDescent\n+   *  - grad_norm1: gradient's L1 norm\n+   *  - first_weight: first parameter in weights\n+   *  - first_gradient: first parameter's gradient\n+   * Default is false.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setVerbose(value: Boolean): this.type = set(verbose, value)\n+  setDefault(verbose -> false)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++\n+        (if ($(fitBias)) Array.fill(1)(0.0) else Array.empty[Double]))\n+\n+    val data = instances.map{ case OldLabeledPoint(label, features) => (label, features) }\n+\n+    // optimize coefficients with gradient descent\n+    val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+      $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+    val updater = $(solver) match {\n+      case GD => new FactorizationMachinesUpdater().setVerbose($(verbose))\n+      case AdamW => new AdamWUpdater(coefficientsSize).setVerbose($(verbose))\n+    }\n+\n+    val optimizer = new GradientDescent(gradient, updater)\n+      .setStepSize($(stepSize))\n+      .setNumIterations($(maxIter))\n+      .setRegParam($(regParam))\n+      .setMiniBatchFraction($(miniBatchFraction))\n+      .setConvergenceTol($(tol))\n+    val coefficients = optimizer.optimize(data, initialCoefficients)\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(new FactorizationMachinesModel(uid, coefficients, numFeatures))\n+    model\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachines = defaultCopy(extra)\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachines extends DefaultParamsReadable[FactorizationMachines] {\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachines = super.load(path)\n+\n+  /** String name for \"gd\". */\n+  private[regression] val GD = \"gd\"\n+\n+  /** String name for \"adamW\". */\n+  private[regression] val AdamW = \"adamW\"\n+\n+  /** Set of solvers that FactorizationMachines supports. */\n+  private[regression] val supportedSolvers = Array(GD, AdamW)\n+\n+  /** String name for \"logisticLoss\". */\n+  private[regression] val LogisticLoss = \"logisticLoss\"\n+\n+  /** String name for \"squaredError\". */\n+  private[regression] val SquaredError = \"squaredError\"\n+\n+  /** Set of loss function names that FactorizationMachines supports. */\n+  private[regression] val supportedLosses = Array(SquaredError, LogisticLoss)\n+}\n+\n+/**\n+ * Model produced by [[FactorizationMachines]].\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachinesModel (\n+    @Since(\"3.0.0\") override val uid: String,\n+    @Since(\"3.0.0\") val coefficients: OldVector,\n+    @Since(\"3.0.0\") override val numFeatures: Int)\n+  extends PredictionModel[Vector, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with MLWritable {\n+\n+  /**\n+   * Returns Factorization Machines coefficients\n+   * coefficients concat from 2-way coefficients, 1-way coefficients, global bias\n+   * index 0 ~ numFeatures*numFactors is 2-way coefficients,\n+   *   [i * numFactors + f] denotes i-th feature and f-th factor\n+   * Following indices are 1-way coefficients and global bias.\n+   *\n+   * @return Vector\n+   */\n+  @Since(\"3.0.0\")\n+  def getCoefficients: Vector = coefficients\n+\n+  private lazy val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+    $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+  override def predict(features: Vector): Double = {\n+    val rawPrediction = gradient.getRawPrediction(features, coefficients)\n+    gradient.getPrediction(rawPrediction)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachinesModel = {\n+    copyValues(new FactorizationMachinesModel(\n+      uid, coefficients, numFeatures), extra)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def write: MLWriter =\n+    new FactorizationMachinesModel.FactorizationMachinesModelWriter(this)\n+\n+  override def toString: String = {\n+    s\"FactorizationMachinesModel: \" +\n+      s\"uid = ${super.toString}, lossFunc = ${$(loss)}, numFeatures = $numFeatures, \" +\n+      s\"numFactors = ${$(numFactors)}, fitLinear = ${$(fitLinear)}, fitBias = ${$(fitBias)}\"\n+  }\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachinesModel extends MLReadable[FactorizationMachinesModel] {\n+\n+  @Since(\"3.0.0\")\n+  override def read: MLReader[FactorizationMachinesModel] = new FactorizationMachinesModelReader\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachinesModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FactorizationMachinesModel]] */\n+  private[FactorizationMachinesModel] class FactorizationMachinesModelWriter(\n+      instance: FactorizationMachinesModel) extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numFeatures: Int,\n+        coefficients: OldVector)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val data = Data(instance.numFeatures, instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sparkSession.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class FactorizationMachinesModelReader extends MLReader[FactorizationMachinesModel] {\n+\n+    private val className = classOf[FactorizationMachinesModel].getName\n+\n+    override def load(path: String): FactorizationMachinesModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sparkSession.read.format(\"parquet\").load(dataPath)\n+\n+      val Row(numFeatures: Int, coefficients: OldVector) = data\n+        .select(\"numFeatures\", \"coefficients\").head()\n+      val model = new FactorizationMachinesModel(\n+        metadata.uid, coefficients, numFeatures)\n+      metadata.getAndSetParams(model)\n+      model\n+    }\n+  }\n+}\n+\n+/**\n+ * Factorization Machines base gradient class\n+ * Implementing the raw FM formula, include raw prediction and raw gradient,\n+ * then inherit the base class to implement special gradient class(like logloss, mse).\n+ *\n+ * Factorization Machines raw formula:\n+ * {{{\n+ *   y_{fm} = w_0 + \\sum\\limits^n_{i-1} w_i x_i +\n+ *     \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1}  x_i x_j\n+ * }}}\n+ * the pairwise interactions (2-way term) can be reformulated:\n+ * {{{\n+ *   \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} <v_i, v_j> x_i x_j\n+ *   = \\frac{1}{2}\\sum\\limits^k_{f=1}\n+ *   \\left(\\left( \\sum\\limits^n_{i=1}v_{i,f}x_i \\right)^2 -\n+ *     \\sum\\limits^n_{i=1}v_{i,f}^2x_i^2 \\right)\n+ * }}}\n+ * and the gradients are:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta}y_{fm} = \\left\\{\n+ *   \\begin{align}\n+ *   &1, & if\\ \\theta\\ is\\ w_0 \\\\\n+ *   &x_i, & if\\ \\theta\\ is\\ w_i \\\\\n+ *   &x_i{\\sum}^n_{j=1}v_{j,f}x_j - v_{i,f}x_i^2, & if\\ \\theta\\ is\\ v_{i,j} \\\\\n+ *   \\end{align}\n+ *   \\right.\n+ * }}}\n+ *\n+ * Factorization Machines formula with prediction task:\n+ * {{{\n+ *   \\hat{y} = p\\left( y_{fm} \\right)\n+ * }}}\n+ * p is the prediction function, for binary classification task is sigmoid.\n+ * The loss funcation gradient formula:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( \\hat{y},y \\right) =\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( p\\left( y_{fm} \\right),y \\right) =\n+ *   \\frac{\\partial l}{\\partial \\hat{y}} \\cdot\n+ *   \\frac{\\partial \\hat{y}}{\\partial y_{fm}} \\cdot\n+ *   \\frac{\\partial y_{fm}}{\\partial\\theta}\n+ * }}}\n+ * Last term is same for all task, so be implemented in base gradient class.\n+ * last term named rawGradient in following code, and first two term named multiplier.\n+ */\n+private[ml] abstract class BaseFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int) extends Gradient {\n+\n+  override def compute(\n+      data: OldVector,\n+      label: Double,\n+      weights: OldVector,\n+      cumGradient: OldVector): Double = {\n+    val rawPrediction = getRawPrediction(data, weights)\n+    val rawGradient = getRawGradient(data, weights)\n+    val multiplier = getMultiplier(rawPrediction, label)\n+    axpy(multiplier, rawGradient, cumGradient)\n+    val loss = getLoss(rawPrediction, label)\n+    loss\n+  }\n+\n+  def getPrediction(rawPrediction: Double): Double\n+\n+  protected def getMultiplier(rawPrediction: Double, label: Double): Double\n+\n+  protected def getLoss(rawPrediction: Double, label: Double): Double\n+\n+  private val sumVX = Array.fill(numFactors)(0.0)\n+\n+  def getRawPrediction(data: OldVector, weights: OldVector): Double = {\n+    var rawPrediction = 0.0\n+    val vWeightsSize = numFeatures * numFactors\n+\n+    if (fitBias) rawPrediction += weights(weights.size - 1)\n+    if (fitLinear) {\n+      data.foreachActive { case (index, value) =>\n+        rawPrediction += weights(vWeightsSize + index) * value\n+      }\n+    }\n+    (0 until numFactors).foreach { f =>\n+      var sumSquare = 0.0\n+      var squareSum = 0.0"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Can these be val?",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-20T22:45:19Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factorization\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"regularization for L2\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"mini-batch fraction\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+\n+  /**\n+   * Param for whether to print information per iteration step\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val verbose: BooleanParam = new BooleanParam(this, \"verbose\",\n+    \"whether to print information per iteration step\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getVerbose: Boolean = $(verbose)\n+\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  /**\n+   * Set the verbose.\n+   * Factorization Machines may produce a gradient explosion, so output some log use logInfo.\n+   * logInfo will output\n+   *  - iter_num: current iterate epoch\n+   *  - iter_step_size: step size(aka learning rate)\n+   *  - weights_norm2: weight's L2 norm\n+   *  - cur_tol: current tolerance in GradientDescent\n+   *  - grad_norm1: gradient's L1 norm\n+   *  - first_weight: first parameter in weights\n+   *  - first_gradient: first parameter's gradient\n+   * Default is false.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setVerbose(value: Boolean): this.type = set(verbose, value)\n+  setDefault(verbose -> false)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++\n+        (if ($(fitBias)) Array.fill(1)(0.0) else Array.empty[Double]))\n+\n+    val data = instances.map{ case OldLabeledPoint(label, features) => (label, features) }\n+\n+    // optimize coefficients with gradient descent\n+    val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+      $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+    val updater = $(solver) match {\n+      case GD => new FactorizationMachinesUpdater().setVerbose($(verbose))\n+      case AdamW => new AdamWUpdater(coefficientsSize).setVerbose($(verbose))\n+    }\n+\n+    val optimizer = new GradientDescent(gradient, updater)\n+      .setStepSize($(stepSize))\n+      .setNumIterations($(maxIter))\n+      .setRegParam($(regParam))\n+      .setMiniBatchFraction($(miniBatchFraction))\n+      .setConvergenceTol($(tol))\n+    val coefficients = optimizer.optimize(data, initialCoefficients)\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(new FactorizationMachinesModel(uid, coefficients, numFeatures))\n+    model\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachines = defaultCopy(extra)\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachines extends DefaultParamsReadable[FactorizationMachines] {\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachines = super.load(path)\n+\n+  /** String name for \"gd\". */\n+  private[regression] val GD = \"gd\"\n+\n+  /** String name for \"adamW\". */\n+  private[regression] val AdamW = \"adamW\"\n+\n+  /** Set of solvers that FactorizationMachines supports. */\n+  private[regression] val supportedSolvers = Array(GD, AdamW)\n+\n+  /** String name for \"logisticLoss\". */\n+  private[regression] val LogisticLoss = \"logisticLoss\"\n+\n+  /** String name for \"squaredError\". */\n+  private[regression] val SquaredError = \"squaredError\"\n+\n+  /** Set of loss function names that FactorizationMachines supports. */\n+  private[regression] val supportedLosses = Array(SquaredError, LogisticLoss)\n+}\n+\n+/**\n+ * Model produced by [[FactorizationMachines]].\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachinesModel (\n+    @Since(\"3.0.0\") override val uid: String,\n+    @Since(\"3.0.0\") val coefficients: OldVector,\n+    @Since(\"3.0.0\") override val numFeatures: Int)\n+  extends PredictionModel[Vector, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with MLWritable {\n+\n+  /**\n+   * Returns Factorization Machines coefficients\n+   * coefficients concat from 2-way coefficients, 1-way coefficients, global bias\n+   * index 0 ~ numFeatures*numFactors is 2-way coefficients,\n+   *   [i * numFactors + f] denotes i-th feature and f-th factor\n+   * Following indices are 1-way coefficients and global bias.\n+   *\n+   * @return Vector\n+   */\n+  @Since(\"3.0.0\")\n+  def getCoefficients: Vector = coefficients\n+\n+  private lazy val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+    $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+  override def predict(features: Vector): Double = {\n+    val rawPrediction = gradient.getRawPrediction(features, coefficients)\n+    gradient.getPrediction(rawPrediction)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachinesModel = {\n+    copyValues(new FactorizationMachinesModel(\n+      uid, coefficients, numFeatures), extra)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def write: MLWriter =\n+    new FactorizationMachinesModel.FactorizationMachinesModelWriter(this)\n+\n+  override def toString: String = {\n+    s\"FactorizationMachinesModel: \" +\n+      s\"uid = ${super.toString}, lossFunc = ${$(loss)}, numFeatures = $numFeatures, \" +\n+      s\"numFactors = ${$(numFactors)}, fitLinear = ${$(fitLinear)}, fitBias = ${$(fitBias)}\"\n+  }\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachinesModel extends MLReadable[FactorizationMachinesModel] {\n+\n+  @Since(\"3.0.0\")\n+  override def read: MLReader[FactorizationMachinesModel] = new FactorizationMachinesModelReader\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachinesModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FactorizationMachinesModel]] */\n+  private[FactorizationMachinesModel] class FactorizationMachinesModelWriter(\n+      instance: FactorizationMachinesModel) extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numFeatures: Int,\n+        coefficients: OldVector)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val data = Data(instance.numFeatures, instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sparkSession.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class FactorizationMachinesModelReader extends MLReader[FactorizationMachinesModel] {\n+\n+    private val className = classOf[FactorizationMachinesModel].getName\n+\n+    override def load(path: String): FactorizationMachinesModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sparkSession.read.format(\"parquet\").load(dataPath)\n+\n+      val Row(numFeatures: Int, coefficients: OldVector) = data\n+        .select(\"numFeatures\", \"coefficients\").head()\n+      val model = new FactorizationMachinesModel(\n+        metadata.uid, coefficients, numFeatures)\n+      metadata.getAndSetParams(model)\n+      model\n+    }\n+  }\n+}\n+\n+/**\n+ * Factorization Machines base gradient class\n+ * Implementing the raw FM formula, include raw prediction and raw gradient,\n+ * then inherit the base class to implement special gradient class(like logloss, mse).\n+ *\n+ * Factorization Machines raw formula:\n+ * {{{\n+ *   y_{fm} = w_0 + \\sum\\limits^n_{i-1} w_i x_i +\n+ *     \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1}  x_i x_j\n+ * }}}\n+ * the pairwise interactions (2-way term) can be reformulated:\n+ * {{{\n+ *   \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} <v_i, v_j> x_i x_j\n+ *   = \\frac{1}{2}\\sum\\limits^k_{f=1}\n+ *   \\left(\\left( \\sum\\limits^n_{i=1}v_{i,f}x_i \\right)^2 -\n+ *     \\sum\\limits^n_{i=1}v_{i,f}^2x_i^2 \\right)\n+ * }}}\n+ * and the gradients are:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta}y_{fm} = \\left\\{\n+ *   \\begin{align}\n+ *   &1, & if\\ \\theta\\ is\\ w_0 \\\\\n+ *   &x_i, & if\\ \\theta\\ is\\ w_i \\\\\n+ *   &x_i{\\sum}^n_{j=1}v_{j,f}x_j - v_{i,f}x_i^2, & if\\ \\theta\\ is\\ v_{i,j} \\\\\n+ *   \\end{align}\n+ *   \\right.\n+ * }}}\n+ *\n+ * Factorization Machines formula with prediction task:\n+ * {{{\n+ *   \\hat{y} = p\\left( y_{fm} \\right)\n+ * }}}\n+ * p is the prediction function, for binary classification task is sigmoid.\n+ * The loss funcation gradient formula:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( \\hat{y},y \\right) =\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( p\\left( y_{fm} \\right),y \\right) =\n+ *   \\frac{\\partial l}{\\partial \\hat{y}} \\cdot\n+ *   \\frac{\\partial \\hat{y}}{\\partial y_{fm}} \\cdot\n+ *   \\frac{\\partial y_{fm}}{\\partial\\theta}\n+ * }}}\n+ * Last term is same for all task, so be implemented in base gradient class.\n+ * last term named rawGradient in following code, and first two term named multiplier.\n+ */\n+private[ml] abstract class BaseFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int) extends Gradient {\n+\n+  override def compute(\n+      data: OldVector,\n+      label: Double,\n+      weights: OldVector,\n+      cumGradient: OldVector): Double = {\n+    val rawPrediction = getRawPrediction(data, weights)\n+    val rawGradient = getRawGradient(data, weights)\n+    val multiplier = getMultiplier(rawPrediction, label)\n+    axpy(multiplier, rawGradient, cumGradient)\n+    val loss = getLoss(rawPrediction, label)\n+    loss\n+  }\n+\n+  def getPrediction(rawPrediction: Double): Double\n+\n+  protected def getMultiplier(rawPrediction: Double, label: Double): Double\n+\n+  protected def getLoss(rawPrediction: Double, label: Double): Double\n+\n+  private val sumVX = Array.fill(numFactors)(0.0)\n+\n+  def getRawPrediction(data: OldVector, weights: OldVector): Double = {\n+    var rawPrediction = 0.0\n+    val vWeightsSize = numFeatures * numFactors\n+\n+    if (fitBias) rawPrediction += weights(weights.size - 1)\n+    if (fitLinear) {\n+      data.foreachActive { case (index, value) =>\n+        rawPrediction += weights(vWeightsSize + index) * value\n+      }\n+    }\n+    (0 until numFactors).foreach { f =>\n+      var sumSquare = 0.0\n+      var squareSum = 0.0\n+      data.foreachActive { case (index, value) =>\n+        val vx = weights(index * numFactors + f) * value\n+        sumSquare += vx * vx\n+        squareSum += vx\n+      }\n+      sumVX(f) = squareSum\n+      squareSum = squareSum * squareSum\n+      rawPrediction += 0.5 * (squareSum - sumSquare)\n+    }\n+\n+    rawPrediction\n+  }\n+\n+  private def getRawGradient(data: OldVector, weights: OldVector): OldVector = {\n+    data match {\n+      // Usually Factorization Machines is used, there will be a lot of sparse features.\n+      // So need to optimize the gradient descent of sparse vector.\n+      case data: OldLinalg.SparseVector =>\n+        val gardSize = data.indices.length * numFactors +\n+          (if (fitLinear) data.indices.length else 0) +\n+          (if (fitBias) 1 else 0)\n+        val gradIndex = Array.fill(gardSize)(0)\n+        val gradValue = Array.fill(gardSize)(0.0)\n+        var gradI = 0\n+        val vWeightsSize = numFeatures * numFactors\n+\n+        data.foreachActive { case (index, value) =>\n+          (0 until numFactors).foreach { f =>\n+            gradIndex(gradI) = index * numFactors + f\n+            gradValue(gradI) = value * sumVX(f) - weights(index * numFactors + f) * value * value\n+            gradI += 1\n+          }\n+        }\n+        if (fitLinear) {\n+          data.foreachActive { case (index, value) =>\n+            gradIndex(gradI) = vWeightsSize + index\n+            gradValue(gradI) = value\n+            gradI += 1\n+          }\n+        }\n+        if (fitBias) {\n+          gradIndex(gradI) = weights.size - 1\n+          gradValue(gradI) = 1.0\n+        }\n+\n+        OldVectors.sparse(weights.size, gradIndex, gradValue)\n+      case data: OldLinalg.DenseVector =>\n+        val gradient = Array.fill(weights.size)(0.0)\n+        val vWeightsSize = numFeatures * numFactors\n+\n+        if (fitBias) gradient(weights.size - 1) += 1.0\n+        if (fitLinear) {\n+          data.foreachActive { case (index, value) =>\n+            gradient(vWeightsSize + index) += value\n+          }\n+        }\n+        (0 until numFactors).foreach { f =>\n+          data.foreachActive { case (index, value) =>\n+            gradient(index * numFactors + f) +=\n+              value * sumVX(f) - weights(index * numFactors + f) * value * value\n+          }\n+        }\n+\n+        OldVectors.dense(gradient)\n+    }\n+  }\n+}\n+\n+object BaseFactorizationMachinesGradient {\n+  def parseLoss(\n+      lossFunc: String,\n+      numFactors: Int,\n+      fitBias: Boolean,\n+      fitLinear: Boolean,\n+      numFeatures: Int): BaseFactorizationMachinesGradient = {\n+\n+    import FactorizationMachines._\n+\n+    lossFunc match {\n+      case LogisticLoss =>\n+        new LogisticFactorizationMachinesGradient(numFactors, fitBias, fitLinear, numFeatures)\n+      case SquaredError =>\n+        new MSEFactorizationMachinesGradient(numFactors, fitBias, fitLinear, numFeatures)\n+      case _ => throw new IllegalArgumentException(s\"loss function type $lossFunc is invalidation\")\n+    }\n+  }\n+}\n+\n+/**\n+ * FM with logistic loss\n+ * prediction formula:\n+ * {{{\n+ *   \\hat{y} = \\sigmoid(y_{fm})\n+ * }}}\n+ * loss formula:\n+ * {{{\n+ *   - y * log(\\hat{y}) - (1 - y) * log(1 - \\hat{y})\n+ * }}}\n+ * multiplier formula:\n+ * {{{\n+ *   \\frac{\\partial l}{\\partial \\hat{y}} \\cdot\n+ *   \\frac{\\partial \\hat{y}}{\\partial y_{fm}} =\n+ *   \\hat{y} - y\n+ * }}}\n+ */\n+private[ml] class LogisticFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int)\n+  extends BaseFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int) with Logging {\n+\n+  override def getPrediction(rawPrediction: Double): Double = {\n+    1.0 / (1.0 + math.exp(-rawPrediction))\n+  }\n+\n+  override protected def getMultiplier(rawPrediction: Double, label: Double): Double = {\n+    getPrediction(rawPrediction) - label\n+  }\n+\n+  override protected def getLoss(rawPrediction: Double, label: Double): Double = {\n+    if (label > 0) MLUtils.log1pExp(-rawPrediction)\n+    else MLUtils.log1pExp(rawPrediction)\n+  }\n+}\n+\n+/**\n+ * FM with mse\n+ * prediction formula:\n+ * {{{\n+ *   \\hat{y} = y_{fm}\n+ * }}}\n+ * loss formula:\n+ * {{{\n+ *   (\\hat{y} - y) ^ 2\n+ * }}}\n+ * multiplier formula:\n+ * {{{\n+ *   \\frac{\\partial l}{\\partial \\hat{y}} \\cdot\n+ *   \\frac{\\partial \\hat{y}}{\\partial y_{fm}} =\n+ *   2 * (\\hat{y} - y)\n+ * }}}\n+ */\n+private[ml] class MSEFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int)\n+  extends BaseFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int) with Logging {\n+\n+  override def getPrediction(rawPrediction: Double): Double = {\n+    rawPrediction\n+  }\n+\n+  override protected def getMultiplier(rawPrediction: Double, label: Double): Double = {\n+    2 * (rawPrediction - label)\n+  }\n+\n+  override protected def getLoss(rawPrediction: Double, label: Double): Double = {\n+    (rawPrediction - label) * (rawPrediction - label)\n+  }\n+}\n+\n+private[ml] class FactorizationMachinesUpdater extends Updater with Logging {\n+  var verbose: Boolean = false\n+\n+  def setVerbose(isVerbose: Boolean): this.type = {\n+    this.verbose = isVerbose\n+    this\n+  }\n+\n+  override def compute(\n+      weightsOld: OldVector,\n+      gradient: OldVector,\n+      stepSize: Double,\n+      iter: Int,\n+      regParam: Double\n+    ): (OldVector, Double) = {\n+    // add up both updates from the gradient of the loss (= step) as well as\n+    // the gradient of the regularizer (= regParam * weightsOld)\n+    // w' = w - thisIterStepSize * (gradient + regParam * w)\n+    // w' = (1 - thisIterStepSize * regParam) * w - thisIterStepSize * gradient\n+    val thisIterStepSize = stepSize / math.sqrt(iter)\n+    val brzWeights: BV[Double] = weightsOld.asBreeze.toDenseVector\n+    brzWeights :*= (1.0 - thisIterStepSize * regParam)\n+    brzAxpy(-thisIterStepSize, gradient.asBreeze, brzWeights)\n+    val norm = brzNorm(brzWeights, 2.0)\n+\n+    if (verbose) {\n+      val gradNorm = brzNorm(gradient.asBreeze.toDenseVector)\n+      val solutionVecDiff = brzNorm(weightsOld.asBreeze.toDenseVector - brzWeights)\n+      val curTol = solutionVecDiff / Math.max(brzNorm(brzWeights), 1.0)\n+      logInfo(s\"iter_num: $iter, iter_step_size: $thisIterStepSize\" +\n+        s\", weights_norm2: $norm, cur_tol: $curTol, grad_norm1: $gradNorm\" +\n+        s\", first_weight: ${brzWeights(0)}, first_gradient: ${gradient(0)}\")\n+    }\n+\n+    (Vectors.fromBreeze(brzWeights), 0.5 * regParam * norm * norm)\n+  }\n+}\n+\n+private[ml] class AdamWUpdater(weightSize: Int) extends Updater with Logging {\n+  var beta1: Double = 0.9"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "for a log-loss, should we put it into `ml.classification`? ",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-22T10:44:00Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factorization\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"regularization for L2\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"mini-batch fraction\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))"
  }, {
    "author": {
      "login": "mob-ai"
    },
    "body": "> for a log-loss, should we put it into `ml.classification`?\r\n\r\nI will add a FM Classifier later.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-25T02:42:50Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factorization\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"regularization for L2\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"mini-batch fraction\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "I guess the question is then, do you want to add log-loss later? it won't make sense for a regression",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-25T13:10:25Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factorization\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"regularization for L2\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"mini-batch fraction\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))"
  }, {
    "author": {
      "login": "mob-ai"
    },
    "body": "@srowen When loss is logloss, labels must in {0, 1}. So, I am planning to add FMClassifier for logloss, original FactorizationMachines changes to FMRegressor for mse, then remove loss parameter. What do you think?",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-28T01:49:22Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factorization\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"regularization for L2\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"mini-batch fraction\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "I would implement it all in one pull request then. My concern is that we're coming up on Spark 3 and we would not want to release part of this only to change the API after.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-28T03:01:39Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factorization\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"regularization for L2\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"mini-batch fraction\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))"
  }, {
    "author": {
      "login": "mob-ai"
    },
    "body": "OK, I will implement Classifier and Regressor in this PR. Thank you for your suggest.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-28T03:28:19Z",
    "diffHunk": "@@ -0,0 +1,839 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factorization\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"regularization for L2\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"mini-batch fraction\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: Must be greater than 0 right? the check below is correct",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-25T13:12:49Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: the doc strings should start with a Capital",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-25T13:16:45Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: l2 -> L2\r\nNit: can we say \"strength\" rather than \"parameter\" to clarify a little?\r\nRather than describe what L2 regularization is, what is being regularized? i'm just looking for a tiny bit more specificity. ",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-25T13:17:12Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Maybe I'm just not familiar here, but if this is a regressor, can you even use logistic loss?",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-25T13:18:20Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: where you call `Array.fill(...)(0.0)` just `new Array[Double](...)`. It doesn't matter much but no need to set the array to 0 as this is already its value. I dont' feel strongly about it.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-25T13:19:41Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++"
  }, {
    "author": {
      "login": "zhengruifeng"
    },
    "body": "another nit, `Array.emptyDoubleArray`",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-29T03:33:58Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++"
  }, {
    "author": {
      "login": "mob-ai"
    },
    "body": "> another nit, `Array.emptyDoubleArray`\r\n\r\nThe difference between `Array.emptyDoubleArray` and `Array.empty[Double]` is? ",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-29T07:39:50Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++"
  }, {
    "author": {
      "login": "mob-ai"
    },
    "body": "> another nit, `Array.emptyDoubleArray`\r\n\r\n`Array.emptyDoubleArray` is a val, `Array.empty[Double]` will create a new object. So `Array.emptyDoubleArray` is more efficient, right?",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-30T02:22:01Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "I guess we do not need to define this function. Other impls have this function for call in the `.mllib` side.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-29T03:31:18Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train("
  }, {
    "author": {
      "login": "mob-ai"
    },
    "body": "> I guess we do not need to define this function. Other impls have this function for call in the `.mllib` side.\r\n\r\nYes, ml's Estimator use `.fit()` public function (not `.train()`). But in `.fit() ` source code, it will do something then call a train function, so I need to implement it. The train function is not a public function, users can only use fit function.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-29T07:33:33Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train("
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "BTW, If both FM and MLP still need mini-batch solver, we may move it to the .ml side in the future, to avoid vector conversion.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-29T03:37:17Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++\n+        (if ($(fitBias)) Array.fill(1)(0.0) else Array.empty[Double]))\n+\n+    val data = instances.map { case OldLabeledPoint(label, features) => (label, features) }\n+\n+    // optimize coefficients with gradient descent\n+    val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+      $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+    val updater = $(solver) match {\n+      case GD => new SquaredL2Updater()\n+      case AdamW => new AdamWUpdater(coefficientsSize)\n+    }\n+\n+    val optimizer = new GradientDescent(gradient, updater)"
  }, {
    "author": {
      "login": "mob-ai"
    },
    "body": "> BTW, If both FM and MLP still need mini-batch solver, we may move it to the .ml side in the future, to avoid vector conversion.\r\n\r\nI agree, and I think GradientDescent supports aggregateDepth parameter, seed parameter, weightCol parameter and logInfo loss value per epoch will be better.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-29T07:45:58Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++\n+        (if ($(fitBias)) Array.fill(1)(0.0) else Array.empty[Double]))\n+\n+    val data = instances.map { case OldLabeledPoint(label, features) => (label, features) }\n+\n+    // optimize coefficients with gradient descent\n+    val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+      $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+    val updater = $(solver) match {\n+      case GD => new SquaredL2Updater()\n+      case AdamW => new AdamWUpdater(coefficientsSize)\n+    }\n+\n+    val optimizer = new GradientDescent(gradient, updater)"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "I'd prefer a `ml.vector` here, just call `.asML` method.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-29T03:38:29Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++\n+        (if ($(fitBias)) Array.fill(1)(0.0) else Array.empty[Double]))\n+\n+    val data = instances.map { case OldLabeledPoint(label, features) => (label, features) }\n+\n+    // optimize coefficients with gradient descent\n+    val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+      $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+    val updater = $(solver) match {\n+      case GD => new SquaredL2Updater()\n+      case AdamW => new AdamWUpdater(coefficientsSize)\n+    }\n+\n+    val optimizer = new GradientDescent(gradient, updater)\n+      .setStepSize($(stepSize))\n+      .setNumIterations($(maxIter))\n+      .setRegParam($(regParam))\n+      .setMiniBatchFraction($(miniBatchFraction))\n+      .setConvergenceTol($(tol))\n+    val coefficients = optimizer.optimize(data, initialCoefficients)\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(new FactorizationMachinesModel(uid, coefficients, numFeatures))\n+    model\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachines = defaultCopy(extra)\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachines extends DefaultParamsReadable[FactorizationMachines] {\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachines = super.load(path)\n+\n+  /** String name for \"gd\". */\n+  private[regression] val GD = \"gd\"\n+\n+  /** String name for \"adamW\". */\n+  private[regression] val AdamW = \"adamW\"\n+\n+  /** Set of solvers that FactorizationMachines supports. */\n+  private[regression] val supportedSolvers = Array(GD, AdamW)\n+\n+  /** String name for \"logisticLoss\". */\n+  private[regression] val LogisticLoss = \"logisticLoss\"\n+\n+  /** String name for \"squaredError\". */\n+  private[regression] val SquaredError = \"squaredError\"\n+\n+  /** Set of loss function names that FactorizationMachines supports. */\n+  private[regression] val supportedLosses = Array(SquaredError, LogisticLoss)\n+}\n+\n+/**\n+ * Model produced by [[FactorizationMachines]].\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachinesModel (\n+    @Since(\"3.0.0\") override val uid: String,\n+    @Since(\"3.0.0\") val coefficients: OldVector,"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "will this cause potential thread safety issue?",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-29T03:41:04Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++\n+        (if ($(fitBias)) Array.fill(1)(0.0) else Array.empty[Double]))\n+\n+    val data = instances.map { case OldLabeledPoint(label, features) => (label, features) }\n+\n+    // optimize coefficients with gradient descent\n+    val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+      $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+    val updater = $(solver) match {\n+      case GD => new SquaredL2Updater()\n+      case AdamW => new AdamWUpdater(coefficientsSize)\n+    }\n+\n+    val optimizer = new GradientDescent(gradient, updater)\n+      .setStepSize($(stepSize))\n+      .setNumIterations($(maxIter))\n+      .setRegParam($(regParam))\n+      .setMiniBatchFraction($(miniBatchFraction))\n+      .setConvergenceTol($(tol))\n+    val coefficients = optimizer.optimize(data, initialCoefficients)\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(new FactorizationMachinesModel(uid, coefficients, numFeatures))\n+    model\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachines = defaultCopy(extra)\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachines extends DefaultParamsReadable[FactorizationMachines] {\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachines = super.load(path)\n+\n+  /** String name for \"gd\". */\n+  private[regression] val GD = \"gd\"\n+\n+  /** String name for \"adamW\". */\n+  private[regression] val AdamW = \"adamW\"\n+\n+  /** Set of solvers that FactorizationMachines supports. */\n+  private[regression] val supportedSolvers = Array(GD, AdamW)\n+\n+  /** String name for \"logisticLoss\". */\n+  private[regression] val LogisticLoss = \"logisticLoss\"\n+\n+  /** String name for \"squaredError\". */\n+  private[regression] val SquaredError = \"squaredError\"\n+\n+  /** Set of loss function names that FactorizationMachines supports. */\n+  private[regression] val supportedLosses = Array(SquaredError, LogisticLoss)\n+}\n+\n+/**\n+ * Model produced by [[FactorizationMachines]].\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachinesModel (\n+    @Since(\"3.0.0\") override val uid: String,\n+    @Since(\"3.0.0\") val coefficients: OldVector,\n+    @Since(\"3.0.0\") override val numFeatures: Int)\n+  extends PredictionModel[Vector, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with MLWritable {\n+\n+  /**\n+   * Returns Factorization Machines coefficients\n+   * coefficients concat from 2-way coefficients, 1-way coefficients, global bias\n+   * index 0 ~ numFeatures*numFactors is 2-way coefficients,\n+   *   [i * numFactors + f] denotes i-th feature and f-th factor\n+   * Following indices are 1-way coefficients and global bias.\n+   *\n+   * @return Vector\n+   */\n+  @Since(\"3.0.0\")\n+  def getCoefficients: Vector = coefficients\n+\n+  private lazy val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+    $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+  override def predict(features: Vector): Double = {\n+    val rawPrediction = gradient.getRawPrediction(features, coefficients)\n+    gradient.getPrediction(rawPrediction)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachinesModel = {\n+    copyValues(new FactorizationMachinesModel(\n+      uid, coefficients, numFeatures), extra)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def write: MLWriter =\n+    new FactorizationMachinesModel.FactorizationMachinesModelWriter(this)\n+\n+  override def toString: String = {\n+    s\"FactorizationMachinesModel: \" +\n+      s\"uid = ${super.toString}, lossFunc = ${$(loss)}, numFeatures = $numFeatures, \" +\n+      s\"numFactors = ${$(numFactors)}, fitLinear = ${$(fitLinear)}, fitBias = ${$(fitBias)}\"\n+  }\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachinesModel extends MLReadable[FactorizationMachinesModel] {\n+\n+  @Since(\"3.0.0\")\n+  override def read: MLReader[FactorizationMachinesModel] = new FactorizationMachinesModelReader\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachinesModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FactorizationMachinesModel]] */\n+  private[FactorizationMachinesModel] class FactorizationMachinesModelWriter(\n+      instance: FactorizationMachinesModel) extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numFeatures: Int,\n+        coefficients: OldVector)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val data = Data(instance.numFeatures, instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sparkSession.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class FactorizationMachinesModelReader extends MLReader[FactorizationMachinesModel] {\n+\n+    private val className = classOf[FactorizationMachinesModel].getName\n+\n+    override def load(path: String): FactorizationMachinesModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sparkSession.read.format(\"parquet\").load(dataPath)\n+\n+      val Row(numFeatures: Int, coefficients: OldVector) = data\n+        .select(\"numFeatures\", \"coefficients\").head()\n+      val model = new FactorizationMachinesModel(\n+        metadata.uid, coefficients, numFeatures)\n+      metadata.getAndSetParams(model)\n+      model\n+    }\n+  }\n+}\n+\n+/**\n+ * Factorization Machines base gradient class\n+ * Implementing the raw FM formula, include raw prediction and raw gradient,\n+ * then inherit the base class to implement special gradient class(like logloss, mse).\n+ *\n+ * Factorization Machines raw formula:\n+ * {{{\n+ *   y_{fm} = w_0 + \\sum\\limits^n_{i-1} w_i x_i +\n+ *     \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1}  x_i x_j\n+ * }}}\n+ * the pairwise interactions (2-way term) can be reformulated:\n+ * {{{\n+ *   \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} <v_i, v_j> x_i x_j\n+ *   = \\frac{1}{2}\\sum\\limits^k_{f=1}\n+ *   \\left(\\left( \\sum\\limits^n_{i=1}v_{i,f}x_i \\right)^2 -\n+ *     \\sum\\limits^n_{i=1}v_{i,f}^2x_i^2 \\right)\n+ * }}}\n+ * and the gradients are:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta}y_{fm} = \\left\\{\n+ *   \\begin{align}\n+ *   &1, & if\\ \\theta\\ is\\ w_0 \\\\\n+ *   &x_i, & if\\ \\theta\\ is\\ w_i \\\\\n+ *   &x_i{\\sum}^n_{j=1}v_{j,f}x_j - v_{i,f}x_i^2, & if\\ \\theta\\ is\\ v_{i,j} \\\\\n+ *   \\end{align}\n+ *   \\right.\n+ * }}}\n+ *\n+ * Factorization Machines formula with prediction task:\n+ * {{{\n+ *   \\hat{y} = p\\left( y_{fm} \\right)\n+ * }}}\n+ * p is the prediction function, for binary classification task is sigmoid.\n+ * The loss funcation gradient formula:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( \\hat{y},y \\right) =\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( p\\left( y_{fm} \\right),y \\right) =\n+ *   \\frac{\\partial l}{\\partial \\hat{y}} \\cdot\n+ *   \\frac{\\partial \\hat{y}}{\\partial y_{fm}} \\cdot\n+ *   \\frac{\\partial y_{fm}}{\\partial\\theta}\n+ * }}}\n+ * Last term is same for all task, so be implemented in base gradient class.\n+ * last term named rawGradient in following code, and first two term named multiplier.\n+ */\n+private[ml] abstract class BaseFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int) extends Gradient {\n+\n+  override def compute(\n+      data: OldVector,\n+      label: Double,\n+      weights: OldVector,\n+      cumGradient: OldVector): Double = {\n+    val rawPrediction = getRawPrediction(data, weights)\n+    val rawGradient = getRawGradient(data, weights)\n+    val multiplier = getMultiplier(rawPrediction, label)\n+    axpy(multiplier, rawGradient, cumGradient)\n+    val loss = getLoss(rawPrediction, label)\n+    loss\n+  }\n+\n+  def getPrediction(rawPrediction: Double): Double\n+\n+  protected def getMultiplier(rawPrediction: Double, label: Double): Double\n+\n+  protected def getLoss(rawPrediction: Double, label: Double): Double\n+\n+  private val sumVX = Array.fill(numFactors)(0.0)"
  }, {
    "author": {
      "login": "mob-ai"
    },
    "body": "> will this cause potential thread safety issue?\r\n\r\nI think it is thread safety.\r\n\r\nThe GD calculates the gradient code: (spark/mllib/optimization/GradientDescent.scala: 239)\r\nGD will use treeAggregate to calculate gradient.\r\n\r\n```scala\r\n        .treeAggregate((BDV.zeros[Double](n), 0.0, 0L))(\r\n          seqOp = (c, v) => {\r\n            // c: (grad, loss, count), v: (label, features)\r\n            val l = gradient.compute(v._2, v._1, bcWeights.value, Vectors.fromBreeze(c._1))\r\n            (c._1, c._2 + l, c._3 + 1)\r\n          },\r\n          combOp = (c1, c2) => {\r\n            // c: (grad, loss, count)\r\n            (c1._1 += c2._1, c1._2 + c2._2, c1._3 + c2._3)\r\n          })\r\n```\r\n\r\ntreeAggregate code: (spark/rdd/RDD.scala: 1201)\r\ntreeAggregate will call `mapPartitions` to calculate local gradient pre partition. And in a partition, it will run `it.aggregate`.\r\n\r\n```scala\r\n      val cleanSeqOp = context.clean(seqOp)\r\n      val cleanCombOp = context.clean(combOp)\r\n      val aggregatePartition =\r\n        (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp)\r\n      var partiallyAggregated: RDD[U] = mapPartitions(it => Iterator(aggregatePartition(it)))\r\n```\r\n\r\nscala aggregate code: (scala/collection/TraversableOnce.scala: 214)\r\n`aggregate` only use seqop, I think it means cumulative gradient will be calculated sequentially in a partition.\r\n\r\n```scala\r\n  def aggregate[B](z: =>B)(seqop: (B, A) => B, combop: (B, B) => B): B = foldLeft(z)(seqop)\r\n```\r\n\r\n",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-29T09:17:52Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++\n+        (if ($(fitBias)) Array.fill(1)(0.0) else Array.empty[Double]))\n+\n+    val data = instances.map { case OldLabeledPoint(label, features) => (label, features) }\n+\n+    // optimize coefficients with gradient descent\n+    val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+      $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+    val updater = $(solver) match {\n+      case GD => new SquaredL2Updater()\n+      case AdamW => new AdamWUpdater(coefficientsSize)\n+    }\n+\n+    val optimizer = new GradientDescent(gradient, updater)\n+      .setStepSize($(stepSize))\n+      .setNumIterations($(maxIter))\n+      .setRegParam($(regParam))\n+      .setMiniBatchFraction($(miniBatchFraction))\n+      .setConvergenceTol($(tol))\n+    val coefficients = optimizer.optimize(data, initialCoefficients)\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(new FactorizationMachinesModel(uid, coefficients, numFeatures))\n+    model\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachines = defaultCopy(extra)\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachines extends DefaultParamsReadable[FactorizationMachines] {\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachines = super.load(path)\n+\n+  /** String name for \"gd\". */\n+  private[regression] val GD = \"gd\"\n+\n+  /** String name for \"adamW\". */\n+  private[regression] val AdamW = \"adamW\"\n+\n+  /** Set of solvers that FactorizationMachines supports. */\n+  private[regression] val supportedSolvers = Array(GD, AdamW)\n+\n+  /** String name for \"logisticLoss\". */\n+  private[regression] val LogisticLoss = \"logisticLoss\"\n+\n+  /** String name for \"squaredError\". */\n+  private[regression] val SquaredError = \"squaredError\"\n+\n+  /** Set of loss function names that FactorizationMachines supports. */\n+  private[regression] val supportedLosses = Array(SquaredError, LogisticLoss)\n+}\n+\n+/**\n+ * Model produced by [[FactorizationMachines]].\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachinesModel (\n+    @Since(\"3.0.0\") override val uid: String,\n+    @Since(\"3.0.0\") val coefficients: OldVector,\n+    @Since(\"3.0.0\") override val numFeatures: Int)\n+  extends PredictionModel[Vector, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with MLWritable {\n+\n+  /**\n+   * Returns Factorization Machines coefficients\n+   * coefficients concat from 2-way coefficients, 1-way coefficients, global bias\n+   * index 0 ~ numFeatures*numFactors is 2-way coefficients,\n+   *   [i * numFactors + f] denotes i-th feature and f-th factor\n+   * Following indices are 1-way coefficients and global bias.\n+   *\n+   * @return Vector\n+   */\n+  @Since(\"3.0.0\")\n+  def getCoefficients: Vector = coefficients\n+\n+  private lazy val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+    $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+  override def predict(features: Vector): Double = {\n+    val rawPrediction = gradient.getRawPrediction(features, coefficients)\n+    gradient.getPrediction(rawPrediction)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachinesModel = {\n+    copyValues(new FactorizationMachinesModel(\n+      uid, coefficients, numFeatures), extra)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def write: MLWriter =\n+    new FactorizationMachinesModel.FactorizationMachinesModelWriter(this)\n+\n+  override def toString: String = {\n+    s\"FactorizationMachinesModel: \" +\n+      s\"uid = ${super.toString}, lossFunc = ${$(loss)}, numFeatures = $numFeatures, \" +\n+      s\"numFactors = ${$(numFactors)}, fitLinear = ${$(fitLinear)}, fitBias = ${$(fitBias)}\"\n+  }\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachinesModel extends MLReadable[FactorizationMachinesModel] {\n+\n+  @Since(\"3.0.0\")\n+  override def read: MLReader[FactorizationMachinesModel] = new FactorizationMachinesModelReader\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachinesModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FactorizationMachinesModel]] */\n+  private[FactorizationMachinesModel] class FactorizationMachinesModelWriter(\n+      instance: FactorizationMachinesModel) extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numFeatures: Int,\n+        coefficients: OldVector)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val data = Data(instance.numFeatures, instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sparkSession.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class FactorizationMachinesModelReader extends MLReader[FactorizationMachinesModel] {\n+\n+    private val className = classOf[FactorizationMachinesModel].getName\n+\n+    override def load(path: String): FactorizationMachinesModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sparkSession.read.format(\"parquet\").load(dataPath)\n+\n+      val Row(numFeatures: Int, coefficients: OldVector) = data\n+        .select(\"numFeatures\", \"coefficients\").head()\n+      val model = new FactorizationMachinesModel(\n+        metadata.uid, coefficients, numFeatures)\n+      metadata.getAndSetParams(model)\n+      model\n+    }\n+  }\n+}\n+\n+/**\n+ * Factorization Machines base gradient class\n+ * Implementing the raw FM formula, include raw prediction and raw gradient,\n+ * then inherit the base class to implement special gradient class(like logloss, mse).\n+ *\n+ * Factorization Machines raw formula:\n+ * {{{\n+ *   y_{fm} = w_0 + \\sum\\limits^n_{i-1} w_i x_i +\n+ *     \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1}  x_i x_j\n+ * }}}\n+ * the pairwise interactions (2-way term) can be reformulated:\n+ * {{{\n+ *   \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} <v_i, v_j> x_i x_j\n+ *   = \\frac{1}{2}\\sum\\limits^k_{f=1}\n+ *   \\left(\\left( \\sum\\limits^n_{i=1}v_{i,f}x_i \\right)^2 -\n+ *     \\sum\\limits^n_{i=1}v_{i,f}^2x_i^2 \\right)\n+ * }}}\n+ * and the gradients are:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta}y_{fm} = \\left\\{\n+ *   \\begin{align}\n+ *   &1, & if\\ \\theta\\ is\\ w_0 \\\\\n+ *   &x_i, & if\\ \\theta\\ is\\ w_i \\\\\n+ *   &x_i{\\sum}^n_{j=1}v_{j,f}x_j - v_{i,f}x_i^2, & if\\ \\theta\\ is\\ v_{i,j} \\\\\n+ *   \\end{align}\n+ *   \\right.\n+ * }}}\n+ *\n+ * Factorization Machines formula with prediction task:\n+ * {{{\n+ *   \\hat{y} = p\\left( y_{fm} \\right)\n+ * }}}\n+ * p is the prediction function, for binary classification task is sigmoid.\n+ * The loss funcation gradient formula:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( \\hat{y},y \\right) =\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( p\\left( y_{fm} \\right),y \\right) =\n+ *   \\frac{\\partial l}{\\partial \\hat{y}} \\cdot\n+ *   \\frac{\\partial \\hat{y}}{\\partial y_{fm}} \\cdot\n+ *   \\frac{\\partial y_{fm}}{\\partial\\theta}\n+ * }}}\n+ * Last term is same for all task, so be implemented in base gradient class.\n+ * last term named rawGradient in following code, and first two term named multiplier.\n+ */\n+private[ml] abstract class BaseFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int) extends Gradient {\n+\n+  override def compute(\n+      data: OldVector,\n+      label: Double,\n+      weights: OldVector,\n+      cumGradient: OldVector): Double = {\n+    val rawPrediction = getRawPrediction(data, weights)\n+    val rawGradient = getRawGradient(data, weights)\n+    val multiplier = getMultiplier(rawPrediction, label)\n+    axpy(multiplier, rawGradient, cumGradient)\n+    val loss = getLoss(rawPrediction, label)\n+    loss\n+  }\n+\n+  def getPrediction(rawPrediction: Double): Double\n+\n+  protected def getMultiplier(rawPrediction: Double, label: Double): Double\n+\n+  protected def getLoss(rawPrediction: Double, label: Double): Double\n+\n+  private val sumVX = Array.fill(numFactors)(0.0)"
  }, {
    "author": {
      "login": "zhengruifeng"
    },
    "body": "In this PR, method `getRawPrediction` is not only used in training, but also used in prediction. What if user call method `predict` concurrently?",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-11-08T10:59:26Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++\n+        (if ($(fitBias)) Array.fill(1)(0.0) else Array.empty[Double]))\n+\n+    val data = instances.map { case OldLabeledPoint(label, features) => (label, features) }\n+\n+    // optimize coefficients with gradient descent\n+    val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+      $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+    val updater = $(solver) match {\n+      case GD => new SquaredL2Updater()\n+      case AdamW => new AdamWUpdater(coefficientsSize)\n+    }\n+\n+    val optimizer = new GradientDescent(gradient, updater)\n+      .setStepSize($(stepSize))\n+      .setNumIterations($(maxIter))\n+      .setRegParam($(regParam))\n+      .setMiniBatchFraction($(miniBatchFraction))\n+      .setConvergenceTol($(tol))\n+    val coefficients = optimizer.optimize(data, initialCoefficients)\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(new FactorizationMachinesModel(uid, coefficients, numFeatures))\n+    model\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachines = defaultCopy(extra)\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachines extends DefaultParamsReadable[FactorizationMachines] {\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachines = super.load(path)\n+\n+  /** String name for \"gd\". */\n+  private[regression] val GD = \"gd\"\n+\n+  /** String name for \"adamW\". */\n+  private[regression] val AdamW = \"adamW\"\n+\n+  /** Set of solvers that FactorizationMachines supports. */\n+  private[regression] val supportedSolvers = Array(GD, AdamW)\n+\n+  /** String name for \"logisticLoss\". */\n+  private[regression] val LogisticLoss = \"logisticLoss\"\n+\n+  /** String name for \"squaredError\". */\n+  private[regression] val SquaredError = \"squaredError\"\n+\n+  /** Set of loss function names that FactorizationMachines supports. */\n+  private[regression] val supportedLosses = Array(SquaredError, LogisticLoss)\n+}\n+\n+/**\n+ * Model produced by [[FactorizationMachines]].\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachinesModel (\n+    @Since(\"3.0.0\") override val uid: String,\n+    @Since(\"3.0.0\") val coefficients: OldVector,\n+    @Since(\"3.0.0\") override val numFeatures: Int)\n+  extends PredictionModel[Vector, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with MLWritable {\n+\n+  /**\n+   * Returns Factorization Machines coefficients\n+   * coefficients concat from 2-way coefficients, 1-way coefficients, global bias\n+   * index 0 ~ numFeatures*numFactors is 2-way coefficients,\n+   *   [i * numFactors + f] denotes i-th feature and f-th factor\n+   * Following indices are 1-way coefficients and global bias.\n+   *\n+   * @return Vector\n+   */\n+  @Since(\"3.0.0\")\n+  def getCoefficients: Vector = coefficients\n+\n+  private lazy val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+    $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+  override def predict(features: Vector): Double = {\n+    val rawPrediction = gradient.getRawPrediction(features, coefficients)\n+    gradient.getPrediction(rawPrediction)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachinesModel = {\n+    copyValues(new FactorizationMachinesModel(\n+      uid, coefficients, numFeatures), extra)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def write: MLWriter =\n+    new FactorizationMachinesModel.FactorizationMachinesModelWriter(this)\n+\n+  override def toString: String = {\n+    s\"FactorizationMachinesModel: \" +\n+      s\"uid = ${super.toString}, lossFunc = ${$(loss)}, numFeatures = $numFeatures, \" +\n+      s\"numFactors = ${$(numFactors)}, fitLinear = ${$(fitLinear)}, fitBias = ${$(fitBias)}\"\n+  }\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachinesModel extends MLReadable[FactorizationMachinesModel] {\n+\n+  @Since(\"3.0.0\")\n+  override def read: MLReader[FactorizationMachinesModel] = new FactorizationMachinesModelReader\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachinesModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FactorizationMachinesModel]] */\n+  private[FactorizationMachinesModel] class FactorizationMachinesModelWriter(\n+      instance: FactorizationMachinesModel) extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numFeatures: Int,\n+        coefficients: OldVector)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val data = Data(instance.numFeatures, instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sparkSession.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class FactorizationMachinesModelReader extends MLReader[FactorizationMachinesModel] {\n+\n+    private val className = classOf[FactorizationMachinesModel].getName\n+\n+    override def load(path: String): FactorizationMachinesModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sparkSession.read.format(\"parquet\").load(dataPath)\n+\n+      val Row(numFeatures: Int, coefficients: OldVector) = data\n+        .select(\"numFeatures\", \"coefficients\").head()\n+      val model = new FactorizationMachinesModel(\n+        metadata.uid, coefficients, numFeatures)\n+      metadata.getAndSetParams(model)\n+      model\n+    }\n+  }\n+}\n+\n+/**\n+ * Factorization Machines base gradient class\n+ * Implementing the raw FM formula, include raw prediction and raw gradient,\n+ * then inherit the base class to implement special gradient class(like logloss, mse).\n+ *\n+ * Factorization Machines raw formula:\n+ * {{{\n+ *   y_{fm} = w_0 + \\sum\\limits^n_{i-1} w_i x_i +\n+ *     \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1}  x_i x_j\n+ * }}}\n+ * the pairwise interactions (2-way term) can be reformulated:\n+ * {{{\n+ *   \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} <v_i, v_j> x_i x_j\n+ *   = \\frac{1}{2}\\sum\\limits^k_{f=1}\n+ *   \\left(\\left( \\sum\\limits^n_{i=1}v_{i,f}x_i \\right)^2 -\n+ *     \\sum\\limits^n_{i=1}v_{i,f}^2x_i^2 \\right)\n+ * }}}\n+ * and the gradients are:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta}y_{fm} = \\left\\{\n+ *   \\begin{align}\n+ *   &1, & if\\ \\theta\\ is\\ w_0 \\\\\n+ *   &x_i, & if\\ \\theta\\ is\\ w_i \\\\\n+ *   &x_i{\\sum}^n_{j=1}v_{j,f}x_j - v_{i,f}x_i^2, & if\\ \\theta\\ is\\ v_{i,j} \\\\\n+ *   \\end{align}\n+ *   \\right.\n+ * }}}\n+ *\n+ * Factorization Machines formula with prediction task:\n+ * {{{\n+ *   \\hat{y} = p\\left( y_{fm} \\right)\n+ * }}}\n+ * p is the prediction function, for binary classification task is sigmoid.\n+ * The loss funcation gradient formula:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( \\hat{y},y \\right) =\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( p\\left( y_{fm} \\right),y \\right) =\n+ *   \\frac{\\partial l}{\\partial \\hat{y}} \\cdot\n+ *   \\frac{\\partial \\hat{y}}{\\partial y_{fm}} \\cdot\n+ *   \\frac{\\partial y_{fm}}{\\partial\\theta}\n+ * }}}\n+ * Last term is same for all task, so be implemented in base gradient class.\n+ * last term named rawGradient in following code, and first two term named multiplier.\n+ */\n+private[ml] abstract class BaseFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int) extends Gradient {\n+\n+  override def compute(\n+      data: OldVector,\n+      label: Double,\n+      weights: OldVector,\n+      cumGradient: OldVector): Double = {\n+    val rawPrediction = getRawPrediction(data, weights)\n+    val rawGradient = getRawGradient(data, weights)\n+    val multiplier = getMultiplier(rawPrediction, label)\n+    axpy(multiplier, rawGradient, cumGradient)\n+    val loss = getLoss(rawPrediction, label)\n+    loss\n+  }\n+\n+  def getPrediction(rawPrediction: Double): Double\n+\n+  protected def getMultiplier(rawPrediction: Double, label: Double): Double\n+\n+  protected def getLoss(rawPrediction: Double, label: Double): Double\n+\n+  private val sumVX = Array.fill(numFactors)(0.0)"
  }, {
    "author": {
      "login": "mob-ai"
    },
    "body": "> In this PR, method `getRawPrediction` is not only used in training, but also used in prediction. What if user call method `predict` concurrently?\r\n\r\nWhen user call method `predict`, val `sumVX` will not be used. (`sumVX` is only used when calculate gradient)",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-11-11T03:13:13Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++\n+        (if ($(fitBias)) Array.fill(1)(0.0) else Array.empty[Double]))\n+\n+    val data = instances.map { case OldLabeledPoint(label, features) => (label, features) }\n+\n+    // optimize coefficients with gradient descent\n+    val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+      $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+    val updater = $(solver) match {\n+      case GD => new SquaredL2Updater()\n+      case AdamW => new AdamWUpdater(coefficientsSize)\n+    }\n+\n+    val optimizer = new GradientDescent(gradient, updater)\n+      .setStepSize($(stepSize))\n+      .setNumIterations($(maxIter))\n+      .setRegParam($(regParam))\n+      .setMiniBatchFraction($(miniBatchFraction))\n+      .setConvergenceTol($(tol))\n+    val coefficients = optimizer.optimize(data, initialCoefficients)\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(new FactorizationMachinesModel(uid, coefficients, numFeatures))\n+    model\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachines = defaultCopy(extra)\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachines extends DefaultParamsReadable[FactorizationMachines] {\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachines = super.load(path)\n+\n+  /** String name for \"gd\". */\n+  private[regression] val GD = \"gd\"\n+\n+  /** String name for \"adamW\". */\n+  private[regression] val AdamW = \"adamW\"\n+\n+  /** Set of solvers that FactorizationMachines supports. */\n+  private[regression] val supportedSolvers = Array(GD, AdamW)\n+\n+  /** String name for \"logisticLoss\". */\n+  private[regression] val LogisticLoss = \"logisticLoss\"\n+\n+  /** String name for \"squaredError\". */\n+  private[regression] val SquaredError = \"squaredError\"\n+\n+  /** Set of loss function names that FactorizationMachines supports. */\n+  private[regression] val supportedLosses = Array(SquaredError, LogisticLoss)\n+}\n+\n+/**\n+ * Model produced by [[FactorizationMachines]].\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachinesModel (\n+    @Since(\"3.0.0\") override val uid: String,\n+    @Since(\"3.0.0\") val coefficients: OldVector,\n+    @Since(\"3.0.0\") override val numFeatures: Int)\n+  extends PredictionModel[Vector, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with MLWritable {\n+\n+  /**\n+   * Returns Factorization Machines coefficients\n+   * coefficients concat from 2-way coefficients, 1-way coefficients, global bias\n+   * index 0 ~ numFeatures*numFactors is 2-way coefficients,\n+   *   [i * numFactors + f] denotes i-th feature and f-th factor\n+   * Following indices are 1-way coefficients and global bias.\n+   *\n+   * @return Vector\n+   */\n+  @Since(\"3.0.0\")\n+  def getCoefficients: Vector = coefficients\n+\n+  private lazy val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+    $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+  override def predict(features: Vector): Double = {\n+    val rawPrediction = gradient.getRawPrediction(features, coefficients)\n+    gradient.getPrediction(rawPrediction)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachinesModel = {\n+    copyValues(new FactorizationMachinesModel(\n+      uid, coefficients, numFeatures), extra)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def write: MLWriter =\n+    new FactorizationMachinesModel.FactorizationMachinesModelWriter(this)\n+\n+  override def toString: String = {\n+    s\"FactorizationMachinesModel: \" +\n+      s\"uid = ${super.toString}, lossFunc = ${$(loss)}, numFeatures = $numFeatures, \" +\n+      s\"numFactors = ${$(numFactors)}, fitLinear = ${$(fitLinear)}, fitBias = ${$(fitBias)}\"\n+  }\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachinesModel extends MLReadable[FactorizationMachinesModel] {\n+\n+  @Since(\"3.0.0\")\n+  override def read: MLReader[FactorizationMachinesModel] = new FactorizationMachinesModelReader\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachinesModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FactorizationMachinesModel]] */\n+  private[FactorizationMachinesModel] class FactorizationMachinesModelWriter(\n+      instance: FactorizationMachinesModel) extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numFeatures: Int,\n+        coefficients: OldVector)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val data = Data(instance.numFeatures, instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sparkSession.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class FactorizationMachinesModelReader extends MLReader[FactorizationMachinesModel] {\n+\n+    private val className = classOf[FactorizationMachinesModel].getName\n+\n+    override def load(path: String): FactorizationMachinesModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sparkSession.read.format(\"parquet\").load(dataPath)\n+\n+      val Row(numFeatures: Int, coefficients: OldVector) = data\n+        .select(\"numFeatures\", \"coefficients\").head()\n+      val model = new FactorizationMachinesModel(\n+        metadata.uid, coefficients, numFeatures)\n+      metadata.getAndSetParams(model)\n+      model\n+    }\n+  }\n+}\n+\n+/**\n+ * Factorization Machines base gradient class\n+ * Implementing the raw FM formula, include raw prediction and raw gradient,\n+ * then inherit the base class to implement special gradient class(like logloss, mse).\n+ *\n+ * Factorization Machines raw formula:\n+ * {{{\n+ *   y_{fm} = w_0 + \\sum\\limits^n_{i-1} w_i x_i +\n+ *     \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1}  x_i x_j\n+ * }}}\n+ * the pairwise interactions (2-way term) can be reformulated:\n+ * {{{\n+ *   \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} <v_i, v_j> x_i x_j\n+ *   = \\frac{1}{2}\\sum\\limits^k_{f=1}\n+ *   \\left(\\left( \\sum\\limits^n_{i=1}v_{i,f}x_i \\right)^2 -\n+ *     \\sum\\limits^n_{i=1}v_{i,f}^2x_i^2 \\right)\n+ * }}}\n+ * and the gradients are:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta}y_{fm} = \\left\\{\n+ *   \\begin{align}\n+ *   &1, & if\\ \\theta\\ is\\ w_0 \\\\\n+ *   &x_i, & if\\ \\theta\\ is\\ w_i \\\\\n+ *   &x_i{\\sum}^n_{j=1}v_{j,f}x_j - v_{i,f}x_i^2, & if\\ \\theta\\ is\\ v_{i,j} \\\\\n+ *   \\end{align}\n+ *   \\right.\n+ * }}}\n+ *\n+ * Factorization Machines formula with prediction task:\n+ * {{{\n+ *   \\hat{y} = p\\left( y_{fm} \\right)\n+ * }}}\n+ * p is the prediction function, for binary classification task is sigmoid.\n+ * The loss funcation gradient formula:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( \\hat{y},y \\right) =\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( p\\left( y_{fm} \\right),y \\right) =\n+ *   \\frac{\\partial l}{\\partial \\hat{y}} \\cdot\n+ *   \\frac{\\partial \\hat{y}}{\\partial y_{fm}} \\cdot\n+ *   \\frac{\\partial y_{fm}}{\\partial\\theta}\n+ * }}}\n+ * Last term is same for all task, so be implemented in base gradient class.\n+ * last term named rawGradient in following code, and first two term named multiplier.\n+ */\n+private[ml] abstract class BaseFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int) extends Gradient {\n+\n+  override def compute(\n+      data: OldVector,\n+      label: Double,\n+      weights: OldVector,\n+      cumGradient: OldVector): Double = {\n+    val rawPrediction = getRawPrediction(data, weights)\n+    val rawGradient = getRawGradient(data, weights)\n+    val multiplier = getMultiplier(rawPrediction, label)\n+    axpy(multiplier, rawGradient, cumGradient)\n+    val loss = getLoss(rawPrediction, label)\n+    loss\n+  }\n+\n+  def getPrediction(rawPrediction: Double): Double\n+\n+  protected def getMultiplier(rawPrediction: Double, label: Double): Double\n+\n+  protected def getLoss(rawPrediction: Double, label: Double): Double\n+\n+  private val sumVX = Array.fill(numFactors)(0.0)"
  }, {
    "author": {
      "login": "mob-ai"
    },
    "body": "But I will change it to avoid more worry.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-11-11T03:51:41Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++\n+        (if ($(fitBias)) Array.fill(1)(0.0) else Array.empty[Double]))\n+\n+    val data = instances.map { case OldLabeledPoint(label, features) => (label, features) }\n+\n+    // optimize coefficients with gradient descent\n+    val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+      $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+    val updater = $(solver) match {\n+      case GD => new SquaredL2Updater()\n+      case AdamW => new AdamWUpdater(coefficientsSize)\n+    }\n+\n+    val optimizer = new GradientDescent(gradient, updater)\n+      .setStepSize($(stepSize))\n+      .setNumIterations($(maxIter))\n+      .setRegParam($(regParam))\n+      .setMiniBatchFraction($(miniBatchFraction))\n+      .setConvergenceTol($(tol))\n+    val coefficients = optimizer.optimize(data, initialCoefficients)\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(new FactorizationMachinesModel(uid, coefficients, numFeatures))\n+    model\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachines = defaultCopy(extra)\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachines extends DefaultParamsReadable[FactorizationMachines] {\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachines = super.load(path)\n+\n+  /** String name for \"gd\". */\n+  private[regression] val GD = \"gd\"\n+\n+  /** String name for \"adamW\". */\n+  private[regression] val AdamW = \"adamW\"\n+\n+  /** Set of solvers that FactorizationMachines supports. */\n+  private[regression] val supportedSolvers = Array(GD, AdamW)\n+\n+  /** String name for \"logisticLoss\". */\n+  private[regression] val LogisticLoss = \"logisticLoss\"\n+\n+  /** String name for \"squaredError\". */\n+  private[regression] val SquaredError = \"squaredError\"\n+\n+  /** Set of loss function names that FactorizationMachines supports. */\n+  private[regression] val supportedLosses = Array(SquaredError, LogisticLoss)\n+}\n+\n+/**\n+ * Model produced by [[FactorizationMachines]].\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachinesModel (\n+    @Since(\"3.0.0\") override val uid: String,\n+    @Since(\"3.0.0\") val coefficients: OldVector,\n+    @Since(\"3.0.0\") override val numFeatures: Int)\n+  extends PredictionModel[Vector, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with MLWritable {\n+\n+  /**\n+   * Returns Factorization Machines coefficients\n+   * coefficients concat from 2-way coefficients, 1-way coefficients, global bias\n+   * index 0 ~ numFeatures*numFactors is 2-way coefficients,\n+   *   [i * numFactors + f] denotes i-th feature and f-th factor\n+   * Following indices are 1-way coefficients and global bias.\n+   *\n+   * @return Vector\n+   */\n+  @Since(\"3.0.0\")\n+  def getCoefficients: Vector = coefficients\n+\n+  private lazy val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+    $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+  override def predict(features: Vector): Double = {\n+    val rawPrediction = gradient.getRawPrediction(features, coefficients)\n+    gradient.getPrediction(rawPrediction)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachinesModel = {\n+    copyValues(new FactorizationMachinesModel(\n+      uid, coefficients, numFeatures), extra)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def write: MLWriter =\n+    new FactorizationMachinesModel.FactorizationMachinesModelWriter(this)\n+\n+  override def toString: String = {\n+    s\"FactorizationMachinesModel: \" +\n+      s\"uid = ${super.toString}, lossFunc = ${$(loss)}, numFeatures = $numFeatures, \" +\n+      s\"numFactors = ${$(numFactors)}, fitLinear = ${$(fitLinear)}, fitBias = ${$(fitBias)}\"\n+  }\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachinesModel extends MLReadable[FactorizationMachinesModel] {\n+\n+  @Since(\"3.0.0\")\n+  override def read: MLReader[FactorizationMachinesModel] = new FactorizationMachinesModelReader\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachinesModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FactorizationMachinesModel]] */\n+  private[FactorizationMachinesModel] class FactorizationMachinesModelWriter(\n+      instance: FactorizationMachinesModel) extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numFeatures: Int,\n+        coefficients: OldVector)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val data = Data(instance.numFeatures, instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sparkSession.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class FactorizationMachinesModelReader extends MLReader[FactorizationMachinesModel] {\n+\n+    private val className = classOf[FactorizationMachinesModel].getName\n+\n+    override def load(path: String): FactorizationMachinesModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sparkSession.read.format(\"parquet\").load(dataPath)\n+\n+      val Row(numFeatures: Int, coefficients: OldVector) = data\n+        .select(\"numFeatures\", \"coefficients\").head()\n+      val model = new FactorizationMachinesModel(\n+        metadata.uid, coefficients, numFeatures)\n+      metadata.getAndSetParams(model)\n+      model\n+    }\n+  }\n+}\n+\n+/**\n+ * Factorization Machines base gradient class\n+ * Implementing the raw FM formula, include raw prediction and raw gradient,\n+ * then inherit the base class to implement special gradient class(like logloss, mse).\n+ *\n+ * Factorization Machines raw formula:\n+ * {{{\n+ *   y_{fm} = w_0 + \\sum\\limits^n_{i-1} w_i x_i +\n+ *     \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1}  x_i x_j\n+ * }}}\n+ * the pairwise interactions (2-way term) can be reformulated:\n+ * {{{\n+ *   \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} <v_i, v_j> x_i x_j\n+ *   = \\frac{1}{2}\\sum\\limits^k_{f=1}\n+ *   \\left(\\left( \\sum\\limits^n_{i=1}v_{i,f}x_i \\right)^2 -\n+ *     \\sum\\limits^n_{i=1}v_{i,f}^2x_i^2 \\right)\n+ * }}}\n+ * and the gradients are:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta}y_{fm} = \\left\\{\n+ *   \\begin{align}\n+ *   &1, & if\\ \\theta\\ is\\ w_0 \\\\\n+ *   &x_i, & if\\ \\theta\\ is\\ w_i \\\\\n+ *   &x_i{\\sum}^n_{j=1}v_{j,f}x_j - v_{i,f}x_i^2, & if\\ \\theta\\ is\\ v_{i,j} \\\\\n+ *   \\end{align}\n+ *   \\right.\n+ * }}}\n+ *\n+ * Factorization Machines formula with prediction task:\n+ * {{{\n+ *   \\hat{y} = p\\left( y_{fm} \\right)\n+ * }}}\n+ * p is the prediction function, for binary classification task is sigmoid.\n+ * The loss funcation gradient formula:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( \\hat{y},y \\right) =\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( p\\left( y_{fm} \\right),y \\right) =\n+ *   \\frac{\\partial l}{\\partial \\hat{y}} \\cdot\n+ *   \\frac{\\partial \\hat{y}}{\\partial y_{fm}} \\cdot\n+ *   \\frac{\\partial y_{fm}}{\\partial\\theta}\n+ * }}}\n+ * Last term is same for all task, so be implemented in base gradient class.\n+ * last term named rawGradient in following code, and first two term named multiplier.\n+ */\n+private[ml] abstract class BaseFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int) extends Gradient {\n+\n+  override def compute(\n+      data: OldVector,\n+      label: Double,\n+      weights: OldVector,\n+      cumGradient: OldVector): Double = {\n+    val rawPrediction = getRawPrediction(data, weights)\n+    val rawGradient = getRawGradient(data, weights)\n+    val multiplier = getMultiplier(rawPrediction, label)\n+    axpy(multiplier, rawGradient, cumGradient)\n+    val loss = getLoss(rawPrediction, label)\n+    loss\n+  }\n+\n+  def getPrediction(rawPrediction: Double): Double\n+\n+  protected def getMultiplier(rawPrediction: Double, label: Double): Double\n+\n+  protected def getLoss(rawPrediction: Double, label: Double): Double\n+\n+  private val sumVX = Array.fill(numFactors)(0.0)"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "these params are not exposed to end users?",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-29T03:46:49Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++\n+        (if ($(fitBias)) Array.fill(1)(0.0) else Array.empty[Double]))\n+\n+    val data = instances.map { case OldLabeledPoint(label, features) => (label, features) }\n+\n+    // optimize coefficients with gradient descent\n+    val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+      $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+    val updater = $(solver) match {\n+      case GD => new SquaredL2Updater()\n+      case AdamW => new AdamWUpdater(coefficientsSize)\n+    }\n+\n+    val optimizer = new GradientDescent(gradient, updater)\n+      .setStepSize($(stepSize))\n+      .setNumIterations($(maxIter))\n+      .setRegParam($(regParam))\n+      .setMiniBatchFraction($(miniBatchFraction))\n+      .setConvergenceTol($(tol))\n+    val coefficients = optimizer.optimize(data, initialCoefficients)\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(new FactorizationMachinesModel(uid, coefficients, numFeatures))\n+    model\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachines = defaultCopy(extra)\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachines extends DefaultParamsReadable[FactorizationMachines] {\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachines = super.load(path)\n+\n+  /** String name for \"gd\". */\n+  private[regression] val GD = \"gd\"\n+\n+  /** String name for \"adamW\". */\n+  private[regression] val AdamW = \"adamW\"\n+\n+  /** Set of solvers that FactorizationMachines supports. */\n+  private[regression] val supportedSolvers = Array(GD, AdamW)\n+\n+  /** String name for \"logisticLoss\". */\n+  private[regression] val LogisticLoss = \"logisticLoss\"\n+\n+  /** String name for \"squaredError\". */\n+  private[regression] val SquaredError = \"squaredError\"\n+\n+  /** Set of loss function names that FactorizationMachines supports. */\n+  private[regression] val supportedLosses = Array(SquaredError, LogisticLoss)\n+}\n+\n+/**\n+ * Model produced by [[FactorizationMachines]].\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachinesModel (\n+    @Since(\"3.0.0\") override val uid: String,\n+    @Since(\"3.0.0\") val coefficients: OldVector,\n+    @Since(\"3.0.0\") override val numFeatures: Int)\n+  extends PredictionModel[Vector, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with MLWritable {\n+\n+  /**\n+   * Returns Factorization Machines coefficients\n+   * coefficients concat from 2-way coefficients, 1-way coefficients, global bias\n+   * index 0 ~ numFeatures*numFactors is 2-way coefficients,\n+   *   [i * numFactors + f] denotes i-th feature and f-th factor\n+   * Following indices are 1-way coefficients and global bias.\n+   *\n+   * @return Vector\n+   */\n+  @Since(\"3.0.0\")\n+  def getCoefficients: Vector = coefficients\n+\n+  private lazy val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+    $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+  override def predict(features: Vector): Double = {\n+    val rawPrediction = gradient.getRawPrediction(features, coefficients)\n+    gradient.getPrediction(rawPrediction)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachinesModel = {\n+    copyValues(new FactorizationMachinesModel(\n+      uid, coefficients, numFeatures), extra)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def write: MLWriter =\n+    new FactorizationMachinesModel.FactorizationMachinesModelWriter(this)\n+\n+  override def toString: String = {\n+    s\"FactorizationMachinesModel: \" +\n+      s\"uid = ${super.toString}, lossFunc = ${$(loss)}, numFeatures = $numFeatures, \" +\n+      s\"numFactors = ${$(numFactors)}, fitLinear = ${$(fitLinear)}, fitBias = ${$(fitBias)}\"\n+  }\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachinesModel extends MLReadable[FactorizationMachinesModel] {\n+\n+  @Since(\"3.0.0\")\n+  override def read: MLReader[FactorizationMachinesModel] = new FactorizationMachinesModelReader\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachinesModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FactorizationMachinesModel]] */\n+  private[FactorizationMachinesModel] class FactorizationMachinesModelWriter(\n+      instance: FactorizationMachinesModel) extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numFeatures: Int,\n+        coefficients: OldVector)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val data = Data(instance.numFeatures, instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sparkSession.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class FactorizationMachinesModelReader extends MLReader[FactorizationMachinesModel] {\n+\n+    private val className = classOf[FactorizationMachinesModel].getName\n+\n+    override def load(path: String): FactorizationMachinesModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sparkSession.read.format(\"parquet\").load(dataPath)\n+\n+      val Row(numFeatures: Int, coefficients: OldVector) = data\n+        .select(\"numFeatures\", \"coefficients\").head()\n+      val model = new FactorizationMachinesModel(\n+        metadata.uid, coefficients, numFeatures)\n+      metadata.getAndSetParams(model)\n+      model\n+    }\n+  }\n+}\n+\n+/**\n+ * Factorization Machines base gradient class\n+ * Implementing the raw FM formula, include raw prediction and raw gradient,\n+ * then inherit the base class to implement special gradient class(like logloss, mse).\n+ *\n+ * Factorization Machines raw formula:\n+ * {{{\n+ *   y_{fm} = w_0 + \\sum\\limits^n_{i-1} w_i x_i +\n+ *     \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1}  x_i x_j\n+ * }}}\n+ * the pairwise interactions (2-way term) can be reformulated:\n+ * {{{\n+ *   \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} <v_i, v_j> x_i x_j\n+ *   = \\frac{1}{2}\\sum\\limits^k_{f=1}\n+ *   \\left(\\left( \\sum\\limits^n_{i=1}v_{i,f}x_i \\right)^2 -\n+ *     \\sum\\limits^n_{i=1}v_{i,f}^2x_i^2 \\right)\n+ * }}}\n+ * and the gradients are:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta}y_{fm} = \\left\\{\n+ *   \\begin{align}\n+ *   &1, & if\\ \\theta\\ is\\ w_0 \\\\\n+ *   &x_i, & if\\ \\theta\\ is\\ w_i \\\\\n+ *   &x_i{\\sum}^n_{j=1}v_{j,f}x_j - v_{i,f}x_i^2, & if\\ \\theta\\ is\\ v_{i,j} \\\\\n+ *   \\end{align}\n+ *   \\right.\n+ * }}}\n+ *\n+ * Factorization Machines formula with prediction task:\n+ * {{{\n+ *   \\hat{y} = p\\left( y_{fm} \\right)\n+ * }}}\n+ * p is the prediction function, for binary classification task is sigmoid.\n+ * The loss funcation gradient formula:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( \\hat{y},y \\right) =\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( p\\left( y_{fm} \\right),y \\right) =\n+ *   \\frac{\\partial l}{\\partial \\hat{y}} \\cdot\n+ *   \\frac{\\partial \\hat{y}}{\\partial y_{fm}} \\cdot\n+ *   \\frac{\\partial y_{fm}}{\\partial\\theta}\n+ * }}}\n+ * Last term is same for all task, so be implemented in base gradient class.\n+ * last term named rawGradient in following code, and first two term named multiplier.\n+ */\n+private[ml] abstract class BaseFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int) extends Gradient {\n+\n+  override def compute(\n+      data: OldVector,\n+      label: Double,\n+      weights: OldVector,\n+      cumGradient: OldVector): Double = {\n+    val rawPrediction = getRawPrediction(data, weights)\n+    val rawGradient = getRawGradient(data, weights)\n+    val multiplier = getMultiplier(rawPrediction, label)\n+    axpy(multiplier, rawGradient, cumGradient)\n+    val loss = getLoss(rawPrediction, label)\n+    loss\n+  }\n+\n+  def getPrediction(rawPrediction: Double): Double\n+\n+  protected def getMultiplier(rawPrediction: Double, label: Double): Double\n+\n+  protected def getLoss(rawPrediction: Double, label: Double): Double\n+\n+  private val sumVX = Array.fill(numFactors)(0.0)\n+\n+  def getRawPrediction(data: OldVector, weights: OldVector): Double = {\n+    var rawPrediction = 0.0\n+    val vWeightsSize = numFeatures * numFactors\n+\n+    if (fitBias) rawPrediction += weights(weights.size - 1)\n+    if (fitLinear) {\n+      data.foreachActive { case (index, value) =>\n+        rawPrediction += weights(vWeightsSize + index) * value\n+      }\n+    }\n+    (0 until numFactors).foreach { f =>\n+      var sumSquare = 0.0\n+      var sum = 0.0\n+      data.foreachActive { case (index, value) =>\n+        val vx = weights(index * numFactors + f) * value\n+        sumSquare += vx * vx\n+        sum += vx\n+      }\n+      sumVX(f) = sum\n+      rawPrediction += 0.5 * (sum * sum - sumSquare)\n+    }\n+\n+    rawPrediction\n+  }\n+\n+  private def getRawGradient(data: OldVector, weights: OldVector): OldVector = {\n+    data match {\n+      // Usually Factorization Machines is used, there will be a lot of sparse features.\n+      // So need to optimize the gradient descent of sparse vector.\n+      case data: OldLinalg.SparseVector =>\n+        val gardSize = data.indices.length * numFactors +\n+          (if (fitLinear) data.indices.length else 0) +\n+          (if (fitBias) 1 else 0)\n+        val gradIndex = Array.fill(gardSize)(0)\n+        val gradValue = Array.fill(gardSize)(0.0)\n+        var gradI = 0\n+        val vWeightsSize = numFeatures * numFactors\n+\n+        data.foreachActive { case (index, value) =>\n+          (0 until numFactors).foreach { f =>\n+            gradIndex(gradI) = index * numFactors + f\n+            gradValue(gradI) = value * sumVX(f) - weights(index * numFactors + f) * value * value\n+            gradI += 1\n+          }\n+        }\n+        if (fitLinear) {\n+          data.foreachActive { case (index, value) =>\n+            gradIndex(gradI) = vWeightsSize + index\n+            gradValue(gradI) = value\n+            gradI += 1\n+          }\n+        }\n+        if (fitBias) {\n+          gradIndex(gradI) = weights.size - 1\n+          gradValue(gradI) = 1.0\n+        }\n+\n+        OldVectors.sparse(weights.size, gradIndex, gradValue)\n+      case data: OldLinalg.DenseVector =>\n+        val gradient = Array.fill(weights.size)(0.0)\n+        val vWeightsSize = numFeatures * numFactors\n+\n+        if (fitBias) gradient(weights.size - 1) += 1.0\n+        if (fitLinear) {\n+          data.foreachActive { case (index, value) =>\n+            gradient(vWeightsSize + index) += value\n+          }\n+        }\n+        (0 until numFactors).foreach { f =>\n+          data.foreachActive { case (index, value) =>\n+            gradient(index * numFactors + f) +=\n+              value * sumVX(f) - weights(index * numFactors + f) * value * value\n+          }\n+        }\n+\n+        OldVectors.dense(gradient)\n+    }\n+  }\n+}\n+\n+object BaseFactorizationMachinesGradient {\n+  def parseLoss(\n+      lossFunc: String,\n+      numFactors: Int,\n+      fitBias: Boolean,\n+      fitLinear: Boolean,\n+      numFeatures: Int): BaseFactorizationMachinesGradient = {\n+\n+    import FactorizationMachines._\n+\n+    lossFunc match {\n+      case LogisticLoss =>\n+        new LogisticFactorizationMachinesGradient(numFactors, fitBias, fitLinear, numFeatures)\n+      case SquaredError =>\n+        new MSEFactorizationMachinesGradient(numFactors, fitBias, fitLinear, numFeatures)\n+      case _ => throw new IllegalArgumentException(s\"loss function type $lossFunc is invalidation\")\n+    }\n+  }\n+}\n+\n+/**\n+ * FM with logistic loss\n+ * prediction formula:\n+ * {{{\n+ *   \\hat{y} = \\sigmoid(y_{fm})\n+ * }}}\n+ * loss formula:\n+ * {{{\n+ *   - y * log(\\hat{y}) - (1 - y) * log(1 - \\hat{y})\n+ * }}}\n+ * multiplier formula:\n+ * {{{\n+ *   \\frac{\\partial l}{\\partial \\hat{y}} \\cdot\n+ *   \\frac{\\partial \\hat{y}}{\\partial y_{fm}} =\n+ *   \\hat{y} - y\n+ * }}}\n+ */\n+private[ml] class LogisticFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int)\n+  extends BaseFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int) with Logging {\n+\n+  override def getPrediction(rawPrediction: Double): Double = {\n+    1.0 / (1.0 + math.exp(-rawPrediction))\n+  }\n+\n+  override protected def getMultiplier(rawPrediction: Double, label: Double): Double = {\n+    getPrediction(rawPrediction) - label\n+  }\n+\n+  override protected def getLoss(rawPrediction: Double, label: Double): Double = {\n+    if (label > 0) MLUtils.log1pExp(-rawPrediction)\n+    else MLUtils.log1pExp(rawPrediction)\n+  }\n+}\n+\n+/**\n+ * FM with mse\n+ * prediction formula:\n+ * {{{\n+ *   \\hat{y} = y_{fm}\n+ * }}}\n+ * loss formula:\n+ * {{{\n+ *   (\\hat{y} - y) ^ 2\n+ * }}}\n+ * multiplier formula:\n+ * {{{\n+ *   \\frac{\\partial l}{\\partial \\hat{y}} \\cdot\n+ *   \\frac{\\partial \\hat{y}}{\\partial y_{fm}} =\n+ *   2 * (\\hat{y} - y)\n+ * }}}\n+ */\n+private[ml] class MSEFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int)\n+  extends BaseFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int) with Logging {\n+\n+  override def getPrediction(rawPrediction: Double): Double = {\n+    rawPrediction\n+  }\n+\n+  override protected def getMultiplier(rawPrediction: Double, label: Double): Double = {\n+    2 * (rawPrediction - label)\n+  }\n+\n+  override protected def getLoss(rawPrediction: Double, label: Double): Double = {\n+    (rawPrediction - label) * (rawPrediction - label)\n+  }\n+}\n+\n+private[ml] class AdamWUpdater(weightSize: Int) extends Updater with Logging {\n+  val beta1: Double = 0.9"
  }, {
    "author": {
      "login": "mob-ai"
    },
    "body": "> these params are not exposed to end users?\r\n\r\nI also considered this. But I was warried about that if someone adds better solver, then the solver's parameters will be too much. And in AdamW, these parameters almost not be tuning. So I am not exposed these parameters for now.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-10-29T08:21:09Z",
    "diffHunk": "@@ -0,0 +1,757 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[regression] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver with HasLoss {\n+\n+  import FactorizationMachines._\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the parameter of l2-regularization term, \" +\n+      \"which prevents overfitting by adding sum of squares of all the parameters\",\n+    ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  /**\n+   * The loss function to be optimized.\n+   * Supported options: \"logisticLoss\" and \"squaredError\".\n+   * Default: \"logisticLoss\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val loss: Param[String] = new Param[String](this, \"loss\", \"The loss function to\" +\n+    s\" be optimized. Supported options: ${supportedLosses.mkString(\", \")}. (Default logisticLoss)\",\n+    ParamValidators.inArray[String](supportedLosses))\n+}\n+\n+/**\n+ * Factorization Machines\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachines @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FactorizationMachines, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with DefaultParamsWritable with Logging {\n+\n+  import FactorizationMachines._\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fm\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  /**\n+   * Sets the value of param [[loss]].\n+   * - \"logisticLoss\" is used to classification, label must be in {0, 1}.\n+   * - \"squaredError\" is used to regression.\n+   * Default is logisticLoss.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setLoss(value: String): this.type = set(loss, value)\n+  setDefault(loss -> LogisticLoss)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FactorizationMachinesModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FactorizationMachinesModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver, loss)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    // initialize coefficients\n+    val coefficientsSize = $(numFactors) * numFeatures +\n+      (if ($(fitLinear)) numFeatures else 0) +\n+      (if ($(fitBias)) 1 else 0)\n+    val initialCoefficients =\n+      Vectors.dense(Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) Array.fill(numFeatures)(0.0) else Array.empty[Double]) ++\n+        (if ($(fitBias)) Array.fill(1)(0.0) else Array.empty[Double]))\n+\n+    val data = instances.map { case OldLabeledPoint(label, features) => (label, features) }\n+\n+    // optimize coefficients with gradient descent\n+    val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+      $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+    val updater = $(solver) match {\n+      case GD => new SquaredL2Updater()\n+      case AdamW => new AdamWUpdater(coefficientsSize)\n+    }\n+\n+    val optimizer = new GradientDescent(gradient, updater)\n+      .setStepSize($(stepSize))\n+      .setNumIterations($(maxIter))\n+      .setRegParam($(regParam))\n+      .setMiniBatchFraction($(miniBatchFraction))\n+      .setConvergenceTol($(tol))\n+    val coefficients = optimizer.optimize(data, initialCoefficients)\n+\n+    if (handlePersistence) instances.unpersist()\n+\n+    val model = copyValues(new FactorizationMachinesModel(uid, coefficients, numFeatures))\n+    model\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachines = defaultCopy(extra)\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachines extends DefaultParamsReadable[FactorizationMachines] {\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachines = super.load(path)\n+\n+  /** String name for \"gd\". */\n+  private[regression] val GD = \"gd\"\n+\n+  /** String name for \"adamW\". */\n+  private[regression] val AdamW = \"adamW\"\n+\n+  /** Set of solvers that FactorizationMachines supports. */\n+  private[regression] val supportedSolvers = Array(GD, AdamW)\n+\n+  /** String name for \"logisticLoss\". */\n+  private[regression] val LogisticLoss = \"logisticLoss\"\n+\n+  /** String name for \"squaredError\". */\n+  private[regression] val SquaredError = \"squaredError\"\n+\n+  /** Set of loss function names that FactorizationMachines supports. */\n+  private[regression] val supportedLosses = Array(SquaredError, LogisticLoss)\n+}\n+\n+/**\n+ * Model produced by [[FactorizationMachines]].\n+ */\n+@Since(\"3.0.0\")\n+class FactorizationMachinesModel (\n+    @Since(\"3.0.0\") override val uid: String,\n+    @Since(\"3.0.0\") val coefficients: OldVector,\n+    @Since(\"3.0.0\") override val numFeatures: Int)\n+  extends PredictionModel[Vector, FactorizationMachinesModel]\n+  with FactorizationMachinesParams with MLWritable {\n+\n+  /**\n+   * Returns Factorization Machines coefficients\n+   * coefficients concat from 2-way coefficients, 1-way coefficients, global bias\n+   * index 0 ~ numFeatures*numFactors is 2-way coefficients,\n+   *   [i * numFactors + f] denotes i-th feature and f-th factor\n+   * Following indices are 1-way coefficients and global bias.\n+   *\n+   * @return Vector\n+   */\n+  @Since(\"3.0.0\")\n+  def getCoefficients: Vector = coefficients\n+\n+  private lazy val gradient = BaseFactorizationMachinesGradient.parseLoss(\n+    $(loss), $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+  override def predict(features: Vector): Double = {\n+    val rawPrediction = gradient.getRawPrediction(features, coefficients)\n+    gradient.getPrediction(rawPrediction)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FactorizationMachinesModel = {\n+    copyValues(new FactorizationMachinesModel(\n+      uid, coefficients, numFeatures), extra)\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def write: MLWriter =\n+    new FactorizationMachinesModel.FactorizationMachinesModelWriter(this)\n+\n+  override def toString: String = {\n+    s\"FactorizationMachinesModel: \" +\n+      s\"uid = ${super.toString}, lossFunc = ${$(loss)}, numFeatures = $numFeatures, \" +\n+      s\"numFactors = ${$(numFactors)}, fitLinear = ${$(fitLinear)}, fitBias = ${$(fitBias)}\"\n+  }\n+}\n+\n+@Since(\"3.0.0\")\n+object FactorizationMachinesModel extends MLReadable[FactorizationMachinesModel] {\n+\n+  @Since(\"3.0.0\")\n+  override def read: MLReader[FactorizationMachinesModel] = new FactorizationMachinesModelReader\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FactorizationMachinesModel = super.load(path)\n+\n+  /** [[MLWriter]] instance for [[FactorizationMachinesModel]] */\n+  private[FactorizationMachinesModel] class FactorizationMachinesModelWriter(\n+      instance: FactorizationMachinesModel) extends MLWriter with Logging {\n+\n+    private case class Data(\n+        numFeatures: Int,\n+        coefficients: OldVector)\n+\n+    override protected def saveImpl(path: String): Unit = {\n+      DefaultParamsWriter.saveMetadata(instance, path, sc)\n+      val data = Data(instance.numFeatures, instance.coefficients)\n+      val dataPath = new Path(path, \"data\").toString\n+      sparkSession.createDataFrame(Seq(data)).repartition(1).write.parquet(dataPath)\n+    }\n+  }\n+\n+  private class FactorizationMachinesModelReader extends MLReader[FactorizationMachinesModel] {\n+\n+    private val className = classOf[FactorizationMachinesModel].getName\n+\n+    override def load(path: String): FactorizationMachinesModel = {\n+      val metadata = DefaultParamsReader.loadMetadata(path, sc, className)\n+      val dataPath = new Path(path, \"data\").toString\n+      val data = sparkSession.read.format(\"parquet\").load(dataPath)\n+\n+      val Row(numFeatures: Int, coefficients: OldVector) = data\n+        .select(\"numFeatures\", \"coefficients\").head()\n+      val model = new FactorizationMachinesModel(\n+        metadata.uid, coefficients, numFeatures)\n+      metadata.getAndSetParams(model)\n+      model\n+    }\n+  }\n+}\n+\n+/**\n+ * Factorization Machines base gradient class\n+ * Implementing the raw FM formula, include raw prediction and raw gradient,\n+ * then inherit the base class to implement special gradient class(like logloss, mse).\n+ *\n+ * Factorization Machines raw formula:\n+ * {{{\n+ *   y_{fm} = w_0 + \\sum\\limits^n_{i-1} w_i x_i +\n+ *     \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1}  x_i x_j\n+ * }}}\n+ * the pairwise interactions (2-way term) can be reformulated:\n+ * {{{\n+ *   \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} <v_i, v_j> x_i x_j\n+ *   = \\frac{1}{2}\\sum\\limits^k_{f=1}\n+ *   \\left(\\left( \\sum\\limits^n_{i=1}v_{i,f}x_i \\right)^2 -\n+ *     \\sum\\limits^n_{i=1}v_{i,f}^2x_i^2 \\right)\n+ * }}}\n+ * and the gradients are:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta}y_{fm} = \\left\\{\n+ *   \\begin{align}\n+ *   &1, & if\\ \\theta\\ is\\ w_0 \\\\\n+ *   &x_i, & if\\ \\theta\\ is\\ w_i \\\\\n+ *   &x_i{\\sum}^n_{j=1}v_{j,f}x_j - v_{i,f}x_i^2, & if\\ \\theta\\ is\\ v_{i,j} \\\\\n+ *   \\end{align}\n+ *   \\right.\n+ * }}}\n+ *\n+ * Factorization Machines formula with prediction task:\n+ * {{{\n+ *   \\hat{y} = p\\left( y_{fm} \\right)\n+ * }}}\n+ * p is the prediction function, for binary classification task is sigmoid.\n+ * The loss funcation gradient formula:\n+ * {{{\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( \\hat{y},y \\right) =\n+ *   \\frac{\\partial}{\\partial\\theta} l\\left( p\\left( y_{fm} \\right),y \\right) =\n+ *   \\frac{\\partial l}{\\partial \\hat{y}} \\cdot\n+ *   \\frac{\\partial \\hat{y}}{\\partial y_{fm}} \\cdot\n+ *   \\frac{\\partial y_{fm}}{\\partial\\theta}\n+ * }}}\n+ * Last term is same for all task, so be implemented in base gradient class.\n+ * last term named rawGradient in following code, and first two term named multiplier.\n+ */\n+private[ml] abstract class BaseFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int) extends Gradient {\n+\n+  override def compute(\n+      data: OldVector,\n+      label: Double,\n+      weights: OldVector,\n+      cumGradient: OldVector): Double = {\n+    val rawPrediction = getRawPrediction(data, weights)\n+    val rawGradient = getRawGradient(data, weights)\n+    val multiplier = getMultiplier(rawPrediction, label)\n+    axpy(multiplier, rawGradient, cumGradient)\n+    val loss = getLoss(rawPrediction, label)\n+    loss\n+  }\n+\n+  def getPrediction(rawPrediction: Double): Double\n+\n+  protected def getMultiplier(rawPrediction: Double, label: Double): Double\n+\n+  protected def getLoss(rawPrediction: Double, label: Double): Double\n+\n+  private val sumVX = Array.fill(numFactors)(0.0)\n+\n+  def getRawPrediction(data: OldVector, weights: OldVector): Double = {\n+    var rawPrediction = 0.0\n+    val vWeightsSize = numFeatures * numFactors\n+\n+    if (fitBias) rawPrediction += weights(weights.size - 1)\n+    if (fitLinear) {\n+      data.foreachActive { case (index, value) =>\n+        rawPrediction += weights(vWeightsSize + index) * value\n+      }\n+    }\n+    (0 until numFactors).foreach { f =>\n+      var sumSquare = 0.0\n+      var sum = 0.0\n+      data.foreachActive { case (index, value) =>\n+        val vx = weights(index * numFactors + f) * value\n+        sumSquare += vx * vx\n+        sum += vx\n+      }\n+      sumVX(f) = sum\n+      rawPrediction += 0.5 * (sum * sum - sumSquare)\n+    }\n+\n+    rawPrediction\n+  }\n+\n+  private def getRawGradient(data: OldVector, weights: OldVector): OldVector = {\n+    data match {\n+      // Usually Factorization Machines is used, there will be a lot of sparse features.\n+      // So need to optimize the gradient descent of sparse vector.\n+      case data: OldLinalg.SparseVector =>\n+        val gardSize = data.indices.length * numFactors +\n+          (if (fitLinear) data.indices.length else 0) +\n+          (if (fitBias) 1 else 0)\n+        val gradIndex = Array.fill(gardSize)(0)\n+        val gradValue = Array.fill(gardSize)(0.0)\n+        var gradI = 0\n+        val vWeightsSize = numFeatures * numFactors\n+\n+        data.foreachActive { case (index, value) =>\n+          (0 until numFactors).foreach { f =>\n+            gradIndex(gradI) = index * numFactors + f\n+            gradValue(gradI) = value * sumVX(f) - weights(index * numFactors + f) * value * value\n+            gradI += 1\n+          }\n+        }\n+        if (fitLinear) {\n+          data.foreachActive { case (index, value) =>\n+            gradIndex(gradI) = vWeightsSize + index\n+            gradValue(gradI) = value\n+            gradI += 1\n+          }\n+        }\n+        if (fitBias) {\n+          gradIndex(gradI) = weights.size - 1\n+          gradValue(gradI) = 1.0\n+        }\n+\n+        OldVectors.sparse(weights.size, gradIndex, gradValue)\n+      case data: OldLinalg.DenseVector =>\n+        val gradient = Array.fill(weights.size)(0.0)\n+        val vWeightsSize = numFeatures * numFactors\n+\n+        if (fitBias) gradient(weights.size - 1) += 1.0\n+        if (fitLinear) {\n+          data.foreachActive { case (index, value) =>\n+            gradient(vWeightsSize + index) += value\n+          }\n+        }\n+        (0 until numFactors).foreach { f =>\n+          data.foreachActive { case (index, value) =>\n+            gradient(index * numFactors + f) +=\n+              value * sumVX(f) - weights(index * numFactors + f) * value * value\n+          }\n+        }\n+\n+        OldVectors.dense(gradient)\n+    }\n+  }\n+}\n+\n+object BaseFactorizationMachinesGradient {\n+  def parseLoss(\n+      lossFunc: String,\n+      numFactors: Int,\n+      fitBias: Boolean,\n+      fitLinear: Boolean,\n+      numFeatures: Int): BaseFactorizationMachinesGradient = {\n+\n+    import FactorizationMachines._\n+\n+    lossFunc match {\n+      case LogisticLoss =>\n+        new LogisticFactorizationMachinesGradient(numFactors, fitBias, fitLinear, numFeatures)\n+      case SquaredError =>\n+        new MSEFactorizationMachinesGradient(numFactors, fitBias, fitLinear, numFeatures)\n+      case _ => throw new IllegalArgumentException(s\"loss function type $lossFunc is invalidation\")\n+    }\n+  }\n+}\n+\n+/**\n+ * FM with logistic loss\n+ * prediction formula:\n+ * {{{\n+ *   \\hat{y} = \\sigmoid(y_{fm})\n+ * }}}\n+ * loss formula:\n+ * {{{\n+ *   - y * log(\\hat{y}) - (1 - y) * log(1 - \\hat{y})\n+ * }}}\n+ * multiplier formula:\n+ * {{{\n+ *   \\frac{\\partial l}{\\partial \\hat{y}} \\cdot\n+ *   \\frac{\\partial \\hat{y}}{\\partial y_{fm}} =\n+ *   \\hat{y} - y\n+ * }}}\n+ */\n+private[ml] class LogisticFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int)\n+  extends BaseFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int) with Logging {\n+\n+  override def getPrediction(rawPrediction: Double): Double = {\n+    1.0 / (1.0 + math.exp(-rawPrediction))\n+  }\n+\n+  override protected def getMultiplier(rawPrediction: Double, label: Double): Double = {\n+    getPrediction(rawPrediction) - label\n+  }\n+\n+  override protected def getLoss(rawPrediction: Double, label: Double): Double = {\n+    if (label > 0) MLUtils.log1pExp(-rawPrediction)\n+    else MLUtils.log1pExp(rawPrediction)\n+  }\n+}\n+\n+/**\n+ * FM with mse\n+ * prediction formula:\n+ * {{{\n+ *   \\hat{y} = y_{fm}\n+ * }}}\n+ * loss formula:\n+ * {{{\n+ *   (\\hat{y} - y) ^ 2\n+ * }}}\n+ * multiplier formula:\n+ * {{{\n+ *   \\frac{\\partial l}{\\partial \\hat{y}} \\cdot\n+ *   \\frac{\\partial \\hat{y}}{\\partial y_{fm}} =\n+ *   2 * (\\hat{y} - y)\n+ * }}}\n+ */\n+private[ml] class MSEFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int)\n+  extends BaseFactorizationMachinesGradient(\n+    numFactors: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean,\n+    numFeatures: Int) with Logging {\n+\n+  override def getPrediction(rawPrediction: Double): Double = {\n+    rawPrediction\n+  }\n+\n+  override protected def getMultiplier(rawPrediction: Double, label: Double): Double = {\n+    2 * (rawPrediction - label)\n+  }\n+\n+  override protected def getLoss(rawPrediction: Double, label: Double): Double = {\n+    (rawPrediction - label) * (rawPrediction - label)\n+  }\n+}\n+\n+private[ml] class AdamWUpdater(weightSize: Int) extends Updater with Logging {\n+  val beta1: Double = 0.9"
  }],
  "prId": 26124
}]