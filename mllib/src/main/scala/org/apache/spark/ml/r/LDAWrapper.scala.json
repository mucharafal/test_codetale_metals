[{
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "since model is referenced and persisted, is there a need to handle trainingLogLikelihood and logPrior separately like this, and writing to metadata, instead of just getting from the model when fetching for the summary?",
    "commit": "e133ee64961beaf10ccccb7885ece76ded021ae5",
    "createdAt": "2017-01-05T05:47:07Z",
    "diffHunk": "@@ -172,6 +187,8 @@ private[r] object LDAWrapper extends MLReadable[LDAWrapper] {\n       model,\n       ldaModel.logLikelihood(preprocessedData),\n       ldaModel.logPerplexity(preprocessedData),\n+      trainingLogLikelihood,\n+      logPrior,"
  }, {
    "author": {
      "login": "wangmiao1981"
    },
    "body": "The first version, I got them from the model in the `LDAWrapper` class. However, when I read `logPrior`, I found that the loaded `logPrior` is not the same as the value before save. So, I followed the logLikelihood and logPerplexity to save it in the metadata. ",
    "commit": "e133ee64961beaf10ccccb7885ece76ded021ae5",
    "createdAt": "2017-01-05T07:05:47Z",
    "diffHunk": "@@ -172,6 +187,8 @@ private[r] object LDAWrapper extends MLReadable[LDAWrapper] {\n       model,\n       ldaModel.logLikelihood(preprocessedData),\n       ldaModel.logPerplexity(preprocessedData),\n+      trainingLogLikelihood,\n+      logPrior,"
  }, {
    "author": {
      "login": "wangmiao1981"
    },
    "body": "With the same dataset, Scala side tests:\r\nOriginal LogPrior:-3.3387459952856338\r\nLogPrior from saved model: -0.9202435107654922\r\n",
    "commit": "e133ee64961beaf10ccccb7885ece76ded021ae5",
    "createdAt": "2017-01-05T22:32:38Z",
    "diffHunk": "@@ -172,6 +187,8 @@ private[r] object LDAWrapper extends MLReadable[LDAWrapper] {\n       model,\n       ldaModel.logLikelihood(preprocessedData),\n       ldaModel.logPerplexity(preprocessedData),\n+      trainingLogLikelihood,\n+      logPrior,"
  }, {
    "author": {
      "login": "felixcheung"
    },
    "body": "that's odd, is that an issue with model persistence?\r\n",
    "commit": "e133ee64961beaf10ccccb7885ece76ded021ae5",
    "createdAt": "2017-01-06T06:34:01Z",
    "diffHunk": "@@ -172,6 +187,8 @@ private[r] object LDAWrapper extends MLReadable[LDAWrapper] {\n       model,\n       ldaModel.logLikelihood(preprocessedData),\n       ldaModel.logPerplexity(preprocessedData),\n+      trainingLogLikelihood,\n+      logPrior,"
  }, {
    "author": {
      "login": "wangmiao1981"
    },
    "body": "LogPrior is calculated based on the serialized topics etc, which are also used by the trainingLikelyhood. But the trainingLikelyhood is the same for both original and loaded model. Let me debug more. It looks like a bug. The original MLLIB implementation doesn't serialize the two parameters as they can be calculated from other saved values. In addition, there is no unit test for comparing the two values, which could be the reason of not catching this issue. ",
    "commit": "e133ee64961beaf10ccccb7885ece76ded021ae5",
    "createdAt": "2017-01-06T17:29:53Z",
    "diffHunk": "@@ -172,6 +187,8 @@ private[r] object LDAWrapper extends MLReadable[LDAWrapper] {\n       model,\n       ldaModel.logLikelihood(preprocessedData),\n       ldaModel.logPerplexity(preprocessedData),\n+      trainingLogLikelihood,\n+      logPrior,"
  }],
  "prId": 16464
}, {
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "shouldn't it just set `optimizer` - without having to check if it is `== em`?",
    "commit": "e133ee64961beaf10ccccb7885ece76ded021ae5",
    "createdAt": "2017-01-06T06:28:55Z",
    "diffHunk": "@@ -123,6 +126,10 @@ private[r] object LDAWrapper extends MLReadable[LDAWrapper] {\n       .setMaxIter(maxIter)\n       .setSubsamplingRate(subsamplingRate)\n \n+    if (optimizer == \"em\") {"
  }, {
    "author": {
      "login": "wangmiao1981"
    },
    "body": "If it is \"online\", it is not necessary to set the optimizer. But setting it anyway will make the code clean. I will do it.",
    "commit": "e133ee64961beaf10ccccb7885ece76ded021ae5",
    "createdAt": "2017-01-06T17:26:07Z",
    "diffHunk": "@@ -123,6 +126,10 @@ private[r] object LDAWrapper extends MLReadable[LDAWrapper] {\n       .setMaxIter(maxIter)\n       .setSubsamplingRate(subsamplingRate)\n \n+    if (optimizer == \"em\") {"
  }],
  "prId": 16464
}, {
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "`distributedModel`?",
    "commit": "e133ee64961beaf10ccccb7885ece76ded021ae5",
    "createdAt": "2017-01-10T19:08:19Z",
    "diffHunk": "@@ -45,6 +45,11 @@ private[r] class LDAWrapper private (\n   import LDAWrapper._\n \n   private val lda: LDAModel = pipeline.stages.last.asInstanceOf[LDAModel]\n+  private val distributedMoel = lda.isDistributed match {"
  }, {
    "author": {
      "login": "wangmiao1981"
    },
    "body": "Fixed. Thanks!",
    "commit": "e133ee64961beaf10ccccb7885ece76ded021ae5",
    "createdAt": "2017-01-10T19:16:53Z",
    "diffHunk": "@@ -45,6 +45,11 @@ private[r] class LDAWrapper private (\n   import LDAWrapper._\n \n   private val lda: LDAModel = pipeline.stages.last.asInstanceOf[LDAModel]\n+  private val distributedMoel = lda.isDistributed match {"
  }],
  "prId": 16464
}]