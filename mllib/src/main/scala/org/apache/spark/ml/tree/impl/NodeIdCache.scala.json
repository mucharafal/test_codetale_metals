[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "When does the final one get unpersisted?\n",
    "commit": "48749bef90d05af0f7fedd5715a67b15a6f24ce7",
    "createdAt": "2015-07-14T21:49:00Z",
    "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.ml.tree.{LearningNode, Split}\n+import org.apache.spark.mllib.tree.impl.BaggedPoint\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+\n+/**\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+private[tree] case class NodeIndexUpdater(split: Split, nodeIndex: Int) {\n+\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeature Binned feature value.\n+   * @param splits Split information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeature: Int, splits: Array[Split]): Int = {\n+    if (split.shouldGoLeft(binnedFeature, splits)) {\n+      LearningNode.leftChildIndex(nodeIndex)\n+    } else {\n+      LearningNode.rightChildIndex(nodeIndex)\n+    }\n+  }\n+}\n+\n+/**\n+ * Each TreePoint belongs to a particular node per tree.\n+ * Each row in the nodeIdsForInstances RDD is an array over trees of the node index\n+ * in each tree. Initially, values should all be 1 for root node.\n+ * The nodeIdsForInstances RDD needs to be updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *                           (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+private[spark] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  private var prevNodeIdsForInstances: RDD[Array[Int]] = null\n+\n+  // To keep track of the past checkpointed RDDs.\n+  private val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  private var rddUpdateCount = 0\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.\n+   * @param data The RDD of training rows.\n+   * @param nodeIdUpdaters A map of node index updaters.\n+   *                       The key is the indices of nodes that we want to update.\n+   * @param splits  Split information needed to find child node indices.\n+   */\n+  def updateNodeIndices(\n+      data: RDD[BaggedPoint[TreePoint]],\n+      nodeIdUpdaters: Array[mutable.Map[Int, NodeIndexUpdater]],\n+      splits: Array[Array[Split]]): Unit = {\n+    if (prevNodeIdsForInstances != null) {\n+      // Unpersist the previous one if one exists.\n+      prevNodeIdsForInstances.unpersist()",
    "line": 100
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "Good point.  Maybe I should do this in a follow-up PR and fix it in the old API as well.\n",
    "commit": "48749bef90d05af0f7fedd5715a67b15a6f24ce7",
    "createdAt": "2015-07-17T00:36:23Z",
    "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.ml.tree.{LearningNode, Split}\n+import org.apache.spark.mllib.tree.impl.BaggedPoint\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+\n+/**\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+private[tree] case class NodeIndexUpdater(split: Split, nodeIndex: Int) {\n+\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeature Binned feature value.\n+   * @param splits Split information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeature: Int, splits: Array[Split]): Int = {\n+    if (split.shouldGoLeft(binnedFeature, splits)) {\n+      LearningNode.leftChildIndex(nodeIndex)\n+    } else {\n+      LearningNode.rightChildIndex(nodeIndex)\n+    }\n+  }\n+}\n+\n+/**\n+ * Each TreePoint belongs to a particular node per tree.\n+ * Each row in the nodeIdsForInstances RDD is an array over trees of the node index\n+ * in each tree. Initially, values should all be 1 for root node.\n+ * The nodeIdsForInstances RDD needs to be updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *                           (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+private[spark] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  private var prevNodeIdsForInstances: RDD[Array[Int]] = null\n+\n+  // To keep track of the past checkpointed RDDs.\n+  private val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  private var rddUpdateCount = 0\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.\n+   * @param data The RDD of training rows.\n+   * @param nodeIdUpdaters A map of node index updaters.\n+   *                       The key is the indices of nodes that we want to update.\n+   * @param splits  Split information needed to find child node indices.\n+   */\n+  def updateNodeIndices(\n+      data: RDD[BaggedPoint[TreePoint]],\n+      nodeIdUpdaters: Array[mutable.Map[Int, NodeIndexUpdater]],\n+      splits: Array[Array[Split]]): Unit = {\n+    if (prevNodeIdsForInstances != null) {\n+      // Unpersist the previous one if one exists.\n+      prevNodeIdsForInstances.unpersist()",
    "line": 100
  }],
  "prId": 7294
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Use `case (point, id) =>` instead for readability.\n",
    "commit": "48749bef90d05af0f7fedd5715a67b15a6f24ce7",
    "createdAt": "2015-07-14T21:49:01Z",
    "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.ml.tree.{LearningNode, Split}\n+import org.apache.spark.mllib.tree.impl.BaggedPoint\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+\n+/**\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+private[tree] case class NodeIndexUpdater(split: Split, nodeIndex: Int) {\n+\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeature Binned feature value.\n+   * @param splits Split information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeature: Int, splits: Array[Split]): Int = {\n+    if (split.shouldGoLeft(binnedFeature, splits)) {\n+      LearningNode.leftChildIndex(nodeIndex)\n+    } else {\n+      LearningNode.rightChildIndex(nodeIndex)\n+    }\n+  }\n+}\n+\n+/**\n+ * Each TreePoint belongs to a particular node per tree.\n+ * Each row in the nodeIdsForInstances RDD is an array over trees of the node index\n+ * in each tree. Initially, values should all be 1 for root node.\n+ * The nodeIdsForInstances RDD needs to be updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *                           (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+private[spark] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  private var prevNodeIdsForInstances: RDD[Array[Int]] = null\n+\n+  // To keep track of the past checkpointed RDDs.\n+  private val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  private var rddUpdateCount = 0\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.\n+   * @param data The RDD of training rows.\n+   * @param nodeIdUpdaters A map of node index updaters.\n+   *                       The key is the indices of nodes that we want to update.\n+   * @param splits  Split information needed to find child node indices.\n+   */\n+  def updateNodeIndices(\n+      data: RDD[BaggedPoint[TreePoint]],\n+      nodeIdUpdaters: Array[mutable.Map[Int, NodeIndexUpdater]],\n+      splits: Array[Array[Split]]): Unit = {\n+    if (prevNodeIdsForInstances != null) {\n+      // Unpersist the previous one if one exists.\n+      prevNodeIdsForInstances.unpersist()\n+    }\n+\n+    prevNodeIdsForInstances = nodeIdsForInstances\n+    nodeIdsForInstances = data.zip(nodeIdsForInstances).map {\n+      dataPoint => {"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "trying to change as little as possible : )\n",
    "commit": "48749bef90d05af0f7fedd5715a67b15a6f24ce7",
    "createdAt": "2015-07-17T00:37:22Z",
    "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.ml.tree.{LearningNode, Split}\n+import org.apache.spark.mllib.tree.impl.BaggedPoint\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+\n+/**\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+private[tree] case class NodeIndexUpdater(split: Split, nodeIndex: Int) {\n+\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeature Binned feature value.\n+   * @param splits Split information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeature: Int, splits: Array[Split]): Int = {\n+    if (split.shouldGoLeft(binnedFeature, splits)) {\n+      LearningNode.leftChildIndex(nodeIndex)\n+    } else {\n+      LearningNode.rightChildIndex(nodeIndex)\n+    }\n+  }\n+}\n+\n+/**\n+ * Each TreePoint belongs to a particular node per tree.\n+ * Each row in the nodeIdsForInstances RDD is an array over trees of the node index\n+ * in each tree. Initially, values should all be 1 for root node.\n+ * The nodeIdsForInstances RDD needs to be updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *                           (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+private[spark] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  private var prevNodeIdsForInstances: RDD[Array[Int]] = null\n+\n+  // To keep track of the past checkpointed RDDs.\n+  private val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  private var rddUpdateCount = 0\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.\n+   * @param data The RDD of training rows.\n+   * @param nodeIdUpdaters A map of node index updaters.\n+   *                       The key is the indices of nodes that we want to update.\n+   * @param splits  Split information needed to find child node indices.\n+   */\n+  def updateNodeIndices(\n+      data: RDD[BaggedPoint[TreePoint]],\n+      nodeIdUpdaters: Array[mutable.Map[Int, NodeIndexUpdater]],\n+      splits: Array[Array[Split]]): Unit = {\n+    if (prevNodeIdsForInstances != null) {\n+      // Unpersist the previous one if one exists.\n+      prevNodeIdsForInstances.unpersist()\n+    }\n+\n+    prevNodeIdsForInstances = nodeIdsForInstances\n+    nodeIdsForInstances = data.zip(nodeIdsForInstances).map {\n+      dataPoint => {"
  }],
  "prId": 7294
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remember `sc` as a member variable?\n",
    "commit": "48749bef90d05af0f7fedd5715a67b15a6f24ce7",
    "createdAt": "2015-07-14T21:49:03Z",
    "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.ml.tree.{LearningNode, Split}\n+import org.apache.spark.mllib.tree.impl.BaggedPoint\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+\n+/**\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+private[tree] case class NodeIndexUpdater(split: Split, nodeIndex: Int) {\n+\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeature Binned feature value.\n+   * @param splits Split information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeature: Int, splits: Array[Split]): Int = {\n+    if (split.shouldGoLeft(binnedFeature, splits)) {\n+      LearningNode.leftChildIndex(nodeIndex)\n+    } else {\n+      LearningNode.rightChildIndex(nodeIndex)\n+    }\n+  }\n+}\n+\n+/**\n+ * Each TreePoint belongs to a particular node per tree.\n+ * Each row in the nodeIdsForInstances RDD is an array over trees of the node index\n+ * in each tree. Initially, values should all be 1 for root node.\n+ * The nodeIdsForInstances RDD needs to be updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *                           (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+private[spark] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  private var prevNodeIdsForInstances: RDD[Array[Int]] = null\n+\n+  // To keep track of the past checkpointed RDDs.\n+  private val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  private var rddUpdateCount = 0\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.\n+   * @param data The RDD of training rows.\n+   * @param nodeIdUpdaters A map of node index updaters.\n+   *                       The key is the indices of nodes that we want to update.\n+   * @param splits  Split information needed to find child node indices.\n+   */\n+  def updateNodeIndices(\n+      data: RDD[BaggedPoint[TreePoint]],\n+      nodeIdUpdaters: Array[mutable.Map[Int, NodeIndexUpdater]],\n+      splits: Array[Array[Split]]): Unit = {\n+    if (prevNodeIdsForInstances != null) {\n+      // Unpersist the previous one if one exists.\n+      prevNodeIdsForInstances.unpersist()\n+    }\n+\n+    prevNodeIdsForInstances = nodeIdsForInstances\n+    nodeIdsForInstances = data.zip(nodeIdsForInstances).map {\n+      dataPoint => {\n+        var treeId = 0\n+        while (treeId < nodeIdUpdaters.length) {\n+          val nodeIdUpdater = nodeIdUpdaters(treeId).getOrElse(dataPoint._2(treeId), null)\n+          if (nodeIdUpdater != null) {\n+            val featureIndex = nodeIdUpdater.split.featureIndex\n+            val newNodeIndex = nodeIdUpdater.updateNodeIndex(\n+              binnedFeature = dataPoint._1.datum.binnedFeatures(featureIndex),\n+              splits = splits(featureIndex))\n+            dataPoint._2(treeId) = newNodeIndex\n+          }\n+          treeId += 1\n+        }\n+        dataPoint._2\n+      }\n+    }\n+\n+    // Keep on persisting new ones.\n+    nodeIdsForInstances.persist(StorageLevel.MEMORY_AND_DISK)\n+    rddUpdateCount += 1\n+\n+    // Handle checkpointing if the directory is not None.\n+    if (nodeIdsForInstances.sparkContext.getCheckpointDir.nonEmpty &&"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "I might just store a Boolean for this one.  For the other usage, do you know if it's a bad idea to store an instance of a FileSystem?\n\n```\nval fs = FileSystem.get(old.sparkContext.hadoopConfiguration)\n```\n",
    "commit": "48749bef90d05af0f7fedd5715a67b15a6f24ce7",
    "createdAt": "2015-07-17T00:52:09Z",
    "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.ml.tree.{LearningNode, Split}\n+import org.apache.spark.mllib.tree.impl.BaggedPoint\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+\n+/**\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+private[tree] case class NodeIndexUpdater(split: Split, nodeIndex: Int) {\n+\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeature Binned feature value.\n+   * @param splits Split information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeature: Int, splits: Array[Split]): Int = {\n+    if (split.shouldGoLeft(binnedFeature, splits)) {\n+      LearningNode.leftChildIndex(nodeIndex)\n+    } else {\n+      LearningNode.rightChildIndex(nodeIndex)\n+    }\n+  }\n+}\n+\n+/**\n+ * Each TreePoint belongs to a particular node per tree.\n+ * Each row in the nodeIdsForInstances RDD is an array over trees of the node index\n+ * in each tree. Initially, values should all be 1 for root node.\n+ * The nodeIdsForInstances RDD needs to be updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *                           (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+private[spark] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  private var prevNodeIdsForInstances: RDD[Array[Int]] = null\n+\n+  // To keep track of the past checkpointed RDDs.\n+  private val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  private var rddUpdateCount = 0\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.\n+   * @param data The RDD of training rows.\n+   * @param nodeIdUpdaters A map of node index updaters.\n+   *                       The key is the indices of nodes that we want to update.\n+   * @param splits  Split information needed to find child node indices.\n+   */\n+  def updateNodeIndices(\n+      data: RDD[BaggedPoint[TreePoint]],\n+      nodeIdUpdaters: Array[mutable.Map[Int, NodeIndexUpdater]],\n+      splits: Array[Array[Split]]): Unit = {\n+    if (prevNodeIdsForInstances != null) {\n+      // Unpersist the previous one if one exists.\n+      prevNodeIdsForInstances.unpersist()\n+    }\n+\n+    prevNodeIdsForInstances = nodeIdsForInstances\n+    nodeIdsForInstances = data.zip(nodeIdsForInstances).map {\n+      dataPoint => {\n+        var treeId = 0\n+        while (treeId < nodeIdUpdaters.length) {\n+          val nodeIdUpdater = nodeIdUpdaters(treeId).getOrElse(dataPoint._2(treeId), null)\n+          if (nodeIdUpdater != null) {\n+            val featureIndex = nodeIdUpdater.split.featureIndex\n+            val newNodeIndex = nodeIdUpdater.updateNodeIndex(\n+              binnedFeature = dataPoint._1.datum.binnedFeatures(featureIndex),\n+              splits = splits(featureIndex))\n+            dataPoint._2(treeId) = newNodeIndex\n+          }\n+          treeId += 1\n+        }\n+        dataPoint._2\n+      }\n+    }\n+\n+    // Keep on persisting new ones.\n+    nodeIdsForInstances.persist(StorageLevel.MEMORY_AND_DISK)\n+    rddUpdateCount += 1\n+\n+    // Handle checkpointing if the directory is not None.\n+    if (nodeIdsForInstances.sparkContext.getCheckpointDir.nonEmpty &&"
  }],
  "prId": 7294
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`.get(1).get` -> `(1)`\n",
    "commit": "48749bef90d05af0f7fedd5715a67b15a6f24ce7",
    "createdAt": "2015-07-14T21:49:05Z",
    "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.ml.tree.{LearningNode, Split}\n+import org.apache.spark.mllib.tree.impl.BaggedPoint\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+\n+/**\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+private[tree] case class NodeIndexUpdater(split: Split, nodeIndex: Int) {\n+\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeature Binned feature value.\n+   * @param splits Split information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeature: Int, splits: Array[Split]): Int = {\n+    if (split.shouldGoLeft(binnedFeature, splits)) {\n+      LearningNode.leftChildIndex(nodeIndex)\n+    } else {\n+      LearningNode.rightChildIndex(nodeIndex)\n+    }\n+  }\n+}\n+\n+/**\n+ * Each TreePoint belongs to a particular node per tree.\n+ * Each row in the nodeIdsForInstances RDD is an array over trees of the node index\n+ * in each tree. Initially, values should all be 1 for root node.\n+ * The nodeIdsForInstances RDD needs to be updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *                           (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+private[spark] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  private var prevNodeIdsForInstances: RDD[Array[Int]] = null\n+\n+  // To keep track of the past checkpointed RDDs.\n+  private val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  private var rddUpdateCount = 0\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.\n+   * @param data The RDD of training rows.\n+   * @param nodeIdUpdaters A map of node index updaters.\n+   *                       The key is the indices of nodes that we want to update.\n+   * @param splits  Split information needed to find child node indices.\n+   */\n+  def updateNodeIndices(\n+      data: RDD[BaggedPoint[TreePoint]],\n+      nodeIdUpdaters: Array[mutable.Map[Int, NodeIndexUpdater]],\n+      splits: Array[Array[Split]]): Unit = {\n+    if (prevNodeIdsForInstances != null) {\n+      // Unpersist the previous one if one exists.\n+      prevNodeIdsForInstances.unpersist()\n+    }\n+\n+    prevNodeIdsForInstances = nodeIdsForInstances\n+    nodeIdsForInstances = data.zip(nodeIdsForInstances).map {\n+      dataPoint => {\n+        var treeId = 0\n+        while (treeId < nodeIdUpdaters.length) {\n+          val nodeIdUpdater = nodeIdUpdaters(treeId).getOrElse(dataPoint._2(treeId), null)\n+          if (nodeIdUpdater != null) {\n+            val featureIndex = nodeIdUpdater.split.featureIndex\n+            val newNodeIndex = nodeIdUpdater.updateNodeIndex(\n+              binnedFeature = dataPoint._1.datum.binnedFeatures(featureIndex),\n+              splits = splits(featureIndex))\n+            dataPoint._2(treeId) = newNodeIndex\n+          }\n+          treeId += 1\n+        }\n+        dataPoint._2\n+      }\n+    }\n+\n+    // Keep on persisting new ones.\n+    nodeIdsForInstances.persist(StorageLevel.MEMORY_AND_DISK)\n+    rddUpdateCount += 1\n+\n+    // Handle checkpointing if the directory is not None.\n+    if (nodeIdsForInstances.sparkContext.getCheckpointDir.nonEmpty &&\n+      (rddUpdateCount % checkpointInterval) == 0) {\n+      // Let's see if we can delete previous checkpoints.\n+      var canDelete = true\n+      while (checkpointQueue.size > 1 && canDelete) {\n+        // We can delete the oldest checkpoint iff\n+        // the next checkpoint actually exists in the file system.\n+        if (checkpointQueue.get(1).get.getCheckpointFile.isDefined) {"
  }],
  "prId": 7294
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "minor: Maybe it is useful to catch IOException. If something is wrong (like permissions), output an error log and keep going.\n",
    "commit": "48749bef90d05af0f7fedd5715a67b15a6f24ce7",
    "createdAt": "2015-07-14T21:49:07Z",
    "diffHunk": "@@ -0,0 +1,174 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.tree.impl\n+\n+import scala.collection.mutable\n+\n+import org.apache.hadoop.fs.{Path, FileSystem}\n+\n+import org.apache.spark.annotation.DeveloperApi\n+import org.apache.spark.ml.tree.{LearningNode, Split}\n+import org.apache.spark.mllib.tree.impl.BaggedPoint\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.storage.StorageLevel\n+\n+\n+/**\n+ * This is used by the node id cache to find the child id that a data point would belong to.\n+ * @param split Split information.\n+ * @param nodeIndex The current node index of a data point that this will update.\n+ */\n+private[tree] case class NodeIndexUpdater(split: Split, nodeIndex: Int) {\n+\n+  /**\n+   * Determine a child node index based on the feature value and the split.\n+   * @param binnedFeature Binned feature value.\n+   * @param splits Split information to convert the bin indices to approximate feature values.\n+   * @return Child node index to update to.\n+   */\n+  def updateNodeIndex(binnedFeature: Int, splits: Array[Split]): Int = {\n+    if (split.shouldGoLeft(binnedFeature, splits)) {\n+      LearningNode.leftChildIndex(nodeIndex)\n+    } else {\n+      LearningNode.rightChildIndex(nodeIndex)\n+    }\n+  }\n+}\n+\n+/**\n+ * Each TreePoint belongs to a particular node per tree.\n+ * Each row in the nodeIdsForInstances RDD is an array over trees of the node index\n+ * in each tree. Initially, values should all be 1 for root node.\n+ * The nodeIdsForInstances RDD needs to be updated at each iteration.\n+ * @param nodeIdsForInstances The initial values in the cache\n+ *                           (should be an Array of all 1's (meaning the root nodes)).\n+ * @param checkpointInterval The checkpointing interval\n+ *                           (how often should the cache be checkpointed.).\n+ */\n+private[spark] class NodeIdCache(\n+  var nodeIdsForInstances: RDD[Array[Int]],\n+  val checkpointInterval: Int) {\n+\n+  // Keep a reference to a previous node Ids for instances.\n+  // Because we will keep on re-persisting updated node Ids,\n+  // we want to unpersist the previous RDD.\n+  private var prevNodeIdsForInstances: RDD[Array[Int]] = null\n+\n+  // To keep track of the past checkpointed RDDs.\n+  private val checkpointQueue = mutable.Queue[RDD[Array[Int]]]()\n+  private var rddUpdateCount = 0\n+\n+  /**\n+   * Update the node index values in the cache.\n+   * This updates the RDD and its lineage.\n+   * TODO: Passing bin information to executors seems unnecessary and costly.\n+   * @param data The RDD of training rows.\n+   * @param nodeIdUpdaters A map of node index updaters.\n+   *                       The key is the indices of nodes that we want to update.\n+   * @param splits  Split information needed to find child node indices.\n+   */\n+  def updateNodeIndices(\n+      data: RDD[BaggedPoint[TreePoint]],\n+      nodeIdUpdaters: Array[mutable.Map[Int, NodeIndexUpdater]],\n+      splits: Array[Array[Split]]): Unit = {\n+    if (prevNodeIdsForInstances != null) {\n+      // Unpersist the previous one if one exists.\n+      prevNodeIdsForInstances.unpersist()\n+    }\n+\n+    prevNodeIdsForInstances = nodeIdsForInstances\n+    nodeIdsForInstances = data.zip(nodeIdsForInstances).map {\n+      dataPoint => {\n+        var treeId = 0\n+        while (treeId < nodeIdUpdaters.length) {\n+          val nodeIdUpdater = nodeIdUpdaters(treeId).getOrElse(dataPoint._2(treeId), null)\n+          if (nodeIdUpdater != null) {\n+            val featureIndex = nodeIdUpdater.split.featureIndex\n+            val newNodeIndex = nodeIdUpdater.updateNodeIndex(\n+              binnedFeature = dataPoint._1.datum.binnedFeatures(featureIndex),\n+              splits = splits(featureIndex))\n+            dataPoint._2(treeId) = newNodeIndex\n+          }\n+          treeId += 1\n+        }\n+        dataPoint._2\n+      }\n+    }\n+\n+    // Keep on persisting new ones.\n+    nodeIdsForInstances.persist(StorageLevel.MEMORY_AND_DISK)\n+    rddUpdateCount += 1\n+\n+    // Handle checkpointing if the directory is not None.\n+    if (nodeIdsForInstances.sparkContext.getCheckpointDir.nonEmpty &&\n+      (rddUpdateCount % checkpointInterval) == 0) {\n+      // Let's see if we can delete previous checkpoints.\n+      var canDelete = true\n+      while (checkpointQueue.size > 1 && canDelete) {\n+        // We can delete the oldest checkpoint iff\n+        // the next checkpoint actually exists in the file system.\n+        if (checkpointQueue.get(1).get.getCheckpointFile.isDefined) {\n+          val old = checkpointQueue.dequeue()\n+          // Since the old checkpoint is not deleted by Spark, we'll manually delete it here.\n+          val fs = FileSystem.get(old.sparkContext.hadoopConfiguration)\n+          fs.delete(new Path(old.getCheckpointFile.get), true)"
  }],
  "prId": 7294
}]