[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Do you mean to use ClassifierParams, not PredictorParams?  If so, then Predictor already extends PredictorParams.\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-07T22:09:51Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.impl.estimator.{PredictionModel, Predictor}\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Model produced by [[OneVsAll]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsAllModel[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],\n+    M <: ClassificationModel[FeaturesType, M]](\n+      override val parent: OneVsAll[FeaturesType, E, M],\n+      val models: Array[M])\n+  extends PredictionModel[FeaturesType, OneVsAllModel[FeaturesType, E, M]]\n+  with ClassifierParams {"
  }, {
    "author": {
      "login": "harsha2010"
    },
    "body": "I do need the rawPredictionCol which is a parameter in ClassifierParams.\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-07T22:18:31Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.impl.estimator.{PredictionModel, Predictor}\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Model produced by [[OneVsAll]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsAllModel[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],\n+    M <: ClassificationModel[FeaturesType, M]](\n+      override val parent: OneVsAll[FeaturesType, E, M],\n+      val models: Array[M])\n+  extends PredictionModel[FeaturesType, OneVsAllModel[FeaturesType, E, M]]\n+  with ClassifierParams {"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "You're not keeping the rawPredictionCol around (i.e., it is not in the DataFrame output by OneVsAllModel.transform), so you could set it arbitrarily.\n\nAlso, \"OneVsAllModel.rawPredictionCol\" should refer to the rawPrediction output by OneVsAllModel, not the column names which the base classifier outputs.  We'll add that in a later PR, right?\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-08T01:16:04Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.impl.estimator.{PredictionModel, Predictor}\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Model produced by [[OneVsAll]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsAllModel[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],\n+    M <: ClassificationModel[FeaturesType, M]](\n+      override val parent: OneVsAll[FeaturesType, E, M],\n+      val models: Array[M])\n+  extends PredictionModel[FeaturesType, OneVsAllModel[FeaturesType, E, M]]\n+  with ClassifierParams {"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "It should be simpler to append the new column using dataset.withColumn.  Please check out other spark.ml classes which use withColumn\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-07T22:09:52Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.impl.estimator.{PredictionModel, Predictor}\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Model produced by [[OneVsAll]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsAllModel[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],\n+    M <: ClassificationModel[FeaturesType, M]](\n+      override val parent: OneVsAll[FeaturesType, E, M],\n+      val models: Array[M])\n+  extends PredictionModel[FeaturesType, OneVsAllModel[FeaturesType, E, M]]\n+  with ClassifierParams {\n+\n+  override protected def featuresDataType: DataType = parent.featuresDataType\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, featuresDataType)\n+  }\n+\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    // TODO: Use DataFrame expressions to leverage performance here\n+    val predictions = models.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>\n+        a ++ b\n+      }\n+    }.map(_.maxBy(_._2))\n+\n+    // ensure that we pass through columns that are part of the original dataset.\n+    val results = dataset.select(col(\"*\")).rdd.zip(predictions).map { case (row, (label, _)) =>"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "override protected\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-07T22:09:54Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.impl.estimator.{PredictionModel, Predictor}\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Model produced by [[OneVsAll]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsAllModel[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],\n+    M <: ClassificationModel[FeaturesType, M]](\n+      override val parent: OneVsAll[FeaturesType, E, M],\n+      val models: Array[M])\n+  extends PredictionModel[FeaturesType, OneVsAllModel[FeaturesType, E, M]]\n+  with ClassifierParams {\n+\n+  override protected def featuresDataType: DataType = parent.featuresDataType\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, featuresDataType)\n+  }\n+\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    // TODO: Use DataFrame expressions to leverage performance here\n+    val predictions = models.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>\n+        a ++ b\n+      }\n+    }.map(_.maxBy(_._2))\n+\n+    // ensure that we pass through columns that are part of the original dataset.\n+    val results = dataset.select(col(\"*\")).rdd.zip(predictions).map { case (row, (label, _)) =>\n+      Row.fromSeq(row.toSeq ++ List(label.toDouble))\n+    }\n+\n+    // the output schema should retain all input fields and add prediction column.\n+    val outputSchema = SchemaUtils.appendColumn(parentSchema, $(predictionCol), DoubleType)\n+    sqlCtx.createDataFrame(results, outputSchema)\n+  }\n+\n+  def predict(features: FeaturesType): Double = {"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Is this needed?\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-07T22:09:55Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.impl.estimator.{PredictionModel, Predictor}\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Model produced by [[OneVsAll]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsAllModel[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],\n+    M <: ClassificationModel[FeaturesType, M]](\n+      override val parent: OneVsAll[FeaturesType, E, M],\n+      val models: Array[M])\n+  extends PredictionModel[FeaturesType, OneVsAllModel[FeaturesType, E, M]]\n+  with ClassifierParams {\n+\n+  override protected def featuresDataType: DataType = parent.featuresDataType\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, featuresDataType)\n+  }\n+\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    // TODO: Use DataFrame expressions to leverage performance here\n+    val predictions = models.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>\n+        a ++ b\n+      }\n+    }.map(_.maxBy(_._2))\n+\n+    // ensure that we pass through columns that are part of the original dataset.\n+    val results = dataset.select(col(\"*\")).rdd.zip(predictions).map { case (row, (label, _)) =>\n+      Row.fromSeq(row.toSeq ++ List(label.toDouble))\n+    }\n+\n+    // the output schema should retain all input fields and add prediction column.\n+    val outputSchema = SchemaUtils.appendColumn(parentSchema, $(predictionCol), DoubleType)\n+    sqlCtx.createDataFrame(results, outputSchema)\n+  }\n+\n+  def predict(features: FeaturesType): Double = {\n+    throw new UnsupportedOperationException(\"Ensemble Classifier does not support predict,\" +\n+      \" use transform instead\")\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Reduction of Multiclass Classification to Binary Classification.\n+ * Performs reduction using one against all strategy.\n+ * For a multiclass classification with k classes, train k models (one per class).\n+ * Each example is scored against all k models and the model with highest score\n+ * is picked to label the example.\n+ *\n+ * Classifier parameters like featuresCol, predictionCol and rawPredictionCol are\n+ * set directly on the OneVsRest and are ignored on the underlying classifier.\n+ *\n+ */\n+@Experimental\n+class OneVsAll[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],\n+    M <: ClassificationModel[FeaturesType, M]]\n+  (val classifier: Classifier[FeaturesType, E, M])\n+  (implicit m: ClassTag[M])"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "ditto\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-07T22:09:56Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.impl.estimator.{PredictionModel, Predictor}\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Model produced by [[OneVsAll]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsAllModel[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],\n+    M <: ClassificationModel[FeaturesType, M]](\n+      override val parent: OneVsAll[FeaturesType, E, M],\n+      val models: Array[M])\n+  extends PredictionModel[FeaturesType, OneVsAllModel[FeaturesType, E, M]]\n+  with ClassifierParams {\n+\n+  override protected def featuresDataType: DataType = parent.featuresDataType\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, featuresDataType)\n+  }\n+\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    // TODO: Use DataFrame expressions to leverage performance here\n+    val predictions = models.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>\n+        a ++ b\n+      }\n+    }.map(_.maxBy(_._2))\n+\n+    // ensure that we pass through columns that are part of the original dataset.\n+    val results = dataset.select(col(\"*\")).rdd.zip(predictions).map { case (row, (label, _)) =>\n+      Row.fromSeq(row.toSeq ++ List(label.toDouble))\n+    }\n+\n+    // the output schema should retain all input fields and add prediction column.\n+    val outputSchema = SchemaUtils.appendColumn(parentSchema, $(predictionCol), DoubleType)\n+    sqlCtx.createDataFrame(results, outputSchema)\n+  }\n+\n+  def predict(features: FeaturesType): Double = {\n+    throw new UnsupportedOperationException(\"Ensemble Classifier does not support predict,\" +\n+      \" use transform instead\")\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Reduction of Multiclass Classification to Binary Classification.\n+ * Performs reduction using one against all strategy.\n+ * For a multiclass classification with k classes, train k models (one per class).\n+ * Each example is scored against all k models and the model with highest score\n+ * is picked to label the example.\n+ *\n+ * Classifier parameters like featuresCol, predictionCol and rawPredictionCol are\n+ * set directly on the OneVsRest and are ignored on the underlying classifier.\n+ *\n+ */\n+@Experimental\n+class OneVsAll[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],\n+    M <: ClassificationModel[FeaturesType, M]]\n+  (val classifier: Classifier[FeaturesType, E, M])\n+  (implicit m: ClassTag[M])\n+  extends Predictor[FeaturesType, OneVsAll[FeaturesType, E, M], OneVsAllModel[FeaturesType, E, M]]\n+  with ClassifierParams {"
  }, {
    "author": {
      "login": "harsha2010"
    },
    "body": "All the base classifier parameters are being set by OneVsAll.\nIt needs access to rawPredictionCol for example and needs to be able to set/ read classifier params instead of just params.\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-07T22:22:18Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.impl.estimator.{PredictionModel, Predictor}\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Model produced by [[OneVsAll]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsAllModel[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],\n+    M <: ClassificationModel[FeaturesType, M]](\n+      override val parent: OneVsAll[FeaturesType, E, M],\n+      val models: Array[M])\n+  extends PredictionModel[FeaturesType, OneVsAllModel[FeaturesType, E, M]]\n+  with ClassifierParams {\n+\n+  override protected def featuresDataType: DataType = parent.featuresDataType\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, featuresDataType)\n+  }\n+\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    // TODO: Use DataFrame expressions to leverage performance here\n+    val predictions = models.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>\n+        a ++ b\n+      }\n+    }.map(_.maxBy(_._2))\n+\n+    // ensure that we pass through columns that are part of the original dataset.\n+    val results = dataset.select(col(\"*\")).rdd.zip(predictions).map { case (row, (label, _)) =>\n+      Row.fromSeq(row.toSeq ++ List(label.toDouble))\n+    }\n+\n+    // the output schema should retain all input fields and add prediction column.\n+    val outputSchema = SchemaUtils.appendColumn(parentSchema, $(predictionCol), DoubleType)\n+    sqlCtx.createDataFrame(results, outputSchema)\n+  }\n+\n+  def predict(features: FeaturesType): Double = {\n+    throw new UnsupportedOperationException(\"Ensemble Classifier does not support predict,\" +\n+      \" use transform instead\")\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Reduction of Multiclass Classification to Binary Classification.\n+ * Performs reduction using one against all strategy.\n+ * For a multiclass classification with k classes, train k models (one per class).\n+ * Each example is scored against all k models and the model with highest score\n+ * is picked to label the example.\n+ *\n+ * Classifier parameters like featuresCol, predictionCol and rawPredictionCol are\n+ * set directly on the OneVsRest and are ignored on the underlying classifier.\n+ *\n+ */\n+@Experimental\n+class OneVsAll[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],\n+    M <: ClassificationModel[FeaturesType, M]]\n+  (val classifier: Classifier[FeaturesType, E, M])\n+  (implicit m: ClassTag[M])\n+  extends Predictor[FeaturesType, OneVsAll[FeaturesType, E, M], OneVsAllModel[FeaturesType, E, M]]\n+  with ClassifierParams {"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "Why can't it call `classifier.copy().setRawPredictionCol(\"arbitraryName\")`?\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-08T01:17:36Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.impl.estimator.{PredictionModel, Predictor}\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Model produced by [[OneVsAll]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsAllModel[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],\n+    M <: ClassificationModel[FeaturesType, M]](\n+      override val parent: OneVsAll[FeaturesType, E, M],\n+      val models: Array[M])\n+  extends PredictionModel[FeaturesType, OneVsAllModel[FeaturesType, E, M]]\n+  with ClassifierParams {\n+\n+  override protected def featuresDataType: DataType = parent.featuresDataType\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, featuresDataType)\n+  }\n+\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    // TODO: Use DataFrame expressions to leverage performance here\n+    val predictions = models.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>\n+        a ++ b\n+      }\n+    }.map(_.maxBy(_._2))\n+\n+    // ensure that we pass through columns that are part of the original dataset.\n+    val results = dataset.select(col(\"*\")).rdd.zip(predictions).map { case (row, (label, _)) =>\n+      Row.fromSeq(row.toSeq ++ List(label.toDouble))\n+    }\n+\n+    // the output schema should retain all input fields and add prediction column.\n+    val outputSchema = SchemaUtils.appendColumn(parentSchema, $(predictionCol), DoubleType)\n+    sqlCtx.createDataFrame(results, outputSchema)\n+  }\n+\n+  def predict(features: FeaturesType): Double = {\n+    throw new UnsupportedOperationException(\"Ensemble Classifier does not support predict,\" +\n+      \" use transform instead\")\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Reduction of Multiclass Classification to Binary Classification.\n+ * Performs reduction using one against all strategy.\n+ * For a multiclass classification with k classes, train k models (one per class).\n+ * Each example is scored against all k models and the model with highest score\n+ * is picked to label the example.\n+ *\n+ * Classifier parameters like featuresCol, predictionCol and rawPredictionCol are\n+ * set directly on the OneVsRest and are ignored on the underlying classifier.\n+ *\n+ */\n+@Experimental\n+class OneVsAll[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],\n+    M <: ClassificationModel[FeaturesType, M]]\n+  (val classifier: Classifier[FeaturesType, E, M])\n+  (implicit m: ClassTag[M])\n+  extends Predictor[FeaturesType, OneVsAll[FeaturesType, E, M], OneVsAllModel[FeaturesType, E, M]]\n+  with ClassifierParams {"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "This should search for the maximum value, not the number of distinct ones.  Later on, there's the assumption that classes use indices 0,...,numClasses-1\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-07T22:09:57Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.impl.estimator.{PredictionModel, Predictor}\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Model produced by [[OneVsAll]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsAllModel[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],\n+    M <: ClassificationModel[FeaturesType, M]](\n+      override val parent: OneVsAll[FeaturesType, E, M],\n+      val models: Array[M])\n+  extends PredictionModel[FeaturesType, OneVsAllModel[FeaturesType, E, M]]\n+  with ClassifierParams {\n+\n+  override protected def featuresDataType: DataType = parent.featuresDataType\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, featuresDataType)\n+  }\n+\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    // TODO: Use DataFrame expressions to leverage performance here\n+    val predictions = models.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>\n+        a ++ b\n+      }\n+    }.map(_.maxBy(_._2))\n+\n+    // ensure that we pass through columns that are part of the original dataset.\n+    val results = dataset.select(col(\"*\")).rdd.zip(predictions).map { case (row, (label, _)) =>\n+      Row.fromSeq(row.toSeq ++ List(label.toDouble))\n+    }\n+\n+    // the output schema should retain all input fields and add prediction column.\n+    val outputSchema = SchemaUtils.appendColumn(parentSchema, $(predictionCol), DoubleType)\n+    sqlCtx.createDataFrame(results, outputSchema)\n+  }\n+\n+  def predict(features: FeaturesType): Double = {\n+    throw new UnsupportedOperationException(\"Ensemble Classifier does not support predict,\" +\n+      \" use transform instead\")\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Reduction of Multiclass Classification to Binary Classification.\n+ * Performs reduction using one against all strategy.\n+ * For a multiclass classification with k classes, train k models (one per class).\n+ * Each example is scored against all k models and the model with highest score\n+ * is picked to label the example.\n+ *\n+ * Classifier parameters like featuresCol, predictionCol and rawPredictionCol are\n+ * set directly on the OneVsRest and are ignored on the underlying classifier.\n+ *\n+ */\n+@Experimental\n+class OneVsAll[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],\n+    M <: ClassificationModel[FeaturesType, M]]\n+  (val classifier: Classifier[FeaturesType, E, M])\n+  (implicit m: ClassTag[M])\n+  extends Predictor[FeaturesType, OneVsAll[FeaturesType, E, M], OneVsAllModel[FeaturesType, E, M]]\n+  with ClassifierParams {\n+\n+  override private[ml] def featuresDataType: DataType = classifier.featuresDataType\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = true, featuresDataType)\n+  }\n+\n+  override protected def train(dataset: DataFrame): OneVsAllModel[FeaturesType, E, M] = {\n+    // determine number of classes either from metadata if provided, or via computation.\n+    val labelSchema = dataset.schema($(labelCol))\n+    val computeNumClasses: () => Int = () => {\n+      dataset.select($(labelCol)).distinct.count().toInt"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "These should be Predictor and PredictionModel (not Classifier, ClassificationModel).\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-08T01:16:28Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.impl.estimator.{PredictionModel, Predictor}\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Model produced by [[OneVsAll]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsAllModel[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],"
  }, {
    "author": {
      "login": "harsha2010"
    },
    "body": "The input to the reduction is a Classifier & ClassifierModel, not a Predictor, PredictorModel right? The type signature simply indicates the input. OneVsAllModel itself implements Predictor not Classifier, but OneVsAllModel needs an Array of ClassificationModels as input (i.e. prediction models with the ability to output rawPrediction scores)\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-08T01:41:19Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.impl.estimator.{PredictionModel, Predictor}\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Model produced by [[OneVsAll]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsAllModel[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "But the type parameters E and M are for OneVsAll and OneVsAllModel, which are of types Predictor and PredictionModel.  (It bothers me that this compiles, hm...)  Those type parameters should not affect what OneVsAllModel stores internally (a ClassificationModel).\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-08T02:17:47Z",
    "diffHunk": "@@ -0,0 +1,165 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import scala.language.existentials\n+import scala.reflect.ClassTag\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.impl.estimator.{PredictionModel, Predictor}\n+import org.apache.spark.ml.param.ParamMap\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Model produced by [[OneVsAll]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsAllModel[\n+    FeaturesType,\n+    E <: Classifier[FeaturesType, E, M],"
  }],
  "prId": 5830
}]