[{
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "4 indent\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2015-10-22T19:13:33Z",
    "diffHunk": "@@ -151,42 +160,39 @@ private[ann] object AffineLayerModel {\n    * Creates a model of Affine layer\n    * @param layer layer properties\n    * @param weights vector with weights\n-   * @param position position of weights in the vector\n    * @return model of Affine layer\n    */\n-  def apply(layer: AffineLayer, weights: Vector, position: Int): AffineLayerModel = {\n-    val (w, b) = unroll(weights, position, layer.numIn, layer.numOut)\n+  def apply(layer: AffineLayer, weights: BDV[Double]): AffineLayerModel = {\n+    val (w, b) = unroll(weights, layer.numIn, layer.numOut)\n     new AffineLayerModel(w, b)\n   }\n \n   /**\n    * Creates a model of Affine layer\n    * @param layer layer properties\n-   * @param seed seed\n+   * @param weights vector for weights initialization\n+   * @param random random number generator\n    * @return model of Affine layer\n    */\n-  def apply(layer: AffineLayer, seed: Long): AffineLayerModel = {\n-    val (w, b) = randomWeights(layer.numIn, layer.numOut, seed)\n+  def apply(layer: AffineLayer, weights: BDV[Double], random: Random): AffineLayerModel = {\n+    val (w, b) = randomWeights(layer.numIn, layer.numOut, weights, random)\n     new AffineLayerModel(w, b)\n   }\n \n   /**\n    * Unrolls the weights from the vector\n    * @param weights vector with weights\n-   * @param position position of weights for this layer\n    * @param numIn number of layer inputs\n    * @param numOut number of layer outputs\n    * @return matrix A and vector b\n    */\n   def unroll(\n-    weights: Vector,\n-    position: Int,\n+    weights: BDV[Double],"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "`Layer` is really implementing the Factory pattern for `LayerModel`. Should we consider renaming to use proper names?\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2015-10-22T19:19:33Z",
    "diffHunk": "@@ -32,20 +32,33 @@ import org.apache.spark.util.random.XORShiftRandom\n  *\n  */\n private[ann] trait Layer extends Serializable {\n+\n+  /**\n+   * Number of weights that is used to allocate memory for the weights vector\n+   */\n+  val weightSize: Int\n+\n+  /**\n+   * Returns the output size given the input size (not counting the stack size).\n+   * Output size is used to allocate memory for the output.\n+   * @param inputSize input size\n+   * @return output size\n+   */\n+  def outputSize(inputSize: Int): Int\n+\n   /**\n    * Returns the instance of the layer based on weights provided\n    * @param weights vector with layer weights\n-   * @param position position of weights in the vector\n    * @return the layer model\n    */\n-  def getInstance(weights: Vector, position: Int): LayerModel\n-\n+  def instance(weights: BDV[Double]): LayerModel"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "Since `w` does not resize if `delta` changes, I don't think we need to worry about resizing `ones` here and can instead just make L130 lazy init `ones`\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2015-10-22T19:23:19Z",
    "diffHunk": "@@ -111,32 +126,26 @@ private[ann] class AffineLayer(val numIn: Int, val numOut: Int) extends Layer {\n  * @param b bias (vector b)\n  */\n private[ann] class AffineLayerModel private(w: BDM[Double], b: BDV[Double]) extends LayerModel {\n-  val size = w.size + b.length\n-  val gwb = new Array[Double](size)\n-  private lazy val gw: BDM[Double] = new BDM[Double](w.rows, w.cols, gwb)\n-  private lazy val gb: BDV[Double] = new BDV[Double](gwb, w.size)\n-  private var z: BDM[Double] = null\n-  private var d: BDM[Double] = null\n+\n   private var ones: BDV[Double] = null\n \n-  override def eval(data: BDM[Double]): BDM[Double] = {\n-    if (z == null || z.cols != data.cols) z = new BDM[Double](w.rows, data.cols)\n-    z(::, *) := b\n-    BreezeUtil.dgemm(1.0, w, data, 1.0, z)\n-    z\n+  override def eval(data: BDM[Double], output: BDM[Double]): Unit = {\n+    output(::, *) := b\n+    BreezeUtil.dgemm(1.0, w, data, 1.0, output)\n   }\n \n-  override def prevDelta(nextDelta: BDM[Double], input: BDM[Double]): BDM[Double] = {\n-    if (d == null || d.cols != nextDelta.cols) d = new BDM[Double](w.cols, nextDelta.cols)\n-    BreezeUtil.dgemm(1.0, w.t, nextDelta, 0.0, d)\n-    d\n+  override def prevDelta(nextDelta: BDM[Double], input: BDM[Double], delta: BDM[Double]): Unit = {\n+    BreezeUtil.dgemm(1.0, w.t, nextDelta, 0.0, delta)\n   }\n \n-  override def grad(delta: BDM[Double], input: BDM[Double]): Array[Double] = {\n-    BreezeUtil.dgemm(1.0 / input.cols, delta, input.t, 0.0, gw)\n+  override def grad(delta: BDM[Double], input: BDM[Double], cumGrad: BDV[Double]): Unit = {\n+    // compute gradient of weights\n+    val cumGradientOfWeights = new BDM[Double](w.rows, w.cols, cumGrad.data, cumGrad.offset)\n+    BreezeUtil.dgemm(1.0 / input.cols, delta, input.t, 1.0, cumGradientOfWeights)\n     if (ones == null || ones.length != delta.cols) ones = BDV.ones[Double](delta.cols)",
    "line": 207
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "nit: I would prefer to line break at arguments. Not sure if it's part of official style guide but I can't recall seeing breaking on the return type elsewhere in codebase\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2015-10-22T19:27:29Z",
    "diffHunk": "@@ -208,14 +214,18 @@ private[ann] object AffineLayerModel {\n    * Generate random weights for the layer\n    * @param numIn number of inputs\n    * @param numOut number of outputs\n-   * @param seed seed\n+   * @param weights vector for weights initialization\n+   * @param random random number generator\n    * @return (matrix A, vector b)\n    */\n-  def randomWeights(numIn: Int, numOut: Int, seed: Long = 11L): (BDM[Double], BDV[Double]) = {\n-    val rand: XORShiftRandom = new XORShiftRandom(seed)\n-    val weights = BDM.fill[Double](numOut, numIn){ (rand.nextDouble * 4.8 - 2.4) / numIn }\n-    val bias = BDV.fill[Double](numOut){ (rand.nextDouble * 4.8 - 2.4) / numIn }\n-    (weights, bias)\n+  def randomWeights(numIn: Int, numOut: Int, weights: BDV[Double], random: Random):\n+  (BDM[Double], BDV[Double]) = {"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "Since I don't see `InPlace` being mixed in anywhere except `LayerModel`, can we just replace the empty `InPlace` trait with a `inPlace : Boolean` flag defined in the `LayerModel` interface and reduce unnecessary abstraction? \n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2015-10-22T19:36:53Z",
    "diffHunk": "@@ -546,65 +446,83 @@ private[ml] object FeedForwardTopology {\n  * Model of Feed Forward Neural Network.\n  * Implements forward, gradient computation and can return weights in vector format.\n  * @param layerModels models of layers\n- * @param topology topology of the network\n+ * @param layers topology of the network\n  */\n private[ml] class FeedForwardModel private(\n     val layerModels: Array[LayerModel],\n-    val topology: FeedForwardTopology) extends TopologyModel {\n+    val layers: Array[Layer]) extends TopologyModel {\n+\n+  private var outputs: Array[BDM[Double]] = null\n+  private var deltas: Array[BDM[Double]] = null\n+\n   override def forward(data: BDM[Double]): Array[BDM[Double]] = {\n-    val outputs = new Array[BDM[Double]](layerModels.length)\n-    outputs(0) = layerModels(0).eval(data)\n+    // Initialize output arrays for all layers. Special treatment for InPlace\n+    val currentBatchSize = data.cols\n+    // TODO: allocate outputs as one big array and then create BDMs from it\n+    if (outputs == null || outputs(0).cols != currentBatchSize) {\n+      outputs = new Array[BDM[Double]](layerModels.length)\n+      var inputSize = data.rows\n+      for (i <- 0 until layerModels.length) {\n+        layerModels(i) match {\n+          case inPlace: InPlace => outputs(i) = outputs(i - 1)"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "nit: Arguments should be 4-indented on new line\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2015-10-22T19:37:24Z",
    "diffHunk": "@@ -546,65 +446,83 @@ private[ml] object FeedForwardTopology {\n  * Model of Feed Forward Neural Network.\n  * Implements forward, gradient computation and can return weights in vector format.\n  * @param layerModels models of layers\n- * @param topology topology of the network\n+ * @param layers topology of the network\n  */\n private[ml] class FeedForwardModel private(\n     val layerModels: Array[LayerModel],\n-    val topology: FeedForwardTopology) extends TopologyModel {\n+    val layers: Array[Layer]) extends TopologyModel {\n+\n+  private var outputs: Array[BDM[Double]] = null\n+  private var deltas: Array[BDM[Double]] = null\n+\n   override def forward(data: BDM[Double]): Array[BDM[Double]] = {\n-    val outputs = new Array[BDM[Double]](layerModels.length)\n-    outputs(0) = layerModels(0).eval(data)\n+    // Initialize output arrays for all layers. Special treatment for InPlace\n+    val currentBatchSize = data.cols\n+    // TODO: allocate outputs as one big array and then create BDMs from it\n+    if (outputs == null || outputs(0).cols != currentBatchSize) {\n+      outputs = new Array[BDM[Double]](layerModels.length)\n+      var inputSize = data.rows\n+      for (i <- 0 until layerModels.length) {\n+        layerModels(i) match {\n+          case inPlace: InPlace => outputs(i) = outputs(i - 1)\n+          case _ => {\n+            val outputSize = layers(i).outputSize(inputSize)\n+            outputs(i) = new BDM[Double](outputSize, currentBatchSize)\n+            inputSize = outputSize\n+          }\n+        }\n+      }\n+    }\n+    layerModels(0).eval(data, outputs(0))\n     for (i <- 1 until layerModels.length) {\n-      outputs(i) = layerModels(i).eval(outputs(i-1))\n+      layerModels(i).eval(outputs(i - 1), outputs(i))\n     }\n     outputs\n   }\n \n-  override def computeGradient(\n-    data: BDM[Double],\n-    target: BDM[Double],\n-    cumGradient: Vector,\n-    realBatchSize: Int): Double = {\n+  override def computeGradient(data: BDM[Double],"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "extra leading whitespace\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2015-10-22T19:38:12Z",
    "diffHunk": "@@ -546,65 +446,83 @@ private[ml] object FeedForwardTopology {\n  * Model of Feed Forward Neural Network.\n  * Implements forward, gradient computation and can return weights in vector format.\n  * @param layerModels models of layers\n- * @param topology topology of the network\n+ * @param layers topology of the network\n  */\n private[ml] class FeedForwardModel private(\n     val layerModels: Array[LayerModel],\n-    val topology: FeedForwardTopology) extends TopologyModel {\n+    val layers: Array[Layer]) extends TopologyModel {\n+\n+  private var outputs: Array[BDM[Double]] = null\n+  private var deltas: Array[BDM[Double]] = null\n+\n   override def forward(data: BDM[Double]): Array[BDM[Double]] = {\n-    val outputs = new Array[BDM[Double]](layerModels.length)\n-    outputs(0) = layerModels(0).eval(data)\n+    // Initialize output arrays for all layers. Special treatment for InPlace\n+    val currentBatchSize = data.cols\n+    // TODO: allocate outputs as one big array and then create BDMs from it\n+    if (outputs == null || outputs(0).cols != currentBatchSize) {\n+      outputs = new Array[BDM[Double]](layerModels.length)\n+      var inputSize = data.rows\n+      for (i <- 0 until layerModels.length) {\n+        layerModels(i) match {\n+          case inPlace: InPlace => outputs(i) = outputs(i - 1)\n+          case _ => {\n+            val outputSize = layers(i).outputSize(inputSize)\n+            outputs(i) = new BDM[Double](outputSize, currentBatchSize)\n+            inputSize = outputSize\n+          }\n+        }\n+      }\n+    }\n+    layerModels(0).eval(data, outputs(0))\n     for (i <- 1 until layerModels.length) {\n-      outputs(i) = layerModels(i).eval(outputs(i-1))\n+      layerModels(i).eval(outputs(i - 1), outputs(i))\n     }\n     outputs\n   }\n \n-  override def computeGradient(\n-    data: BDM[Double],\n-    target: BDM[Double],\n-    cumGradient: Vector,\n-    realBatchSize: Int): Double = {\n+  override def computeGradient(data: BDM[Double],\n+                               target: BDM[Double],\n+                               cumGradient: Vector,\n+                               realBatchSize: Int): Double = {\n     val outputs = forward(data)\n-    val deltas = new Array[BDM[Double]](layerModels.length)\n+    val currentBatchSize = data.cols\n+    // TODO: allocate deltas as one big array and then create BDMs from it\n+    if (deltas == null || deltas(0).cols != currentBatchSize) {\n+      deltas = new Array[BDM[Double]](layerModels.length)\n+      var inputSize = data.rows\n+      for (i <- 0 until layerModels.length - 1) {\n+          val outputSize = layers(i).outputSize(inputSize)\n+          deltas(i) = new BDM[Double](outputSize, currentBatchSize)\n+          inputSize = outputSize\n+      }\n+    }\n     val L = layerModels.length - 1\n-    val (newE, newError) = layerModels.last match {\n-      case flm: FunctionalLayerModel => flm.error(outputs.last, target)\n+    // TODO: explain why delta of top layer is null (because it might contain loss+layer)\n+    val loss = layerModels.last match {\n+      case levelWithError: LossFunction => levelWithError.loss(outputs.last, target, deltas(L - 1))\n       case _ =>\n-        throw new UnsupportedOperationException(\"Non-functional layer not supported at the top\")\n+        throw new UnsupportedOperationException(\"Top layer is required to have objective.\")\n     }\n-    deltas(L) = new BDM[Double](0, 0)\n-    deltas(L - 1) = newE\n     for (i <- (L - 2) to (0, -1)) {\n-      deltas(i) = layerModels(i + 1).prevDelta(deltas(i + 1), outputs(i + 1))\n+       layerModels(i + 1).prevDelta(deltas(i + 1), outputs(i + 1), deltas(i))"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "I found it slightly confusing that the `apply` method of a companion object is mutating in place since companion object `apply`s are typically used to implement default constructors; we should consider renaming `apply` -> `call` if you think that's clearer\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2015-10-22T19:42:46Z",
    "diffHunk": "@@ -226,44 +236,21 @@ private[ann] trait ActivationFunction extends Serializable {\n \n   /**\n    * Implements a function\n-   * @param x input data\n-   * @param y output data\n    */\n-  def eval(x: BDM[Double], y: BDM[Double]): Unit\n+  def eval: Double => Double\n \n   /**\n    * Implements a derivative of a function (needed for the back propagation)\n-   * @param x input data\n-   * @param y output data\n    */\n-  def derivative(x: BDM[Double], y: BDM[Double]): Unit\n-\n-  /**\n-   * Implements a cross entropy error of a function.\n-   * Needed if the functional layer that contains this function is the output layer\n-   * of the network.\n-   * @param target target output\n-   * @param output computed output\n-   * @param result intermediate result\n-   * @return cross-entropy\n-   */\n-  def crossEntropy(target: BDM[Double], output: BDM[Double], result: BDM[Double]): Double\n-\n-  /**\n-   * Implements a mean squared error of a function\n-   * @param target target output\n-   * @param output computed output\n-   * @param result intermediate result\n-   * @return mean squared error\n-   */\n-  def squared(target: BDM[Double], output: BDM[Double], result: BDM[Double]): Double\n+  def derivative: Double => Double\n }\n \n /**\n- * Implements in-place application of functions\n+ * Implements in-place application of functions in the arrays\n  */\n-private[ann] object ActivationFunction {\n+private[ann] object UniversalFunction {"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "+1 on @feynmanliang 's suggestion. Btw, it is not clear to me what `UniversalFunction` means here. If we call it `ApplyInPlace`, then we can keep the `apply` method name.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:31Z",
    "diffHunk": "@@ -226,44 +236,21 @@ private[ann] trait ActivationFunction extends Serializable {\n \n   /**\n    * Implements a function\n-   * @param x input data\n-   * @param y output data\n    */\n-  def eval(x: BDM[Double], y: BDM[Double]): Unit\n+  def eval: Double => Double\n \n   /**\n    * Implements a derivative of a function (needed for the back propagation)\n-   * @param x input data\n-   * @param y output data\n    */\n-  def derivative(x: BDM[Double], y: BDM[Double]): Unit\n-\n-  /**\n-   * Implements a cross entropy error of a function.\n-   * Needed if the functional layer that contains this function is the output layer\n-   * of the network.\n-   * @param target target output\n-   * @param output computed output\n-   * @param result intermediate result\n-   * @return cross-entropy\n-   */\n-  def crossEntropy(target: BDM[Double], output: BDM[Double], result: BDM[Double]): Double\n-\n-  /**\n-   * Implements a mean squared error of a function\n-   * @param target target output\n-   * @param output computed output\n-   * @param result intermediate result\n-   * @return mean squared error\n-   */\n-  def squared(target: BDM[Double], output: BDM[Double], result: BDM[Double]): Double\n+  def derivative: Double => Double\n }\n \n /**\n- * Implements in-place application of functions\n+ * Implements in-place application of functions in the arrays\n  */\n-private[ann] object ActivationFunction {\n+private[ann] object UniversalFunction {"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "nit: Space before `{`\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2015-10-22T19:43:36Z",
    "diffHunk": "@@ -466,6 +352,15 @@ private[ann] trait Topology extends Serializable{\n  */\n private[ann] trait TopologyModel extends Serializable{"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "nit: maybe instead of `softmax`, have an `isBinaryClassification` flag to make intention clear\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2015-10-22T19:50:41Z",
    "diffHunk": "@@ -532,8 +427,13 @@ private[ml] object FeedForwardTopology {\n     for(i <- 0 until layerSizes.length - 1){\n       layers(i * 2) = new AffineLayer(layerSizes(i), layerSizes(i + 1))\n       layers(i * 2 + 1) =\n-        if (softmax && i == layerSizes.length - 2) {\n-          new FunctionalLayer(new SoftmaxFunction())\n+        if (i == layerSizes.length - 2) {\n+          if (softmax) {"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`getOutputSize`\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:05Z",
    "diffHunk": "@@ -32,20 +32,43 @@ import org.apache.spark.util.random.XORShiftRandom\n  *\n  */\n private[ann] trait Layer extends Serializable {\n+\n+  /**\n+   * Number of weights that is used to allocate memory for the weights vector\n+   */\n+  val weightSize: Int\n+\n+  /**\n+   * Returns the output size given the input size (not counting the stack size).\n+   * Output size is used to allocate memory for the output.\n+   * @param inputSize input size\n+   * @return output size\n+   */\n+  def outputSize(inputSize: Int): Int"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "- `createModel` and rename `weights` to `initialWeights`\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:08Z",
    "diffHunk": "@@ -32,20 +32,43 @@ import org.apache.spark.util.random.XORShiftRandom\n  *\n  */\n private[ann] trait Layer extends Serializable {\n+\n+  /**\n+   * Number of weights that is used to allocate memory for the weights vector\n+   */\n+  val weightSize: Int\n+\n+  /**\n+   * Returns the output size given the input size (not counting the stack size).\n+   * Output size is used to allocate memory for the output.\n+   * @param inputSize input size\n+   * @return output size\n+   */\n+  def outputSize(inputSize: Int): Int\n+\n+  /**\n+   * If true, the memory is not allocated for the output of this layer.\n+   * The memory allocated to the previous layer is used to write the output of this layer.\n+   * Developer can set this to true if computing delta of a previous layer\n+   * does not involve its output, so the current layer can write there.\n+   * This also mean that both layers have the same number of outputs.\n+   */\n+  val inPlace: Boolean\n+\n   /**\n-   * Returns the instance of the layer based on weights provided\n+   * Returns the instance of the layer based on weights provided.\n+   * Size of weights must be equal to weightSize\n    * @param weights vector with layer weights\n-   * @param position position of weights in the vector\n    * @return the layer model\n    */\n-  def getInstance(weights: Vector, position: Int): LayerModel\n-\n+  def model(weights: BDV[Double]): LayerModel"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "document the behavior when the output matrix is not large enough\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:10Z",
    "diffHunk": "@@ -54,39 +77,32 @@ private[ann] trait Layer extends Serializable {\n  * Can return weights in Vector format.\n  */\n private[ann] trait LayerModel extends Serializable {\n-  /**\n-   * number of weights\n-   */\n-  val size: Int\n \n+  val weights: BDV[Double]\n   /**\n    * Evaluates the data (process the data through the layer)\n    * @param data data\n-   * @return processed data\n+   * @param output output to write to"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "minor: still useful to return the output instead of `Unit`\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:12Z",
    "diffHunk": "@@ -54,39 +77,32 @@ private[ann] trait Layer extends Serializable {\n  * Can return weights in Vector format.\n  */\n private[ann] trait LayerModel extends Serializable {\n-  /**\n-   * number of weights\n-   */\n-  val size: Int\n \n+  val weights: BDV[Double]\n   /**\n    * Evaluates the data (process the data through the layer)\n    * @param data data\n-   * @return processed data\n+   * @param output output to write to\n    */\n-  def eval(data: BDM[Double]): BDM[Double]\n+  def eval(data: BDM[Double], output: BDM[Double]): Unit"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "- document what input parameters will be changed in-place\n- `prevDelta`. We can change the method name to `computePrevDelta` (using a verb)\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:13Z",
    "diffHunk": "@@ -54,39 +77,32 @@ private[ann] trait Layer extends Serializable {\n  * Can return weights in Vector format.\n  */\n private[ann] trait LayerModel extends Serializable {\n-  /**\n-   * number of weights\n-   */\n-  val size: Int\n \n+  val weights: BDV[Double]\n   /**\n    * Evaluates the data (process the data through the layer)\n    * @param data data\n-   * @return processed data\n+   * @param output output to write to\n    */\n-  def eval(data: BDM[Double]): BDM[Double]\n+  def eval(data: BDM[Double], output: BDM[Double]): Unit\n \n   /**\n    * Computes the delta for back propagation\n-   * @param nextDelta delta of the next layer\n-   * @param input input data\n+   * @param delta delta of this layer\n+   * @param output output of this layer\n+   * @param pDelta storage for the result, the previous delta"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "(minor) Could return `prevDelta` instead of `Unit`\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:15Z",
    "diffHunk": "@@ -54,39 +77,32 @@ private[ann] trait Layer extends Serializable {\n  * Can return weights in Vector format.\n  */\n private[ann] trait LayerModel extends Serializable {\n-  /**\n-   * number of weights\n-   */\n-  val size: Int\n \n+  val weights: BDV[Double]\n   /**\n    * Evaluates the data (process the data through the layer)\n    * @param data data\n-   * @return processed data\n+   * @param output output to write to\n    */\n-  def eval(data: BDM[Double]): BDM[Double]\n+  def eval(data: BDM[Double], output: BDM[Double]): Unit\n \n   /**\n    * Computes the delta for back propagation\n-   * @param nextDelta delta of the next layer\n-   * @param input input data\n+   * @param delta delta of this layer\n+   * @param output output of this layer\n+   * @param pDelta storage for the result, the previous delta\n    * @return delta\n    */\n-  def prevDelta(nextDelta: BDM[Double], input: BDM[Double]): BDM[Double]\n+  def prevDelta(delta: BDM[Double], output: BDM[Double], pDelta: BDM[Double]): Unit"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "same here. document clearly if this going to be modified in-place\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:16Z",
    "diffHunk": "@@ -54,39 +77,32 @@ private[ann] trait Layer extends Serializable {\n  * Can return weights in Vector format.\n  */\n private[ann] trait LayerModel extends Serializable {\n-  /**\n-   * number of weights\n-   */\n-  val size: Int\n \n+  val weights: BDV[Double]\n   /**\n    * Evaluates the data (process the data through the layer)\n    * @param data data\n-   * @return processed data\n+   * @param output output to write to\n    */\n-  def eval(data: BDM[Double]): BDM[Double]\n+  def eval(data: BDM[Double], output: BDM[Double]): Unit\n \n   /**\n    * Computes the delta for back propagation\n-   * @param nextDelta delta of the next layer\n-   * @param input input data\n+   * @param delta delta of this layer\n+   * @param output output of this layer\n+   * @param pDelta storage for the result, the previous delta\n    * @return delta\n    */\n-  def prevDelta(nextDelta: BDM[Double], input: BDM[Double]): BDM[Double]\n+  def prevDelta(delta: BDM[Double], output: BDM[Double], pDelta: BDM[Double]): Unit\n \n   /**\n    * Computes the gradient\n    * @param delta delta for this layer\n    * @param input input data\n+   * @param cumGrad cumulative gradient"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "need to update the doc\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:17Z",
    "diffHunk": "@@ -54,39 +77,32 @@ private[ann] trait Layer extends Serializable {\n  * Can return weights in Vector format.\n  */\n private[ann] trait LayerModel extends Serializable {\n-  /**\n-   * number of weights\n-   */\n-  val size: Int\n \n+  val weights: BDV[Double]\n   /**\n    * Evaluates the data (process the data through the layer)\n    * @param data data\n-   * @return processed data\n+   * @param output output to write to\n    */\n-  def eval(data: BDM[Double]): BDM[Double]\n+  def eval(data: BDM[Double], output: BDM[Double]): Unit\n \n   /**\n    * Computes the delta for back propagation\n-   * @param nextDelta delta of the next layer\n-   * @param input input data\n+   * @param delta delta of this layer\n+   * @param output output of this layer\n+   * @param pDelta storage for the result, the previous delta\n    * @return delta\n    */\n-  def prevDelta(nextDelta: BDM[Double], input: BDM[Double]): BDM[Double]\n+  def prevDelta(delta: BDM[Double], output: BDM[Double], pDelta: BDM[Double]): Unit\n \n   /**\n    * Computes the gradient\n    * @param delta delta for this layer\n    * @param input input data\n+   * @param cumGrad cumulative gradient\n    * @return gradient"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "This is not the same signature (arg names) as defined in the interface. If you use intellij, you should see a warning here.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:21Z",
    "diffHunk": "@@ -96,50 +112,50 @@ private[ann] trait LayerModel extends Serializable {\n  */\n private[ann] class AffineLayer(val numIn: Int, val numOut: Int) extends Layer {\n \n-  override def getInstance(weights: Vector, position: Int): LayerModel = {\n-    AffineLayerModel(this, weights, position)\n-  }\n+  override val weightSize = numIn * numOut + numOut\n \n-  override def getInstance(seed: Long = 11L): LayerModel = {\n-    AffineLayerModel(this, seed)\n-  }\n+  override def outputSize(inputSize: Int): Int = numOut\n+\n+  override val inPlace = false\n+\n+  override def model(weights: BDV[Double]): LayerModel = new AffineLayerModel(weights, this)\n+\n+  override def initModel(weights: BDV[Double], random: Random): LayerModel =\n+    AffineLayerModel(this, weights, random)\n }\n \n /**\n- * Model of Affine layer y=A*x+b\n- * @param w weights (matrix A)\n- * @param b bias (vector b)\n+ * Model of Affine layer\n+ * @param weights weights\n+ * @param layer layer properties\n  */\n-private[ann] class AffineLayerModel private(w: BDM[Double], b: BDV[Double]) extends LayerModel {\n-  val size = w.size + b.length\n-  val gwb = new Array[Double](size)\n-  private lazy val gw: BDM[Double] = new BDM[Double](w.rows, w.cols, gwb)\n-  private lazy val gb: BDV[Double] = new BDV[Double](gwb, w.size)\n-  private var z: BDM[Double] = null\n-  private var d: BDM[Double] = null\n+private[ann] class AffineLayerModel private[ann] (\n+    val weights: BDV[Double],\n+    val layer: AffineLayer) extends LayerModel {\n+  val w = new BDM[Double](layer.numOut, layer.numIn, weights.data, weights.offset)\n+  val b =\n+    new BDV[Double](weights.data, weights.offset + (layer.numOut * layer.numIn), 1, layer.numOut)\n+\n   private var ones: BDV[Double] = null\n \n-  override def eval(data: BDM[Double]): BDM[Double] = {\n-    if (z == null || z.cols != data.cols) z = new BDM[Double](w.rows, data.cols)\n-    z(::, *) := b\n-    BreezeUtil.dgemm(1.0, w, data, 1.0, z)\n-    z\n+  override def eval(data: BDM[Double], output: BDM[Double]): Unit = {\n+    output(::, *) := b\n+    BreezeUtil.dgemm(1.0, w, data, 1.0, output)\n   }\n \n-  override def prevDelta(nextDelta: BDM[Double], input: BDM[Double]): BDM[Double] = {\n-    if (d == null || d.cols != nextDelta.cols) d = new BDM[Double](w.cols, nextDelta.cols)\n-    BreezeUtil.dgemm(1.0, w.t, nextDelta, 0.0, d)\n-    d\n+  override def prevDelta(nextDelta: BDM[Double], input: BDM[Double], delta: BDM[Double]): Unit = {"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "4-space indentation\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:26Z",
    "diffHunk": "@@ -150,72 +166,32 @@ private[ann] object AffineLayerModel {\n   /**\n    * Creates a model of Affine layer\n    * @param layer layer properties\n-   * @param weights vector with weights\n-   * @param position position of weights in the vector\n+   * @param weights vector for weights initialization\n+   * @param random random number generator\n    * @return model of Affine layer\n    */\n-  def apply(layer: AffineLayer, weights: Vector, position: Int): AffineLayerModel = {\n-    val (w, b) = unroll(weights, position, layer.numIn, layer.numOut)\n-    new AffineLayerModel(w, b)\n+  def apply(layer: AffineLayer, weights: BDV[Double], random: Random): AffineLayerModel = {\n+    randomWeights(layer.numIn, layer.numOut, weights, random)\n+    new AffineLayerModel(weights, layer)\n   }\n \n   /**\n-   * Creates a model of Affine layer\n-   * @param layer layer properties\n-   * @param seed seed\n-   * @return model of Affine layer\n-   */\n-  def apply(layer: AffineLayer, seed: Long): AffineLayerModel = {\n-    val (w, b) = randomWeights(layer.numIn, layer.numOut, seed)\n-    new AffineLayerModel(w, b)\n-  }\n-\n-  /**\n-   * Unrolls the weights from the vector\n-   * @param weights vector with weights\n-   * @param position position of weights for this layer\n-   * @param numIn number of layer inputs\n-   * @param numOut number of layer outputs\n-   * @return matrix A and vector b\n-   */\n-  def unroll(\n-    weights: Vector,\n-    position: Int,\n-    numIn: Int,\n-    numOut: Int): (BDM[Double], BDV[Double]) = {\n-    val weightsCopy = weights.toArray\n-    // TODO: the array is not copied to BDMs, make sure this is OK!\n-    val a = new BDM[Double](numOut, numIn, weightsCopy, position)\n-    val b = new BDV[Double](weightsCopy, position + (numOut * numIn), 1, numOut)\n-    (a, b)\n-  }\n-\n-  /**\n-   * Roll the layer weights into a vector\n-   * @param a matrix A\n-   * @param b vector b\n-   * @return vector of weights\n-   */\n-  def roll(a: BDM[Double], b: BDV[Double]): Vector = {\n-    val result = new Array[Double](a.size + b.length)\n-    // TODO: make sure that we need to copy!\n-    System.arraycopy(a.toArray, 0, result, 0, a.size)\n-    System.arraycopy(b.toArray, 0, result, a.size, b.length)\n-    Vectors.dense(result)\n-  }\n-\n-  /**\n-   * Generate random weights for the layer\n+   * Initialize weights\n    * @param numIn number of inputs\n    * @param numOut number of outputs\n-   * @param seed seed\n-   * @return (matrix A, vector b)\n+   * @param weights vector for weights initialization\n+   * @param random random number generator\n    */\n-  def randomWeights(numIn: Int, numOut: Int, seed: Long = 11L): (BDM[Double], BDV[Double]) = {\n-    val rand: XORShiftRandom = new XORShiftRandom(seed)\n-    val weights = BDM.fill[Double](numOut, numIn){ (rand.nextDouble * 4.8 - 2.4) / numIn }\n-    val bias = BDV.fill[Double](numOut){ (rand.nextDouble * 4.8 - 2.4) / numIn }\n-    (weights, bias)\n+  def randomWeights(\n+    numIn: Int,"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "comment on `4.8` and `2.4`\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:28Z",
    "diffHunk": "@@ -150,72 +166,32 @@ private[ann] object AffineLayerModel {\n   /**\n    * Creates a model of Affine layer\n    * @param layer layer properties\n-   * @param weights vector with weights\n-   * @param position position of weights in the vector\n+   * @param weights vector for weights initialization\n+   * @param random random number generator\n    * @return model of Affine layer\n    */\n-  def apply(layer: AffineLayer, weights: Vector, position: Int): AffineLayerModel = {\n-    val (w, b) = unroll(weights, position, layer.numIn, layer.numOut)\n-    new AffineLayerModel(w, b)\n+  def apply(layer: AffineLayer, weights: BDV[Double], random: Random): AffineLayerModel = {\n+    randomWeights(layer.numIn, layer.numOut, weights, random)\n+    new AffineLayerModel(weights, layer)\n   }\n \n   /**\n-   * Creates a model of Affine layer\n-   * @param layer layer properties\n-   * @param seed seed\n-   * @return model of Affine layer\n-   */\n-  def apply(layer: AffineLayer, seed: Long): AffineLayerModel = {\n-    val (w, b) = randomWeights(layer.numIn, layer.numOut, seed)\n-    new AffineLayerModel(w, b)\n-  }\n-\n-  /**\n-   * Unrolls the weights from the vector\n-   * @param weights vector with weights\n-   * @param position position of weights for this layer\n-   * @param numIn number of layer inputs\n-   * @param numOut number of layer outputs\n-   * @return matrix A and vector b\n-   */\n-  def unroll(\n-    weights: Vector,\n-    position: Int,\n-    numIn: Int,\n-    numOut: Int): (BDM[Double], BDV[Double]) = {\n-    val weightsCopy = weights.toArray\n-    // TODO: the array is not copied to BDMs, make sure this is OK!\n-    val a = new BDM[Double](numOut, numIn, weightsCopy, position)\n-    val b = new BDV[Double](weightsCopy, position + (numOut * numIn), 1, numOut)\n-    (a, b)\n-  }\n-\n-  /**\n-   * Roll the layer weights into a vector\n-   * @param a matrix A\n-   * @param b vector b\n-   * @return vector of weights\n-   */\n-  def roll(a: BDM[Double], b: BDV[Double]): Vector = {\n-    val result = new Array[Double](a.size + b.length)\n-    // TODO: make sure that we need to copy!\n-    System.arraycopy(a.toArray, 0, result, 0, a.size)\n-    System.arraycopy(b.toArray, 0, result, a.size, b.length)\n-    Vectors.dense(result)\n-  }\n-\n-  /**\n-   * Generate random weights for the layer\n+   * Initialize weights\n    * @param numIn number of inputs\n    * @param numOut number of outputs\n-   * @param seed seed\n-   * @return (matrix A, vector b)\n+   * @param weights vector for weights initialization\n+   * @param random random number generator\n    */\n-  def randomWeights(numIn: Int, numOut: Int, seed: Long = 11L): (BDM[Double], BDV[Double]) = {\n-    val rand: XORShiftRandom = new XORShiftRandom(seed)\n-    val weights = BDM.fill[Double](numOut, numIn){ (rand.nextDouble * 4.8 - 2.4) / numIn }\n-    val bias = BDV.fill[Double](numOut){ (rand.nextDouble * 4.8 - 2.4) / numIn }\n-    (weights, bias)\n+  def randomWeights(\n+    numIn: Int,\n+    numOut: Int,\n+    weights: BDV[Double],\n+    random: Random): Unit = {\n+    var i = 0\n+    while (i < weights.length) {\n+      weights(i) = (random.nextDouble * 4.8 - 2.4) / numIn"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Use `math` instead. `Math` is deprecated.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:35Z",
    "diffHunk": "@@ -294,89 +248,13 @@ private[ann] object ActivationFunction {\n }\n \n /**\n- * Implements SoftMax activation function\n- */\n-private[ann] class SoftmaxFunction extends ActivationFunction {\n-  override def eval(x: BDM[Double], y: BDM[Double]): Unit = {\n-    var j = 0\n-    // find max value to make sure later that exponent is computable\n-    while (j < x.cols) {\n-      var i = 0\n-      var max = Double.MinValue\n-      while (i < x.rows) {\n-        if (x(i, j) > max) {\n-          max = x(i, j)\n-        }\n-        i += 1\n-      }\n-      var sum = 0.0\n-      i = 0\n-      while (i < x.rows) {\n-        val res = Math.exp(x(i, j) - max)\n-        y(i, j) = res\n-        sum += res\n-        i += 1\n-      }\n-      i = 0\n-      while (i < x.rows) {\n-        y(i, j) /= sum\n-        i += 1\n-      }\n-      j += 1\n-    }\n-  }\n-\n-  override def crossEntropy(\n-    output: BDM[Double],\n-    target: BDM[Double],\n-    result: BDM[Double]): Double = {\n-    def m(o: Double, t: Double): Double = o - t\n-    ActivationFunction(output, target, result, m)\n-    -Bsum( target :* Blog(output)) / output.cols\n-  }\n-\n-  override def derivative(x: BDM[Double], y: BDM[Double]): Unit = {\n-    def sd(z: Double): Double = (1 - z) * z\n-    ActivationFunction(x, y, sd)\n-  }\n-\n-  override def squared(output: BDM[Double], target: BDM[Double], result: BDM[Double]): Double = {\n-    throw new UnsupportedOperationException(\"Sorry, squared error is not defined for SoftMax.\")\n-  }\n-}\n-\n-/**\n  * Implements Sigmoid activation function\n  */\n private[ann] class SigmoidFunction extends ActivationFunction {\n-  override def eval(x: BDM[Double], y: BDM[Double]): Unit = {\n-    def s(z: Double): Double = Bsigmoid(z)\n-    ActivationFunction(x, y, s)\n-  }\n \n-  override def crossEntropy(\n-    output: BDM[Double],\n-    target: BDM[Double],\n-    result: BDM[Double]): Double = {\n-    def m(o: Double, t: Double): Double = o - t\n-    ActivationFunction(output, target, result, m)\n-    -Bsum(target :* Blog(output)) / output.cols\n-  }\n+  override def eval: (Double) => Double = x => 1.0 / (1 + Math.exp(-x))"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`BDV.zeros`\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:41Z",
    "diffHunk": "@@ -654,12 +513,19 @@ private[ann] object FeedForwardModel {\n   def apply(topology: FeedForwardTopology, seed: Long = 11L): FeedForwardModel = {\n     val layers = topology.layers\n     val layerModels = new Array[LayerModel](layers.length)\n+    var totalSize = 0\n+    for (i <- 0 until topology.layers.length) {\n+      totalSize += topology.layers(i).weightSize\n+    }\n+    val weights = new BDV[Double](new Array[Double](totalSize))"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "We use `11L` in many places. It would be nice to generate a default seed based on the class name, e.g., `this.getClass.getName.hashCode.toLong`.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:44Z",
    "diffHunk": "@@ -774,8 +640,8 @@ private[ml] class FeedForwardTrainer(\n     val inputSize: Int,\n     val outputSize: Int) extends Serializable {\n \n-  // TODO: what if we need to pass random seed?\n-  private var _weights = topology.getInstance(11L).weights()\n+  private var _seed = 11L"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Not necessary to have this line.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:46Z",
    "diffHunk": "@@ -783,6 +649,22 @@ private[ml] class FeedForwardTrainer(\n   private var optimizer: Optimizer = LBFGSOptimizer.setConvergenceTol(1e-4).setNumIterations(100)\n \n   /**\n+   * Returns seed\n+   * @return seed"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Not necessary to have `@param` and `@return`.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:47Z",
    "diffHunk": "@@ -783,6 +649,22 @@ private[ml] class FeedForwardTrainer(\n   private var optimizer: Optimizer = LBFGSOptimizer.setConvergenceTol(1e-4).setNumIterations(100)\n \n   /**\n+   * Returns seed\n+   * @return seed\n+   */\n+  def getSeed: Long = _seed\n+\n+  /**\n+   * Sets seed\n+   * @param value seed"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`FeedForwardTrainer` => `self.type`\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:44:49Z",
    "diffHunk": "@@ -783,6 +649,22 @@ private[ml] class FeedForwardTrainer(\n   private var optimizer: Optimizer = LBFGSOptimizer.setConvergenceTol(1e-4).setNumIterations(100)\n \n   /**\n+   * Returns seed\n+   * @return seed\n+   */\n+  def getSeed: Long = _seed\n+\n+  /**\n+   * Sets seed\n+   * @param value seed\n+   * @return trainer\n+   */\n+  def setSeed(value: Long): FeedForwardTrainer = {"
  }],
  "prId": 9229
}]