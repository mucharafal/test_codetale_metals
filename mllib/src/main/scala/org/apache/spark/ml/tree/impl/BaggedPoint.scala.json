[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "You can just store subsampleCounts * sampleWeights as a field, and then these two constructor params won't become fields. No need to compute it each time. On the other hand I don't mind this change as it reduces memory usage -- ints are smaller than doubles.\r\n\r\nOn the other other hand, if you often call this method with one sample, it could be more efficient, to even provide a second constructor taking an Int rather than array. Then you do need to store the array of doubles internally.\r\n\r\nI don't feel strongly about it",
    "commit": "7d6654ed0b03afe6af623a909d6ed2f1bc85ac8d",
    "createdAt": "2018-12-21T14:25:34Z",
    "diffHunk": "@@ -33,13 +33,20 @@ import org.apache.spark.util.random.XORShiftRandom\n  * this datum has 1 copy, 0 copies, and 4 copies in the 3 subsamples, respectively.\n  *\n  * @param datum  Data instance\n- * @param subsampleWeights  Weight of this instance in each subsampled dataset.\n- *\n- * TODO: This does not currently support (Double) weighted instances.  Once MLlib has weighted\n- *       dataset support, update.  (We store subsampleWeights as Double for this future extension.)\n+ * @param subsampleCounts  Number of samples of this instance in each subsampled dataset.\n+ * @param sampleWeight The weight of this instance.\n  */\n-private[spark] class BaggedPoint[Datum](val datum: Datum, val subsampleWeights: Array[Double])\n-  extends Serializable\n+private[spark] class BaggedPoint[Datum](\n+    val datum: Datum,\n+    val subsampleCounts: Array[Int],\n+    val sampleWeight: Double) extends Serializable {\n+\n+  /**\n+   * Subsample counts weighted by the sample weight.\n+   */\n+  def weightedCounts: Array[Double] = subsampleCounts.map(_ * sampleWeight)"
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "I was going to say I'm not sure as well - as you said there is a small trade off (also I would mention that for clarity in code I think it is a bit clearer what the field is this way as opposed to doing the computation beforehand).  However, I noticed this field isn't even used anywhere, it's just there for clarity, in here:\r\n\r\nmllib/src/main/scala/org/apache/spark/ml/tree/impl/RandomForest.scala\r\n\r\nwe get a single subsample count and weight and they are passed separately to another method orderedBinSeqOp or mixedBinSeqOp\r\n\r\nI guess it's nice to have this method for clarity or debugging but if it's not used maybe I should just get rid of it altogether?  What are your thoughts on this?",
    "commit": "7d6654ed0b03afe6af623a909d6ed2f1bc85ac8d",
    "createdAt": "2019-01-07T04:55:23Z",
    "diffHunk": "@@ -33,13 +33,20 @@ import org.apache.spark.util.random.XORShiftRandom\n  * this datum has 1 copy, 0 copies, and 4 copies in the 3 subsamples, respectively.\n  *\n  * @param datum  Data instance\n- * @param subsampleWeights  Weight of this instance in each subsampled dataset.\n- *\n- * TODO: This does not currently support (Double) weighted instances.  Once MLlib has weighted\n- *       dataset support, update.  (We store subsampleWeights as Double for this future extension.)\n+ * @param subsampleCounts  Number of samples of this instance in each subsampled dataset.\n+ * @param sampleWeight The weight of this instance.\n  */\n-private[spark] class BaggedPoint[Datum](val datum: Datum, val subsampleWeights: Array[Double])\n-  extends Serializable\n+private[spark] class BaggedPoint[Datum](\n+    val datum: Datum,\n+    val subsampleCounts: Array[Int],\n+    val sampleWeight: Double) extends Serializable {\n+\n+  /**\n+   * Subsample counts weighted by the sample weight.\n+   */\n+  def weightedCounts: Array[Double] = subsampleCounts.map(_ * sampleWeight)"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Yes I wouldn't carry it around if it can be avoided. How about a `def subsampleWeight(i: Int)` method instead, that computes it from subsampleCounts * sampleWeight?",
    "commit": "7d6654ed0b03afe6af623a909d6ed2f1bc85ac8d",
    "createdAt": "2019-01-07T13:37:30Z",
    "diffHunk": "@@ -33,13 +33,20 @@ import org.apache.spark.util.random.XORShiftRandom\n  * this datum has 1 copy, 0 copies, and 4 copies in the 3 subsamples, respectively.\n  *\n  * @param datum  Data instance\n- * @param subsampleWeights  Weight of this instance in each subsampled dataset.\n- *\n- * TODO: This does not currently support (Double) weighted instances.  Once MLlib has weighted\n- *       dataset support, update.  (We store subsampleWeights as Double for this future extension.)\n+ * @param subsampleCounts  Number of samples of this instance in each subsampled dataset.\n+ * @param sampleWeight The weight of this instance.\n  */\n-private[spark] class BaggedPoint[Datum](val datum: Datum, val subsampleWeights: Array[Double])\n-  extends Serializable\n+private[spark] class BaggedPoint[Datum](\n+    val datum: Datum,\n+    val subsampleCounts: Array[Int],\n+    val sampleWeight: Double) extends Serializable {\n+\n+  /**\n+   * Subsample counts weighted by the sample weight.\n+   */\n+  def weightedCounts: Array[Double] = subsampleCounts.map(_ * sampleWeight)"
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "done! removed the method.  they are retrieved and passed down to several layers of methods before they get to the aggregator where that computation subsampleCount * sampleWeight happens, so it doesn't make sense to have any method there (unless it is just for some debugging or clarity in the code)",
    "commit": "7d6654ed0b03afe6af623a909d6ed2f1bc85ac8d",
    "createdAt": "2019-01-09T05:04:56Z",
    "diffHunk": "@@ -33,13 +33,20 @@ import org.apache.spark.util.random.XORShiftRandom\n  * this datum has 1 copy, 0 copies, and 4 copies in the 3 subsamples, respectively.\n  *\n  * @param datum  Data instance\n- * @param subsampleWeights  Weight of this instance in each subsampled dataset.\n- *\n- * TODO: This does not currently support (Double) weighted instances.  Once MLlib has weighted\n- *       dataset support, update.  (We store subsampleWeights as Double for this future extension.)\n+ * @param subsampleCounts  Number of samples of this instance in each subsampled dataset.\n+ * @param sampleWeight The weight of this instance.\n  */\n-private[spark] class BaggedPoint[Datum](val datum: Datum, val subsampleWeights: Array[Double])\n-  extends Serializable\n+private[spark] class BaggedPoint[Datum](\n+    val datum: Datum,\n+    val subsampleCounts: Array[Int],\n+    val sampleWeight: Double) extends Serializable {\n+\n+  /**\n+   * Subsample counts weighted by the sample weight.\n+   */\n+  def weightedCounts: Array[Double] = subsampleCounts.map(_ * sampleWeight)"
  }],
  "prId": 21632
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "The while body could be a little tighter as:\r\n```\r\nif (rng.nextDouble() < subsamplingRate) {\r\n  subsampleCounts(subsampleIndex) = 1\r\n}\r\nsubsampleIndex += 1\r\n```",
    "commit": "7d6654ed0b03afe6af623a909d6ed2f1bc85ac8d",
    "createdAt": "2018-12-21T14:27:20Z",
    "diffHunk": "@@ -82,16 +92,16 @@ private[spark] object BaggedPoint {\n       val rng = new XORShiftRandom\n       rng.setSeed(seed + partitionIndex + 1)\n       instances.map { instance =>\n-        val subsampleWeights = new Array[Double](numSubsamples)\n+        val subsampleCounts = new Array[Int](numSubsamples)\n         var subsampleIndex = 0\n         while (subsampleIndex < numSubsamples) {\n           val x = rng.nextDouble()\n-          subsampleWeights(subsampleIndex) = {\n-            if (x < subsamplingRate) 1.0 else 0.0\n+          subsampleCounts(subsampleIndex) = {"
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "good suggestion, done!",
    "commit": "7d6654ed0b03afe6af623a909d6ed2f1bc85ac8d",
    "createdAt": "2019-01-07T05:00:50Z",
    "diffHunk": "@@ -82,16 +92,16 @@ private[spark] object BaggedPoint {\n       val rng = new XORShiftRandom\n       rng.setSeed(seed + partitionIndex + 1)\n       instances.map { instance =>\n-        val subsampleWeights = new Array[Double](numSubsamples)\n+        val subsampleCounts = new Array[Int](numSubsamples)\n         var subsampleIndex = 0\n         while (subsampleIndex < numSubsamples) {\n           val x = rng.nextDouble()\n-          subsampleWeights(subsampleIndex) = {\n-            if (x < subsamplingRate) 1.0 else 0.0\n+          subsampleCounts(subsampleIndex) = {"
  }],
  "prId": 21632
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Consider adding a constructor that specifies a sampleWeight of 1.0 so you don't have to set it everywhere you call it with 1.0",
    "commit": "7d6654ed0b03afe6af623a909d6ed2f1bc85ac8d",
    "createdAt": "2018-12-21T14:30:59Z",
    "diffHunk": "@@ -33,13 +33,20 @@ import org.apache.spark.util.random.XORShiftRandom\n  * this datum has 1 copy, 0 copies, and 4 copies in the 3 subsamples, respectively.\n  *\n  * @param datum  Data instance\n- * @param subsampleWeights  Weight of this instance in each subsampled dataset.\n- *\n- * TODO: This does not currently support (Double) weighted instances.  Once MLlib has weighted\n- *       dataset support, update.  (We store subsampleWeights as Double for this future extension.)\n+ * @param subsampleCounts  Number of samples of this instance in each subsampled dataset.\n+ * @param sampleWeight The weight of this instance.\n  */\n-private[spark] class BaggedPoint[Datum](val datum: Datum, val subsampleWeights: Array[Double])\n-  extends Serializable\n+private[spark] class BaggedPoint[Datum](\n+    val datum: Datum,\n+    val subsampleCounts: Array[Int],\n+    val sampleWeight: Double) extends Serializable {"
  }, {
    "author": {
      "login": "imatiach-msft"
    },
    "body": "done!",
    "commit": "7d6654ed0b03afe6af623a909d6ed2f1bc85ac8d",
    "createdAt": "2019-01-11T03:52:22Z",
    "diffHunk": "@@ -33,13 +33,20 @@ import org.apache.spark.util.random.XORShiftRandom\n  * this datum has 1 copy, 0 copies, and 4 copies in the 3 subsamples, respectively.\n  *\n  * @param datum  Data instance\n- * @param subsampleWeights  Weight of this instance in each subsampled dataset.\n- *\n- * TODO: This does not currently support (Double) weighted instances.  Once MLlib has weighted\n- *       dataset support, update.  (We store subsampleWeights as Double for this future extension.)\n+ * @param subsampleCounts  Number of samples of this instance in each subsampled dataset.\n+ * @param sampleWeight The weight of this instance.\n  */\n-private[spark] class BaggedPoint[Datum](val datum: Datum, val subsampleWeights: Array[Double])\n-  extends Serializable\n+private[spark] class BaggedPoint[Datum](\n+    val datum: Datum,\n+    val subsampleCounts: Array[Int],\n+    val sampleWeight: Double) extends Serializable {"
  }],
  "prId": 21632
}]