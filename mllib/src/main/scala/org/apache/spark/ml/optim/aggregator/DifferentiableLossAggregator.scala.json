[{
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "change this to use `getClass.getName`",
    "commit": "29052d3dbc97ad548128c13533de47d5488b4196",
    "createdAt": "2017-02-28T16:08:30Z",
    "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+\n+/**\n+ * A parent trait for aggregators used in fitting MLlib models. This parent trait implements\n+ * some of the common code shared between concrete instances of aggregators. Subclasses of this\n+ * aggregator need only implement the `add` method.\n+ *\n+ * @tparam Datum The type of the instances added to the aggregator to update the loss and gradient.\n+ * @tparam Agg Specialization of [[DifferentiableLossAggregator]]. Classes that subclass this\n+ *             type need to use this parameter to specify the concrete type of the aggregator.\n+ */\n+private[ml] trait DifferentiableLossAggregator[\n+    Datum,\n+    Agg <: DifferentiableLossAggregator[Datum, Agg]] extends Serializable {\n+\n+  self: Agg =>\n+\n+  protected var weightSum: Double = 0.0\n+  protected var lossSum: Double = 0.0\n+\n+  /** The dimension of the gradient array. */\n+  protected val dim: Int\n+\n+  /** Array of gradient values that are mutated when new instances are added to the aggregator. */\n+  protected lazy val gradientSumArray: Array[Double] = Array.ofDim[Double](dim)\n+\n+  /** Add a single data point to this aggregator. */\n+  def add(instance: Datum): Agg\n+\n+  /** Merge two aggregators. The `this` object will be modified in place and returned. */\n+  def merge(other: Agg): Agg = {\n+    require(dim == other.dim, s\"Dimensions mismatch when merging with another \" +\n+      s\"LeastSquaresAggregator. Expecting $dim but got ${other.dim}.\")"
  }],
  "prId": 17094
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "You could add a brief comment explaining what this self type does",
    "commit": "29052d3dbc97ad548128c13533de47d5488b4196",
    "createdAt": "2017-05-25T12:35:40Z",
    "diffHunk": "@@ -0,0 +1,88 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.aggregator\n+\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+\n+/**\n+ * A parent trait for aggregators used in fitting MLlib models. This parent trait implements\n+ * some of the common code shared between concrete instances of aggregators. Subclasses of this\n+ * aggregator need only implement the `add` method.\n+ *\n+ * @tparam Datum The type of the instances added to the aggregator to update the loss and gradient.\n+ * @tparam Agg Specialization of [[DifferentiableLossAggregator]]. Classes that subclass this\n+ *             type need to use this parameter to specify the concrete type of the aggregator.\n+ */\n+private[ml] trait DifferentiableLossAggregator[\n+    Datum,\n+    Agg <: DifferentiableLossAggregator[Datum, Agg]] extends Serializable {\n+\n+  self: Agg =>"
  }],
  "prId": 17094
}]