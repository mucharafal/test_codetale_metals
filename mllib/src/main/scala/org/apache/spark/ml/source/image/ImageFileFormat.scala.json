[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Shall we make another file for this for consistency?",
    "commit": "218ce4cf796308c8705a27889b25100e2b779365",
    "createdAt": "2018-09-04T10:10:21Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.source.image\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+import org.apache.hadoop.mapreduce.Job\n+\n+import org.apache.spark.ml.image.ImageSchema\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, UnsafeRow}\n+import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.execution.datasources.{DataSource, FileFormat, OutputWriterFactory, PartitionedFile}\n+import org.apache.spark.sql.sources.{DataSourceRegister, Filter}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+\n+private[image] class ImageFileFormatOptions("
  }],
  "prId": 22328
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "tiny nit: `s` can be removed",
    "commit": "218ce4cf796308c8705a27889b25100e2b779365",
    "createdAt": "2018-09-04T10:15:38Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.source.image\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+import org.apache.hadoop.mapreduce.Job\n+\n+import org.apache.spark.ml.image.ImageSchema\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, UnsafeRow}\n+import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.execution.datasources.{DataSource, FileFormat, OutputWriterFactory, PartitionedFile}\n+import org.apache.spark.sql.sources.{DataSourceRegister, Filter}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+\n+private[image] class ImageFileFormatOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String]) extends Serializable {\n+\n+  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+\n+  val dropImageFailures = parameters.getOrElse(\"dropImageFailures\", \"false\").toBoolean\n+}\n+\n+private[image] class ImageFileFormat extends FileFormat with DataSourceRegister {\n+\n+  override def inferSchema(\n+      sparkSession: SparkSession,\n+      options: Map[String, String],\n+      files: Seq[FileStatus]): Option[StructType] = Some(ImageSchema.imageSchema)\n+\n+  override def prepareWrite(\n+      sparkSession: SparkSession,\n+      job: Job, options: Map[String, String],\n+      dataSchema: StructType): OutputWriterFactory = {\n+    throw new UnsupportedOperationException(\n+      s\"prepareWrite is not supported for image data source\")"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "The error message is user-facing and users do not know `prepareWrite`. So just say \"Write is not supported\"",
    "commit": "218ce4cf796308c8705a27889b25100e2b779365",
    "createdAt": "2018-09-04T15:47:27Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.source.image\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+import org.apache.hadoop.mapreduce.Job\n+\n+import org.apache.spark.ml.image.ImageSchema\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, UnsafeRow}\n+import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.execution.datasources.{DataSource, FileFormat, OutputWriterFactory, PartitionedFile}\n+import org.apache.spark.sql.sources.{DataSourceRegister, Filter}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+\n+private[image] class ImageFileFormatOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String]) extends Serializable {\n+\n+  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+\n+  val dropImageFailures = parameters.getOrElse(\"dropImageFailures\", \"false\").toBoolean\n+}\n+\n+private[image] class ImageFileFormat extends FileFormat with DataSourceRegister {\n+\n+  override def inferSchema(\n+      sparkSession: SparkSession,\n+      options: Map[String, String],\n+      files: Seq[FileStatus]): Option[StructType] = Some(ImageSchema.imageSchema)\n+\n+  override def prepareWrite(\n+      sparkSession: SparkSession,\n+      job: Job, options: Map[String, String],\n+      dataSchema: StructType): OutputWriterFactory = {\n+    throw new UnsupportedOperationException(\n+      s\"prepareWrite is not supported for image data source\")"
  }],
  "prId": 22328
}, {
  "comments": [{
    "author": {
      "login": "imatiach-msft"
    },
    "body": "hmm, is there any way we could combine the two apis?  I don't like having to support two different implementations.  Or, what is the issue that is blocking us from combining them?",
    "commit": "218ce4cf796308c8705a27889b25100e2b779365",
    "createdAt": "2018-09-04T15:59:38Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.source.image\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+import org.apache.hadoop.mapreduce.Job\n+\n+import org.apache.spark.ml.image.ImageSchema\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, UnsafeRow}\n+import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.execution.datasources.{DataSource, FileFormat, OutputWriterFactory, PartitionedFile}\n+import org.apache.spark.sql.sources.{DataSourceRegister, Filter}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+\n+private[image] class ImageFileFormatOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String]) extends Serializable {\n+\n+  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+\n+  val dropImageFailures = parameters.getOrElse(\"dropImageFailures\", \"false\").toBoolean\n+}\n+\n+private[image] class ImageFileFormat extends FileFormat with DataSourceRegister {\n+\n+  override def inferSchema(\n+      sparkSession: SparkSession,\n+      options: Map[String, String],\n+      files: Seq[FileStatus]): Option[StructType] = Some(ImageSchema.imageSchema)\n+\n+  override def prepareWrite(\n+      sparkSession: SparkSession,\n+      job: Job, options: Map[String, String],\n+      dataSchema: StructType): OutputWriterFactory = {\n+    throw new UnsupportedOperationException(\n+      s\"prepareWrite is not supported for image data source\")\n+  }\n+\n+  override def shortName(): String = \"image\"\n+\n+  override protected def buildReader(",
    "line": 53
  }],
  "prId": 22328
}, {
  "comments": [{
    "author": {
      "login": "imatiach-msft"
    },
    "body": "should the sampling option be ported as well?  It seemed like an important option in case users didn't want to load all images.",
    "commit": "218ce4cf796308c8705a27889b25100e2b779365",
    "createdAt": "2018-09-04T16:05:29Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.source.image\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+import org.apache.hadoop.mapreduce.Job\n+\n+import org.apache.spark.ml.image.ImageSchema\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, UnsafeRow}\n+import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.execution.datasources.{DataSource, FileFormat, OutputWriterFactory, PartitionedFile}\n+import org.apache.spark.sql.sources.{DataSourceRegister, Filter}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+\n+private[image] class ImageFileFormatOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String]) extends Serializable {\n+\n+  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+\n+  val dropImageFailures = parameters.getOrElse(\"dropImageFailures\", \"false\").toBoolean\n+}\n+\n+private[image] class ImageFileFormat extends FileFormat with DataSourceRegister {\n+\n+  override def inferSchema(\n+      sparkSession: SparkSession,\n+      options: Map[String, String],\n+      files: Seq[FileStatus]): Option[StructType] = Some(ImageSchema.imageSchema)\n+\n+  override def prepareWrite(\n+      sparkSession: SparkSession,\n+      job: Job, options: Map[String, String],\n+      dataSchema: StructType): OutputWriterFactory = {\n+    throw new UnsupportedOperationException(\n+      s\"prepareWrite is not supported for image data source\")\n+  }\n+\n+  override def shortName(): String = \"image\"\n+\n+  override protected def buildReader(\n+      sparkSession: SparkSession,\n+      dataSchema: StructType,\n+      partitionSchema: StructType,\n+      requiredSchema: StructType,\n+      filters: Seq[Filter],\n+      options: Map[String, String],\n+      hadoopConf: Configuration): (PartitionedFile) => Iterator[InternalRow] = {",
    "line": 60
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "It won't be addressed in this PR. The best way to support it is to allow data source handle sampling operation. cc @cloud-fan ",
    "commit": "218ce4cf796308c8705a27889b25100e2b779365",
    "createdAt": "2018-09-04T16:27:03Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.source.image\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+import org.apache.hadoop.mapreduce.Job\n+\n+import org.apache.spark.ml.image.ImageSchema\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, UnsafeRow}\n+import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.execution.datasources.{DataSource, FileFormat, OutputWriterFactory, PartitionedFile}\n+import org.apache.spark.sql.sources.{DataSourceRegister, Filter}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+\n+private[image] class ImageFileFormatOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String]) extends Serializable {\n+\n+  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+\n+  val dropImageFailures = parameters.getOrElse(\"dropImageFailures\", \"false\").toBoolean\n+}\n+\n+private[image] class ImageFileFormat extends FileFormat with DataSourceRegister {\n+\n+  override def inferSchema(\n+      sparkSession: SparkSession,\n+      options: Map[String, String],\n+      files: Seq[FileStatus]): Option[StructType] = Some(ImageSchema.imageSchema)\n+\n+  override def prepareWrite(\n+      sparkSession: SparkSession,\n+      job: Job, options: Map[String, String],\n+      dataSchema: StructType): OutputWriterFactory = {\n+    throw new UnsupportedOperationException(\n+      s\"prepareWrite is not supported for image data source\")\n+  }\n+\n+  override def shortName(): String = \"image\"\n+\n+  override protected def buildReader(\n+      sparkSession: SparkSession,\n+      dataSchema: StructType,\n+      partitionSchema: StructType,\n+      requiredSchema: StructType,\n+      filters: Seq[Filter],\n+      options: Map[String, String],\n+      hadoopConf: Configuration): (PartitionedFile) => Iterator[InternalRow] = {",
    "line": 60
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "sample pushdown should be supported by data source v2 in the next release, then we can migrate the image source to data source v2 at that time.",
    "commit": "218ce4cf796308c8705a27889b25100e2b779365",
    "createdAt": "2018-09-05T05:24:42Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.source.image\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+import org.apache.hadoop.mapreduce.Job\n+\n+import org.apache.spark.ml.image.ImageSchema\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, UnsafeRow}\n+import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.execution.datasources.{DataSource, FileFormat, OutputWriterFactory, PartitionedFile}\n+import org.apache.spark.sql.sources.{DataSourceRegister, Filter}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+\n+private[image] class ImageFileFormatOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String]) extends Serializable {\n+\n+  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+\n+  val dropImageFailures = parameters.getOrElse(\"dropImageFailures\", \"false\").toBoolean\n+}\n+\n+private[image] class ImageFileFormat extends FileFormat with DataSourceRegister {\n+\n+  override def inferSchema(\n+      sparkSession: SparkSession,\n+      options: Map[String, String],\n+      files: Seq[FileStatus]): Option[StructType] = Some(ImageSchema.imageSchema)\n+\n+  override def prepareWrite(\n+      sparkSession: SparkSession,\n+      job: Job, options: Map[String, String],\n+      dataSchema: StructType): OutputWriterFactory = {\n+    throw new UnsupportedOperationException(\n+      s\"prepareWrite is not supported for image data source\")\n+  }\n+\n+  override def shortName(): String = \"image\"\n+\n+  override protected def buildReader(\n+      sparkSession: SparkSession,\n+      dataSchema: StructType,\n+      partitionSchema: StructType,\n+      requiredSchema: StructType,\n+      filters: Seq[Filter],\n+      options: Map[String, String],\n+      hadoopConf: Configuration): (PartitionedFile) => Iterator[InternalRow] = {",
    "line": 60
  }],
  "prId": 22328
}, {
  "comments": [{
    "author": {
      "login": "jaceklaskowski"
    },
    "body": "New line after `job: Job`",
    "commit": "218ce4cf796308c8705a27889b25100e2b779365",
    "createdAt": "2018-09-04T19:22:46Z",
    "diffHunk": "@@ -0,0 +1,109 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.source.image\n+\n+import com.google.common.io.{ByteStreams, Closeables}\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.{FileStatus, Path}\n+import org.apache.hadoop.mapreduce.Job\n+\n+import org.apache.spark.ml.image.ImageSchema\n+import org.apache.spark.sql.SparkSession\n+import org.apache.spark.sql.catalyst.InternalRow\n+import org.apache.spark.sql.catalyst.encoders.RowEncoder\n+import org.apache.spark.sql.catalyst.expressions.{AttributeReference, UnsafeRow}\n+import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap\n+import org.apache.spark.sql.execution.datasources.{DataSource, FileFormat, OutputWriterFactory, PartitionedFile}\n+import org.apache.spark.sql.sources.{DataSourceRegister, Filter}\n+import org.apache.spark.sql.types.StructType\n+import org.apache.spark.util.SerializableConfiguration\n+\n+\n+private[image] class ImageFileFormatOptions(\n+    @transient private val parameters: CaseInsensitiveMap[String]) extends Serializable {\n+\n+  def this(parameters: Map[String, String]) = this(CaseInsensitiveMap(parameters))\n+\n+  val dropImageFailures = parameters.getOrElse(\"dropImageFailures\", \"false\").toBoolean\n+}\n+\n+private[image] class ImageFileFormat extends FileFormat with DataSourceRegister {\n+\n+  override def inferSchema(\n+      sparkSession: SparkSession,\n+      options: Map[String, String],\n+      files: Seq[FileStatus]): Option[StructType] = Some(ImageSchema.imageSchema)\n+\n+  override def prepareWrite(\n+      sparkSession: SparkSession,\n+      job: Job, options: Map[String, String],"
  }],
  "prId": 22328
}]