[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: cru -> cur? or current?",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-04-07T12:07:43Z",
    "diffHunk": "@@ -1009,10 +1009,24 @@ private[spark] object RandomForest extends Logging {\n       // sort distinct values\n       val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n \n+      def weightedMean(pre: (Double, Int), cru: (Double, Int)): Double = {"
  }, {
    "author": {
      "login": "facaiy"
    },
    "body": "fixed.",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-04-07T13:09:31Z",
    "diffHunk": "@@ -1009,10 +1009,24 @@ private[spark] object RandomForest extends Logging {\n       // sort distinct values\n       val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n \n+      def weightedMean(pre: (Double, Int), cru: (Double, Int)): Double = {"
  }],
  "prId": 17556
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Was this needed?",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-04-07T12:07:56Z",
    "diffHunk": "@@ -996,7 +996,7 @@ private[spark] object RandomForest extends Logging {\n     require(metadata.isContinuous(featureIndex),\n       \"findSplitsForContinuousFeature can only be used to find splits for a continuous feature.\")\n \n-    val splits = if (featureSamples.isEmpty) {\n+    val splits: Array[Double] = if (featureSamples.isEmpty) {",
    "line": 5
  }, {
    "author": {
      "login": "facaiy"
    },
    "body": "The code block is too long and has 4 exits. Emphasizing its type perhaps is better to be understand, though `splits` is implied by return type.",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-04-07T13:15:40Z",
    "diffHunk": "@@ -996,7 +996,7 @@ private[spark] object RandomForest extends Logging {\n     require(metadata.isContinuous(featureIndex),\n       \"findSplitsForContinuousFeature can only be used to find splits for a continuous feature.\")\n \n-    val splits = if (featureSamples.isEmpty) {\n+    val splits: Array[Double] = if (featureSamples.isEmpty) {",
    "line": 5
  }],
  "prId": 17556
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "I'm probably over-thinking this, but do we have a possible overflow issue in the denominator? like if both are near `Int.MaxValue`. One could be converted `.toDouble` just to make sure",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-04-07T12:10:04Z",
    "diffHunk": "@@ -1009,10 +1009,24 @@ private[spark] object RandomForest extends Logging {\n       // sort distinct values\n       val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n \n+      def weightedMean(pre: (Double, Int), cru: (Double, Int)): Double = {\n+        val (preValue, preCount) = pre\n+        val (curValue, curCount) = cru\n+        (preValue * preCount + curValue * curCount) / (preCount + curCount)"
  }, {
    "author": {
      "login": "facaiy"
    },
    "body": "I agree with you. fixed.",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-04-07T13:10:04Z",
    "diffHunk": "@@ -1009,10 +1009,24 @@ private[spark] object RandomForest extends Logging {\n       // sort distinct values\n       val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n \n+      def weightedMean(pre: (Double, Int), cru: (Double, Int)): Double = {\n+        val (preValue, preCount) = pre\n+        val (curValue, curCount) = cru\n+        (preValue * preCount + curValue * curCount) / (preCount + curCount)"
  }],
  "prId": 17556
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: use () instead of {}\r\nThere are more efficient ways of writing this but not as compact. I think it's OK unless someone suggests this is performance critical here",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-04-07T12:10:58Z",
    "diffHunk": "@@ -1009,10 +1009,24 @@ private[spark] object RandomForest extends Logging {\n       // sort distinct values\n       val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n \n+      def weightedMean(pre: (Double, Int), cru: (Double, Int)): Double = {\n+        val (preValue, preCount) = pre\n+        val (curValue, curCount) = cru\n+        (preValue * preCount + curValue * curCount) / (preCount + curCount)\n+      }\n+\n       // if possible splits is not enough or just enough, just return all possible splits\n       val possibleSplits = valueCounts.length - 1\n-      if (possibleSplits <= numSplits) {\n-        valueCounts.map(_._1).init\n+      if (possibleSplits == 0) {\n+        // constant feature\n+        Array.empty[Double]\n+\n+      } else if (possibleSplits <= numSplits) {\n+        valueCounts\n+          .sliding(2)\n+          .map{x => weightedMean(x(0), x(1))}"
  }, {
    "author": {
      "login": "facaiy"
    },
    "body": "fixed. \r\nDo you mean use `scanLeft`? It's a little complicate and obscure.",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-04-07T13:11:21Z",
    "diffHunk": "@@ -1009,10 +1009,24 @@ private[spark] object RandomForest extends Logging {\n       // sort distinct values\n       val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n \n+      def weightedMean(pre: (Double, Int), cru: (Double, Int)): Double = {\n+        val (preValue, preCount) = pre\n+        val (curValue, curCount) = cru\n+        (preValue * preCount + curValue * curCount) / (preCount + curCount)\n+      }\n+\n       // if possible splits is not enough or just enough, just return all possible splits\n       val possibleSplits = valueCounts.length - 1\n-      if (possibleSplits <= numSplits) {\n-        valueCounts.map(_._1).init\n+      if (possibleSplits == 0) {\n+        // constant feature\n+        Array.empty[Double]\n+\n+      } else if (possibleSplits <= numSplits) {\n+        valueCounts\n+          .sliding(2)\n+          .map{x => weightedMean(x(0), x(1))}"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "No not scanLeft, just manually building the result array and iterating because it's already known ahead of time how big it is.",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-04-07T13:41:22Z",
    "diffHunk": "@@ -1009,10 +1009,24 @@ private[spark] object RandomForest extends Logging {\n       // sort distinct values\n       val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n \n+      def weightedMean(pre: (Double, Int), cru: (Double, Int)): Double = {\n+        val (preValue, preCount) = pre\n+        val (curValue, curCount) = cru\n+        (preValue * preCount + curValue * curCount) / (preCount + curCount)\n+      }\n+\n       // if possible splits is not enough or just enough, just return all possible splits\n       val possibleSplits = valueCounts.length - 1\n-      if (possibleSplits <= numSplits) {\n-        valueCounts.map(_._1).init\n+      if (possibleSplits == 0) {\n+        // constant feature\n+        Array.empty[Double]\n+\n+      } else if (possibleSplits <= numSplits) {\n+        valueCounts\n+          .sliding(2)\n+          .map{x => weightedMean(x(0), x(1))}"
  }],
  "prId": 17556
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "remove this line",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-04-28T05:37:44Z",
    "diffHunk": "@@ -1009,10 +1009,24 @@ private[spark] object RandomForest extends Logging {\n       // sort distinct values\n       val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n \n-      // if possible splits is not enough or just enough, just return all possible splits\n+      def weightedMean(pre: (Double, Int), cur: (Double, Int)): Double = {\n+        val (preValue, preCount) = pre\n+        val (curValue, curCount) = cur\n+        (preValue * preCount + curValue * curCount) / (preCount.toDouble + curCount)\n+      }\n+\n       val possibleSplits = valueCounts.length - 1\n-      if (possibleSplits <= numSplits) {\n-        valueCounts.map(_._1).init\n+      if (possibleSplits == 0) {\n+        // constant feature\n+        Array.empty[Double]\n+"
  }, {
    "author": {
      "login": "facaiy"
    },
    "body": "removed.",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-04-29T02:05:30Z",
    "diffHunk": "@@ -1009,10 +1009,24 @@ private[spark] object RandomForest extends Logging {\n       // sort distinct values\n       val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n \n-      // if possible splits is not enough or just enough, just return all possible splits\n+      def weightedMean(pre: (Double, Int), cur: (Double, Int)): Double = {\n+        val (preValue, preCount) = pre\n+        val (curValue, curCount) = cur\n+        (preValue * preCount + curValue * curCount) / (preCount.toDouble + curCount)\n+      }\n+\n       val possibleSplits = valueCounts.length - 1\n-      if (possibleSplits <= numSplits) {\n-        valueCounts.map(_._1).init\n+      if (possibleSplits == 0) {\n+        // constant feature\n+        Array.empty[Double]\n+"
  }],
  "prId": 17556
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "remove this line",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-04-28T05:38:03Z",
    "diffHunk": "@@ -1037,7 +1051,10 @@ private[spark] object RandomForest extends Logging {\n           // makes the gap between currentCount and targetCount smaller,\n           // previous value is a split threshold.\n           if (previousGap < currentGap) {\n-            splitsBuilder += valueCounts(index - 1)._1\n+            val pre = valueCounts(index - 1)\n+            val cur = valueCounts(index)\n+"
  }, {
    "author": {
      "login": "facaiy"
    },
    "body": "removed",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-04-29T02:05:21Z",
    "diffHunk": "@@ -1037,7 +1051,10 @@ private[spark] object RandomForest extends Logging {\n           // makes the gap between currentCount and targetCount smaller,\n           // previous value is a split threshold.\n           if (previousGap < currentGap) {\n-            splitsBuilder += valueCounts(index - 1)._1\n+            val pre = valueCounts(index - 1)\n+            val cur = valueCounts(index)\n+"
  }],
  "prId": 17556
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Is it worth factoring a method for this? you could just write `(preValue, _) =` here, but, just dereferncing `._1` isn't so bad, and then, wondering if it saves much to make a method.",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-05-01T09:42:46Z",
    "diffHunk": "@@ -1009,10 +1009,24 @@ private[spark] object RandomForest extends Logging {\n       // sort distinct values\n       val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n \n-      // if possible splits is not enough or just enough, just return all possible splits\n+      // perhaps weighted mean is better in the future, see SPARK-16957 and Github PR 17556.\n+      def mean(pre: (Double, Int), cur: (Double, Int)): Double = {\n+        val (preValue, preCount) = pre"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Yeah, we should get rid of this method.",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-05-01T14:51:23Z",
    "diffHunk": "@@ -1009,10 +1009,24 @@ private[spark] object RandomForest extends Logging {\n       // sort distinct values\n       val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n \n-      // if possible splits is not enough or just enough, just return all possible splits\n+      // perhaps weighted mean is better in the future, see SPARK-16957 and Github PR 17556.\n+      def mean(pre: (Double, Int), cur: (Double, Int)): Double = {\n+        val (preValue, preCount) = pre"
  }, {
    "author": {
      "login": "facaiy"
    },
    "body": "removed.",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-05-02T02:16:50Z",
    "diffHunk": "@@ -1009,10 +1009,24 @@ private[spark] object RandomForest extends Logging {\n       // sort distinct values\n       val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n \n-      // if possible splits is not enough or just enough, just return all possible splits\n+      // perhaps weighted mean is better in the future, see SPARK-16957 and Github PR 17556.\n+      def mean(pre: (Double, Int), cur: (Double, Int)): Double = {\n+        val (preValue, preCount) = pre"
  }],
  "prId": 17556
}, {
  "comments": [{
    "author": {
      "login": "facaiy"
    },
    "body": "@srowen  Is it more efficient than sliding?",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-05-02T02:32:15Z",
    "diffHunk": "@@ -1009,10 +1009,17 @@ private[spark] object RandomForest extends Logging {\n       // sort distinct values\n       val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n \n-      // if possible splits is not enough or just enough, just return all possible splits\n       val possibleSplits = valueCounts.length - 1\n-      if (possibleSplits <= numSplits) {\n-        valueCounts.map(_._1).init\n+      if (possibleSplits == 0) {\n+        // constant feature\n+        Array.empty[Double]\n+      } else if (possibleSplits <= numSplits) {\n+        // if possible splits is not enough or just enough, just return all possible splits\n+        val splits = for {\n+          i <- 0 until possibleSplits\n+        } yield (valueCounts(i)._1 + valueCounts(i + 1)._1) / 2"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Good idea. Maybe even just `(0 until possibleSplits).map(...).toArray` which is probably about the same thing anyway. You might write `/ 2.0` to be clear it's floating point division",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-05-02T09:22:52Z",
    "diffHunk": "@@ -1009,10 +1009,17 @@ private[spark] object RandomForest extends Logging {\n       // sort distinct values\n       val valueCounts = valueCountMap.toSeq.sortBy(_._1).toArray\n \n-      // if possible splits is not enough or just enough, just return all possible splits\n       val possibleSplits = valueCounts.length - 1\n-      if (possibleSplits <= numSplits) {\n-        valueCounts.map(_._1).init\n+      if (possibleSplits == 0) {\n+        // constant feature\n+        Array.empty[Double]\n+      } else if (possibleSplits <= numSplits) {\n+        // if possible splits is not enough or just enough, just return all possible splits\n+        val splits = for {\n+          i <- 0 until possibleSplits\n+        } yield (valueCounts(i)._1 + valueCounts(i + 1)._1) / 2"
  }],
  "prId": 17556
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Meh, could likewise be one line like above. No big deal.",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-05-02T09:34:12Z",
    "diffHunk": "@@ -1037,7 +1044,10 @@ private[spark] object RandomForest extends Logging {\n           // makes the gap between currentCount and targetCount smaller,\n           // previous value is a split threshold.\n           if (previousGap < currentGap) {\n-            splitsBuilder += valueCounts(index - 1)._1\n+            val pre = valueCounts(index - 1)\n+            val cur = valueCounts(index)\n+            // perhaps weighted mean will be used later, see SPARK-16957 and Github PR 17556.\n+            splitsBuilder += (pre._1 + cur._1) / 2"
  }, {
    "author": {
      "login": "facaiy"
    },
    "body": "Nice! revised.",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-05-02T10:23:44Z",
    "diffHunk": "@@ -1037,7 +1044,10 @@ private[spark] object RandomForest extends Logging {\n           // makes the gap between currentCount and targetCount smaller,\n           // previous value is a split threshold.\n           if (previousGap < currentGap) {\n-            splitsBuilder += valueCounts(index - 1)._1\n+            val pre = valueCounts(index - 1)\n+            val cur = valueCounts(index)\n+            // perhaps weighted mean will be used later, see SPARK-16957 and Github PR 17556.\n+            splitsBuilder += (pre._1 + cur._1) / 2"
  }],
  "prId": 17556
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "Comments like these tend to just get left around and sit there forever. Unless we file a _new_ JIRA that intends to decide on future behavior, I would like to remove this comment altogether. I'd prefer to just remove it and not create a follow up.",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-05-03T00:51:50Z",
    "diffHunk": "@@ -1037,7 +1042,8 @@ private[spark] object RandomForest extends Logging {\n           // makes the gap between currentCount and targetCount smaller,\n           // previous value is a split threshold.\n           if (previousGap < currentGap) {\n-            splitsBuilder += valueCounts(index - 1)._1\n+            // perhaps weighted mean will be used later, see SPARK-16957 and Github PR 17556."
  }, {
    "author": {
      "login": "facaiy"
    },
    "body": "removed. Thanks for your help! @sethah @srowen ",
    "commit": "591d7900fa25a2c0abdbc0de7dadc105e35dd52d",
    "createdAt": "2017-05-03T08:29:25Z",
    "diffHunk": "@@ -1037,7 +1042,8 @@ private[spark] object RandomForest extends Logging {\n           // makes the gap between currentCount and targetCount smaller,\n           // previous value is a split threshold.\n           if (previousGap < currentGap) {\n-            splitsBuilder += valueCounts(index - 1)._1\n+            // perhaps weighted mean will be used later, see SPARK-16957 and Github PR 17556."
  }],
  "prId": 17556
}]