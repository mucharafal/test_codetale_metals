[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "don't need semicolons\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-11T23:18:23Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import java.util.UUID\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml._\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.MetadataUtils\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRest]].\n+ */\n+private[ml] trait OneVsRestParams extends PredictorParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "\"base classifier\" --> \"base binary classifier\"\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-11T23:18:26Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import java.util.UUID\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml._\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.MetadataUtils\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRest]].\n+ */\n+private[ml] trait OneVsRestParams extends PredictorParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into."
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Remove or add doc\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-11T23:18:28Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import java.util.UUID\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml._\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.MetadataUtils\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRest]].\n+ */\n+private[ml] trait OneVsRestParams extends PredictorParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val classifier: Param[ClassifierType]  =\n+    new Param(this, \"classifier\", \"base binary classifier \")\n+\n+  /** @group getParam */\n+  def getClassifier: ClassifierType = $(classifier)\n+\n+}\n+\n+/**\n+ * Model produced by [[OneVsRest]].\n+ *\n+ * @param parent"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Can we promise here that model(i) corresponds to testing class i vs. the rest?\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-11T23:18:35Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import java.util.UUID\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml._\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.MetadataUtils\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRest]].\n+ */\n+private[ml] trait OneVsRestParams extends PredictorParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val classifier: Param[ClassifierType]  =\n+    new Param(this, \"classifier\", \"base binary classifier \")\n+\n+  /** @group getParam */\n+  def getClassifier: ClassifierType = $(classifier)\n+\n+}\n+\n+/**\n+ * Model produced by [[OneVsRest]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction."
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "make final\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-11T23:18:40Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import java.util.UUID\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml._\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.MetadataUtils\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRest]].\n+ */\n+private[ml] trait OneVsRestParams extends PredictorParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val classifier: Param[ClassifierType]  =\n+    new Param(this, \"classifier\", \"base binary classifier \")\n+\n+  /** @group getParam */\n+  def getClassifier: ClassifierType = $(classifier)\n+\n+}\n+\n+/**\n+ * Model produced by [[OneVsRest]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsRestModel("
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Note for the future: This API may need to change when we introduce a ClassificationModel trait as the public API replacement for the current ClassificationModel abstract class.\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-11T23:18:52Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import java.util.UUID\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml._\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.MetadataUtils\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRest]].\n+ */\n+private[ml] trait OneVsRestParams extends PredictorParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val classifier: Param[ClassifierType]  =\n+    new Param(this, \"classifier\", \"base binary classifier \")\n+\n+  /** @group getParam */\n+  def getClassifier: ClassifierType = $(classifier)\n+\n+}\n+\n+/**\n+ * Model produced by [[OneVsRest]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsRestModel(\n+      override val parent: OneVsRest,\n+      val models: Array[_ <: ClassificationModel[_,_]])"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "scala style: space after comma\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-11T23:18:57Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import java.util.UUID\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml._\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.MetadataUtils\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRest]].\n+ */\n+private[ml] trait OneVsRestParams extends PredictorParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val classifier: Param[ClassifierType]  =\n+    new Param(this, \"classifier\", \"base binary classifier \")\n+\n+  /** @group getParam */\n+  def getClassifier: ClassifierType = $(classifier)\n+\n+}\n+\n+/**\n+ * Model produced by [[OneVsRest]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsRestModel(\n+      override val parent: OneVsRest,\n+      val models: Array[_ <: ClassificationModel[_,_]])\n+  extends Model[OneVsRestModel] with OneVsRestParams {\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, getClassifier.featuresDataType)\n+  }\n+\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    transformSchema(dataset.schema, logging = true)\n+\n+    // determine the input columns: these need to be passed through\n+    val origCols = dataset.schema.map(f => col(f.name))\n+\n+    // add an accumulator column to store predictions of all the models\n+    val accColName = \"mbc$acc\" + UUID.randomUUID().toString\n+    val init: () => Map[Int, Double] = () => {Map()}\n+    val mapType = MapType(IntegerType, DoubleType, false)\n+    val newDataset = dataset.withColumn(accColName,callUDF(init, mapType))"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "It would be nice if this output predictionCol metadata, created from the labelCol metadata if available, or set to NominalAttribute with the correct numValues otherwise.\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-11T23:19:03Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import java.util.UUID\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml._\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.MetadataUtils\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRest]].\n+ */\n+private[ml] trait OneVsRestParams extends PredictorParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val classifier: Param[ClassifierType]  =\n+    new Param(this, \"classifier\", \"base binary classifier \")\n+\n+  /** @group getParam */\n+  def getClassifier: ClassifierType = $(classifier)\n+\n+}\n+\n+/**\n+ * Model produced by [[OneVsRest]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsRestModel(\n+      override val parent: OneVsRest,\n+      val models: Array[_ <: ClassificationModel[_,_]])\n+  extends Model[OneVsRestModel] with OneVsRestParams {\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, getClassifier.featuresDataType)\n+  }\n+\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    transformSchema(dataset.schema, logging = true)\n+\n+    // determine the input columns: these need to be passed through\n+    val origCols = dataset.schema.map(f => col(f.name))\n+\n+    // add an accumulator column to store predictions of all the models\n+    val accColName = \"mbc$acc\" + UUID.randomUUID().toString\n+    val init: () => Map[Int, Double] = () => {Map()}\n+    val mapType = MapType(IntegerType, DoubleType, false)\n+    val newDataset = dataset.withColumn(accColName,callUDF(init, mapType))\n+\n+    // persist if underlying dataset is not persistent.\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) {\n+      newDataset.persist(StorageLevel.MEMORY_AND_DISK)\n+    }\n+\n+    // update the accumulator column with the result of prediction of models\n+    val aggregatedDataset = models.zipWithIndex.foldLeft[DataFrame](newDataset) {\n+      case (df, (model, index)) => {\n+        val rawPredictionCol = model.getRawPredictionCol\n+        val columns = origCols ++ List(col(rawPredictionCol), col(accColName))\n+\n+        // add temporary column to store intermediate scores and update\n+        val tmpColName = \"mbc$tmp\" + UUID.randomUUID().toString\n+        val update: (Map[Int, Double], Vector) => Map[Int, Double]  =\n+          (predictions: Map[Int, Double], prediction: Vector) => {\n+            predictions + ((index, prediction(1)))\n+        }\n+        val updateUdf = callUDF(update, mapType, col(accColName), col(rawPredictionCol))\n+        val transformedDataset = model.transform(df).select(columns:_*)\n+        val updatedDataset = transformedDataset.withColumn(tmpColName, updateUdf)\n+        val newColumns = origCols ++ List(col(tmpColName))\n+\n+        // switch out the intermediate column with the accumulator column\n+        updatedDataset.select(newColumns:_*).withColumnRenamed(tmpColName, accColName)\n+      }\n+    }\n+\n+    if (handlePersistence) {\n+      newDataset.unpersist()\n+    }\n+\n+    // output the index of the classifier with highest confidence as prediction\n+    val label: Map[Int, Double] => Double = (predictions: Map[Int, Double]) => {\n+      predictions.maxBy(_._2)._1.toDouble\n+    }\n+    aggregatedDataset.withColumn($(predictionCol), callUDF(label, DoubleType, col(accColName)))"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "remove extra newline\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-11T23:19:05Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import java.util.UUID\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml._\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.MetadataUtils\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRest]].\n+ */\n+private[ml] trait OneVsRestParams extends PredictorParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val classifier: Param[ClassifierType]  =\n+    new Param(this, \"classifier\", \"base binary classifier \")\n+\n+  /** @group getParam */\n+  def getClassifier: ClassifierType = $(classifier)\n+\n+}\n+\n+/**\n+ * Model produced by [[OneVsRest]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsRestModel(\n+      override val parent: OneVsRest,\n+      val models: Array[_ <: ClassificationModel[_,_]])\n+  extends Model[OneVsRestModel] with OneVsRestParams {\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, getClassifier.featuresDataType)\n+  }\n+\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    transformSchema(dataset.schema, logging = true)\n+\n+    // determine the input columns: these need to be passed through\n+    val origCols = dataset.schema.map(f => col(f.name))\n+\n+    // add an accumulator column to store predictions of all the models\n+    val accColName = \"mbc$acc\" + UUID.randomUUID().toString\n+    val init: () => Map[Int, Double] = () => {Map()}\n+    val mapType = MapType(IntegerType, DoubleType, false)\n+    val newDataset = dataset.withColumn(accColName,callUDF(init, mapType))\n+\n+    // persist if underlying dataset is not persistent.\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) {\n+      newDataset.persist(StorageLevel.MEMORY_AND_DISK)\n+    }\n+\n+    // update the accumulator column with the result of prediction of models\n+    val aggregatedDataset = models.zipWithIndex.foldLeft[DataFrame](newDataset) {\n+      case (df, (model, index)) => {\n+        val rawPredictionCol = model.getRawPredictionCol\n+        val columns = origCols ++ List(col(rawPredictionCol), col(accColName))\n+\n+        // add temporary column to store intermediate scores and update\n+        val tmpColName = \"mbc$tmp\" + UUID.randomUUID().toString\n+        val update: (Map[Int, Double], Vector) => Map[Int, Double]  =\n+          (predictions: Map[Int, Double], prediction: Vector) => {\n+            predictions + ((index, prediction(1)))\n+        }\n+        val updateUdf = callUDF(update, mapType, col(accColName), col(rawPredictionCol))\n+        val transformedDataset = model.transform(df).select(columns:_*)\n+        val updatedDataset = transformedDataset.withColumn(tmpColName, updateUdf)\n+        val newColumns = origCols ++ List(col(tmpColName))\n+\n+        // switch out the intermediate column with the accumulator column\n+        updatedDataset.select(newColumns:_*).withColumnRenamed(tmpColName, accColName)\n+      }\n+    }\n+\n+    if (handlePersistence) {\n+      newDataset.unpersist()\n+    }\n+\n+    // output the index of the classifier with highest confidence as prediction\n+    val label: Map[Int, Double] => Double = (predictions: Map[Int, Double]) => {\n+      predictions.maxBy(_._2)._1.toDouble\n+    }\n+    aggregatedDataset.withColumn($(predictionCol), callUDF(label, DoubleType, col(accColName)))\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Reduction of Multiclass Classification to Binary Classification.\n+ * Performs reduction using one against all strategy.\n+ * For a multiclass classification with k classes, train k models (one per class).\n+ * Each example is scored against all k models and the model with highest score\n+ * is picked to label the example.\n+ *"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "make final\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-11T23:19:07Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import java.util.UUID\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml._\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.MetadataUtils\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRest]].\n+ */\n+private[ml] trait OneVsRestParams extends PredictorParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val classifier: Param[ClassifierType]  =\n+    new Param(this, \"classifier\", \"base binary classifier \")\n+\n+  /** @group getParam */\n+  def getClassifier: ClassifierType = $(classifier)\n+\n+}\n+\n+/**\n+ * Model produced by [[OneVsRest]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsRestModel(\n+      override val parent: OneVsRest,\n+      val models: Array[_ <: ClassificationModel[_,_]])\n+  extends Model[OneVsRestModel] with OneVsRestParams {\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, getClassifier.featuresDataType)\n+  }\n+\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    transformSchema(dataset.schema, logging = true)\n+\n+    // determine the input columns: these need to be passed through\n+    val origCols = dataset.schema.map(f => col(f.name))\n+\n+    // add an accumulator column to store predictions of all the models\n+    val accColName = \"mbc$acc\" + UUID.randomUUID().toString\n+    val init: () => Map[Int, Double] = () => {Map()}\n+    val mapType = MapType(IntegerType, DoubleType, false)\n+    val newDataset = dataset.withColumn(accColName,callUDF(init, mapType))\n+\n+    // persist if underlying dataset is not persistent.\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) {\n+      newDataset.persist(StorageLevel.MEMORY_AND_DISK)\n+    }\n+\n+    // update the accumulator column with the result of prediction of models\n+    val aggregatedDataset = models.zipWithIndex.foldLeft[DataFrame](newDataset) {\n+      case (df, (model, index)) => {\n+        val rawPredictionCol = model.getRawPredictionCol\n+        val columns = origCols ++ List(col(rawPredictionCol), col(accColName))\n+\n+        // add temporary column to store intermediate scores and update\n+        val tmpColName = \"mbc$tmp\" + UUID.randomUUID().toString\n+        val update: (Map[Int, Double], Vector) => Map[Int, Double]  =\n+          (predictions: Map[Int, Double], prediction: Vector) => {\n+            predictions + ((index, prediction(1)))\n+        }\n+        val updateUdf = callUDF(update, mapType, col(accColName), col(rawPredictionCol))\n+        val transformedDataset = model.transform(df).select(columns:_*)\n+        val updatedDataset = transformedDataset.withColumn(tmpColName, updateUdf)\n+        val newColumns = origCols ++ List(col(tmpColName))\n+\n+        // switch out the intermediate column with the accumulator column\n+        updatedDataset.select(newColumns:_*).withColumnRenamed(tmpColName, accColName)\n+      }\n+    }\n+\n+    if (handlePersistence) {\n+      newDataset.unpersist()\n+    }\n+\n+    // output the index of the classifier with highest confidence as prediction\n+    val label: Map[Int, Double] => Double = (predictions: Map[Int, Double]) => {\n+      predictions.maxBy(_._2)._1.toDouble\n+    }\n+    aggregatedDataset.withColumn($(predictionCol), callUDF(label, DoubleType, col(accColName)))\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Reduction of Multiclass Classification to Binary Classification.\n+ * Performs reduction using one against all strategy.\n+ * For a multiclass classification with k classes, train k models (one per class).\n+ * Each example is scored against all k models and the model with highest score\n+ * is picked to label the example.\n+ *\n+ */\n+@Experimental\n+class OneVsRest extends Estimator[OneVsRestModel] with OneVsRestParams {"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Not used\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-11T23:19:10Z",
    "diffHunk": "@@ -0,0 +1,194 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+import java.util.UUID\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, Experimental}\n+import org.apache.spark.ml._\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.MetadataUtils\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRest]].\n+ */\n+private[ml] trait OneVsRestParams extends PredictorParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val classifier: Param[ClassifierType]  =\n+    new Param(this, \"classifier\", \"base binary classifier \")\n+\n+  /** @group getParam */\n+  def getClassifier: ClassifierType = $(classifier)\n+\n+}\n+\n+/**\n+ * Model produced by [[OneVsRest]].\n+ *\n+ * @param parent\n+ * @param models the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+class OneVsRestModel(\n+      override val parent: OneVsRest,\n+      val models: Array[_ <: ClassificationModel[_,_]])\n+  extends Model[OneVsRestModel] with OneVsRestParams {\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, getClassifier.featuresDataType)\n+  }\n+\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    transformSchema(dataset.schema, logging = true)\n+\n+    // determine the input columns: these need to be passed through\n+    val origCols = dataset.schema.map(f => col(f.name))\n+\n+    // add an accumulator column to store predictions of all the models\n+    val accColName = \"mbc$acc\" + UUID.randomUUID().toString\n+    val init: () => Map[Int, Double] = () => {Map()}\n+    val mapType = MapType(IntegerType, DoubleType, false)\n+    val newDataset = dataset.withColumn(accColName,callUDF(init, mapType))\n+\n+    // persist if underlying dataset is not persistent.\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) {\n+      newDataset.persist(StorageLevel.MEMORY_AND_DISK)\n+    }\n+\n+    // update the accumulator column with the result of prediction of models\n+    val aggregatedDataset = models.zipWithIndex.foldLeft[DataFrame](newDataset) {\n+      case (df, (model, index)) => {\n+        val rawPredictionCol = model.getRawPredictionCol\n+        val columns = origCols ++ List(col(rawPredictionCol), col(accColName))\n+\n+        // add temporary column to store intermediate scores and update\n+        val tmpColName = \"mbc$tmp\" + UUID.randomUUID().toString\n+        val update: (Map[Int, Double], Vector) => Map[Int, Double]  =\n+          (predictions: Map[Int, Double], prediction: Vector) => {\n+            predictions + ((index, prediction(1)))\n+        }\n+        val updateUdf = callUDF(update, mapType, col(accColName), col(rawPredictionCol))\n+        val transformedDataset = model.transform(df).select(columns:_*)\n+        val updatedDataset = transformedDataset.withColumn(tmpColName, updateUdf)\n+        val newColumns = origCols ++ List(col(tmpColName))\n+\n+        // switch out the intermediate column with the accumulator column\n+        updatedDataset.select(newColumns:_*).withColumnRenamed(tmpColName, accColName)\n+      }\n+    }\n+\n+    if (handlePersistence) {\n+      newDataset.unpersist()\n+    }\n+\n+    // output the index of the classifier with highest confidence as prediction\n+    val label: Map[Int, Double] => Double = (predictions: Map[Int, Double]) => {\n+      predictions.maxBy(_._2)._1.toDouble\n+    }\n+    aggregatedDataset.withColumn($(predictionCol), callUDF(label, DoubleType, col(accColName)))\n+  }\n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Reduction of Multiclass Classification to Binary Classification.\n+ * Performs reduction using one against all strategy.\n+ * For a multiclass classification with k classes, train k models (one per class).\n+ * Each example is scored against all k models and the model with highest score\n+ * is picked to label the example.\n+ *\n+ */\n+@Experimental\n+class OneVsRest extends Estimator[OneVsRestModel] with OneVsRestParams {\n+\n+  /** @group setParam */\n+  // TODO: Find a better way to do this. Existential Types don't work with Java API so cast needed.\n+  def setClassifier(value: Classifier[_,_,_]): this.type = {\n+    set(classifier, value.asInstanceOf[ClassifierType])\n+  }\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = true, getClassifier.featuresDataType)\n+  }\n+\n+  override def fit(dataset: DataFrame): OneVsRestModel = {\n+    // determine number of classes either from metadata if provided, or via computation.\n+    val labelSchema = dataset.schema($(labelCol))\n+    val computeNumClasses: () => Int = () => {\n+      val Row(maxLabelIndex: Double) = dataset.agg(max($(labelCol))).head()\n+      // classes are assumed to be numbered from 0,...,maxLabelIndex\n+      maxLabelIndex.toInt + 1\n+    }\n+    val numClasses = MetadataUtils.getNumClasses(labelSchema).fold(computeNumClasses())(identity)\n+\n+    val multiclassLabeled = dataset.select($(labelCol), $(featuresCol))\n+\n+    // persist if underlying dataset is not persistent.\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) {\n+      multiclassLabeled.persist(StorageLevel.MEMORY_AND_DISK)\n+    }\n+\n+    // create k columns, one for each binary classifier.\n+    val models = Range(0, numClasses).par.map { index =>\n+\n+      val label: Double => Double = (label: Double) => {\n+        if (label.toInt == index) 1.0 else 0.0\n+      }\n+\n+      // generate new label metadata for the binary problem.\n+      // TODO: use when ... otherwise after SPARK-7321 is merged\n+      val labelUDF = callUDF(label, DoubleType, col($(labelCol)))\n+      val newLabelMeta = BinaryAttribute.defaultAttr.withName(\"label\").toMetadata()\n+      val skipFeatures: Any => Boolean = (name: Any) => name.toString.equals(featuresCol.name)"
  }],
  "prId": 5830
}]