[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "It should be okay to use `._`.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:14Z",
    "diffHunk": "@@ -22,7 +22,8 @@ import scala.collection.JavaConverters._\n import org.apache.spark.annotation.{Experimental, Since}\n import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n import org.apache.spark.ml.ann.{FeedForwardTopology, FeedForwardTrainer}\n-import org.apache.spark.ml.param.{IntArrayParam, IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.{DoubleParam, IntArrayParam, IntParam, Param, ParamMap,"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "We should have the same doc as it is defined in `optimizer`.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:17Z",
    "diffHunk": "@@ -65,7 +66,42 @@ private[ml] trait MultilayerPerceptronParams extends PredictorParams\n   /** @group getParam */\n   final def getBlockSize: Int = $(blockSize)\n \n-  setDefault(maxIter -> 100, tol -> 1e-4, layers -> Array(1, 1), blockSize -> 128)\n+  /**\n+   * Optimizer setup."
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "We use `solver` in GLM. It might be better to be consistent.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:19Z",
    "diffHunk": "@@ -65,7 +66,42 @@ private[ml] trait MultilayerPerceptronParams extends PredictorParams\n   /** @group getParam */\n   final def getBlockSize: Int = $(blockSize)\n \n-  setDefault(maxIter -> 100, tol -> 1e-4, layers -> Array(1, 1), blockSize -> 128)\n+  /**\n+   * Optimizer setup.\n+   * @group expertParam\n+   */\n+  final val optimizer: Param[String] = new Param[String](this, \"optimizer\","
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Mention which one is the default.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:21Z",
    "diffHunk": "@@ -65,7 +66,42 @@ private[ml] trait MultilayerPerceptronParams extends PredictorParams\n   /** @group getParam */\n   final def getBlockSize: Int = $(blockSize)\n \n-  setDefault(maxIter -> 100, tol -> 1e-4, layers -> Array(1, 1), blockSize -> 128)\n+  /**\n+   * Optimizer setup.\n+   * @group expertParam\n+   */\n+  final val optimizer: Param[String] = new Param[String](this, \"optimizer\",\n+    \" Allows setting the optimizer: minibatch gradient descent (GD) or LBFGS. \" +"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "use lowercase letters: `gd` and `l-bfgs`. Note that we already used `l-bfgs` in `LinearRegression`, so we should do the same here.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:23Z",
    "diffHunk": "@@ -65,7 +66,42 @@ private[ml] trait MultilayerPerceptronParams extends PredictorParams\n   /** @group getParam */\n   final def getBlockSize: Int = $(blockSize)\n \n-  setDefault(maxIter -> 100, tol -> 1e-4, layers -> Array(1, 1), blockSize -> 128)\n+  /**\n+   * Optimizer setup.\n+   * @group expertParam\n+   */\n+  final val optimizer: Param[String] = new Param[String](this, \"optimizer\",\n+    \" Allows setting the optimizer: minibatch gradient descent (GD) or LBFGS. \" +\n+      \" The latter is recommended one. \",\n+    ParamValidators.inArray[String](Array(\"GD\", \"LBFGS\")))"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "- use `stepSize` instead to be consistent with other algorithms under `spark.ml`\n- You can get this from `HasStepSize`\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:25Z",
    "diffHunk": "@@ -65,7 +66,42 @@ private[ml] trait MultilayerPerceptronParams extends PredictorParams\n   /** @group getParam */\n   final def getBlockSize: Int = $(blockSize)\n \n-  setDefault(maxIter -> 100, tol -> 1e-4, layers -> Array(1, 1), blockSize -> 128)\n+  /**\n+   * Optimizer setup.\n+   * @group expertParam\n+   */\n+  final val optimizer: Param[String] = new Param[String](this, \"optimizer\",\n+    \" Allows setting the optimizer: minibatch gradient descent (GD) or LBFGS. \" +\n+      \" The latter is recommended one. \",\n+    ParamValidators.inArray[String](Array(\"GD\", \"LBFGS\")))\n+\n+  /** @group getParam */\n+  final def getOptimizer: String = $(optimizer)\n+\n+  /**\n+   * Learning rate.\n+   * @group expertParam\n+   */\n+  final val learningRate: DoubleParam = new DoubleParam(this, \"learning rate\","
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Is it okay to change it to `coefficients`? We did this for linear models to avoid confusion with instance weights.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:27Z",
    "diffHunk": "@@ -65,7 +66,42 @@ private[ml] trait MultilayerPerceptronParams extends PredictorParams\n   /** @group getParam */\n   final def getBlockSize: Int = $(blockSize)\n \n-  setDefault(maxIter -> 100, tol -> 1e-4, layers -> Array(1, 1), blockSize -> 128)\n+  /**\n+   * Optimizer setup.\n+   * @group expertParam\n+   */\n+  final val optimizer: Param[String] = new Param[String](this, \"optimizer\",\n+    \" Allows setting the optimizer: minibatch gradient descent (GD) or LBFGS. \" +\n+      \" The latter is recommended one. \",\n+    ParamValidators.inArray[String](Array(\"GD\", \"LBFGS\")))\n+\n+  /** @group getParam */\n+  final def getOptimizer: String = $(optimizer)\n+\n+  /**\n+   * Learning rate.\n+   * @group expertParam\n+   */\n+  final val learningRate: DoubleParam = new DoubleParam(this, \"learning rate\",\n+    \" Sets the learning rate for gradient descent optimizer \",\n+    ParamValidators.inRange(0, 1))\n+\n+  /** @group getParam */\n+  final def getLearningRate: Double = $(learningRate)\n+\n+  /**\n+   * Model weights. Can be returned either after training or after explicit setting\n+   * @group expertParam\n+   */\n+  final val weights: Param[Vector] = new Param[Vector](this, \"weights\","
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Shall we use a more reasonable default value for `layers`? Or we should leave it empty and require users to set it.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:30Z",
    "diffHunk": "@@ -65,7 +66,42 @@ private[ml] trait MultilayerPerceptronParams extends PredictorParams\n   /** @group getParam */\n   final def getBlockSize: Int = $(blockSize)\n \n-  setDefault(maxIter -> 100, tol -> 1e-4, layers -> Array(1, 1), blockSize -> 128)\n+  /**\n+   * Optimizer setup.\n+   * @group expertParam\n+   */\n+  final val optimizer: Param[String] = new Param[String](this, \"optimizer\",\n+    \" Allows setting the optimizer: minibatch gradient descent (GD) or LBFGS. \" +\n+      \" The latter is recommended one. \",\n+    ParamValidators.inArray[String](Array(\"GD\", \"LBFGS\")))\n+\n+  /** @group getParam */\n+  final def getOptimizer: String = $(optimizer)\n+\n+  /**\n+   * Learning rate.\n+   * @group expertParam\n+   */\n+  final val learningRate: DoubleParam = new DoubleParam(this, \"learning rate\",\n+    \" Sets the learning rate for gradient descent optimizer \",\n+    ParamValidators.inRange(0, 1))\n+\n+  /** @group getParam */\n+  final def getLearningRate: Double = $(learningRate)\n+\n+  /**\n+   * Model weights. Can be returned either after training or after explicit setting\n+   * @group expertParam\n+   */\n+  final val weights: Param[Vector] = new Param[Vector](this, \"weights\",\n+    \" Sets the weights of the model \")\n+\n+  /** @group getParam */\n+  final def getWeights: Vector = $(weights)\n+\n+\n+  setDefault(maxIter -> 100, tol -> 1e-4, layers -> Array(1, 1), blockSize -> 128,"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "If the default is `null`, we don't need to set it.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:32Z",
    "diffHunk": "@@ -65,7 +66,42 @@ private[ml] trait MultilayerPerceptronParams extends PredictorParams\n   /** @group getParam */\n   final def getBlockSize: Int = $(blockSize)\n \n-  setDefault(maxIter -> 100, tol -> 1e-4, layers -> Array(1, 1), blockSize -> 128)\n+  /**\n+   * Optimizer setup.\n+   * @group expertParam\n+   */\n+  final val optimizer: Param[String] = new Param[String](this, \"optimizer\",\n+    \" Allows setting the optimizer: minibatch gradient descent (GD) or LBFGS. \" +\n+      \" The latter is recommended one. \",\n+    ParamValidators.inArray[String](Array(\"GD\", \"LBFGS\")))\n+\n+  /** @group getParam */\n+  final def getOptimizer: String = $(optimizer)\n+\n+  /**\n+   * Learning rate.\n+   * @group expertParam\n+   */\n+  final val learningRate: DoubleParam = new DoubleParam(this, \"learning rate\",\n+    \" Sets the learning rate for gradient descent optimizer \",\n+    ParamValidators.inRange(0, 1))\n+\n+  /** @group getParam */\n+  final def getLearningRate: Double = $(learningRate)\n+\n+  /**\n+   * Model weights. Can be returned either after training or after explicit setting\n+   * @group expertParam\n+   */\n+  final val weights: Param[Vector] = new Param[Vector](this, \"weights\",\n+    \" Sets the weights of the model \")\n+\n+  /** @group getParam */\n+  final def getWeights: Vector = $(weights)\n+\n+\n+  setDefault(maxIter -> 100, tol -> 1e-4, layers -> Array(1, 1), blockSize -> 128,\n+    optimizer -> \"LBFGS\", learningRate -> 0.03, weights -> null)"
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "- `Sets`\n- This should be an expert param.\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:35Z",
    "diffHunk": "@@ -141,12 +178,19 @@ class MultilayerPerceptronClassifier @Since(\"1.5.0\") (\n   def setTol(value: Double): this.type = set(tol, value)\n \n   /**\n-   * Set the seed for weights initialization.\n+   * Set the seed for weights initialization if weights are not set\n    * @group setParam\n    */\n   @Since(\"1.5.0\")\n   def setSeed(value: Long): this.type = set(seed, value)\n \n+  /**\n+   * Set the model weights."
  }],
  "prId": 9229
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "If we don't have `weights` set, this should be `isDefined(weights)`\n",
    "commit": "94dcec08b5bf7cb1af054e7e27b258ab0ce870a9",
    "createdAt": "2016-03-15T17:45:37Z",
    "diffHunk": "@@ -164,11 +208,18 @@ class MultilayerPerceptronClassifier @Since(\"1.5.0\") (\n     val lpData = extractLabeledPoints(dataset)\n     val data = lpData.map(lp => LabelConverter.encodeLabeledPoint(lp, labels))\n     val topology = FeedForwardTopology.multiLayerPerceptron(myLayers, true)\n-    val FeedForwardTrainer = new FeedForwardTrainer(topology, myLayers(0), myLayers.last)\n-    FeedForwardTrainer.LBFGSOptimizer.setConvergenceTol($(tol)).setNumIterations($(maxIter))\n-    FeedForwardTrainer.setStackSize($(blockSize))\n-    val mlpModel = FeedForwardTrainer.train(data)\n-    new MultilayerPerceptronClassificationModel(uid, myLayers, mlpModel.weights())\n+    val trainer = new FeedForwardTrainer(topology, myLayers(0), myLayers.last)\n+    if ($(weights) != null) {"
  }],
  "prId": 9229
}]