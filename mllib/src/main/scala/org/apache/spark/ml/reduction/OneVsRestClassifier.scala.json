[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove space before `;`\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:30Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "space after `,`\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:33Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove extra space after `<:` and add space after `E,`\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:35Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "For the class name and param name, maybe we can rename `OneVsRestClassifier` to `OneVsRest` and then rename `baseClassifer` to `classifier`. The code should read fine:\n\n``` scala\nval ovr = new OneVsRest()\n  .setClassifier(svm)\n```\n\n``` python\novr = OneVsRest(classifier=svm)\n```\n\nBtw, we should also mention that `featuresCol` and `labelCol` are ignored in the base classifiers.\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:37Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  ="
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove `/regressor`\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:39Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  =\n+    new Param(this, \"baseClassifier\", \"base binary classifier/regressor \")"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`getOrDefault` -> `$`\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:40Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  =\n+    new Param(this, \"baseClassifier\", \"base binary classifier/regressor \")\n+\n+  /** @group getParam */\n+  def getBaseClassifier: ClassifierType = getOrDefault(baseClassifier)"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Missing doc.\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:42Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  =\n+    new Param(this, \"baseClassifier\", \"base binary classifier/regressor \")\n+\n+  /** @group getParam */\n+  def getBaseClassifier: ClassifierType = getOrDefault(baseClassifier)\n+\n+}\n+\n+/**\n+ *"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`baseClassificationModels` -> `baseModels` or simply `models`?\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:44Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  =\n+    new Param(this, \"baseClassifier\", \"base binary classifier/regressor \")\n+\n+  /** @group getParam */\n+  def getBaseClassifier: ClassifierType = getOrDefault(baseClassifier)\n+\n+}\n+\n+/**\n+ *\n+ * @param parent\n+ * @param baseClassificationModels the binary classification models for reduction."
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "This should be public.\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:45Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  =\n+    new Param(this, \"baseClassifier\", \"base binary classifier/regressor \")\n+\n+  /** @group getParam */\n+  def getBaseClassifier: ClassifierType = getOrDefault(baseClassifier)\n+\n+}\n+\n+/**\n+ *\n+ * @param parent\n+ * @param baseClassificationModels the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+private[ml] class OneVsRestModel("
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "We don't need the doc for overridden method.\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:46Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  =\n+    new Param(this, \"baseClassifier\", \"base binary classifier/regressor \")\n+\n+  /** @group getParam */\n+  def getBaseClassifier: ClassifierType = getOrDefault(baseClassifier)\n+\n+}\n+\n+/**\n+ *\n+ * @param parent\n+ * @param baseClassificationModels the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+private[ml] class OneVsRestModel(\n+    override val parent: OneVsRestClassifier,\n+    val baseClassificationModels: Array[Model[_]])\n+  extends Model[OneVsRestModel] with OneVsRestParams {\n+\n+  /**\n+   * Transforms the dataset with provided parameter map as additional parameters."
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "We should leave a TODO here to tune performance. It would be nice if we can use DataFrame expressions in the future to leverage on SQL performance improvements.\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:48Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  =\n+    new Param(this, \"baseClassifier\", \"base binary classifier/regressor \")\n+\n+  /** @group getParam */\n+  def getBaseClassifier: ClassifierType = getOrDefault(baseClassifier)\n+\n+}\n+\n+/**\n+ *\n+ * @param parent\n+ * @param baseClassificationModels the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+private[ml] class OneVsRestModel(\n+    override val parent: OneVsRestClassifier,\n+    val baseClassificationModels: Array[Model[_]])\n+  extends Model[OneVsRestModel] with OneVsRestParams {\n+\n+  /**\n+   * Transforms the dataset with provided parameter map as additional parameters.\n+   * @param dataset input dataset\n+   * @return transformed dataset\n+   */\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    val predictions = baseClassificationModels.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "merge this line into the one above\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:50Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  =\n+    new Param(this, \"baseClassifier\", \"base binary classifier/regressor \")\n+\n+  /** @group getParam */\n+  def getBaseClassifier: ClassifierType = getOrDefault(baseClassifier)\n+\n+}\n+\n+/**\n+ *\n+ * @param parent\n+ * @param baseClassificationModels the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+private[ml] class OneVsRestModel(\n+    override val parent: OneVsRestClassifier,\n+    val baseClassificationModels: Array[Model[_]])\n+  extends Model[OneVsRestModel] with OneVsRestParams {\n+\n+  /**\n+   * Transforms the dataset with provided parameter map as additional parameters.\n+   * @param dataset input dataset\n+   * @return transformed dataset\n+   */\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    val predictions = baseClassificationModels.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>\n+        a ++ b\n+      }\n+    }.\n+      map(_.maxBy(_._2))"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`col(\"*\")` -> `\"*\"`\n\nIs the `(...)` around `(row, (label, _))` necessary?\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:51Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  =\n+    new Param(this, \"baseClassifier\", \"base binary classifier/regressor \")\n+\n+  /** @group getParam */\n+  def getBaseClassifier: ClassifierType = getOrDefault(baseClassifier)\n+\n+}\n+\n+/**\n+ *\n+ * @param parent\n+ * @param baseClassificationModels the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+private[ml] class OneVsRestModel(\n+    override val parent: OneVsRestClassifier,\n+    val baseClassificationModels: Array[Model[_]])\n+  extends Model[OneVsRestModel] with OneVsRestParams {\n+\n+  /**\n+   * Transforms the dataset with provided parameter map as additional parameters.\n+   * @param dataset input dataset\n+   * @return transformed dataset\n+   */\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    val predictions = baseClassificationModels.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>\n+        a ++ b\n+      }\n+    }.\n+      map(_.maxBy(_._2))\n+\n+    // ensure that we pass through columns that are part of the original dataset.\n+    val results = dataset.select(col(\"*\")).rdd.zip(predictions).map { case ((row, (label, _))) =>"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Missing `@Experimental`. See my previous comments on the class name.\n\nShould `OneVsRest` implement `Classifier`? @jkbradley \n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:53Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  =\n+    new Param(this, \"baseClassifier\", \"base binary classifier/regressor \")\n+\n+  /** @group getParam */\n+  def getBaseClassifier: ClassifierType = getOrDefault(baseClassifier)\n+\n+}\n+\n+/**\n+ *\n+ * @param parent\n+ * @param baseClassificationModels the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+private[ml] class OneVsRestModel(\n+    override val parent: OneVsRestClassifier,\n+    val baseClassificationModels: Array[Model[_]])\n+  extends Model[OneVsRestModel] with OneVsRestParams {\n+\n+  /**\n+   * Transforms the dataset with provided parameter map as additional parameters.\n+   * @param dataset input dataset\n+   * @return transformed dataset\n+   */\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    val predictions = baseClassificationModels.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>\n+        a ++ b\n+      }\n+    }.\n+      map(_.maxBy(_._2))\n+\n+    // ensure that we pass through columns that are part of the original dataset.\n+    val results = dataset.select(col(\"*\")).rdd.zip(predictions).map { case ((row, (label, _))) =>\n+      Row.fromSeq(row.toSeq ++ List(label.toDouble))\n+    }\n+\n+    // the output schema should retain all input fields and add prediction column.\n+    val outputSchema = SchemaUtils.appendColumn(parentSchema, $(predictionCol), DoubleType)\n+    sqlCtx.createDataFrame(results, outputSchema)\n+  }\n+\n+  @DeveloperApi\n+  protected def featuresDataType: DataType = new VectorUDT\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, featuresDataType)\n+  }\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Reduction of Multiclass Classification to Binary Classification.\n+ * Performs reduction using one against all strategy.\n+ * For a multiclass classification with k classes, train k models (one per class).\n+ * Each example is scored against all k models and the model with highest score\n+ * is picked to label the example.\n+ *\n+ */\n+class OneVsRestClassifier extends Estimator[OneVsRestModel]"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`distinct` -> `distinct()`\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:56Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  =\n+    new Param(this, \"baseClassifier\", \"base binary classifier/regressor \")\n+\n+  /** @group getParam */\n+  def getBaseClassifier: ClassifierType = getOrDefault(baseClassifier)\n+\n+}\n+\n+/**\n+ *\n+ * @param parent\n+ * @param baseClassificationModels the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+private[ml] class OneVsRestModel(\n+    override val parent: OneVsRestClassifier,\n+    val baseClassificationModels: Array[Model[_]])\n+  extends Model[OneVsRestModel] with OneVsRestParams {\n+\n+  /**\n+   * Transforms the dataset with provided parameter map as additional parameters.\n+   * @param dataset input dataset\n+   * @return transformed dataset\n+   */\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    val predictions = baseClassificationModels.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>\n+        a ++ b\n+      }\n+    }.\n+      map(_.maxBy(_._2))\n+\n+    // ensure that we pass through columns that are part of the original dataset.\n+    val results = dataset.select(col(\"*\")).rdd.zip(predictions).map { case ((row, (label, _))) =>\n+      Row.fromSeq(row.toSeq ++ List(label.toDouble))\n+    }\n+\n+    // the output schema should retain all input fields and add prediction column.\n+    val outputSchema = SchemaUtils.appendColumn(parentSchema, $(predictionCol), DoubleType)\n+    sqlCtx.createDataFrame(results, outputSchema)\n+  }\n+\n+  @DeveloperApi\n+  protected def featuresDataType: DataType = new VectorUDT\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, featuresDataType)\n+  }\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Reduction of Multiclass Classification to Binary Classification.\n+ * Performs reduction using one against all strategy.\n+ * For a multiclass classification with k classes, train k models (one per class).\n+ * Each example is scored against all k models and the model with highest score\n+ * is picked to label the example.\n+ *\n+ */\n+class OneVsRestClassifier extends Estimator[OneVsRestModel]\n+  with OneVsRestParams {\n+\n+  @DeveloperApi\n+  protected def featuresDataType: DataType = new VectorUDT\n+\n+  /** @group setParam */\n+  def setBaseClassifier(value: ClassifierType): this.type = set(baseClassifier, value)\n+\n+  override def fit(dataset: DataFrame): OneVsRestModel = {\n+\n+    // determine number of classes either from metadata if provided, or via computation.\n+    val labelSchema = dataset.schema($(labelCol))\n+    val computeNumClasses: () => Int = () => {\n+      dataset.select($(labelCol)).distinct.count().toInt"
  }, {
    "author": {
      "login": "harsha2010"
    },
    "body": "The distinct signature in DF is without parentheses\n\n/**\n- Returns a new [[DataFrame]] that contains only the unique rows from this [[DataFrame]].\n- @group dfops\n  */\n  override def distinct: DataFrame = Distinct(logicalPlan)\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T14:29:41Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  =\n+    new Param(this, \"baseClassifier\", \"base binary classifier/regressor \")\n+\n+  /** @group getParam */\n+  def getBaseClassifier: ClassifierType = getOrDefault(baseClassifier)\n+\n+}\n+\n+/**\n+ *\n+ * @param parent\n+ * @param baseClassificationModels the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+private[ml] class OneVsRestModel(\n+    override val parent: OneVsRestClassifier,\n+    val baseClassificationModels: Array[Model[_]])\n+  extends Model[OneVsRestModel] with OneVsRestParams {\n+\n+  /**\n+   * Transforms the dataset with provided parameter map as additional parameters.\n+   * @param dataset input dataset\n+   * @return transformed dataset\n+   */\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    val predictions = baseClassificationModels.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>\n+        a ++ b\n+      }\n+    }.\n+      map(_.maxBy(_._2))\n+\n+    // ensure that we pass through columns that are part of the original dataset.\n+    val results = dataset.select(col(\"*\")).rdd.zip(predictions).map { case ((row, (label, _))) =>\n+      Row.fromSeq(row.toSeq ++ List(label.toDouble))\n+    }\n+\n+    // the output schema should retain all input fields and add prediction column.\n+    val outputSchema = SchemaUtils.appendColumn(parentSchema, $(predictionCol), DoubleType)\n+    sqlCtx.createDataFrame(results, outputSchema)\n+  }\n+\n+  @DeveloperApi\n+  protected def featuresDataType: DataType = new VectorUDT\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, featuresDataType)\n+  }\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Reduction of Multiclass Classification to Binary Classification.\n+ * Performs reduction using one against all strategy.\n+ * For a multiclass classification with k classes, train k models (one per class).\n+ * Each example is scored against all k models and the model with highest score\n+ * is picked to label the example.\n+ *\n+ */\n+class OneVsRestClassifier extends Estimator[OneVsRestModel]\n+  with OneVsRestParams {\n+\n+  @DeveloperApi\n+  protected def featuresDataType: DataType = new VectorUDT\n+\n+  /** @group setParam */\n+  def setBaseClassifier(value: ClassifierType): this.type = set(baseClassifier, value)\n+\n+  override def fit(dataset: DataFrame): OneVsRestModel = {\n+\n+    // determine number of classes either from metadata if provided, or via computation.\n+    val labelSchema = dataset.schema($(labelCol))\n+    val computeNumClasses: () => Int = () => {\n+      dataset.select($(labelCol)).distinct.count().toInt"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "minor: add a TODO here to use `when ... otherwise` after SPARK-7321 is merged\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:58Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  =\n+    new Param(this, \"baseClassifier\", \"base binary classifier/regressor \")\n+\n+  /** @group getParam */\n+  def getBaseClassifier: ClassifierType = getOrDefault(baseClassifier)\n+\n+}\n+\n+/**\n+ *\n+ * @param parent\n+ * @param baseClassificationModels the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+private[ml] class OneVsRestModel(\n+    override val parent: OneVsRestClassifier,\n+    val baseClassificationModels: Array[Model[_]])\n+  extends Model[OneVsRestModel] with OneVsRestParams {\n+\n+  /**\n+   * Transforms the dataset with provided parameter map as additional parameters.\n+   * @param dataset input dataset\n+   * @return transformed dataset\n+   */\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    val predictions = baseClassificationModels.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>\n+        a ++ b\n+      }\n+    }.\n+      map(_.maxBy(_._2))\n+\n+    // ensure that we pass through columns that are part of the original dataset.\n+    val results = dataset.select(col(\"*\")).rdd.zip(predictions).map { case ((row, (label, _))) =>\n+      Row.fromSeq(row.toSeq ++ List(label.toDouble))\n+    }\n+\n+    // the output schema should retain all input fields and add prediction column.\n+    val outputSchema = SchemaUtils.appendColumn(parentSchema, $(predictionCol), DoubleType)\n+    sqlCtx.createDataFrame(results, outputSchema)\n+  }\n+\n+  @DeveloperApi\n+  protected def featuresDataType: DataType = new VectorUDT\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, featuresDataType)\n+  }\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Reduction of Multiclass Classification to Binary Classification.\n+ * Performs reduction using one against all strategy.\n+ * For a multiclass classification with k classes, train k models (one per class).\n+ * Each example is scored against all k models and the model with highest score\n+ * is picked to label the example.\n+ *\n+ */\n+class OneVsRestClassifier extends Estimator[OneVsRestModel]\n+  with OneVsRestParams {\n+\n+  @DeveloperApi\n+  protected def featuresDataType: DataType = new VectorUDT\n+\n+  /** @group setParam */\n+  def setBaseClassifier(value: ClassifierType): this.type = set(baseClassifier, value)\n+\n+  override def fit(dataset: DataFrame): OneVsRestModel = {\n+\n+    // determine number of classes either from metadata if provided, or via computation.\n+    val labelSchema = dataset.schema($(labelCol))\n+    val computeNumClasses: () => Int = () => {\n+      dataset.select($(labelCol)).distinct.count().toInt\n+    }\n+    val numClasses = MetadataUtils.getNumClasses(labelSchema).fold(computeNumClasses())(identity)\n+\n+    val multiclassLabeled = dataset.select($(labelCol), $(featuresCol))\n+\n+    // persist if underlying dataset is not persistent.\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) {\n+      multiclassLabeled.persist(StorageLevel.MEMORY_AND_DISK)\n+    }\n+\n+    // create k columns, one for each binary classifier.\n+    val models = Range(0, numClasses).par.map { index =>\n+      val labelColName = \"mc2b$\" + index\n+      val label: Double => Double = (label: Double) => {\n+        if (label.toInt == index) 1.0 else 0.0\n+      }\n+\n+      // generate new label for each binary classifier.\n+      // generate new label metadata for the binary problem.\n+      val labelUDF = callUDF(label, DoubleType, col($(labelCol)))"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Also override `featuresCol`, `predictionCol`, etc.\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:25:59Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  =\n+    new Param(this, \"baseClassifier\", \"base binary classifier/regressor \")\n+\n+  /** @group getParam */\n+  def getBaseClassifier: ClassifierType = getOrDefault(baseClassifier)\n+\n+}\n+\n+/**\n+ *\n+ * @param parent\n+ * @param baseClassificationModels the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+private[ml] class OneVsRestModel(\n+    override val parent: OneVsRestClassifier,\n+    val baseClassificationModels: Array[Model[_]])\n+  extends Model[OneVsRestModel] with OneVsRestParams {\n+\n+  /**\n+   * Transforms the dataset with provided parameter map as additional parameters.\n+   * @param dataset input dataset\n+   * @return transformed dataset\n+   */\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    val predictions = baseClassificationModels.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>\n+        a ++ b\n+      }\n+    }.\n+      map(_.maxBy(_._2))\n+\n+    // ensure that we pass through columns that are part of the original dataset.\n+    val results = dataset.select(col(\"*\")).rdd.zip(predictions).map { case ((row, (label, _))) =>\n+      Row.fromSeq(row.toSeq ++ List(label.toDouble))\n+    }\n+\n+    // the output schema should retain all input fields and add prediction column.\n+    val outputSchema = SchemaUtils.appendColumn(parentSchema, $(predictionCol), DoubleType)\n+    sqlCtx.createDataFrame(results, outputSchema)\n+  }\n+\n+  @DeveloperApi\n+  protected def featuresDataType: DataType = new VectorUDT\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, featuresDataType)\n+  }\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Reduction of Multiclass Classification to Binary Classification.\n+ * Performs reduction using one against all strategy.\n+ * For a multiclass classification with k classes, train k models (one per class).\n+ * Each example is scored against all k models and the model with highest score\n+ * is picked to label the example.\n+ *\n+ */\n+class OneVsRestClassifier extends Estimator[OneVsRestModel]\n+  with OneVsRestParams {\n+\n+  @DeveloperApi\n+  protected def featuresDataType: DataType = new VectorUDT\n+\n+  /** @group setParam */\n+  def setBaseClassifier(value: ClassifierType): this.type = set(baseClassifier, value)\n+\n+  override def fit(dataset: DataFrame): OneVsRestModel = {\n+\n+    // determine number of classes either from metadata if provided, or via computation.\n+    val labelSchema = dataset.schema($(labelCol))\n+    val computeNumClasses: () => Int = () => {\n+      dataset.select($(labelCol)).distinct.count().toInt\n+    }\n+    val numClasses = MetadataUtils.getNumClasses(labelSchema).fold(computeNumClasses())(identity)\n+\n+    val multiclassLabeled = dataset.select($(labelCol), $(featuresCol))\n+\n+    // persist if underlying dataset is not persistent.\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) {\n+      multiclassLabeled.persist(StorageLevel.MEMORY_AND_DISK)\n+    }\n+\n+    // create k columns, one for each binary classifier.\n+    val models = Range(0, numClasses).par.map { index =>\n+      val labelColName = \"mc2b$\" + index\n+      val label: Double => Double = (label: Double) => {\n+        if (label.toInt == index) 1.0 else 0.0\n+      }\n+\n+      // generate new label for each binary classifier.\n+      // generate new label metadata for the binary problem.\n+      val labelUDF = callUDF(label, DoubleType, col($(labelCol)))\n+      val newLabelMeta = BinaryAttribute.defaultAttr.withName(\"label\").toMetadata()\n+      val labelUDFWithNewMeta = labelUDF.as(labelColName, newLabelMeta)\n+      val trainingDataset = multiclassLabeled.withColumn(labelColName, labelUDFWithNewMeta)\n+      val classifier = getBaseClassifier\n+      classifier.fit(trainingDataset, classifier.labelCol -> labelColName)"
  }],
  "prId": 5830
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Need to copy the param values (e.g., featuresCol) over. See `copyValues`.\n",
    "commit": "5f4b495e41324ca423aaea1b4cce3c782e13147c",
    "createdAt": "2015-05-06T08:26:01Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.reduction\n+\n+\n+import scala.language.existentials\n+\n+import org.apache.spark.annotation.{AlphaComponent, DeveloperApi}\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.attribute.BinaryAttribute\n+import org.apache.spark.ml.classification.{ClassificationModel, Classifier, ClassifierParams}\n+import org.apache.spark.ml.param.Param\n+import org.apache.spark.ml.util.{MetadataUtils, SchemaUtils}\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{DataFrame, Row}\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for [[OneVsRestClassifier]].\n+ */\n+private[ml] trait OneVsRestParams extends ClassifierParams {\n+\n+  type ClassifierType = Classifier[F, E, M] forSome {\n+    type F ;\n+    type M <: ClassificationModel[F,M];\n+    type E <:  Classifier[F, E,M]\n+  }\n+\n+  /**\n+   * param for the base classifier that we reduce multiclass classification into.\n+   * @group param\n+   */\n+  val baseClassifier: Param[ClassifierType]  =\n+    new Param(this, \"baseClassifier\", \"base binary classifier/regressor \")\n+\n+  /** @group getParam */\n+  def getBaseClassifier: ClassifierType = getOrDefault(baseClassifier)\n+\n+}\n+\n+/**\n+ *\n+ * @param parent\n+ * @param baseClassificationModels the binary classification models for reduction.\n+ */\n+@AlphaComponent\n+private[ml] class OneVsRestModel(\n+    override val parent: OneVsRestClassifier,\n+    val baseClassificationModels: Array[Model[_]])\n+  extends Model[OneVsRestModel] with OneVsRestParams {\n+\n+  /**\n+   * Transforms the dataset with provided parameter map as additional parameters.\n+   * @param dataset input dataset\n+   * @return transformed dataset\n+   */\n+  override def transform(dataset: DataFrame): DataFrame = {\n+    // Check schema\n+    val parentSchema = dataset.schema\n+    transformSchema(parentSchema, logging = true)\n+    val sqlCtx = dataset.sqlContext\n+\n+    // score each model on every data point and pick the model with highest score\n+    val predictions = baseClassificationModels.zipWithIndex.par.map { case (model, index) =>\n+      val output = model.transform(dataset)\n+      output.select($(rawPredictionCol)).map { case Row(p: Vector) => List((index, p(1))) }\n+    }.reduce[RDD[List[(Int, Double)]]] { case (x, y) =>\n+      x.zip(y).map { case ((a, b)) =>\n+        a ++ b\n+      }\n+    }.\n+      map(_.maxBy(_._2))\n+\n+    // ensure that we pass through columns that are part of the original dataset.\n+    val results = dataset.select(col(\"*\")).rdd.zip(predictions).map { case ((row, (label, _))) =>\n+      Row.fromSeq(row.toSeq ++ List(label.toDouble))\n+    }\n+\n+    // the output schema should retain all input fields and add prediction column.\n+    val outputSchema = SchemaUtils.appendColumn(parentSchema, $(predictionCol), DoubleType)\n+    sqlCtx.createDataFrame(results, outputSchema)\n+  }\n+\n+  @DeveloperApi\n+  protected def featuresDataType: DataType = new VectorUDT\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema, fitting = false, featuresDataType)\n+  }\n+\n+}\n+\n+/**\n+ * :: Experimental ::\n+ *\n+ * Reduction of Multiclass Classification to Binary Classification.\n+ * Performs reduction using one against all strategy.\n+ * For a multiclass classification with k classes, train k models (one per class).\n+ * Each example is scored against all k models and the model with highest score\n+ * is picked to label the example.\n+ *\n+ */\n+class OneVsRestClassifier extends Estimator[OneVsRestModel]\n+  with OneVsRestParams {\n+\n+  @DeveloperApi\n+  protected def featuresDataType: DataType = new VectorUDT\n+\n+  /** @group setParam */\n+  def setBaseClassifier(value: ClassifierType): this.type = set(baseClassifier, value)\n+\n+  override def fit(dataset: DataFrame): OneVsRestModel = {\n+\n+    // determine number of classes either from metadata if provided, or via computation.\n+    val labelSchema = dataset.schema($(labelCol))\n+    val computeNumClasses: () => Int = () => {\n+      dataset.select($(labelCol)).distinct.count().toInt\n+    }\n+    val numClasses = MetadataUtils.getNumClasses(labelSchema).fold(computeNumClasses())(identity)\n+\n+    val multiclassLabeled = dataset.select($(labelCol), $(featuresCol))\n+\n+    // persist if underlying dataset is not persistent.\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    if (handlePersistence) {\n+      multiclassLabeled.persist(StorageLevel.MEMORY_AND_DISK)\n+    }\n+\n+    // create k columns, one for each binary classifier.\n+    val models = Range(0, numClasses).par.map { index =>\n+      val labelColName = \"mc2b$\" + index\n+      val label: Double => Double = (label: Double) => {\n+        if (label.toInt == index) 1.0 else 0.0\n+      }\n+\n+      // generate new label for each binary classifier.\n+      // generate new label metadata for the binary problem.\n+      val labelUDF = callUDF(label, DoubleType, col($(labelCol)))\n+      val newLabelMeta = BinaryAttribute.defaultAttr.withName(\"label\").toMetadata()\n+      val labelUDFWithNewMeta = labelUDF.as(labelColName, newLabelMeta)\n+      val trainingDataset = multiclassLabeled.withColumn(labelColName, labelUDFWithNewMeta)\n+      val classifier = getBaseClassifier\n+      classifier.fit(trainingDataset, classifier.labelCol -> labelColName)\n+    }.toArray[Model[_]]\n+\n+    if (handlePersistence) {\n+      multiclassLabeled.unpersist()\n+    }\n+\n+    new OneVsRestModel(this, models)"
  }],
  "prId": 5830
}]