[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "private?",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-16T13:56:49Z",
    "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",",
    "line": 44
  }, {
    "author": {
      "login": "zhengruifeng"
    },
    "body": "It should not be private, this param is expected to be exposed to users.",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-17T03:11:29Z",
    "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",",
    "line": 44
  }],
  "prId": 25160
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: you probably want to clarify what 'scales' means here. You divide through by the IQR?\r\nAlso the IQR isn't necessarily 25%-75% because it's configurable.",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-16T13:59:59Z",
    "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to interquartile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to interquartile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * This Scaler removes the median and scales the data according to the quantile range"
  }, {
    "author": {
      "login": "zhengruifeng"
    },
    "body": "Yes, after optional remove the median, the features will be divided by the quantile range.\r\nIQR is a special quantile range, from 25% to 75%. But if the `lower`/`upper` are set to other values, then the range is not IQR.",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-17T03:17:17Z",
    "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to interquartile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to interquartile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * This Scaler removes the median and scales the data according to the quantile range"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "OK can you say it's IQR by default but can be configured, something like that?",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-17T12:46:06Z",
    "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to interquartile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to interquartile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * This Scaler removes the median and scales the data according to the quantile range"
  }],
  "prId": 25160
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "This might be clearer as:\r\n```\r\nif (localAgg == null) {\r\n  Iterator.empty\r\n} else {\r\n  ...\r\n}\r\n```",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-16T14:00:42Z",
    "diffHunk": "@@ -0,0 +1,290 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to interquartile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to interquartile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * This Scaler removes the median and scales the data according to the quantile range\n+ * (defaults to IQR: Interquartile Range). The IQR is the range between the 1st quartile\n+ * (25th quantile) and the 3rd quartile (75th quantile).\n+ * Centering and scaling happen independently on each feature by computing the relevant\n+ * statistics on the samples in the training set. Median and interquartile range are then\n+ * stored to be used on later data using the transform method.\n+ * Standardization of a dataset is a common requirement for many machine learning estimators.\n+ * Typically this is done by removing the mean and scaling to unit variance. However,\n+ * outliers can often influence the sample mean / variance in a negative way.\n+ * In such cases, the median and the interquartile range often give better results.\n+ */\n+@Since(\"3.0.0\")\n+class RobustScaler (override val uid: String)\n+  extends Estimator[RobustScalerModel] with RobustScalerParams with DefaultParamsWritable {\n+\n+  def this() = this(Identifiable.randomUID(\"robustScal\"))\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  /** @group setParam */\n+  def setLower(value: Double): this.type = set(lower, value)\n+\n+  /** @group setParam */\n+  def setUpper(value: Double): this.type = set(upper, value)\n+\n+  /** @group setParam */\n+  def setWithCentering(value: Boolean): this.type = set(withCentering, value)\n+\n+  /** @group setParam */\n+  def setWithScaling(value: Boolean): this.type = set(withScaling, value)\n+\n+  override def fit(dataset: Dataset[_]): RobustScalerModel = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val summaries = dataset.select($(inputCol)).rdd.map {\n+      case Row(vec: Vector) => vec\n+    }.mapPartitions { iter =>\n+      var localAgg: Array[QuantileSummaries] = null\n+      while (iter.hasNext) {\n+        val vec = iter.next()\n+        if (localAgg == null) {\n+          localAgg = Array.fill(vec.size)(\n+            new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, 0.001))\n+        }\n+        require(vec.size == localAgg.length)\n+        var i = 0\n+        while (i < vec.size) {\n+          localAgg(i) = localAgg(i).insert(vec(i))\n+          i += 1\n+        }\n+      }\n+\n+      if (localAgg != null) {"
  }],
  "prId": 25160
}, {
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "can we move this outside in order to avoid this check every time? we can change this to a `do` ... `while` maybe",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-20T10:02:14Z",
    "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * Lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"Lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * Upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"Upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to quantile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to quantile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * RobustScaler removes the median and scales the data according to the quantile range.\n+ * The quantile range is by default IQR (Interquartile Range, quantile range between the\n+ * 1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured.\n+ * Centering and scaling happen independently on each feature by computing the relevant\n+ * statistics on the samples in the training set. Median and quantile range are then\n+ * stored to be used on later data using the transform method.\n+ * Standardization of a dataset is a common requirement for many machine learning estimators.\n+ * Typically this is done by removing the mean and scaling to unit variance. However,\n+ * outliers can often influence the sample mean / variance in a negative way.\n+ * In such cases, the median and the quantile range often give better results.\n+ */\n+@Since(\"3.0.0\")\n+class RobustScaler (override val uid: String)\n+  extends Estimator[RobustScalerModel] with RobustScalerParams with DefaultParamsWritable {\n+\n+  def this() = this(Identifiable.randomUID(\"robustScal\"))\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  /** @group setParam */\n+  def setLower(value: Double): this.type = set(lower, value)\n+\n+  /** @group setParam */\n+  def setUpper(value: Double): this.type = set(upper, value)\n+\n+  /** @group setParam */\n+  def setWithCentering(value: Boolean): this.type = set(withCentering, value)\n+\n+  /** @group setParam */\n+  def setWithScaling(value: Boolean): this.type = set(withScaling, value)\n+\n+  override def fit(dataset: Dataset[_]): RobustScalerModel = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val summaries = dataset.select($(inputCol)).rdd.map {\n+      case Row(vec: Vector) => vec\n+    }.mapPartitions { iter =>\n+      var agg: Array[QuantileSummaries] = null\n+      while (iter.hasNext) {\n+        val vec = iter.next()\n+        if (agg == null) {",
    "line": 153
  }, {
    "author": {
      "login": "zhengruifeng"
    },
    "body": "we don't know the `numFeatures` before read the first item\r\nThe only approach I found to avoid follwing check is something like this:\r\n```\r\nval agg = \r\nif (iter.hasNext) {\r\nval vec = iter.next;\r\nval agg2 = Array.fill(vec.size)(new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, 0.001))\r\n// update agg2 with first vec\r\nagg2\r\nwhile(iiter.haxNext) {\r\n//update on following vecs\r\n}\r\nagg2\r\n} else {\r\nnull\r\n}\r\n```\r\nbut I think it too complex, so I tend to keep it.\r\n",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-22T09:14:30Z",
    "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * Lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"Lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * Upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"Upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to quantile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to quantile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * RobustScaler removes the median and scales the data according to the quantile range.\n+ * The quantile range is by default IQR (Interquartile Range, quantile range between the\n+ * 1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured.\n+ * Centering and scaling happen independently on each feature by computing the relevant\n+ * statistics on the samples in the training set. Median and quantile range are then\n+ * stored to be used on later data using the transform method.\n+ * Standardization of a dataset is a common requirement for many machine learning estimators.\n+ * Typically this is done by removing the mean and scaling to unit variance. However,\n+ * outliers can often influence the sample mean / variance in a negative way.\n+ * In such cases, the median and the quantile range often give better results.\n+ */\n+@Since(\"3.0.0\")\n+class RobustScaler (override val uid: String)\n+  extends Estimator[RobustScalerModel] with RobustScalerParams with DefaultParamsWritable {\n+\n+  def this() = this(Identifiable.randomUID(\"robustScal\"))\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  /** @group setParam */\n+  def setLower(value: Double): this.type = set(lower, value)\n+\n+  /** @group setParam */\n+  def setUpper(value: Double): this.type = set(upper, value)\n+\n+  /** @group setParam */\n+  def setWithCentering(value: Boolean): this.type = set(withCentering, value)\n+\n+  /** @group setParam */\n+  def setWithScaling(value: Boolean): this.type = set(withScaling, value)\n+\n+  override def fit(dataset: Dataset[_]): RobustScalerModel = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val summaries = dataset.select($(inputCol)).rdd.map {\n+      case Row(vec: Vector) => vec\n+    }.mapPartitions { iter =>\n+      var agg: Array[QuantileSummaries] = null\n+      while (iter.hasNext) {\n+        val vec = iter.next()\n+        if (agg == null) {",
    "line": 153
  }],
  "prId": 25160
}, {
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "can we add a meaningful error message here?",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-20T10:05:19Z",
    "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * Lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"Lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * Upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"Upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to quantile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to quantile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * RobustScaler removes the median and scales the data according to the quantile range.\n+ * The quantile range is by default IQR (Interquartile Range, quantile range between the\n+ * 1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured.\n+ * Centering and scaling happen independently on each feature by computing the relevant\n+ * statistics on the samples in the training set. Median and quantile range are then\n+ * stored to be used on later data using the transform method.\n+ * Standardization of a dataset is a common requirement for many machine learning estimators.\n+ * Typically this is done by removing the mean and scaling to unit variance. However,\n+ * outliers can often influence the sample mean / variance in a negative way.\n+ * In such cases, the median and the quantile range often give better results.\n+ */\n+@Since(\"3.0.0\")\n+class RobustScaler (override val uid: String)\n+  extends Estimator[RobustScalerModel] with RobustScalerParams with DefaultParamsWritable {\n+\n+  def this() = this(Identifiable.randomUID(\"robustScal\"))\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  /** @group setParam */\n+  def setLower(value: Double): this.type = set(lower, value)\n+\n+  /** @group setParam */\n+  def setUpper(value: Double): this.type = set(upper, value)\n+\n+  /** @group setParam */\n+  def setWithCentering(value: Boolean): this.type = set(withCentering, value)\n+\n+  /** @group setParam */\n+  def setWithScaling(value: Boolean): this.type = set(withScaling, value)\n+\n+  override def fit(dataset: Dataset[_]): RobustScalerModel = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val summaries = dataset.select($(inputCol)).rdd.map {\n+      case Row(vec: Vector) => vec\n+    }.mapPartitions { iter =>\n+      var agg: Array[QuantileSummaries] = null\n+      while (iter.hasNext) {\n+        val vec = iter.next()\n+        if (agg == null) {\n+          agg = Array.fill(vec.size)(\n+            new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, 0.001))\n+        }\n+        require(vec.size == agg.length)"
  }],
  "prId": 25160
}, {
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "`agg.map(_.compress())`?",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-20T10:08:04Z",
    "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * Lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"Lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * Upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"Upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to quantile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to quantile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * RobustScaler removes the median and scales the data according to the quantile range.\n+ * The quantile range is by default IQR (Interquartile Range, quantile range between the\n+ * 1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured.\n+ * Centering and scaling happen independently on each feature by computing the relevant\n+ * statistics on the samples in the training set. Median and quantile range are then\n+ * stored to be used on later data using the transform method.\n+ * Standardization of a dataset is a common requirement for many machine learning estimators.\n+ * Typically this is done by removing the mean and scaling to unit variance. However,\n+ * outliers can often influence the sample mean / variance in a negative way.\n+ * In such cases, the median and the quantile range often give better results.\n+ */\n+@Since(\"3.0.0\")\n+class RobustScaler (override val uid: String)\n+  extends Estimator[RobustScalerModel] with RobustScalerParams with DefaultParamsWritable {\n+\n+  def this() = this(Identifiable.randomUID(\"robustScal\"))\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  /** @group setParam */\n+  def setLower(value: Double): this.type = set(lower, value)\n+\n+  /** @group setParam */\n+  def setUpper(value: Double): this.type = set(upper, value)\n+\n+  /** @group setParam */\n+  def setWithCentering(value: Boolean): this.type = set(withCentering, value)\n+\n+  /** @group setParam */\n+  def setWithScaling(value: Boolean): this.type = set(withScaling, value)\n+\n+  override def fit(dataset: Dataset[_]): RobustScalerModel = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val summaries = dataset.select($(inputCol)).rdd.map {\n+      case Row(vec: Vector) => vec\n+    }.mapPartitions { iter =>\n+      var agg: Array[QuantileSummaries] = null\n+      while (iter.hasNext) {\n+        val vec = iter.next()\n+        if (agg == null) {\n+          agg = Array.fill(vec.size)(\n+            new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, 0.001))\n+        }\n+        require(vec.size == agg.length)\n+        var i = 0\n+        while (i < vec.size) {\n+          agg(i) = agg(i).insert(vec(i))\n+          i += 1\n+        }\n+      }\n+\n+      if (agg == null) {\n+        Iterator.empty\n+      } else {\n+        var i = 0"
  }, {
    "author": {
      "login": "zhengruifeng"
    },
    "body": "I tend to keep current impl, since this can avoid create a tmp array, and should be a little faster",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-22T09:23:26Z",
    "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * Lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"Lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * Upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"Upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to quantile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to quantile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * RobustScaler removes the median and scales the data according to the quantile range.\n+ * The quantile range is by default IQR (Interquartile Range, quantile range between the\n+ * 1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured.\n+ * Centering and scaling happen independently on each feature by computing the relevant\n+ * statistics on the samples in the training set. Median and quantile range are then\n+ * stored to be used on later data using the transform method.\n+ * Standardization of a dataset is a common requirement for many machine learning estimators.\n+ * Typically this is done by removing the mean and scaling to unit variance. However,\n+ * outliers can often influence the sample mean / variance in a negative way.\n+ * In such cases, the median and the quantile range often give better results.\n+ */\n+@Since(\"3.0.0\")\n+class RobustScaler (override val uid: String)\n+  extends Estimator[RobustScalerModel] with RobustScalerParams with DefaultParamsWritable {\n+\n+  def this() = this(Identifiable.randomUID(\"robustScal\"))\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  /** @group setParam */\n+  def setLower(value: Double): this.type = set(lower, value)\n+\n+  /** @group setParam */\n+  def setUpper(value: Double): this.type = set(upper, value)\n+\n+  /** @group setParam */\n+  def setWithCentering(value: Boolean): this.type = set(withCentering, value)\n+\n+  /** @group setParam */\n+  def setWithScaling(value: Boolean): this.type = set(withScaling, value)\n+\n+  override def fit(dataset: Dataset[_]): RobustScalerModel = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val summaries = dataset.select($(inputCol)).rdd.map {\n+      case Row(vec: Vector) => vec\n+    }.mapPartitions { iter =>\n+      var agg: Array[QuantileSummaries] = null\n+      while (iter.hasNext) {\n+        val vec = iter.next()\n+        if (agg == null) {\n+          agg = Array.fill(vec.size)(\n+            new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, 0.001))\n+        }\n+        require(vec.size == agg.length)\n+        var i = 0\n+        while (i < vec.size) {\n+          agg(i) = agg(i).insert(vec(i))\n+          i += 1\n+        }\n+      }\n+\n+      if (agg == null) {\n+        Iterator.empty\n+      } else {\n+        var i = 0"
  }, {
    "author": {
      "login": "mgaido91"
    },
    "body": "I think the perf gain is quite irrelevant compared to the operations and the objects created in compress...",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-22T09:30:31Z",
    "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * Lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"Lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * Upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"Upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to quantile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to quantile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * RobustScaler removes the median and scales the data according to the quantile range.\n+ * The quantile range is by default IQR (Interquartile Range, quantile range between the\n+ * 1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured.\n+ * Centering and scaling happen independently on each feature by computing the relevant\n+ * statistics on the samples in the training set. Median and quantile range are then\n+ * stored to be used on later data using the transform method.\n+ * Standardization of a dataset is a common requirement for many machine learning estimators.\n+ * Typically this is done by removing the mean and scaling to unit variance. However,\n+ * outliers can often influence the sample mean / variance in a negative way.\n+ * In such cases, the median and the quantile range often give better results.\n+ */\n+@Since(\"3.0.0\")\n+class RobustScaler (override val uid: String)\n+  extends Estimator[RobustScalerModel] with RobustScalerParams with DefaultParamsWritable {\n+\n+  def this() = this(Identifiable.randomUID(\"robustScal\"))\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  /** @group setParam */\n+  def setLower(value: Double): this.type = set(lower, value)\n+\n+  /** @group setParam */\n+  def setUpper(value: Double): this.type = set(upper, value)\n+\n+  /** @group setParam */\n+  def setWithCentering(value: Boolean): this.type = set(withCentering, value)\n+\n+  /** @group setParam */\n+  def setWithScaling(value: Boolean): this.type = set(withScaling, value)\n+\n+  override def fit(dataset: Dataset[_]): RobustScalerModel = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val summaries = dataset.select($(inputCol)).rdd.map {\n+      case Row(vec: Vector) => vec\n+    }.mapPartitions { iter =>\n+      var agg: Array[QuantileSummaries] = null\n+      while (iter.hasNext) {\n+        val vec = iter.next()\n+        if (agg == null) {\n+          agg = Array.fill(vec.size)(\n+            new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, 0.001))\n+        }\n+        require(vec.size == agg.length)\n+        var i = 0\n+        while (i < vec.size) {\n+          agg(i) = agg(i).insert(vec(i))\n+          i += 1\n+        }\n+      }\n+\n+      if (agg == null) {\n+        Iterator.empty\n+      } else {\n+        var i = 0"
  }],
  "prId": 25160
}, {
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "why are you adding `compress` here? AFAIK it is not needed here",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-20T10:09:27Z",
    "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * Lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"Lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * Upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"Upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to quantile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to quantile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * RobustScaler removes the median and scales the data according to the quantile range.\n+ * The quantile range is by default IQR (Interquartile Range, quantile range between the\n+ * 1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured.\n+ * Centering and scaling happen independently on each feature by computing the relevant\n+ * statistics on the samples in the training set. Median and quantile range are then\n+ * stored to be used on later data using the transform method.\n+ * Standardization of a dataset is a common requirement for many machine learning estimators.\n+ * Typically this is done by removing the mean and scaling to unit variance. However,\n+ * outliers can often influence the sample mean / variance in a negative way.\n+ * In such cases, the median and the quantile range often give better results.\n+ */\n+@Since(\"3.0.0\")\n+class RobustScaler (override val uid: String)\n+  extends Estimator[RobustScalerModel] with RobustScalerParams with DefaultParamsWritable {\n+\n+  def this() = this(Identifiable.randomUID(\"robustScal\"))\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  /** @group setParam */\n+  def setLower(value: Double): this.type = set(lower, value)\n+\n+  /** @group setParam */\n+  def setUpper(value: Double): this.type = set(upper, value)\n+\n+  /** @group setParam */\n+  def setWithCentering(value: Boolean): this.type = set(withCentering, value)\n+\n+  /** @group setParam */\n+  def setWithScaling(value: Boolean): this.type = set(withScaling, value)\n+\n+  override def fit(dataset: Dataset[_]): RobustScalerModel = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val summaries = dataset.select($(inputCol)).rdd.map {\n+      case Row(vec: Vector) => vec\n+    }.mapPartitions { iter =>\n+      var agg: Array[QuantileSummaries] = null\n+      while (iter.hasNext) {\n+        val vec = iter.next()\n+        if (agg == null) {\n+          agg = Array.fill(vec.size)(\n+            new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, 0.001))\n+        }\n+        require(vec.size == agg.length)\n+        var i = 0\n+        while (i < vec.size) {\n+          agg(i) = agg(i).insert(vec(i))\n+          i += 1\n+        }\n+      }\n+\n+      if (agg == null) {\n+        Iterator.empty\n+      } else {\n+        var i = 0\n+        while (i < agg.length) {\n+          agg(i) = agg(i).compress()\n+          i += 1\n+        }\n+        Iterator.single(agg)\n+      }\n+    }.treeReduce { (agg1, agg2) =>\n+      require(agg1.length == agg2.length)\n+      var i = 0\n+      while (i < agg1.length) {\n+        agg1(i) = agg1(i).merge(agg2(i)).compress()"
  }, {
    "author": {
      "login": "zhengruifeng"
    },
    "body": "Yes, I added this to confirm compression, I will remove it.",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-22T09:24:00Z",
    "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * Lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"Lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * Upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"Upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to quantile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to quantile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * RobustScaler removes the median and scales the data according to the quantile range.\n+ * The quantile range is by default IQR (Interquartile Range, quantile range between the\n+ * 1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured.\n+ * Centering and scaling happen independently on each feature by computing the relevant\n+ * statistics on the samples in the training set. Median and quantile range are then\n+ * stored to be used on later data using the transform method.\n+ * Standardization of a dataset is a common requirement for many machine learning estimators.\n+ * Typically this is done by removing the mean and scaling to unit variance. However,\n+ * outliers can often influence the sample mean / variance in a negative way.\n+ * In such cases, the median and the quantile range often give better results.\n+ */\n+@Since(\"3.0.0\")\n+class RobustScaler (override val uid: String)\n+  extends Estimator[RobustScalerModel] with RobustScalerParams with DefaultParamsWritable {\n+\n+  def this() = this(Identifiable.randomUID(\"robustScal\"))\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  /** @group setParam */\n+  def setLower(value: Double): this.type = set(lower, value)\n+\n+  /** @group setParam */\n+  def setUpper(value: Double): this.type = set(upper, value)\n+\n+  /** @group setParam */\n+  def setWithCentering(value: Boolean): this.type = set(withCentering, value)\n+\n+  /** @group setParam */\n+  def setWithScaling(value: Boolean): this.type = set(withScaling, value)\n+\n+  override def fit(dataset: Dataset[_]): RobustScalerModel = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val summaries = dataset.select($(inputCol)).rdd.map {\n+      case Row(vec: Vector) => vec\n+    }.mapPartitions { iter =>\n+      var agg: Array[QuantileSummaries] = null\n+      while (iter.hasNext) {\n+        val vec = iter.next()\n+        if (agg == null) {\n+          agg = Array.fill(vec.size)(\n+            new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, 0.001))\n+        }\n+        require(vec.size == agg.length)\n+        var i = 0\n+        while (i < vec.size) {\n+          agg(i) = agg(i).insert(vec(i))\n+          i += 1\n+        }\n+      }\n+\n+      if (agg == null) {\n+        Iterator.empty\n+      } else {\n+        var i = 0\n+        while (i < agg.length) {\n+          agg(i) = agg(i).compress()\n+          i += 1\n+        }\n+        Iterator.single(agg)\n+      }\n+    }.treeReduce { (agg1, agg2) =>\n+      require(agg1.length == agg2.length)\n+      var i = 0\n+      while (i < agg1.length) {\n+        agg1(i) = agg1(i).merge(agg2(i)).compress()"
  }],
  "prId": 25160
}, {
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "nit:\r\n```\r\nval func = StandardScalerModel.getTransformFunc(\r\n  shift, scale, $(withCentering), $(withScaling))\r\n```",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-20T10:12:22Z",
    "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * Lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"Lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * Upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"Upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to quantile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to quantile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * RobustScaler removes the median and scales the data according to the quantile range.\n+ * The quantile range is by default IQR (Interquartile Range, quantile range between the\n+ * 1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured.\n+ * Centering and scaling happen independently on each feature by computing the relevant\n+ * statistics on the samples in the training set. Median and quantile range are then\n+ * stored to be used on later data using the transform method.\n+ * Standardization of a dataset is a common requirement for many machine learning estimators.\n+ * Typically this is done by removing the mean and scaling to unit variance. However,\n+ * outliers can often influence the sample mean / variance in a negative way.\n+ * In such cases, the median and the quantile range often give better results.\n+ */\n+@Since(\"3.0.0\")\n+class RobustScaler (override val uid: String)\n+  extends Estimator[RobustScalerModel] with RobustScalerParams with DefaultParamsWritable {\n+\n+  def this() = this(Identifiable.randomUID(\"robustScal\"))\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  /** @group setParam */\n+  def setLower(value: Double): this.type = set(lower, value)\n+\n+  /** @group setParam */\n+  def setUpper(value: Double): this.type = set(upper, value)\n+\n+  /** @group setParam */\n+  def setWithCentering(value: Boolean): this.type = set(withCentering, value)\n+\n+  /** @group setParam */\n+  def setWithScaling(value: Boolean): this.type = set(withScaling, value)\n+\n+  override def fit(dataset: Dataset[_]): RobustScalerModel = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val summaries = dataset.select($(inputCol)).rdd.map {\n+      case Row(vec: Vector) => vec\n+    }.mapPartitions { iter =>\n+      var agg: Array[QuantileSummaries] = null\n+      while (iter.hasNext) {\n+        val vec = iter.next()\n+        if (agg == null) {\n+          agg = Array.fill(vec.size)(\n+            new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, 0.001))\n+        }\n+        require(vec.size == agg.length)\n+        var i = 0\n+        while (i < vec.size) {\n+          agg(i) = agg(i).insert(vec(i))\n+          i += 1\n+        }\n+      }\n+\n+      if (agg == null) {\n+        Iterator.empty\n+      } else {\n+        var i = 0\n+        while (i < agg.length) {\n+          agg(i) = agg(i).compress()\n+          i += 1\n+        }\n+        Iterator.single(agg)\n+      }\n+    }.treeReduce { (agg1, agg2) =>\n+      require(agg1.length == agg2.length)\n+      var i = 0\n+      while (i < agg1.length) {\n+        agg1(i) = agg1(i).merge(agg2(i)).compress()\n+        i += 1\n+      }\n+      agg1\n+    }\n+\n+    val (range, median) = summaries.map { s =>\n+      (s.query($(upper)).get - s.query($(lower)).get,\n+        s.query(0.5).get)\n+    }.unzip\n+\n+    copyValues(new RobustScalerModel(uid, Vectors.dense(range).compressed,\n+      Vectors.dense(median).compressed).setParent(this))\n+  }\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): RobustScaler = defaultCopy(extra)\n+}\n+\n+@Since(\"3.0.0\")\n+object RobustScaler extends DefaultParamsReadable[RobustScaler] {\n+\n+  override def load(path: String): RobustScaler = super.load(path)\n+}\n+\n+/**\n+ * Model fitted by [[RobustScaler]].\n+ *\n+ * @param range quantile range for each original column during fitting\n+ * @param median median value for each original column during fitting\n+ */\n+@Since(\"3.0.0\")\n+class RobustScalerModel private[ml] (\n+    override val uid: String,\n+    val range: Vector,\n+    val median: Vector)\n+  extends Model[RobustScalerModel] with RobustScalerParams with MLWritable {\n+\n+  import RobustScalerModel._\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val shift = if ($(withCentering)) median.toArray else Array.emptyDoubleArray\n+    val scale = if ($(withScaling)) {\n+      range.toArray.map { v => if (v == 0) 0.0 else 1.0 / v }\n+    } else Array.emptyDoubleArray\n+\n+    val func = StandardScalerModel.getTransformFunc(shift, scale,"
  }, {
    "author": {
      "login": "zhengruifeng"
    },
    "body": "I am not sure, but is it a convention? It is easy to find similar indent/style other places.",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-22T09:27:19Z",
    "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * Lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"Lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * Upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"Upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to quantile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to quantile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * RobustScaler removes the median and scales the data according to the quantile range.\n+ * The quantile range is by default IQR (Interquartile Range, quantile range between the\n+ * 1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured.\n+ * Centering and scaling happen independently on each feature by computing the relevant\n+ * statistics on the samples in the training set. Median and quantile range are then\n+ * stored to be used on later data using the transform method.\n+ * Standardization of a dataset is a common requirement for many machine learning estimators.\n+ * Typically this is done by removing the mean and scaling to unit variance. However,\n+ * outliers can often influence the sample mean / variance in a negative way.\n+ * In such cases, the median and the quantile range often give better results.\n+ */\n+@Since(\"3.0.0\")\n+class RobustScaler (override val uid: String)\n+  extends Estimator[RobustScalerModel] with RobustScalerParams with DefaultParamsWritable {\n+\n+  def this() = this(Identifiable.randomUID(\"robustScal\"))\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  /** @group setParam */\n+  def setLower(value: Double): this.type = set(lower, value)\n+\n+  /** @group setParam */\n+  def setUpper(value: Double): this.type = set(upper, value)\n+\n+  /** @group setParam */\n+  def setWithCentering(value: Boolean): this.type = set(withCentering, value)\n+\n+  /** @group setParam */\n+  def setWithScaling(value: Boolean): this.type = set(withScaling, value)\n+\n+  override def fit(dataset: Dataset[_]): RobustScalerModel = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val summaries = dataset.select($(inputCol)).rdd.map {\n+      case Row(vec: Vector) => vec\n+    }.mapPartitions { iter =>\n+      var agg: Array[QuantileSummaries] = null\n+      while (iter.hasNext) {\n+        val vec = iter.next()\n+        if (agg == null) {\n+          agg = Array.fill(vec.size)(\n+            new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, 0.001))\n+        }\n+        require(vec.size == agg.length)\n+        var i = 0\n+        while (i < vec.size) {\n+          agg(i) = agg(i).insert(vec(i))\n+          i += 1\n+        }\n+      }\n+\n+      if (agg == null) {\n+        Iterator.empty\n+      } else {\n+        var i = 0\n+        while (i < agg.length) {\n+          agg(i) = agg(i).compress()\n+          i += 1\n+        }\n+        Iterator.single(agg)\n+      }\n+    }.treeReduce { (agg1, agg2) =>\n+      require(agg1.length == agg2.length)\n+      var i = 0\n+      while (i < agg1.length) {\n+        agg1(i) = agg1(i).merge(agg2(i)).compress()\n+        i += 1\n+      }\n+      agg1\n+    }\n+\n+    val (range, median) = summaries.map { s =>\n+      (s.query($(upper)).get - s.query($(lower)).get,\n+        s.query(0.5).get)\n+    }.unzip\n+\n+    copyValues(new RobustScalerModel(uid, Vectors.dense(range).compressed,\n+      Vectors.dense(median).compressed).setParent(this))\n+  }\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): RobustScaler = defaultCopy(extra)\n+}\n+\n+@Since(\"3.0.0\")\n+object RobustScaler extends DefaultParamsReadable[RobustScaler] {\n+\n+  override def load(path: String): RobustScaler = super.load(path)\n+}\n+\n+/**\n+ * Model fitted by [[RobustScaler]].\n+ *\n+ * @param range quantile range for each original column during fitting\n+ * @param median median value for each original column during fitting\n+ */\n+@Since(\"3.0.0\")\n+class RobustScalerModel private[ml] (\n+    override val uid: String,\n+    val range: Vector,\n+    val median: Vector)\n+  extends Model[RobustScalerModel] with RobustScalerParams with MLWritable {\n+\n+  import RobustScalerModel._\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val shift = if ($(withCentering)) median.toArray else Array.emptyDoubleArray\n+    val scale = if ($(withScaling)) {\n+      range.toArray.map { v => if (v == 0) 0.0 else 1.0 / v }\n+    } else Array.emptyDoubleArray\n+\n+    val func = StandardScalerModel.getTransformFunc(shift, scale,"
  }, {
    "author": {
      "login": "mgaido91"
    },
    "body": "I always saw the above mentioned style. Maybe SQL part is more strict on this.",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-22T09:31:28Z",
    "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * Lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"Lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * Upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"Upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to quantile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to quantile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * RobustScaler removes the median and scales the data according to the quantile range.\n+ * The quantile range is by default IQR (Interquartile Range, quantile range between the\n+ * 1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured.\n+ * Centering and scaling happen independently on each feature by computing the relevant\n+ * statistics on the samples in the training set. Median and quantile range are then\n+ * stored to be used on later data using the transform method.\n+ * Standardization of a dataset is a common requirement for many machine learning estimators.\n+ * Typically this is done by removing the mean and scaling to unit variance. However,\n+ * outliers can often influence the sample mean / variance in a negative way.\n+ * In such cases, the median and the quantile range often give better results.\n+ */\n+@Since(\"3.0.0\")\n+class RobustScaler (override val uid: String)\n+  extends Estimator[RobustScalerModel] with RobustScalerParams with DefaultParamsWritable {\n+\n+  def this() = this(Identifiable.randomUID(\"robustScal\"))\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  /** @group setParam */\n+  def setLower(value: Double): this.type = set(lower, value)\n+\n+  /** @group setParam */\n+  def setUpper(value: Double): this.type = set(upper, value)\n+\n+  /** @group setParam */\n+  def setWithCentering(value: Boolean): this.type = set(withCentering, value)\n+\n+  /** @group setParam */\n+  def setWithScaling(value: Boolean): this.type = set(withScaling, value)\n+\n+  override def fit(dataset: Dataset[_]): RobustScalerModel = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val summaries = dataset.select($(inputCol)).rdd.map {\n+      case Row(vec: Vector) => vec\n+    }.mapPartitions { iter =>\n+      var agg: Array[QuantileSummaries] = null\n+      while (iter.hasNext) {\n+        val vec = iter.next()\n+        if (agg == null) {\n+          agg = Array.fill(vec.size)(\n+            new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, 0.001))\n+        }\n+        require(vec.size == agg.length)\n+        var i = 0\n+        while (i < vec.size) {\n+          agg(i) = agg(i).insert(vec(i))\n+          i += 1\n+        }\n+      }\n+\n+      if (agg == null) {\n+        Iterator.empty\n+      } else {\n+        var i = 0\n+        while (i < agg.length) {\n+          agg(i) = agg(i).compress()\n+          i += 1\n+        }\n+        Iterator.single(agg)\n+      }\n+    }.treeReduce { (agg1, agg2) =>\n+      require(agg1.length == agg2.length)\n+      var i = 0\n+      while (i < agg1.length) {\n+        agg1(i) = agg1(i).merge(agg2(i)).compress()\n+        i += 1\n+      }\n+      agg1\n+    }\n+\n+    val (range, median) = summaries.map { s =>\n+      (s.query($(upper)).get - s.query($(lower)).get,\n+        s.query(0.5).get)\n+    }.unzip\n+\n+    copyValues(new RobustScalerModel(uid, Vectors.dense(range).compressed,\n+      Vectors.dense(median).compressed).setParent(this))\n+  }\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): RobustScaler = defaultCopy(extra)\n+}\n+\n+@Since(\"3.0.0\")\n+object RobustScaler extends DefaultParamsReadable[RobustScaler] {\n+\n+  override def load(path: String): RobustScaler = super.load(path)\n+}\n+\n+/**\n+ * Model fitted by [[RobustScaler]].\n+ *\n+ * @param range quantile range for each original column during fitting\n+ * @param median median value for each original column during fitting\n+ */\n+@Since(\"3.0.0\")\n+class RobustScalerModel private[ml] (\n+    override val uid: String,\n+    val range: Vector,\n+    val median: Vector)\n+  extends Model[RobustScalerModel] with RobustScalerParams with MLWritable {\n+\n+  import RobustScalerModel._\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val shift = if ($(withCentering)) median.toArray else Array.emptyDoubleArray\n+    val scale = if ($(withScaling)) {\n+      range.toArray.map { v => if (v == 0) 0.0 else 1.0 / v }\n+    } else Array.emptyDoubleArray\n+\n+    val func = StandardScalerModel.getTransformFunc(shift, scale,"
  }, {
    "author": {
      "login": "zhengruifeng"
    },
    "body": "I am neutral on this, and will follow you advice.",
    "commit": "45aa724d3f352c265840817d1b690162865f3ec1",
    "createdAt": "2019-07-22T09:39:16Z",
    "diffHunk": "@@ -0,0 +1,292 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.feature\n+\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.ml.util._\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.catalyst.util.QuantileSummaries\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types.{StructField, StructType}\n+\n+/**\n+ * Params for [[RobustScaler]] and [[RobustScalerModel]].\n+ */\n+private[feature] trait RobustScalerParams extends Params with HasInputCol with HasOutputCol {\n+\n+  /**\n+   * Lower quantile to calculate quantile range, shared by all features\n+   * Default: 0.25\n+   * @group param\n+   */\n+  val lower: DoubleParam = new DoubleParam(this, \"lower\",\n+    \"Lower quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getLower: Double = $(lower)\n+\n+  setDefault(lower -> 0.25)\n+\n+  /**\n+   * Upper quantile to calculate quantile range, shared by all features\n+   * Default: 0.75\n+   * @group param\n+   */\n+  val upper: DoubleParam = new DoubleParam(this, \"upper\",\n+    \"Upper quantile to calculate quantile range\",\n+    ParamValidators.inRange(0, 1, false, false))\n+\n+  /** @group getParam */\n+  def getUpper: Double = $(upper)\n+\n+  setDefault(upper -> 0.75)\n+\n+  /**\n+   * Whether to center the data with median before scaling.\n+   * It will build a dense output, so take care when applying to sparse input.\n+   * Default: false\n+   * @group param\n+   */\n+  val withCentering: BooleanParam = new BooleanParam(this, \"withCentering\",\n+    \"Whether to center data with median\")\n+\n+  /** @group getParam */\n+  def getWithCentering: Boolean = $(withCentering)\n+\n+  setDefault(withCentering -> false)\n+\n+  /**\n+   * Whether to scale the data to quantile range.\n+   * Default: true\n+   * @group param\n+   */\n+  val withScaling: BooleanParam = new BooleanParam(this, \"withScaling\",\n+    \"Whether to scale the data to quantile range\")\n+\n+  /** @group getParam */\n+  def getWithScaling: Boolean = $(withScaling)\n+\n+  setDefault(withScaling -> true)\n+\n+  /** Validates and transforms the input schema. */\n+  protected def validateAndTransformSchema(schema: StructType): StructType = {\n+    require($(lower) < $(upper), s\"The specified lower quantile(${$(lower)}) is \" +\n+      s\"larger or equal to upper quantile(${$(upper)})\")\n+    SchemaUtils.checkColumnType(schema, $(inputCol), new VectorUDT)\n+    require(!schema.fieldNames.contains($(outputCol)),\n+      s\"Output column ${$(outputCol)} already exists.\")\n+    val outputFields = schema.fields :+ StructField($(outputCol), new VectorUDT, false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+\n+/**\n+ * Scale features using statistics that are robust to outliers.\n+ * RobustScaler removes the median and scales the data according to the quantile range.\n+ * The quantile range is by default IQR (Interquartile Range, quantile range between the\n+ * 1st quartile = 25th quantile and the 3rd quartile = 75th quantile) but can be configured.\n+ * Centering and scaling happen independently on each feature by computing the relevant\n+ * statistics on the samples in the training set. Median and quantile range are then\n+ * stored to be used on later data using the transform method.\n+ * Standardization of a dataset is a common requirement for many machine learning estimators.\n+ * Typically this is done by removing the mean and scaling to unit variance. However,\n+ * outliers can often influence the sample mean / variance in a negative way.\n+ * In such cases, the median and the quantile range often give better results.\n+ */\n+@Since(\"3.0.0\")\n+class RobustScaler (override val uid: String)\n+  extends Estimator[RobustScalerModel] with RobustScalerParams with DefaultParamsWritable {\n+\n+  def this() = this(Identifiable.randomUID(\"robustScal\"))\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  /** @group setParam */\n+  def setLower(value: Double): this.type = set(lower, value)\n+\n+  /** @group setParam */\n+  def setUpper(value: Double): this.type = set(upper, value)\n+\n+  /** @group setParam */\n+  def setWithCentering(value: Boolean): this.type = set(withCentering, value)\n+\n+  /** @group setParam */\n+  def setWithScaling(value: Boolean): this.type = set(withScaling, value)\n+\n+  override def fit(dataset: Dataset[_]): RobustScalerModel = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val summaries = dataset.select($(inputCol)).rdd.map {\n+      case Row(vec: Vector) => vec\n+    }.mapPartitions { iter =>\n+      var agg: Array[QuantileSummaries] = null\n+      while (iter.hasNext) {\n+        val vec = iter.next()\n+        if (agg == null) {\n+          agg = Array.fill(vec.size)(\n+            new QuantileSummaries(QuantileSummaries.defaultCompressThreshold, 0.001))\n+        }\n+        require(vec.size == agg.length)\n+        var i = 0\n+        while (i < vec.size) {\n+          agg(i) = agg(i).insert(vec(i))\n+          i += 1\n+        }\n+      }\n+\n+      if (agg == null) {\n+        Iterator.empty\n+      } else {\n+        var i = 0\n+        while (i < agg.length) {\n+          agg(i) = agg(i).compress()\n+          i += 1\n+        }\n+        Iterator.single(agg)\n+      }\n+    }.treeReduce { (agg1, agg2) =>\n+      require(agg1.length == agg2.length)\n+      var i = 0\n+      while (i < agg1.length) {\n+        agg1(i) = agg1(i).merge(agg2(i)).compress()\n+        i += 1\n+      }\n+      agg1\n+    }\n+\n+    val (range, median) = summaries.map { s =>\n+      (s.query($(upper)).get - s.query($(lower)).get,\n+        s.query(0.5).get)\n+    }.unzip\n+\n+    copyValues(new RobustScalerModel(uid, Vectors.dense(range).compressed,\n+      Vectors.dense(median).compressed).setParent(this))\n+  }\n+\n+  override def transformSchema(schema: StructType): StructType = {\n+    validateAndTransformSchema(schema)\n+  }\n+\n+  override def copy(extra: ParamMap): RobustScaler = defaultCopy(extra)\n+}\n+\n+@Since(\"3.0.0\")\n+object RobustScaler extends DefaultParamsReadable[RobustScaler] {\n+\n+  override def load(path: String): RobustScaler = super.load(path)\n+}\n+\n+/**\n+ * Model fitted by [[RobustScaler]].\n+ *\n+ * @param range quantile range for each original column during fitting\n+ * @param median median value for each original column during fitting\n+ */\n+@Since(\"3.0.0\")\n+class RobustScalerModel private[ml] (\n+    override val uid: String,\n+    val range: Vector,\n+    val median: Vector)\n+  extends Model[RobustScalerModel] with RobustScalerParams with MLWritable {\n+\n+  import RobustScalerModel._\n+\n+  /** @group setParam */\n+  def setInputCol(value: String): this.type = set(inputCol, value)\n+\n+  /** @group setParam */\n+  def setOutputCol(value: String): this.type = set(outputCol, value)\n+\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+\n+    val shift = if ($(withCentering)) median.toArray else Array.emptyDoubleArray\n+    val scale = if ($(withScaling)) {\n+      range.toArray.map { v => if (v == 0) 0.0 else 1.0 / v }\n+    } else Array.emptyDoubleArray\n+\n+    val func = StandardScalerModel.getTransformFunc(shift, scale,"
  }],
  "prId": 25160
}]