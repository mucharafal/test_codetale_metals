[{
  "comments": [{
    "author": {
      "login": "ebernhardson"
    },
    "body": "This doesn't seem right, there is no overlap between the calculation of dcg and max_dcg. The question asked here should be if the label at predicted(i) is \"good\". When treating the labels as binary relevant/not relevant I suppose that might use a threshold, but better would be to move away from a binary dcg and use the full equation from the docblock. I understand though that you are not looking to make major updates to the code from mllib, so it would probably be reasonable for someone to fix this in a followup.",
    "commit": "fa2155af8947347a2fc1e565cf05a19529022266",
    "createdAt": "2017-04-26T03:51:20Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.evaluation\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Column, DataFrame}\n+import org.apache.spark.sql.functions.{mean, sum}\n+import org.apache.spark.sql.functions.udf\n+import org.apache.spark.sql.types.DoubleType\n+\n+@Since(\"2.2.0\")\n+class RankingMetrics(\n+  predictionAndObservations: DataFrame, predictionCol: String, labelCol: String)\n+  extends Logging with Serializable {\n+\n+  /**\n+   * Compute the Mean Percentile Rank (MPR) of all the queries.\n+   *\n+   * See the following paper for detail (\"Expected percentile rank\" in the paper):\n+   * Hu, Y., Y. Koren, and C. Volinsky. “Collaborative Filtering for Implicit Feedback Datasets.”\n+   * In 2008 Eighth IEEE International Conference on Data Mining, 263–72, 2008.\n+   * doi:10.1109/ICDM.2008.22.\n+   *\n+   * @return the mean percentile rank\n+   */\n+  lazy val meanPercentileRank: Double = {\n+\n+    def rank = udf((predicted: Seq[Any], actual: Any) => {\n+      val l_i = predicted.indexOf(actual)\n+\n+      if (l_i == -1) {\n+        1\n+      } else {\n+        l_i.toDouble / predicted.size\n+      }\n+    }, DoubleType)\n+\n+    val R_prime = predictionAndObservations.count()\n+    val predictionColumn: Column = predictionAndObservations.col(predictionCol)\n+    val labelColumn: Column = predictionAndObservations.col(labelCol)\n+\n+    val rankSum: Double = predictionAndObservations\n+      .withColumn(\"rank\", rank(predictionColumn, labelColumn))\n+      .agg(sum(\"rank\")).first().getDouble(0)\n+\n+    rankSum / R_prime\n+  }\n+\n+  /**\n+   * Compute the average precision of all the queries, truncated at ranking position k.\n+   *\n+   * If for a query, the ranking algorithm returns n (n is less than k) results, the precision\n+   * value will be computed as #(relevant items retrieved) / k. This formula also applies when\n+   * the size of the ground truth set is less than k.\n+   *\n+   * If a query has an empty ground truth set, zero will be used as precision together with\n+   * a log warning.\n+   *\n+   * See the following paper for detail:\n+   *\n+   * IR evaluation methods for retrieving highly relevant documents. K. Jarvelin and J. Kekalainen\n+   *\n+   * @param k the position to compute the truncated precision, must be positive\n+   * @return the average precision at the first k ranking positions\n+   */\n+  @Since(\"2.2.0\")\n+  def precisionAt(k: Int): Double = {\n+    require(k > 0, \"ranking position k should be positive\")\n+\n+    def precisionAtK = udf((predicted: Seq[Any], actual: Seq[Any]) => {\n+      val actualSet = actual.toSet\n+      if (actualSet.nonEmpty) {\n+        val n = math.min(predicted.length, k)\n+        var i = 0\n+        var cnt = 0\n+        while (i < n) {\n+          if (actualSet.contains(predicted(i))) {\n+            cnt += 1\n+          }\n+          i += 1\n+        }\n+        cnt.toDouble / k\n+      } else {\n+        logWarning(\"Empty ground truth set, check input data\")\n+        0.0\n+      }\n+    }, DoubleType)\n+\n+    val predictionColumn: Column = predictionAndObservations.col(predictionCol)\n+    val labelColumn: Column = predictionAndObservations.col(labelCol)\n+\n+    predictionAndObservations\n+      .withColumn(\"predictionAtK\", precisionAtK(predictionColumn, labelColumn))\n+      .agg(mean(\"predictionAtK\")).first().getDouble(0)\n+  }\n+\n+  /**\n+   * Returns the mean average precision (MAP) of all the queries.\n+   * If a query has an empty ground truth set, the average precision will be zero and a log\n+   * warning is generated.\n+   */\n+  lazy val meanAveragePrecision: Double = {\n+\n+    def map = udf((predicted: Seq[Any], actual: Seq[Any]) => {\n+      val actualSet = actual.toSet\n+      if (actualSet.nonEmpty) {\n+        var i = 0\n+        var cnt = 0\n+        var precSum = 0.0\n+        val n = predicted.length\n+        while (i < n) {\n+          if (actualSet.contains(predicted(i))) {\n+            cnt += 1\n+            precSum += cnt.toDouble / (i + 1)\n+          }\n+          i += 1\n+        }\n+        precSum / actualSet.size\n+      } else {\n+        logWarning(\"Empty ground truth set, check input data\")\n+        0.0\n+      }\n+    }, DoubleType)\n+\n+    val predictionColumn: Column = predictionAndObservations.col(predictionCol)\n+    val labelColumn: Column = predictionAndObservations.col(labelCol)\n+\n+    predictionAndObservations\n+      .withColumn(\"MAP\", map(predictionColumn, labelColumn))\n+      .agg(mean(\"MAP\")).first().getDouble(0)\n+  }\n+\n+  /**\n+   * Compute the average NDCG value of all the queries, truncated at ranking position k.\n+   * The discounted cumulative gain at position k is computed as:\n+   *    sum,,i=1,,^k^ (2^{relevance of ''i''th item}^ - 1) / log(i + 1),\n+   * and the NDCG is obtained by dividing the DCG value on the ground truth set. In the current\n+   * implementation, the relevance value is binary.\n+\n+   * If a query has an empty ground truth set, zero will be used as ndcg together with\n+   * a log warning.\n+   *\n+   * See the following paper for detail:\n+   *\n+   * IR evaluation methods for retrieving highly relevant documents. K. Jarvelin and J. Kekalainen\n+   *\n+   * @param k the position to compute the truncated ndcg, must be positive\n+   * @return the average ndcg at the first k ranking positions\n+   */\n+  @Since(\"2.2.0\")\n+  def ndcgAt(k: Int): Double = {\n+    require(k > 0, \"ranking position k should be positive\")\n+\n+    def ndcgAtK = udf((predicted: Seq[Any], actual: Seq[Any]) => {\n+      val actualSet = actual.toSet\n+\n+      if (actualSet.nonEmpty) {\n+        val labSetSize = actualSet.size\n+        val n = math.min(math.max(predicted.length, labSetSize), k)\n+        var maxDcg = 0.0\n+        var dcg = 0.0\n+        var i = 0\n+        while (i < n) {\n+          val gain = 1.0 / math.log(i + 2)\n+          if (i < predicted.length && actualSet.contains(predicted(i))) {",
    "line": 180
  }, {
    "author": {
      "login": "daniloascione"
    },
    "body": "Yes, this should be fixed in another PR to keep changes isolated. FYI, the original JIRA for this is [here](https://issues.apache.org/jira/browse/SPARK-3568).",
    "commit": "fa2155af8947347a2fc1e565cf05a19529022266",
    "createdAt": "2017-04-27T15:04:39Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.evaluation\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Column, DataFrame}\n+import org.apache.spark.sql.functions.{mean, sum}\n+import org.apache.spark.sql.functions.udf\n+import org.apache.spark.sql.types.DoubleType\n+\n+@Since(\"2.2.0\")\n+class RankingMetrics(\n+  predictionAndObservations: DataFrame, predictionCol: String, labelCol: String)\n+  extends Logging with Serializable {\n+\n+  /**\n+   * Compute the Mean Percentile Rank (MPR) of all the queries.\n+   *\n+   * See the following paper for detail (\"Expected percentile rank\" in the paper):\n+   * Hu, Y., Y. Koren, and C. Volinsky. “Collaborative Filtering for Implicit Feedback Datasets.”\n+   * In 2008 Eighth IEEE International Conference on Data Mining, 263–72, 2008.\n+   * doi:10.1109/ICDM.2008.22.\n+   *\n+   * @return the mean percentile rank\n+   */\n+  lazy val meanPercentileRank: Double = {\n+\n+    def rank = udf((predicted: Seq[Any], actual: Any) => {\n+      val l_i = predicted.indexOf(actual)\n+\n+      if (l_i == -1) {\n+        1\n+      } else {\n+        l_i.toDouble / predicted.size\n+      }\n+    }, DoubleType)\n+\n+    val R_prime = predictionAndObservations.count()\n+    val predictionColumn: Column = predictionAndObservations.col(predictionCol)\n+    val labelColumn: Column = predictionAndObservations.col(labelCol)\n+\n+    val rankSum: Double = predictionAndObservations\n+      .withColumn(\"rank\", rank(predictionColumn, labelColumn))\n+      .agg(sum(\"rank\")).first().getDouble(0)\n+\n+    rankSum / R_prime\n+  }\n+\n+  /**\n+   * Compute the average precision of all the queries, truncated at ranking position k.\n+   *\n+   * If for a query, the ranking algorithm returns n (n is less than k) results, the precision\n+   * value will be computed as #(relevant items retrieved) / k. This formula also applies when\n+   * the size of the ground truth set is less than k.\n+   *\n+   * If a query has an empty ground truth set, zero will be used as precision together with\n+   * a log warning.\n+   *\n+   * See the following paper for detail:\n+   *\n+   * IR evaluation methods for retrieving highly relevant documents. K. Jarvelin and J. Kekalainen\n+   *\n+   * @param k the position to compute the truncated precision, must be positive\n+   * @return the average precision at the first k ranking positions\n+   */\n+  @Since(\"2.2.0\")\n+  def precisionAt(k: Int): Double = {\n+    require(k > 0, \"ranking position k should be positive\")\n+\n+    def precisionAtK = udf((predicted: Seq[Any], actual: Seq[Any]) => {\n+      val actualSet = actual.toSet\n+      if (actualSet.nonEmpty) {\n+        val n = math.min(predicted.length, k)\n+        var i = 0\n+        var cnt = 0\n+        while (i < n) {\n+          if (actualSet.contains(predicted(i))) {\n+            cnt += 1\n+          }\n+          i += 1\n+        }\n+        cnt.toDouble / k\n+      } else {\n+        logWarning(\"Empty ground truth set, check input data\")\n+        0.0\n+      }\n+    }, DoubleType)\n+\n+    val predictionColumn: Column = predictionAndObservations.col(predictionCol)\n+    val labelColumn: Column = predictionAndObservations.col(labelCol)\n+\n+    predictionAndObservations\n+      .withColumn(\"predictionAtK\", precisionAtK(predictionColumn, labelColumn))\n+      .agg(mean(\"predictionAtK\")).first().getDouble(0)\n+  }\n+\n+  /**\n+   * Returns the mean average precision (MAP) of all the queries.\n+   * If a query has an empty ground truth set, the average precision will be zero and a log\n+   * warning is generated.\n+   */\n+  lazy val meanAveragePrecision: Double = {\n+\n+    def map = udf((predicted: Seq[Any], actual: Seq[Any]) => {\n+      val actualSet = actual.toSet\n+      if (actualSet.nonEmpty) {\n+        var i = 0\n+        var cnt = 0\n+        var precSum = 0.0\n+        val n = predicted.length\n+        while (i < n) {\n+          if (actualSet.contains(predicted(i))) {\n+            cnt += 1\n+            precSum += cnt.toDouble / (i + 1)\n+          }\n+          i += 1\n+        }\n+        precSum / actualSet.size\n+      } else {\n+        logWarning(\"Empty ground truth set, check input data\")\n+        0.0\n+      }\n+    }, DoubleType)\n+\n+    val predictionColumn: Column = predictionAndObservations.col(predictionCol)\n+    val labelColumn: Column = predictionAndObservations.col(labelCol)\n+\n+    predictionAndObservations\n+      .withColumn(\"MAP\", map(predictionColumn, labelColumn))\n+      .agg(mean(\"MAP\")).first().getDouble(0)\n+  }\n+\n+  /**\n+   * Compute the average NDCG value of all the queries, truncated at ranking position k.\n+   * The discounted cumulative gain at position k is computed as:\n+   *    sum,,i=1,,^k^ (2^{relevance of ''i''th item}^ - 1) / log(i + 1),\n+   * and the NDCG is obtained by dividing the DCG value on the ground truth set. In the current\n+   * implementation, the relevance value is binary.\n+\n+   * If a query has an empty ground truth set, zero will be used as ndcg together with\n+   * a log warning.\n+   *\n+   * See the following paper for detail:\n+   *\n+   * IR evaluation methods for retrieving highly relevant documents. K. Jarvelin and J. Kekalainen\n+   *\n+   * @param k the position to compute the truncated ndcg, must be positive\n+   * @return the average ndcg at the first k ranking positions\n+   */\n+  @Since(\"2.2.0\")\n+  def ndcgAt(k: Int): Double = {\n+    require(k > 0, \"ranking position k should be positive\")\n+\n+    def ndcgAtK = udf((predicted: Seq[Any], actual: Seq[Any]) => {\n+      val actualSet = actual.toSet\n+\n+      if (actualSet.nonEmpty) {\n+        val labSetSize = actualSet.size\n+        val n = math.min(math.max(predicted.length, labSetSize), k)\n+        var maxDcg = 0.0\n+        var dcg = 0.0\n+        var i = 0\n+        while (i < n) {\n+          val gain = 1.0 / math.log(i + 2)\n+          if (i < predicted.length && actualSet.contains(predicted(i))) {",
    "line": 180
  }],
  "prId": 16618
}, {
  "comments": [{
    "author": {
      "login": "bantmen"
    },
    "body": "Shouldn't this be a sum instead of count?\r\n(I know this is old/closed but other people might be referring to this code)\r\n",
    "commit": "fa2155af8947347a2fc1e565cf05a19529022266",
    "createdAt": "2019-10-03T15:50:08Z",
    "diffHunk": "@@ -0,0 +1,202 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.evaluation\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.{Column, DataFrame}\n+import org.apache.spark.sql.functions.{mean, sum}\n+import org.apache.spark.sql.functions.udf\n+import org.apache.spark.sql.types.DoubleType\n+\n+@Since(\"2.2.0\")\n+class RankingMetrics(\n+  predictionAndObservations: DataFrame, predictionCol: String, labelCol: String)\n+  extends Logging with Serializable {\n+\n+  /**\n+   * Compute the Mean Percentile Rank (MPR) of all the queries.\n+   *\n+   * See the following paper for detail (\"Expected percentile rank\" in the paper):\n+   * Hu, Y., Y. Koren, and C. Volinsky. “Collaborative Filtering for Implicit Feedback Datasets.”\n+   * In 2008 Eighth IEEE International Conference on Data Mining, 263–72, 2008.\n+   * doi:10.1109/ICDM.2008.22.\n+   *\n+   * @return the mean percentile rank\n+   */\n+  lazy val meanPercentileRank: Double = {\n+\n+    def rank = udf((predicted: Seq[Any], actual: Any) => {\n+      val l_i = predicted.indexOf(actual)\n+\n+      if (l_i == -1) {\n+        1\n+      } else {\n+        l_i.toDouble / predicted.size\n+      }\n+    }, DoubleType)\n+\n+    val R_prime = predictionAndObservations.count()",
    "line": 53
  }],
  "prId": 16618
}]