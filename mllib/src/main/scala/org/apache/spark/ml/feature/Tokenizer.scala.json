[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Add doc and update code style. What's the case when we match a token with regex but its length is zero? Should we control it in the regex, e.g., `\\d{5,}`?\n\nBtw, it is not intuitive that the min value is excluded. Could we remove \"excluded\" and set the default value to `1`? And it might be better to call it `minTokenLength`, if we want to keep it.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-02-24T07:14:31Z",
    "diffHunk": "@@ -39,3 +39,66 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens using a regex.\n+ * Optional additional parameters include enabling lowercase stabdarization, a minimum character\n+ * size for tokens as well as an array of stop words to remove from the results.\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  val lowerCase = new BooleanParam(this, \n+      \"lowerCase\", \n+      \"enable case folding to lower case\", \n+      Some(true))\n+  def setLowercase(value: Boolean) = set(lowerCase, value)\n+  def getLowercase: Boolean = get(lowerCase)\n+\n+  val minLength = new IntParam(this, "
  }, {
    "body": "I removed excluded as it is indeed unusual and set the default value to 1 which is standard\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-03T00:18:16Z",
    "diffHunk": "@@ -39,3 +39,66 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens using a regex.\n+ * Optional additional parameters include enabling lowercase stabdarization, a minimum character\n+ * size for tokens as well as an array of stop words to remove from the results.\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  val lowerCase = new BooleanParam(this, \n+      \"lowerCase\", \n+      \"enable case folding to lower case\", \n+      Some(true))\n+  def setLowercase(value: Boolean) = set(lowerCase, value)\n+  def getLowercase: Boolean = get(lowerCase)\n+\n+  val minLength = new IntParam(this, "
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "From the doc, it is not clear to me whether I should specify groups to extract in the regex or it repeatedly matches the entire regex.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-02-24T07:14:35Z",
    "diffHunk": "@@ -39,3 +39,66 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens using a regex."
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Add doc with `@group param` to group methods in the generated html doc, e.g., https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/param/sharedParams.scala#L65\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-02-24T07:14:36Z",
    "diffHunk": "@@ -39,3 +39,66 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens using a regex.\n+ * Optional additional parameters include enabling lowercase stabdarization, a minimum character\n+ * size for tokens as well as an array of stop words to remove from the results.\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  val lowerCase = new BooleanParam(this, "
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "We don't chop down args in a method call.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-02-24T07:14:37Z",
    "diffHunk": "@@ -39,3 +39,66 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens using a regex.\n+ * Optional additional parameters include enabling lowercase stabdarization, a minimum character\n+ * size for tokens as well as an array of stop words to remove from the results.\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  val lowerCase = new BooleanParam(this, \n+      \"lowerCase\", "
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "It is not clear to me whether this is applied before regex matching or after.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-02-24T07:14:39Z",
    "diffHunk": "@@ -39,3 +39,66 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens using a regex.\n+ * Optional additional parameters include enabling lowercase stabdarization, a minimum character\n+ * size for tokens as well as an array of stop words to remove from the results.\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  val lowerCase = new BooleanParam(this, \n+      \"lowerCase\", \n+      \"enable case folding to lower case\", "
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`regEx` -> `regex`. The latter is more common. Or we can use `tokenRegex` to be more specific.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-02-24T07:14:41Z",
    "diffHunk": "@@ -39,3 +39,66 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens using a regex.\n+ * Optional additional parameters include enabling lowercase stabdarization, a minimum character\n+ * size for tokens as well as an array of stop words to remove from the results.\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  val lowerCase = new BooleanParam(this, \n+      \"lowerCase\", \n+      \"enable case folding to lower case\", \n+      Some(true))\n+  def setLowercase(value: Boolean) = set(lowerCase, value)\n+  def getLowercase: Boolean = get(lowerCase)\n+\n+  val minLength = new IntParam(this, \n+      \"minLength\", \n+      \"minimum token length (excluded)\", \n+      Some(0))\n+  def setMinLength(value: Int) = set(minLength, value)\n+  def getMinLength: Int = get(minLength)\n+\n+  val regEx = new Param(this, "
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Using `Regex` as param type is not Java/Python friendly. We should use plain string.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-02-24T07:14:42Z",
    "diffHunk": "@@ -39,3 +39,66 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens using a regex.\n+ * Optional additional parameters include enabling lowercase stabdarization, a minimum character\n+ * size for tokens as well as an array of stop words to remove from the results.\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  val lowerCase = new BooleanParam(this, \n+      \"lowerCase\", \n+      \"enable case folding to lower case\", \n+      Some(true))\n+  def setLowercase(value: Boolean) = set(lowerCase, value)\n+  def getLowercase: Boolean = get(lowerCase)\n+\n+  val minLength = new IntParam(this, \n+      \"minLength\", \n+      \"minimum token length (excluded)\", \n+      Some(0))\n+  def setMinLength(value: Int) = set(minLength, value)\n+  def getMinLength: Int = get(minLength)\n+\n+  val regEx = new Param(this, \n+      \"regEx\", \n+      \"RegEx used for tokenizing\", \n+      Some(\"\\\\p{L}+|[^\\\\p{L}\\\\s]+\".r))"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "I would leave this function to a stopword filter.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-02-24T07:14:44Z",
    "diffHunk": "@@ -39,3 +39,66 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens using a regex.\n+ * Optional additional parameters include enabling lowercase stabdarization, a minimum character\n+ * size for tokens as well as an array of stop words to remove from the results.\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  val lowerCase = new BooleanParam(this, \n+      \"lowerCase\", \n+      \"enable case folding to lower case\", \n+      Some(true))\n+  def setLowercase(value: Boolean) = set(lowerCase, value)\n+  def getLowercase: Boolean = get(lowerCase)\n+\n+  val minLength = new IntParam(this, \n+      \"minLength\", \n+      \"minimum token length (excluded)\", \n+      Some(0))\n+  def setMinLength(value: Int) = set(minLength, value)\n+  def getMinLength: Int = get(minLength)\n+\n+  val regEx = new Param(this, \n+      \"regEx\", \n+      \"RegEx used for tokenizing\", \n+      Some(\"\\\\p{L}+|[^\\\\p{L}\\\\s]+\".r))\n+  def setRegex(value: scala.util.matching.Regex) = set(regEx, value)\n+  def getRegex: scala.util.matching.Regex = get(regEx)\n+\n+  val stopWords = new Param(this, "
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove extra blank line\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-02-24T07:14:45Z",
    "diffHunk": "@@ -39,3 +39,66 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens using a regex.\n+ * Optional additional parameters include enabling lowercase stabdarization, a minimum character\n+ * size for tokens as well as an array of stop words to remove from the results.\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  val lowerCase = new BooleanParam(this, \n+      \"lowerCase\", \n+      \"enable case folding to lower case\", \n+      Some(true))\n+  def setLowercase(value: Boolean) = set(lowerCase, value)\n+  def getLowercase: Boolean = get(lowerCase)\n+\n+  val minLength = new IntParam(this, \n+      \"minLength\", \n+      \"minimum token length (excluded)\", \n+      Some(0))\n+  def setMinLength(value: Int) = set(minLength, value)\n+  def getMinLength: Int = get(minLength)\n+\n+  val regEx = new Param(this, \n+      \"regEx\", \n+      \"RegEx used for tokenizing\", \n+      Some(\"\\\\p{L}+|[^\\\\p{L}\\\\s]+\".r))\n+  def setRegex(value: scala.util.matching.Regex) = set(regEx, value)\n+  def getRegex: scala.util.matching.Regex = get(regEx)\n+\n+  val stopWords = new Param(this, \n+      \"stopWords\", \n+      \"array of tokens to filter from results\", \n+      Some(Array[String]()))\n+  def setStopWords(value: Array[String]) = set(stopWords, value)\n+  def getStopWords: Array[String] = get(stopWords)\n+\n+"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "remove this line too\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-02-24T07:14:47Z",
    "diffHunk": "@@ -39,3 +39,66 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens using a regex.\n+ * Optional additional parameters include enabling lowercase stabdarization, a minimum character\n+ * size for tokens as well as an array of stop words to remove from the results.\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  val lowerCase = new BooleanParam(this, \n+      \"lowerCase\", \n+      \"enable case folding to lower case\", \n+      Some(true))\n+  def setLowercase(value: Boolean) = set(lowerCase, value)\n+  def getLowercase: Boolean = get(lowerCase)\n+\n+  val minLength = new IntParam(this, \n+      \"minLength\", \n+      \"minimum token length (excluded)\", \n+      Some(0))\n+  def setMinLength(value: Int) = set(minLength, value)\n+  def getMinLength: Int = get(minLength)\n+\n+  val regEx = new Param(this, \n+      \"regEx\", \n+      \"RegEx used for tokenizing\", \n+      Some(\"\\\\p{L}+|[^\\\\p{L}\\\\s]+\".r))\n+  def setRegex(value: scala.util.matching.Regex) = set(regEx, value)\n+  def getRegex: scala.util.matching.Regex = get(regEx)\n+\n+  val stopWords = new Param(this, \n+      \"stopWords\", \n+      \"array of tokens to filter from results\", \n+      Some(Array[String]()))\n+  def setStopWords(value: Array[String]) = set(stopWords, value)\n+  def getStopWords: Array[String] = get(stopWords)\n+\n+\n+  override protected def createTransformFunc(paramMap: ParamMap): String => Seq[String] = { x =>\n+"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Try to use `val` if possible:\n\n```\nval str = if (paramMap(lowerCase)) x.toLowerCase else x\n```\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-02-24T07:14:48Z",
    "diffHunk": "@@ -39,3 +39,66 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens using a regex.\n+ * Optional additional parameters include enabling lowercase stabdarization, a minimum character\n+ * size for tokens as well as an array of stop words to remove from the results.\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  val lowerCase = new BooleanParam(this, \n+      \"lowerCase\", \n+      \"enable case folding to lower case\", \n+      Some(true))\n+  def setLowercase(value: Boolean) = set(lowerCase, value)\n+  def getLowercase: Boolean = get(lowerCase)\n+\n+  val minLength = new IntParam(this, \n+      \"minLength\", \n+      \"minimum token length (excluded)\", \n+      Some(0))\n+  def setMinLength(value: Int) = set(minLength, value)\n+  def getMinLength: Int = get(minLength)\n+\n+  val regEx = new Param(this, \n+      \"regEx\", \n+      \"RegEx used for tokenizing\", \n+      Some(\"\\\\p{L}+|[^\\\\p{L}\\\\s]+\".r))\n+  def setRegex(value: scala.util.matching.Regex) = set(regEx, value)\n+  def getRegex: scala.util.matching.Regex = get(regEx)\n+\n+  val stopWords = new Param(this, \n+      \"stopWords\", \n+      \"array of tokens to filter from results\", \n+      Some(Array[String]()))\n+  def setStopWords(value: Array[String]) = set(stopWords, value)\n+  def getStopWords: Array[String] = get(stopWords)\n+\n+\n+  override protected def createTransformFunc(paramMap: ParamMap): String => Seq[String] = { x =>\n+\n+    var string = x"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Do not use infix (https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide#SparkCodeStyleGuide-InfixMethods).\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-02-24T07:14:50Z",
    "diffHunk": "@@ -39,3 +39,66 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens using a regex.\n+ * Optional additional parameters include enabling lowercase stabdarization, a minimum character\n+ * size for tokens as well as an array of stop words to remove from the results.\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  val lowerCase = new BooleanParam(this, \n+      \"lowerCase\", \n+      \"enable case folding to lower case\", \n+      Some(true))\n+  def setLowercase(value: Boolean) = set(lowerCase, value)\n+  def getLowercase: Boolean = get(lowerCase)\n+\n+  val minLength = new IntParam(this, \n+      \"minLength\", \n+      \"minimum token length (excluded)\", \n+      Some(0))\n+  def setMinLength(value: Int) = set(minLength, value)\n+  def getMinLength: Int = get(minLength)\n+\n+  val regEx = new Param(this, \n+      \"regEx\", \n+      \"RegEx used for tokenizing\", \n+      Some(\"\\\\p{L}+|[^\\\\p{L}\\\\s]+\".r))\n+  def setRegex(value: scala.util.matching.Regex) = set(regEx, value)\n+  def getRegex: scala.util.matching.Regex = get(regEx)\n+\n+  val stopWords = new Param(this, \n+      \"stopWords\", \n+      \"array of tokens to filter from results\", \n+      Some(Array[String]()))\n+  def setStopWords(value: Array[String]) = set(stopWords, value)\n+  def getStopWords: Array[String] = get(stopWords)\n+\n+\n+  override protected def createTransformFunc(paramMap: ParamMap): String => Seq[String] = { x =>\n+\n+    var string = x\n+    if (paramMap(lowerCase)) {\n+      string = string.toLowerCase\n+    }\n+    var tokens = (paramMap(regEx) findAllIn string).toList"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "space before `(paramMap` and after `> 0)`\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-02-24T07:14:57Z",
    "diffHunk": "@@ -39,3 +39,66 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens using a regex.\n+ * Optional additional parameters include enabling lowercase stabdarization, a minimum character\n+ * size for tokens as well as an array of stop words to remove from the results.\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  val lowerCase = new BooleanParam(this, \n+      \"lowerCase\", \n+      \"enable case folding to lower case\", \n+      Some(true))\n+  def setLowercase(value: Boolean) = set(lowerCase, value)\n+  def getLowercase: Boolean = get(lowerCase)\n+\n+  val minLength = new IntParam(this, \n+      \"minLength\", \n+      \"minimum token length (excluded)\", \n+      Some(0))\n+  def setMinLength(value: Int) = set(minLength, value)\n+  def getMinLength: Int = get(minLength)\n+\n+  val regEx = new Param(this, \n+      \"regEx\", \n+      \"RegEx used for tokenizing\", \n+      Some(\"\\\\p{L}+|[^\\\\p{L}\\\\s]+\".r))\n+  def setRegex(value: scala.util.matching.Regex) = set(regEx, value)\n+  def getRegex: scala.util.matching.Regex = get(regEx)\n+\n+  val stopWords = new Param(this, \n+      \"stopWords\", \n+      \"array of tokens to filter from results\", \n+      Some(Array[String]()))\n+  def setStopWords(value: Array[String]) = set(stopWords, value)\n+  def getStopWords: Array[String] = get(stopWords)\n+\n+\n+  override protected def createTransformFunc(paramMap: ParamMap): String => Seq[String] = { x =>\n+\n+    var string = x\n+    if (paramMap(lowerCase)) {\n+      string = string.toLowerCase\n+    }\n+    var tokens = (paramMap(regEx) findAllIn string).toList\n+    \n+    if(paramMap(minLength) > 0){"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`stopWords` is an `Array[String]`and hence `contains` has linear time complexity. We should convert it into a set first. As I mentioned before, I think the stop words could be handled separately.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-02-24T07:14:59Z",
    "diffHunk": "@@ -39,3 +39,66 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens using a regex.\n+ * Optional additional parameters include enabling lowercase stabdarization, a minimum character\n+ * size for tokens as well as an array of stop words to remove from the results.\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  val lowerCase = new BooleanParam(this, \n+      \"lowerCase\", \n+      \"enable case folding to lower case\", \n+      Some(true))\n+  def setLowercase(value: Boolean) = set(lowerCase, value)\n+  def getLowercase: Boolean = get(lowerCase)\n+\n+  val minLength = new IntParam(this, \n+      \"minLength\", \n+      \"minimum token length (excluded)\", \n+      Some(0))\n+  def setMinLength(value: Int) = set(minLength, value)\n+  def getMinLength: Int = get(minLength)\n+\n+  val regEx = new Param(this, \n+      \"regEx\", \n+      \"RegEx used for tokenizing\", \n+      Some(\"\\\\p{L}+|[^\\\\p{L}\\\\s]+\".r))\n+  def setRegex(value: scala.util.matching.Regex) = set(regEx, value)\n+  def getRegex: scala.util.matching.Regex = get(regEx)\n+\n+  val stopWords = new Param(this, \n+      \"stopWords\", \n+      \"array of tokens to filter from results\", \n+      Some(Array[String]()))\n+  def setStopWords(value: Array[String]) = set(stopWords, value)\n+  def getStopWords: Array[String] = get(stopWords)\n+\n+\n+  override protected def createTransformFunc(paramMap: ParamMap): String => Seq[String] = { x =>\n+\n+    var string = x\n+    if (paramMap(lowerCase)) {\n+      string = string.toLowerCase\n+    }\n+    var tokens = (paramMap(regEx) findAllIn string).toList\n+    \n+    if(paramMap(minLength) > 0){\n+      tokens = tokens.filter(_.length > paramMap(minLength))\n+    }\n+    if(paramMap(stopWords).length > 0){\n+      tokens = tokens.filter(!paramMap(stopWords).contains(_))"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "This is not necessary at this time. It doesn't affect any user code because we don't expect users to treat tokenizers in a generic way.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-18T14:07:47Z",
    "diffHunk": "@@ -39,3 +39,67 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens either by repeatedly matching the regex(default) \n+ * or using it to split the text (set matching to false). Optional parameters also allow to fold\n+ * the text to lowercase prior to it being tokenized and to filer tokens using a minimal length. \n+ * It returns an array of strings that can be empty.\n+ * The default parameters are regex = \"\\\\p{L}+|[^\\\\p{L}\\\\s]+\", matching = true, \n+ * lowercase = false, minTokenLength = 1\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends Tokenizer {"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "space before `(true)`\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-18T14:07:49Z",
    "diffHunk": "@@ -39,3 +39,67 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens either by repeatedly matching the regex(default) \n+ * or using it to split the text (set matching to false). Optional parameters also allow to fold\n+ * the text to lowercase prior to it being tokenized and to filer tokens using a minimal length. \n+ * It returns an array of strings that can be empty.\n+ * The default parameters are regex = \"\\\\p{L}+|[^\\\\p{L}\\\\s]+\", matching = true, \n+ * lowercase = false, minTokenLength = 1\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends Tokenizer {\n+\n+  /**\n+   * param for minimum token length, default is one to avoid returning empty strings\n+   * @group param\n+   */\n+  val minTokenLength = new IntParam(this, \"minLength\", \"minimum token length\", Some(1))\n+\n+  /** @group setParam */\n+  def setMinTokenLength(value: Int) = set(minTokenLength, value)\n+\n+  /** @group getParam */\n+  def getMinTokenLength: Int = get(minTokenLength)\n+\n+  /**\n+   * param sets regex as splitting on gaps(true) or matching tokens (false)"
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Please append `: IntParam` to `minTokenLength`. See SPARK-6428. Please also update other method declarations.\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-20T19:19:36Z",
    "diffHunk": "@@ -39,3 +39,67 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens either by repeatedly matching the regex(default) \n+ * or using it to split the text (set matching to false). Optional parameters also allow to fold\n+ * the text to lowercase prior to it being tokenized and to filer tokens using a minimal length. \n+ * It returns an array of strings that can be empty.\n+ * The default parameters are regex = \"\\\\p{L}+|[^\\\\p{L}\\\\s]+\", matching = true, \n+ * lowercase = false, minTokenLength = 1\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  /**\n+   * param for minimum token length, default is one to avoid returning empty strings\n+   * @group param\n+   */",
    "line": 29
  }],
  "prId": 4504
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`: this.type`\n",
    "commit": "716d2579f0f0829753cdd5882d22a5e497486a3e",
    "createdAt": "2015-03-20T19:19:38Z",
    "diffHunk": "@@ -39,3 +39,67 @@ class Tokenizer extends UnaryTransformer[String, Seq[String], Tokenizer] {\n \n   override protected def outputDataType: DataType = new ArrayType(StringType, false)\n }\n+\n+/**\n+ * :: AlphaComponent ::\n+ * A regex based tokenizer that extracts tokens either by repeatedly matching the regex(default) \n+ * or using it to split the text (set matching to false). Optional parameters also allow to fold\n+ * the text to lowercase prior to it being tokenized and to filer tokens using a minimal length. \n+ * It returns an array of strings that can be empty.\n+ * The default parameters are regex = \"\\\\p{L}+|[^\\\\p{L}\\\\s]+\", matching = true, \n+ * lowercase = false, minTokenLength = 1\n+ */\n+@AlphaComponent\n+class RegexTokenizer extends UnaryTransformer[String, Seq[String], RegexTokenizer] {\n+\n+  /**\n+   * param for minimum token length, default is one to avoid returning empty strings\n+   * @group param\n+   */\n+  val minTokenLength = new IntParam(this, \"minLength\", \"minimum token length\", Some(1))\n+\n+  /** @group setParam */\n+  def setMinTokenLength(value: Int) = set(minTokenLength, value)"
  }],
  "prId": 4504
}]