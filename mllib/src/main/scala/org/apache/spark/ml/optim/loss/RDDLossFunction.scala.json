[{
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "use `fromBreeze`",
    "commit": "29052d3dbc97ad548128c13533de47d5488b4196",
    "createdAt": "2017-02-28T03:42:56Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.ml.optim.loss\n+\n+import scala.reflect.ClassTag\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import breeze.optimize.DiffFunction\n+\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.ml.feature.Instance\n+import org.apache.spark.ml.linalg.{BLAS, Vector, Vectors}\n+import org.apache.spark.ml.optim.aggregator.DifferentiableLossAggregator\n+import org.apache.spark.rdd.RDD\n+\n+/**\n+ * This class computes the gradient and loss of a differentiable loss function by mapping a\n+ * [[DifferentiableLossAggregator]] over an [[RDD]] of [[Instance]]s. The loss function is the\n+ * sum of the loss computed on a single instance across all points in the RDD. Therefore, the actual\n+ * analytical form of the loss function is specified by the aggregator, which computes each points\n+ * contribution to the overall loss.\n+ *\n+ * A differentiable regularization component can also be added by providing a\n+ * [[DifferentiableRegularization]] loss function.\n+ *\n+ * @param instances\n+ * @param getAggregator A function which gets a new loss aggregator in every tree aggregate step.\n+ * @param regularization An option representing the regularization loss function to apply to the\n+ *                       coefficients.\n+ * @param aggregationDepth The aggregation depth of the tree aggregation step.\n+ * @tparam Agg Specialization of [[DifferentiableLossAggregator]], representing the concrete type\n+ *             of the aggregator.\n+ */\n+private[ml] class RDDLossFunction[Agg <: DifferentiableLossAggregator[Instance, Agg]: ClassTag](\n+    instances: RDD[Instance],\n+    getAggregator: (Broadcast[Vector] => Agg),\n+    regularization: Option[DifferentiableRegularization[Array[Double]]],\n+    aggregationDepth: Int = 2)\n+  extends DiffFunction[BDV[Double]] {\n+\n+  override def calculate(coefficients: BDV[Double]): (Double, BDV[Double]) = {\n+    val bcCoefficients = instances.context.broadcast(Vectors.dense(coefficients.data))"
  }],
  "prId": 17094
}]