[{
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "I think `GD`, `AdamW`, `supportedSolvers` should be defined in an object",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-11-01T03:19:47Z",
    "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[ml] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver {\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"Dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the magnitude of L2-regularization\", ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /** String name for \"gd\". */\n+  private[ml] val GD = \"gd\""
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "ditto",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-11-01T03:20:19Z",
    "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[ml] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver {\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"Dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the magnitude of L2-regularization\", ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /** String name for \"gd\". */\n+  private[ml] val GD = \"gd\"\n+\n+  /** String name for \"adamW\". */\n+  private[ml] val AdamW = \"adamW\"\n+\n+  /** Set of solvers that FactorizationMachines supports. */\n+  private[ml] val supportedSolvers = Array(GD, AdamW)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  private[ml] def parseSolver(solver: String, coefficientsSize: Int): Updater = {\n+    solver match {\n+      case GD => new SquaredL2Updater()\n+      case AdamW => new AdamWUpdater(coefficientsSize)\n+    }\n+  }\n+}\n+\n+/**\n+ * Params for FMRegressor\n+ */\n+private[regression] trait FMRegressorParams extends FactorizationMachinesParams {\n+}\n+\n+/**\n+ * Factorization Machines learning algorithm for regression.\n+ * It supports normal gradient descent and AdamW solver.\n+ *\n+ * The implementation is based upon:\n+ * <a href=\"https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\">\n+ * S. Rendle. \"Factorization machines\" 2010</a>.\n+ *\n+ * FM is able to estimate interactions even in problems with huge sparsity\n+ * (like advertising and recommendation system).\n+ * FM formula is:\n+ * {{{\n+ *   y = w_0 + \\sum\\limits^n_{i-1} w_i x_i +\n+ *     \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} \\langle v_i, v_j \\rangle x_i x_j\n+ * }}}\n+ * First two terms denote global bias and linear term (as same as linear regression),\n+ * and last term denotes pairwise interactions term. {{{v_i}}} describes the i-th variable\n+ * with k factors.\n+ *\n+ * FM regression model uses MSE loss which can be solved by gradient descent method, and\n+ * regularization terms like L2 are usually added to the loss function to prevent overfitting.\n+ */\n+@Since(\"3.0.0\")\n+class FMRegressor @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FMRegressor, FMRegressorModel]\n+  with FMRegressorParams with DefaultParamsWritable with Logging {\n+\n+  import org.apache.spark.ml.regression.BaseFactorizationMachinesGradient.{SquaredError, parseLoss}\n+  import org.apache.spark.ml.regression.FMRegressor.initCoefficients"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "`ParamValidators.inRange(0, 1, false, true)` already checks input value",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-11-01T03:21:06Z",
    "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[ml] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver {\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"Dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the magnitude of L2-regularization\", ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /** String name for \"gd\". */\n+  private[ml] val GD = \"gd\"\n+\n+  /** String name for \"adamW\". */\n+  private[ml] val AdamW = \"adamW\"\n+\n+  /** Set of solvers that FactorizationMachines supports. */\n+  private[ml] val supportedSolvers = Array(GD, AdamW)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  private[ml] def parseSolver(solver: String, coefficientsSize: Int): Updater = {\n+    solver match {\n+      case GD => new SquaredL2Updater()\n+      case AdamW => new AdamWUpdater(coefficientsSize)\n+    }\n+  }\n+}\n+\n+/**\n+ * Params for FMRegressor\n+ */\n+private[regression] trait FMRegressorParams extends FactorizationMachinesParams {\n+}\n+\n+/**\n+ * Factorization Machines learning algorithm for regression.\n+ * It supports normal gradient descent and AdamW solver.\n+ *\n+ * The implementation is based upon:\n+ * <a href=\"https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\">\n+ * S. Rendle. \"Factorization machines\" 2010</a>.\n+ *\n+ * FM is able to estimate interactions even in problems with huge sparsity\n+ * (like advertising and recommendation system).\n+ * FM formula is:\n+ * {{{\n+ *   y = w_0 + \\sum\\limits^n_{i-1} w_i x_i +\n+ *     \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} \\langle v_i, v_j \\rangle x_i x_j\n+ * }}}\n+ * First two terms denote global bias and linear term (as same as linear regression),\n+ * and last term denotes pairwise interactions term. {{{v_i}}} describes the i-th variable\n+ * with k factors.\n+ *\n+ * FM regression model uses MSE loss which can be solved by gradient descent method, and\n+ * regularization terms like L2 are usually added to the loss function to prevent overfitting.\n+ */\n+@Since(\"3.0.0\")\n+class FMRegressor @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FMRegressor, FMRegressorModel]\n+  with FMRegressorParams with DefaultParamsWritable with Logging {\n+\n+  import org.apache.spark.ml.regression.BaseFactorizationMachinesGradient.{SquaredError, parseLoss}\n+  import org.apache.spark.ml.regression.FMRegressor.initCoefficients\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fmr\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "ditto",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-11-01T03:21:36Z",
    "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.regression.{LabeledPoint => OldLabeledPoint}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[ml] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver {\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"Dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the magnitude of L2-regularization\", ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /** String name for \"gd\". */\n+  private[ml] val GD = \"gd\"\n+\n+  /** String name for \"adamW\". */\n+  private[ml] val AdamW = \"adamW\"\n+\n+  /** Set of solvers that FactorizationMachines supports. */\n+  private[ml] val supportedSolvers = Array(GD, AdamW)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+\n+  private[ml] def parseSolver(solver: String, coefficientsSize: Int): Updater = {\n+    solver match {\n+      case GD => new SquaredL2Updater()\n+      case AdamW => new AdamWUpdater(coefficientsSize)\n+    }\n+  }\n+}\n+\n+/**\n+ * Params for FMRegressor\n+ */\n+private[regression] trait FMRegressorParams extends FactorizationMachinesParams {\n+}\n+\n+/**\n+ * Factorization Machines learning algorithm for regression.\n+ * It supports normal gradient descent and AdamW solver.\n+ *\n+ * The implementation is based upon:\n+ * <a href=\"https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\">\n+ * S. Rendle. \"Factorization machines\" 2010</a>.\n+ *\n+ * FM is able to estimate interactions even in problems with huge sparsity\n+ * (like advertising and recommendation system).\n+ * FM formula is:\n+ * {{{\n+ *   y = w_0 + \\sum\\limits^n_{i-1} w_i x_i +\n+ *     \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} \\langle v_i, v_j \\rangle x_i x_j\n+ * }}}\n+ * First two terms denote global bias and linear term (as same as linear regression),\n+ * and last term denotes pairwise interactions term. {{{v_i}}} describes the i-th variable\n+ * with k factors.\n+ *\n+ * FM regression model uses MSE loss which can be solved by gradient descent method, and\n+ * regularization terms like L2 are usually added to the loss function to prevent overfitting.\n+ */\n+@Since(\"3.0.0\")\n+class FMRegressor @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FMRegressor, FMRegressorModel]\n+  with FMRegressorParams with DefaultParamsWritable with Logging {\n+\n+  import org.apache.spark.ml.regression.BaseFactorizationMachinesGradient.{SquaredError, parseLoss}\n+  import org.apache.spark.ml.regression.FMRegressor.initCoefficients\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fmr\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setNumFactors(value: Int): this.type = set(numFactors, value)\n+  setDefault(numFactors -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = {\n+    require(value > 0 && value <= 1.0,\n+      s\"Fraction for mini-batch SGD must be in range (0, 1] but got $value\")\n+    set(miniBatchFraction, value)\n+  }\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Default is adamW.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FMRegressorModel = {\n+    val handlePersistence = dataset.rdd.getStorageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FMRegressorModel = instrumented { instr =>\n+    val instances: RDD[OldLabeledPoint] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          OldLabeledPoint(label, features)\n+      }\n+\n+    if (handlePersistence) instances.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, numFactors, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver)\n+\n+    val numFeatures = instances.first().features.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    val data = instances.map { case OldLabeledPoint(label, features) => (label, features) }\n+\n+    // initialize coefficients\n+    val (initialCoefficients, coefficientsSize) = initCoefficients(\n+      numFeatures, $(numFactors), $(fitBias), $(fitLinear), $(initStd))\n+\n+    // optimize coefficients with gradient descent\n+    val gradient = parseLoss(\n+      SquaredError, $(numFactors), $(fitBias), $(fitLinear), numFeatures)\n+\n+    val updater = parseSolver($(solver), coefficientsSize)\n+\n+    val optimizer = new GradientDescent(gradient, updater)\n+      .setStepSize($(stepSize))\n+      .setNumIterations($(maxIter))\n+      .setRegParam($(regParam))\n+      .setMiniBatchFraction($(miniBatchFraction))\n+      .setConvergenceTol($(tol))\n+    val coefficients = optimizer.optimize(data, initialCoefficients)"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "why not extending `HasRegParam`?",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-11-08T10:41:06Z",
    "diffHunk": "@@ -0,0 +1,786 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.regression.FactorizationMachines._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[ml] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver {\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"Dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",",
    "line": 96
  }, {
    "author": {
      "login": "mob-ai"
    },
    "body": "> why not extending `HasRegParam`?\r\n\r\nBecause I want to change `regParam` doc (FM regParams only for L2), but `HasRegParam` has `final` tag, I can't override it.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-11-11T02:48:51Z",
    "diffHunk": "@@ -0,0 +1,786 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.regression.FactorizationMachines._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[ml] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver {\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"Dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",",
    "line": 96
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "Why not extending `HasSolver`?",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-11-08T10:41:37Z",
    "diffHunk": "@@ -0,0 +1,786 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.regression.FactorizationMachines._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[ml] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver {\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"Dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the magnitude of L2-regularization\", ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",",
    "line": 136
  }, {
    "author": {
      "login": "mob-ai"
    },
    "body": "> Why not extending `HasSolver`?\r\n\r\nI already extended `HasSolver`, and I override solver to change doc.",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-11-11T02:51:21Z",
    "diffHunk": "@@ -0,0 +1,786 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.regression.FactorizationMachines._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[ml] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver {\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"Dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the magnitude of L2-regularization\", ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",",
    "line": 136
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "_train -> train",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-11-08T10:41:53Z",
    "diffHunk": "@@ -0,0 +1,786 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.regression.FactorizationMachines._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[ml] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver {\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val numFactors: IntParam = new IntParam(this, \"numFactors\",\n+    \"Dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getNumFactors: Int = $(numFactors)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the magnitude of L2-regularization\", ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+}\n+\n+private[ml] trait FactorizationMachines extends FactorizationMachinesParams {\n+\n+  private[ml] def initCoefficients(numFeatures: Int): OldVector = {\n+    val initialCoefficients =\n+      OldVectors.dense(\n+        Array.fill($(numFactors) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) new Array[Double](numFeatures) else Array.emptyDoubleArray) ++\n+        (if ($(fitBias)) new Array[Double](1) else Array.emptyDoubleArray))\n+    initialCoefficients\n+  }\n+\n+  private[ml] def _train("
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "`Random.nextGaussian()`\r\nWhat about exposing seed to end users, by extending `HasSeed`?",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-11-25T10:23:18Z",
    "diffHunk": "@@ -0,0 +1,796 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.regression.FactorizationMachines._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[ml] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver {\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val factorSize: IntParam = new IntParam(this, \"factorSize\",\n+    \"Dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFactorSize: Int = $(factorSize)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the magnitude of L2-regularization\", ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+}\n+\n+private[ml] trait FactorizationMachines extends FactorizationMachinesParams {\n+\n+  private[ml] def initCoefficients(numFeatures: Int): OldVector = {\n+    val initialCoefficients =\n+      OldVectors.dense(\n+        Array.fill($(factorSize) * numFeatures)(Random.nextGaussian() * $(initStd)) ++"
  }],
  "prId": 26124
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "`FMRegressorModel` -> `FMRegressionModel`",
    "commit": "9bd6cbffa3adef737070c24c4ccf4bbb423a0a05",
    "createdAt": "2019-11-25T10:26:15Z",
    "diffHunk": "@@ -0,0 +1,796 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.regression\n+\n+import scala.util.Random\n+\n+import breeze.linalg.{axpy => brzAxpy, norm => brzNorm, Vector => BV}\n+import breeze.numerics.{sqrt => brzSqrt}\n+import org.apache.hadoop.fs.Path\n+\n+import org.apache.spark.annotation.Since\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.ml.{PredictionModel, Predictor, PredictorParams}\n+import org.apache.spark.ml.linalg._\n+import org.apache.spark.ml.linalg.BLAS._\n+import org.apache.spark.ml.param._\n+import org.apache.spark.ml.param.shared._\n+import org.apache.spark.ml.regression.FactorizationMachines._\n+import org.apache.spark.ml.util._\n+import org.apache.spark.ml.util.Instrumentation.instrumented\n+import org.apache.spark.mllib.{linalg => OldLinalg}\n+import org.apache.spark.mllib.linalg.{Vector => OldVector, Vectors => OldVectors}\n+import org.apache.spark.mllib.linalg.VectorImplicits._\n+import org.apache.spark.mllib.optimization.{Gradient, GradientDescent, SquaredL2Updater, Updater}\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.sql.{Dataset, Row}\n+import org.apache.spark.sql.functions.col\n+import org.apache.spark.storage.StorageLevel\n+\n+/**\n+ * Params for Factorization Machines\n+ */\n+private[ml] trait FactorizationMachinesParams\n+  extends PredictorParams\n+  with HasMaxIter with HasStepSize with HasTol with HasSolver {\n+\n+  /**\n+   * Param for dimensionality of the factors (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val factorSize: IntParam = new IntParam(this, \"factorSize\",\n+    \"Dimensionality of the factor vectors, \" +\n+      \"which are used to get pairwise interactions between variables\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFactorSize: Int = $(factorSize)\n+\n+  /**\n+   * Param for whether to fit global bias term\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitBias: BooleanParam = new BooleanParam(this, \"fitBias\",\n+    \"whether to fit global bias term\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitBias: Boolean = $(fitBias)\n+\n+  /**\n+   * Param for whether to fit linear term (aka 1-way term)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val fitLinear: BooleanParam = new BooleanParam(this, \"fitLinear\",\n+    \"whether to fit linear term (aka 1-way term)\")\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getFitLinear: Boolean = $(fitLinear)\n+\n+  /**\n+   * Param for L2 regularization parameter (&gt;= 0)\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val regParam: DoubleParam = new DoubleParam(this, \"regParam\",\n+    \"the magnitude of L2-regularization\", ParamValidators.gtEq(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getRegParam: Double = $(regParam)\n+\n+  /**\n+   * Param for mini-batch fraction, must be in range (0, 1]\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val miniBatchFraction: DoubleParam = new DoubleParam(this, \"miniBatchFraction\",\n+    \"fraction of the input data set that should be used for one iteration of gradient descent\",\n+    ParamValidators.inRange(0, 1, false, true))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getMiniBatchFraction: Double = $(miniBatchFraction)\n+\n+  /**\n+   * Param for standard deviation of initial coefficients\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final val initStd: DoubleParam = new DoubleParam(this, \"initStd\",\n+    \"standard deviation of initial coefficients\", ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  @Since(\"3.0.0\")\n+  final def getInitStd: Double = $(initStd)\n+\n+  /**\n+   * The solver algorithm for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group param\n+   */\n+  @Since(\"3.0.0\")\n+  final override val solver: Param[String] = new Param[String](this, \"solver\",\n+    \"The solver algorithm for optimization. Supported options: \" +\n+      s\"${supportedSolvers.mkString(\", \")}. (Default adamW)\",\n+    ParamValidators.inArray[String](supportedSolvers))\n+}\n+\n+private[ml] trait FactorizationMachines extends FactorizationMachinesParams {\n+\n+  private[ml] def initCoefficients(numFeatures: Int): OldVector = {\n+    val initialCoefficients =\n+      OldVectors.dense(\n+        Array.fill($(factorSize) * numFeatures)(Random.nextGaussian() * $(initStd)) ++\n+        (if ($(fitLinear)) new Array[Double](numFeatures) else Array.emptyDoubleArray) ++\n+        (if ($(fitBias)) new Array[Double](1) else Array.emptyDoubleArray))\n+    initialCoefficients\n+  }\n+\n+  private[ml] def trainImpl(\n+      data: RDD[(Double, OldVector)],\n+      numFeatures: Int,\n+      loss: String\n+    ): Vector = {\n+\n+    // initialize coefficients\n+    val initialCoefficients = initCoefficients(numFeatures)\n+    val coefficientsSize = initialCoefficients.size\n+\n+    // optimize coefficients with gradient descent\n+    val gradient = parseLoss(loss, $(factorSize), $(fitBias), $(fitLinear), numFeatures)\n+\n+    val updater = parseSolver($(solver), coefficientsSize)\n+\n+    val optimizer = new GradientDescent(gradient, updater)\n+      .setStepSize($(stepSize))\n+      .setNumIterations($(maxIter))\n+      .setRegParam($(regParam))\n+      .setMiniBatchFraction($(miniBatchFraction))\n+      .setConvergenceTol($(tol))\n+    val coefficients = optimizer.optimize(data, initialCoefficients)\n+    coefficients.asML\n+  }\n+}\n+\n+private[ml] object FactorizationMachines {\n+\n+  /** String name for \"gd\". */\n+  val GD = \"gd\"\n+\n+  /** String name for \"adamW\". */\n+  val AdamW = \"adamW\"\n+\n+  /** Set of solvers that FactorizationMachines supports. */\n+  val supportedSolvers = Array(GD, AdamW)\n+\n+  /** String name for \"logisticLoss\". */\n+  val LogisticLoss = \"logisticLoss\"\n+\n+  /** String name for \"squaredError\". */\n+  val SquaredError = \"squaredError\"\n+\n+  /** Set of loss function names that FactorizationMachines supports. */\n+  val supportedRegressorLosses = Array(SquaredError)\n+  val supportedClassifierLosses = Array(LogisticLoss)\n+  val supportedLosses = supportedRegressorLosses ++ supportedClassifierLosses\n+\n+  def parseSolver(solver: String, coefficientsSize: Int): Updater = {\n+    solver match {\n+      case GD => new SquaredL2Updater()\n+      case AdamW => new AdamWUpdater(coefficientsSize)\n+    }\n+  }\n+\n+  def parseLoss(\n+      lossFunc: String,\n+      factorSize: Int,\n+      fitBias: Boolean,\n+      fitLinear: Boolean,\n+      numFeatures: Int): BaseFactorizationMachinesGradient = {\n+    lossFunc match {\n+      case LogisticLoss =>\n+        new LogisticFactorizationMachinesGradient(factorSize, fitBias, fitLinear, numFeatures)\n+      case SquaredError =>\n+        new MSEFactorizationMachinesGradient(factorSize, fitBias, fitLinear, numFeatures)\n+      case _ => throw new IllegalArgumentException(s\"loss function type $lossFunc is invalidation\")\n+    }\n+  }\n+\n+  def splitCoefficients(\n+    coefficients: Vector,\n+    numFeatures: Int,\n+    factorSize: Int,\n+    fitBias: Boolean,\n+    fitLinear: Boolean\n+  ): (Double, Vector, Matrix) = {\n+    val bias = if (fitBias) coefficients(coefficients.size - 1) else 0.0\n+    val linear: Vector = if (fitLinear) {\n+      new DenseVector(coefficients.toArray.slice(numFeatures * factorSize, coefficients.size - 1))\n+    } else {\n+      Vectors.sparse(numFeatures, Seq.empty)\n+    }\n+    val factors = new DenseMatrix(numFeatures, factorSize,\n+      coefficients.toArray.slice(0, numFeatures * factorSize), true)\n+    (bias, linear, factors)\n+  }\n+\n+  def combineCoefficients(\n+    bias: Double,\n+    linear: Vector,\n+    factors: Matrix,\n+    fitBias: Boolean,\n+    fitLinear: Boolean\n+  ): Vector = {\n+    val coefficients = factors.toDense.values ++\n+      (if (fitLinear) linear.toArray else Array.emptyDoubleArray) ++\n+      (if (fitBias) Array(bias) else Array.emptyDoubleArray)\n+    new DenseVector(coefficients)\n+  }\n+}\n+\n+/**\n+ * Params for FMRegressor\n+ */\n+private[regression] trait FMRegressorParams extends FactorizationMachinesParams {\n+}\n+\n+/**\n+ * Factorization Machines learning algorithm for regression.\n+ * It supports normal gradient descent and AdamW solver.\n+ *\n+ * The implementation is based upon:\n+ * <a href=\"https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf\">\n+ * S. Rendle. \"Factorization machines\" 2010</a>.\n+ *\n+ * FM is able to estimate interactions even in problems with huge sparsity\n+ * (like advertising and recommendation system).\n+ * FM formula is:\n+ * {{{\n+ *   y = w_0 + \\sum\\limits^n_{i-1} w_i x_i +\n+ *     \\sum\\limits^n_{i=1} \\sum\\limits^n_{j=i+1} \\langle v_i, v_j \\rangle x_i x_j\n+ * }}}\n+ * First two terms denote global bias and linear term (as same as linear regression),\n+ * and last term denotes pairwise interactions term. {{{v_i}}} describes the i-th variable\n+ * with k factors.\n+ *\n+ * FM regression model uses MSE loss which can be solved by gradient descent method, and\n+ * regularization terms like L2 are usually added to the loss function to prevent overfitting.\n+ */\n+@Since(\"3.0.0\")\n+class FMRegressor @Since(\"3.0.0\") (\n+    @Since(\"3.0.0\") override val uid: String)\n+  extends Predictor[Vector, FMRegressor, FMRegressorModel]\n+  with FactorizationMachines with FMRegressorParams with DefaultParamsWritable with Logging {\n+\n+  @Since(\"3.0.0\")\n+  def this() = this(Identifiable.randomUID(\"fmr\"))\n+\n+  /**\n+   * Set the dimensionality of the factors.\n+   * Default is 8.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFactorSize(value: Int): this.type = set(factorSize, value)\n+  setDefault(factorSize -> 8)\n+\n+  /**\n+   * Set whether to fit global bias term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitBias(value: Boolean): this.type = set(fitBias, value)\n+  setDefault(fitBias -> true)\n+\n+  /**\n+   * Set whether to fit linear term.\n+   * Default is true.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setFitLinear(value: Boolean): this.type = set(fitLinear, value)\n+  setDefault(fitLinear -> true)\n+\n+  /**\n+   * Set the L2 regularization parameter.\n+   * Default is 0.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setRegParam(value: Double): this.type = set(regParam, value)\n+  setDefault(regParam -> 0.0)\n+\n+  /**\n+   * Set the mini-batch fraction parameter.\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMiniBatchFraction(value: Double): this.type = set(miniBatchFraction, value)\n+  setDefault(miniBatchFraction -> 1.0)\n+\n+  /**\n+   * Set the standard deviation of initial coefficients.\n+   * Default is 0.01.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setInitStd(value: Double): this.type = set(initStd, value)\n+  setDefault(initStd -> 0.01)\n+\n+  /**\n+   * Set the maximum number of iterations.\n+   * Default is 100.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setMaxIter(value: Int): this.type = set(maxIter, value)\n+  setDefault(maxIter -> 100)\n+\n+  /**\n+   * Set the initial step size for the first step (like learning rate).\n+   * Default is 1.0.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setStepSize(value: Double): this.type = set(stepSize, value)\n+  setDefault(stepSize -> 1.0)\n+\n+  /**\n+   * Set the convergence tolerance of iterations.\n+   * Default is 1E-6.\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setTol(value: Double): this.type = set(tol, value)\n+  setDefault(tol -> 1E-6)\n+\n+  /**\n+   * Set the solver algorithm used for optimization.\n+   * Supported options: \"gd\", \"adamW\".\n+   * Default: \"adamW\"\n+   *\n+   * @group setParam\n+   */\n+  @Since(\"3.0.0\")\n+  def setSolver(value: String): this.type = set(solver, value)\n+  setDefault(solver -> AdamW)\n+\n+  override protected[spark] def train(dataset: Dataset[_]): FMRegressorModel = {\n+    val handlePersistence = dataset.storageLevel == StorageLevel.NONE\n+    train(dataset, handlePersistence)\n+  }\n+\n+  protected[spark] def train(\n+      dataset: Dataset[_],\n+      handlePersistence: Boolean): FMRegressorModel = instrumented { instr =>\n+    val data: RDD[(Double, OldVector)] =\n+      dataset.select(col($(labelCol)), col($(featuresCol))).rdd.map {\n+        case Row(label: Double, features: Vector) =>\n+          (label, features)\n+      }\n+\n+    if (handlePersistence) data.persist(StorageLevel.MEMORY_AND_DISK)\n+\n+    instr.logPipelineStage(this)\n+    instr.logDataset(dataset)\n+    instr.logParams(this, factorSize, fitBias, fitLinear, regParam,\n+      miniBatchFraction, initStd, maxIter, stepSize, tol, solver)\n+\n+    val numFeatures = data.first()._2.size\n+    instr.logNumFeatures(numFeatures)\n+\n+    val coefficients = trainImpl(data, numFeatures, SquaredError)\n+\n+    val (bias, linear, factors) = splitCoefficients(\n+      coefficients, numFeatures, $(factorSize), $(fitBias), $(fitLinear))\n+\n+    if (handlePersistence) data.unpersist()\n+\n+    copyValues(new FMRegressorModel(uid, bias, linear, factors))\n+  }\n+\n+  @Since(\"3.0.0\")\n+  override def copy(extra: ParamMap): FMRegressor = defaultCopy(extra)\n+}\n+\n+@Since(\"3.0.0\")\n+object FMRegressor extends DefaultParamsReadable[FMRegressor] {\n+\n+  @Since(\"3.0.0\")\n+  override def load(path: String): FMRegressor = super.load(path)\n+}\n+\n+/**\n+ * Model produced by [[FMRegressor]].\n+ */\n+@Since(\"3.0.0\")\n+class FMRegressorModel private[regression] ("
  }],
  "prId": 26124
}]