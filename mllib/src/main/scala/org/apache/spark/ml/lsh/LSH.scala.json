[{
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "This seems include redundant operations.\nFor `DenseVector`, we can directly use its `values: Array[Double]`.\nFor `SparseVector`, we can use Breeze's subtraction op then get the data from the result.\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-19T07:29:48Z",
    "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import scala.util.Random\n+\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg.{Vector, VectorUDT}\n+import org.apache.spark.ml.param.{IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Params for [[LSH]].\n+ */\n+private[ml] trait LSHParams extends HasInputCol with HasOutputCol {\n+  /**\n+   * Param for output dimension.\n+   *\n+   * @group param\n+   */\n+  final val outputDim: IntParam = new IntParam(this, \"outputDim\", \"output dimension\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  final def getOutputDim: Int = $(outputDim)\n+\n+  setDefault(outputDim -> 1)\n+\n+  setDefault(outputCol -> \"lsh_output\")\n+\n+  /**\n+   * Transform the Schema for LSH\n+   * @param schema The schema of the input dataset without outputCol\n+   * @return A derived schema with outputCol added\n+   */\n+  final def transformLSHSchema(schema: StructType): StructType = {\n+    val outputFields = schema.fields :+\n+      StructField($(outputCol), new VectorUDT, nullable = false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+/**\n+ * Model produced by [[LSH]].\n+ */\n+abstract class LSHModel[KeyType, T <: LSHModel[KeyType, T]] private[ml]\n+  extends Model[T] with LSHParams {\n+  override def copy(extra: ParamMap): T = defaultCopy(extra)\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * The hash function of LSH, mapping a predefined KeyType to a Vector\n+   * @return The mapping of LSH function.\n+   */\n+  protected[this] val hashFunction: KeyType => Vector\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different keys using the distance metric corresponding\n+   * to the hashFunction\n+   * @param x One of the point in the metric space\n+   * @param y Another the point in the metric space\n+   * @return The distance between x and y in double\n+   */\n+  protected[ml] def keyDistance(x: KeyType, y: KeyType): Double\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different hash Vectors. By default, the distance is the\n+   * minimum distance of two hash values in any dimension.\n+   *\n+   * @param x One of the hash vector\n+   * @param y Another hash vector\n+   * @return The distance between hash vectors x and y in double\n+   */\n+  protected[ml] def hashDistance(x: Vector, y: Vector): Double = {\n+    (x.asBreeze - y.asBreeze).toArray.map(math.abs).min"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "I am wondering what's API to calculate the difference between two spark Vectors?\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-19T22:39:55Z",
    "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import scala.util.Random\n+\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg.{Vector, VectorUDT}\n+import org.apache.spark.ml.param.{IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Params for [[LSH]].\n+ */\n+private[ml] trait LSHParams extends HasInputCol with HasOutputCol {\n+  /**\n+   * Param for output dimension.\n+   *\n+   * @group param\n+   */\n+  final val outputDim: IntParam = new IntParam(this, \"outputDim\", \"output dimension\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  final def getOutputDim: Int = $(outputDim)\n+\n+  setDefault(outputDim -> 1)\n+\n+  setDefault(outputCol -> \"lsh_output\")\n+\n+  /**\n+   * Transform the Schema for LSH\n+   * @param schema The schema of the input dataset without outputCol\n+   * @return A derived schema with outputCol added\n+   */\n+  final def transformLSHSchema(schema: StructType): StructType = {\n+    val outputFields = schema.fields :+\n+      StructField($(outputCol), new VectorUDT, nullable = false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+/**\n+ * Model produced by [[LSH]].\n+ */\n+abstract class LSHModel[KeyType, T <: LSHModel[KeyType, T]] private[ml]\n+  extends Model[T] with LSHParams {\n+  override def copy(extra: ParamMap): T = defaultCopy(extra)\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * The hash function of LSH, mapping a predefined KeyType to a Vector\n+   * @return The mapping of LSH function.\n+   */\n+  protected[this] val hashFunction: KeyType => Vector\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different keys using the distance metric corresponding\n+   * to the hashFunction\n+   * @param x One of the point in the metric space\n+   * @param y Another the point in the metric space\n+   * @return The distance between x and y in double\n+   */\n+  protected[ml] def keyDistance(x: KeyType, y: KeyType): Double\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different hash Vectors. By default, the distance is the\n+   * minimum distance of two hash values in any dimension.\n+   *\n+   * @param x One of the hash vector\n+   * @param y Another hash vector\n+   * @return The distance between hash vectors x and y in double\n+   */\n+  protected[ml] def hashDistance(x: Vector, y: Vector): Double = {\n+    (x.asBreeze - y.asBreeze).toArray.map(math.abs).min"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "For a pair of `DenseVector`, you can directly use its `values` member and do something like:\n\nx.values.zip(y.values).map(x => math.abs(x._1 - x._2)).min\n\nFor a pair of `SparseVector`, you may not need to conver `(x.asBreeze - y.asBreeze)` back to `Array`, because the resulting array should be sparse too. We can directly map on the Breeze vector, i.e., `(x.asBreeze - y.Breeze).map(math.abs).min`.\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-20T04:08:36Z",
    "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import scala.util.Random\n+\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg.{Vector, VectorUDT}\n+import org.apache.spark.ml.param.{IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Params for [[LSH]].\n+ */\n+private[ml] trait LSHParams extends HasInputCol with HasOutputCol {\n+  /**\n+   * Param for output dimension.\n+   *\n+   * @group param\n+   */\n+  final val outputDim: IntParam = new IntParam(this, \"outputDim\", \"output dimension\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  final def getOutputDim: Int = $(outputDim)\n+\n+  setDefault(outputDim -> 1)\n+\n+  setDefault(outputCol -> \"lsh_output\")\n+\n+  /**\n+   * Transform the Schema for LSH\n+   * @param schema The schema of the input dataset without outputCol\n+   * @return A derived schema with outputCol added\n+   */\n+  final def transformLSHSchema(schema: StructType): StructType = {\n+    val outputFields = schema.fields :+\n+      StructField($(outputCol), new VectorUDT, nullable = false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+/**\n+ * Model produced by [[LSH]].\n+ */\n+abstract class LSHModel[KeyType, T <: LSHModel[KeyType, T]] private[ml]\n+  extends Model[T] with LSHParams {\n+  override def copy(extra: ParamMap): T = defaultCopy(extra)\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * The hash function of LSH, mapping a predefined KeyType to a Vector\n+   * @return The mapping of LSH function.\n+   */\n+  protected[this] val hashFunction: KeyType => Vector\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different keys using the distance metric corresponding\n+   * to the hashFunction\n+   * @param x One of the point in the metric space\n+   * @param y Another the point in the metric space\n+   * @return The distance between x and y in double\n+   */\n+  protected[ml] def keyDistance(x: KeyType, y: KeyType): Double\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different hash Vectors. By default, the distance is the\n+   * minimum distance of two hash values in any dimension.\n+   *\n+   * @param x One of the hash vector\n+   * @param y Another hash vector\n+   * @return The distance between hash vectors x and y in double\n+   */\n+  protected[ml] def hashDistance(x: Vector, y: Vector): Double = {\n+    (x.asBreeze - y.asBreeze).toArray.map(math.abs).min"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Thanks! Since it's generated by hashing, I am assuming it's a pair of dense vector.\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-20T15:27:59Z",
    "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import scala.util.Random\n+\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg.{Vector, VectorUDT}\n+import org.apache.spark.ml.param.{IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Params for [[LSH]].\n+ */\n+private[ml] trait LSHParams extends HasInputCol with HasOutputCol {\n+  /**\n+   * Param for output dimension.\n+   *\n+   * @group param\n+   */\n+  final val outputDim: IntParam = new IntParam(this, \"outputDim\", \"output dimension\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  final def getOutputDim: Int = $(outputDim)\n+\n+  setDefault(outputDim -> 1)\n+\n+  setDefault(outputCol -> \"lsh_output\")\n+\n+  /**\n+   * Transform the Schema for LSH\n+   * @param schema The schema of the input dataset without outputCol\n+   * @return A derived schema with outputCol added\n+   */\n+  final def transformLSHSchema(schema: StructType): StructType = {\n+    val outputFields = schema.fields :+\n+      StructField($(outputCol), new VectorUDT, nullable = false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+/**\n+ * Model produced by [[LSH]].\n+ */\n+abstract class LSHModel[KeyType, T <: LSHModel[KeyType, T]] private[ml]\n+  extends Model[T] with LSHParams {\n+  override def copy(extra: ParamMap): T = defaultCopy(extra)\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * The hash function of LSH, mapping a predefined KeyType to a Vector\n+   * @return The mapping of LSH function.\n+   */\n+  protected[this] val hashFunction: KeyType => Vector\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different keys using the distance metric corresponding\n+   * to the hashFunction\n+   * @param x One of the point in the metric space\n+   * @param y Another the point in the metric space\n+   * @return The distance between x and y in double\n+   */\n+  protected[ml] def keyDistance(x: KeyType, y: KeyType): Double\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different hash Vectors. By default, the distance is the\n+   * minimum distance of two hash values in any dimension.\n+   *\n+   * @param x One of the hash vector\n+   * @param y Another hash vector\n+   * @return The distance between hash vectors x and y in double\n+   */\n+  protected[ml] def hashDistance(x: Vector, y: Vector): Double = {\n+    (x.asBreeze - y.asBreeze).toArray.map(math.abs).min"
  }],
  "prId": 15148
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Usually we use `assert` for this. And more informative error message might be `The number of nearest neighbors cannot be less than 1`.\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-19T07:37:51Z",
    "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import scala.util.Random\n+\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg.{Vector, VectorUDT}\n+import org.apache.spark.ml.param.{IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Params for [[LSH]].\n+ */\n+private[ml] trait LSHParams extends HasInputCol with HasOutputCol {\n+  /**\n+   * Param for output dimension.\n+   *\n+   * @group param\n+   */\n+  final val outputDim: IntParam = new IntParam(this, \"outputDim\", \"output dimension\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  final def getOutputDim: Int = $(outputDim)\n+\n+  setDefault(outputDim -> 1)\n+\n+  setDefault(outputCol -> \"lsh_output\")\n+\n+  /**\n+   * Transform the Schema for LSH\n+   * @param schema The schema of the input dataset without outputCol\n+   * @return A derived schema with outputCol added\n+   */\n+  final def transformLSHSchema(schema: StructType): StructType = {\n+    val outputFields = schema.fields :+\n+      StructField($(outputCol), new VectorUDT, nullable = false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+/**\n+ * Model produced by [[LSH]].\n+ */\n+abstract class LSHModel[KeyType, T <: LSHModel[KeyType, T]] private[ml]\n+  extends Model[T] with LSHParams {\n+  override def copy(extra: ParamMap): T = defaultCopy(extra)\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * The hash function of LSH, mapping a predefined KeyType to a Vector\n+   * @return The mapping of LSH function.\n+   */\n+  protected[this] val hashFunction: KeyType => Vector\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different keys using the distance metric corresponding\n+   * to the hashFunction\n+   * @param x One of the point in the metric space\n+   * @param y Another the point in the metric space\n+   * @return The distance between x and y in double\n+   */\n+  protected[ml] def keyDistance(x: KeyType, y: KeyType): Double\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different hash Vectors. By default, the distance is the\n+   * minimum distance of two hash values in any dimension.\n+   *\n+   * @param x One of the hash vector\n+   * @param y Another hash vector\n+   * @return The distance between hash vectors x and y in double\n+   */\n+  protected[ml] def hashDistance(x: Vector, y: Vector): Double = {\n+    (x.asBreeze - y.asBreeze).toArray.map(math.abs).min\n+  }\n+\n+  /**\n+   * Transforms the input dataset.\n+   */\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    val transformUDF = udf(hashFunction, new VectorUDT)\n+    dataset.withColumn($(outputCol), transformUDF(dataset($(inputCol))))\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Check transform validity and derive the output schema from the input schema.\n+   *\n+   * Typical implementation should first conduct verification on schema change and parameter\n+   * validity, including complex parameter interaction checks.\n+   */\n+  override def transformSchema(schema: StructType): StructType = {\n+    transformLSHSchema(schema)\n+  }\n+\n+  /**\n+   * Given a large dataset and an item, approximately find at most k items which have the closest\n+   * distance to the item.\n+   * @param dataset the dataset to look for the key\n+   * @param key The key to hash for the item\n+   * @param k The maximum number of items closest to the key\n+   * @param distCol The column to store the distance between pairs\n+   * @return A dataset containing at most k items closest to the key. A distCol is added to show\n+   *         the distance between each record and the key.\n+   */\n+  def approxNearestNeighbors(dataset: Dataset[_], key: KeyType, k: Int = 1,\n+                             distCol: String = \"distance\"): Dataset[_] = {\n+    if (k < 1) {"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Done.\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-19T22:40:26Z",
    "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import scala.util.Random\n+\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg.{Vector, VectorUDT}\n+import org.apache.spark.ml.param.{IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Params for [[LSH]].\n+ */\n+private[ml] trait LSHParams extends HasInputCol with HasOutputCol {\n+  /**\n+   * Param for output dimension.\n+   *\n+   * @group param\n+   */\n+  final val outputDim: IntParam = new IntParam(this, \"outputDim\", \"output dimension\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  final def getOutputDim: Int = $(outputDim)\n+\n+  setDefault(outputDim -> 1)\n+\n+  setDefault(outputCol -> \"lsh_output\")\n+\n+  /**\n+   * Transform the Schema for LSH\n+   * @param schema The schema of the input dataset without outputCol\n+   * @return A derived schema with outputCol added\n+   */\n+  final def transformLSHSchema(schema: StructType): StructType = {\n+    val outputFields = schema.fields :+\n+      StructField($(outputCol), new VectorUDT, nullable = false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+/**\n+ * Model produced by [[LSH]].\n+ */\n+abstract class LSHModel[KeyType, T <: LSHModel[KeyType, T]] private[ml]\n+  extends Model[T] with LSHParams {\n+  override def copy(extra: ParamMap): T = defaultCopy(extra)\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * The hash function of LSH, mapping a predefined KeyType to a Vector\n+   * @return The mapping of LSH function.\n+   */\n+  protected[this] val hashFunction: KeyType => Vector\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different keys using the distance metric corresponding\n+   * to the hashFunction\n+   * @param x One of the point in the metric space\n+   * @param y Another the point in the metric space\n+   * @return The distance between x and y in double\n+   */\n+  protected[ml] def keyDistance(x: KeyType, y: KeyType): Double\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different hash Vectors. By default, the distance is the\n+   * minimum distance of two hash values in any dimension.\n+   *\n+   * @param x One of the hash vector\n+   * @param y Another hash vector\n+   * @return The distance between hash vectors x and y in double\n+   */\n+  protected[ml] def hashDistance(x: Vector, y: Vector): Double = {\n+    (x.asBreeze - y.asBreeze).toArray.map(math.abs).min\n+  }\n+\n+  /**\n+   * Transforms the input dataset.\n+   */\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    val transformUDF = udf(hashFunction, new VectorUDT)\n+    dataset.withColumn($(outputCol), transformUDF(dataset($(inputCol))))\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Check transform validity and derive the output schema from the input schema.\n+   *\n+   * Typical implementation should first conduct verification on schema change and parameter\n+   * validity, including complex parameter interaction checks.\n+   */\n+  override def transformSchema(schema: StructType): StructType = {\n+    transformLSHSchema(schema)\n+  }\n+\n+  /**\n+   * Given a large dataset and an item, approximately find at most k items which have the closest\n+   * distance to the item.\n+   * @param dataset the dataset to look for the key\n+   * @param key The key to hash for the item\n+   * @param k The maximum number of items closest to the key\n+   * @param distCol The column to store the distance between pairs\n+   * @return A dataset containing at most k items closest to the key. A distCol is added to show\n+   *         the distance between each record and the key.\n+   */\n+  def approxNearestNeighbors(dataset: Dataset[_], key: KeyType, k: Int = 1,\n+                             distCol: String = \"distance\"): Dataset[_] = {\n+    if (k < 1) {"
  }],
  "prId": 15148
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "You do `hashDistUDF` twice for the dataset. Besides, you might get less than k nearest neighbors in current approach. We can do this like:\n\n```\nval hashDistCol = \"_hash_dist\"\nmodelDataset.withColumn(hashDistCol, hashDistUDF(col($(outputCol))))\n  .sort(hashDistCol)\n  .drop(hashDistCol)\n  .limit(k)\n  .withColumn(distCol, keyDistUDF(col($(inputCol))))\n```\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-19T07:50:07Z",
    "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import scala.util.Random\n+\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg.{Vector, VectorUDT}\n+import org.apache.spark.ml.param.{IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Params for [[LSH]].\n+ */\n+private[ml] trait LSHParams extends HasInputCol with HasOutputCol {\n+  /**\n+   * Param for output dimension.\n+   *\n+   * @group param\n+   */\n+  final val outputDim: IntParam = new IntParam(this, \"outputDim\", \"output dimension\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  final def getOutputDim: Int = $(outputDim)\n+\n+  setDefault(outputDim -> 1)\n+\n+  setDefault(outputCol -> \"lsh_output\")\n+\n+  /**\n+   * Transform the Schema for LSH\n+   * @param schema The schema of the input dataset without outputCol\n+   * @return A derived schema with outputCol added\n+   */\n+  final def transformLSHSchema(schema: StructType): StructType = {\n+    val outputFields = schema.fields :+\n+      StructField($(outputCol), new VectorUDT, nullable = false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+/**\n+ * Model produced by [[LSH]].\n+ */\n+abstract class LSHModel[KeyType, T <: LSHModel[KeyType, T]] private[ml]\n+  extends Model[T] with LSHParams {\n+  override def copy(extra: ParamMap): T = defaultCopy(extra)\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * The hash function of LSH, mapping a predefined KeyType to a Vector\n+   * @return The mapping of LSH function.\n+   */\n+  protected[this] val hashFunction: KeyType => Vector\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different keys using the distance metric corresponding\n+   * to the hashFunction\n+   * @param x One of the point in the metric space\n+   * @param y Another the point in the metric space\n+   * @return The distance between x and y in double\n+   */\n+  protected[ml] def keyDistance(x: KeyType, y: KeyType): Double\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different hash Vectors. By default, the distance is the\n+   * minimum distance of two hash values in any dimension.\n+   *\n+   * @param x One of the hash vector\n+   * @param y Another hash vector\n+   * @return The distance between hash vectors x and y in double\n+   */\n+  protected[ml] def hashDistance(x: Vector, y: Vector): Double = {\n+    (x.asBreeze - y.asBreeze).toArray.map(math.abs).min\n+  }\n+\n+  /**\n+   * Transforms the input dataset.\n+   */\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    val transformUDF = udf(hashFunction, new VectorUDT)\n+    dataset.withColumn($(outputCol), transformUDF(dataset($(inputCol))))\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Check transform validity and derive the output schema from the input schema.\n+   *\n+   * Typical implementation should first conduct verification on schema change and parameter\n+   * validity, including complex parameter interaction checks.\n+   */\n+  override def transformSchema(schema: StructType): StructType = {\n+    transformLSHSchema(schema)\n+  }\n+\n+  /**\n+   * Given a large dataset and an item, approximately find at most k items which have the closest\n+   * distance to the item.\n+   * @param dataset the dataset to look for the key\n+   * @param key The key to hash for the item\n+   * @param k The maximum number of items closest to the key\n+   * @param distCol The column to store the distance between pairs\n+   * @return A dataset containing at most k items closest to the key. A distCol is added to show\n+   *         the distance between each record and the key.\n+   */\n+  def approxNearestNeighbors(dataset: Dataset[_], key: KeyType, k: Int = 1,\n+                             distCol: String = \"distance\"): Dataset[_] = {\n+    if (k < 1) {\n+      throw new Exception(s\"Invalid number of nearest neighbors $k\")\n+    }\n+    // Get Hash Value of the key v\n+    val keyHash = hashFunction(key)\n+    val modelDataset = transform(dataset)\n+\n+    // In the origin dataset, find the hash value u that is closest to v\n+    val hashDistUDF = udf((x: Vector) => hashDistance(x, keyHash), DataTypes.DoubleType)\n+    val nearestHashDataset = modelDataset.select(min(hashDistUDF(col($(outputCol)))))\n+    val nearestHashValue = nearestHashDataset.collect()(0)(0).asInstanceOf[Double]\n+\n+    // Filter the dataset where the hash value equals to u\n+    val modelSubset = modelDataset.filter(hashDistUDF(col($(outputCol))) === nearestHashValue)"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Actually this does not work because number of elements with the same \"hashDistCol\" can be much larger than k. In that case, we are random selecting k elements of the same \"hashDistCol\" value.\n\nTo resolve the issue you mentioned, I am changing nearestHashValue to hashThreshold, which is the maximum \"hashDistCol\" for the top k elements.\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-19T22:43:09Z",
    "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import scala.util.Random\n+\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg.{Vector, VectorUDT}\n+import org.apache.spark.ml.param.{IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Params for [[LSH]].\n+ */\n+private[ml] trait LSHParams extends HasInputCol with HasOutputCol {\n+  /**\n+   * Param for output dimension.\n+   *\n+   * @group param\n+   */\n+  final val outputDim: IntParam = new IntParam(this, \"outputDim\", \"output dimension\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  final def getOutputDim: Int = $(outputDim)\n+\n+  setDefault(outputDim -> 1)\n+\n+  setDefault(outputCol -> \"lsh_output\")\n+\n+  /**\n+   * Transform the Schema for LSH\n+   * @param schema The schema of the input dataset without outputCol\n+   * @return A derived schema with outputCol added\n+   */\n+  final def transformLSHSchema(schema: StructType): StructType = {\n+    val outputFields = schema.fields :+\n+      StructField($(outputCol), new VectorUDT, nullable = false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+/**\n+ * Model produced by [[LSH]].\n+ */\n+abstract class LSHModel[KeyType, T <: LSHModel[KeyType, T]] private[ml]\n+  extends Model[T] with LSHParams {\n+  override def copy(extra: ParamMap): T = defaultCopy(extra)\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * The hash function of LSH, mapping a predefined KeyType to a Vector\n+   * @return The mapping of LSH function.\n+   */\n+  protected[this] val hashFunction: KeyType => Vector\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different keys using the distance metric corresponding\n+   * to the hashFunction\n+   * @param x One of the point in the metric space\n+   * @param y Another the point in the metric space\n+   * @return The distance between x and y in double\n+   */\n+  protected[ml] def keyDistance(x: KeyType, y: KeyType): Double\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different hash Vectors. By default, the distance is the\n+   * minimum distance of two hash values in any dimension.\n+   *\n+   * @param x One of the hash vector\n+   * @param y Another hash vector\n+   * @return The distance between hash vectors x and y in double\n+   */\n+  protected[ml] def hashDistance(x: Vector, y: Vector): Double = {\n+    (x.asBreeze - y.asBreeze).toArray.map(math.abs).min\n+  }\n+\n+  /**\n+   * Transforms the input dataset.\n+   */\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    val transformUDF = udf(hashFunction, new VectorUDT)\n+    dataset.withColumn($(outputCol), transformUDF(dataset($(inputCol))))\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Check transform validity and derive the output schema from the input schema.\n+   *\n+   * Typical implementation should first conduct verification on schema change and parameter\n+   * validity, including complex parameter interaction checks.\n+   */\n+  override def transformSchema(schema: StructType): StructType = {\n+    transformLSHSchema(schema)\n+  }\n+\n+  /**\n+   * Given a large dataset and an item, approximately find at most k items which have the closest\n+   * distance to the item.\n+   * @param dataset the dataset to look for the key\n+   * @param key The key to hash for the item\n+   * @param k The maximum number of items closest to the key\n+   * @param distCol The column to store the distance between pairs\n+   * @return A dataset containing at most k items closest to the key. A distCol is added to show\n+   *         the distance between each record and the key.\n+   */\n+  def approxNearestNeighbors(dataset: Dataset[_], key: KeyType, k: Int = 1,\n+                             distCol: String = \"distance\"): Dataset[_] = {\n+    if (k < 1) {\n+      throw new Exception(s\"Invalid number of nearest neighbors $k\")\n+    }\n+    // Get Hash Value of the key v\n+    val keyHash = hashFunction(key)\n+    val modelDataset = transform(dataset)\n+\n+    // In the origin dataset, find the hash value u that is closest to v\n+    val hashDistUDF = udf((x: Vector) => hashDistance(x, keyHash), DataTypes.DoubleType)\n+    val nearestHashDataset = modelDataset.select(min(hashDistUDF(col($(outputCol)))))\n+    val nearestHashValue = nearestHashDataset.collect()(0)(0).asInstanceOf[Double]\n+\n+    // Filter the dataset where the hash value equals to u\n+    val modelSubset = modelDataset.filter(hashDistUDF(col($(outputCol))) === nearestHashValue)"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Yeah, I think we can replace the `limit` above to a `filter` to choose the elements failed in this range.\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-20T03:51:16Z",
    "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import scala.util.Random\n+\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg.{Vector, VectorUDT}\n+import org.apache.spark.ml.param.{IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Params for [[LSH]].\n+ */\n+private[ml] trait LSHParams extends HasInputCol with HasOutputCol {\n+  /**\n+   * Param for output dimension.\n+   *\n+   * @group param\n+   */\n+  final val outputDim: IntParam = new IntParam(this, \"outputDim\", \"output dimension\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  final def getOutputDim: Int = $(outputDim)\n+\n+  setDefault(outputDim -> 1)\n+\n+  setDefault(outputCol -> \"lsh_output\")\n+\n+  /**\n+   * Transform the Schema for LSH\n+   * @param schema The schema of the input dataset without outputCol\n+   * @return A derived schema with outputCol added\n+   */\n+  final def transformLSHSchema(schema: StructType): StructType = {\n+    val outputFields = schema.fields :+\n+      StructField($(outputCol), new VectorUDT, nullable = false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+/**\n+ * Model produced by [[LSH]].\n+ */\n+abstract class LSHModel[KeyType, T <: LSHModel[KeyType, T]] private[ml]\n+  extends Model[T] with LSHParams {\n+  override def copy(extra: ParamMap): T = defaultCopy(extra)\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * The hash function of LSH, mapping a predefined KeyType to a Vector\n+   * @return The mapping of LSH function.\n+   */\n+  protected[this] val hashFunction: KeyType => Vector\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different keys using the distance metric corresponding\n+   * to the hashFunction\n+   * @param x One of the point in the metric space\n+   * @param y Another the point in the metric space\n+   * @return The distance between x and y in double\n+   */\n+  protected[ml] def keyDistance(x: KeyType, y: KeyType): Double\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different hash Vectors. By default, the distance is the\n+   * minimum distance of two hash values in any dimension.\n+   *\n+   * @param x One of the hash vector\n+   * @param y Another hash vector\n+   * @return The distance between hash vectors x and y in double\n+   */\n+  protected[ml] def hashDistance(x: Vector, y: Vector): Double = {\n+    (x.asBreeze - y.asBreeze).toArray.map(math.abs).min\n+  }\n+\n+  /**\n+   * Transforms the input dataset.\n+   */\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    val transformUDF = udf(hashFunction, new VectorUDT)\n+    dataset.withColumn($(outputCol), transformUDF(dataset($(inputCol))))\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Check transform validity and derive the output schema from the input schema.\n+   *\n+   * Typical implementation should first conduct verification on schema change and parameter\n+   * validity, including complex parameter interaction checks.\n+   */\n+  override def transformSchema(schema: StructType): StructType = {\n+    transformLSHSchema(schema)\n+  }\n+\n+  /**\n+   * Given a large dataset and an item, approximately find at most k items which have the closest\n+   * distance to the item.\n+   * @param dataset the dataset to look for the key\n+   * @param key The key to hash for the item\n+   * @param k The maximum number of items closest to the key\n+   * @param distCol The column to store the distance between pairs\n+   * @return A dataset containing at most k items closest to the key. A distCol is added to show\n+   *         the distance between each record and the key.\n+   */\n+  def approxNearestNeighbors(dataset: Dataset[_], key: KeyType, k: Int = 1,\n+                             distCol: String = \"distance\"): Dataset[_] = {\n+    if (k < 1) {\n+      throw new Exception(s\"Invalid number of nearest neighbors $k\")\n+    }\n+    // Get Hash Value of the key v\n+    val keyHash = hashFunction(key)\n+    val modelDataset = transform(dataset)\n+\n+    // In the origin dataset, find the hash value u that is closest to v\n+    val hashDistUDF = udf((x: Vector) => hashDistance(x, keyHash), DataTypes.DoubleType)\n+    val nearestHashDataset = modelDataset.select(min(hashDistUDF(col($(outputCol)))))\n+    val nearestHashValue = nearestHashDataset.collect()(0)(0).asInstanceOf[Double]\n+\n+    // Filter the dataset where the hash value equals to u\n+    val modelSubset = modelDataset.filter(hashDistUDF(col($(outputCol))) === nearestHashValue)"
  }],
  "prId": 15148
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Do we need this? I think we already do dedup operation in Analyzer for self-join.\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-19T08:01:57Z",
    "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import scala.util.Random\n+\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg.{Vector, VectorUDT}\n+import org.apache.spark.ml.param.{IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Params for [[LSH]].\n+ */\n+private[ml] trait LSHParams extends HasInputCol with HasOutputCol {\n+  /**\n+   * Param for output dimension.\n+   *\n+   * @group param\n+   */\n+  final val outputDim: IntParam = new IntParam(this, \"outputDim\", \"output dimension\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  final def getOutputDim: Int = $(outputDim)\n+\n+  setDefault(outputDim -> 1)\n+\n+  setDefault(outputCol -> \"lsh_output\")\n+\n+  /**\n+   * Transform the Schema for LSH\n+   * @param schema The schema of the input dataset without outputCol\n+   * @return A derived schema with outputCol added\n+   */\n+  final def transformLSHSchema(schema: StructType): StructType = {\n+    val outputFields = schema.fields :+\n+      StructField($(outputCol), new VectorUDT, nullable = false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+/**\n+ * Model produced by [[LSH]].\n+ */\n+abstract class LSHModel[KeyType, T <: LSHModel[KeyType, T]] private[ml]\n+  extends Model[T] with LSHParams {\n+  override def copy(extra: ParamMap): T = defaultCopy(extra)\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * The hash function of LSH, mapping a predefined KeyType to a Vector\n+   * @return The mapping of LSH function.\n+   */\n+  protected[this] val hashFunction: KeyType => Vector\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different keys using the distance metric corresponding\n+   * to the hashFunction\n+   * @param x One of the point in the metric space\n+   * @param y Another the point in the metric space\n+   * @return The distance between x and y in double\n+   */\n+  protected[ml] def keyDistance(x: KeyType, y: KeyType): Double\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different hash Vectors. By default, the distance is the\n+   * minimum distance of two hash values in any dimension.\n+   *\n+   * @param x One of the hash vector\n+   * @param y Another hash vector\n+   * @return The distance between hash vectors x and y in double\n+   */\n+  protected[ml] def hashDistance(x: Vector, y: Vector): Double = {\n+    (x.asBreeze - y.asBreeze).toArray.map(math.abs).min\n+  }\n+\n+  /**\n+   * Transforms the input dataset.\n+   */\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    val transformUDF = udf(hashFunction, new VectorUDT)\n+    dataset.withColumn($(outputCol), transformUDF(dataset($(inputCol))))\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Check transform validity and derive the output schema from the input schema.\n+   *\n+   * Typical implementation should first conduct verification on schema change and parameter\n+   * validity, including complex parameter interaction checks.\n+   */\n+  override def transformSchema(schema: StructType): StructType = {\n+    transformLSHSchema(schema)\n+  }\n+\n+  /**\n+   * Given a large dataset and an item, approximately find at most k items which have the closest\n+   * distance to the item.\n+   * @param dataset the dataset to look for the key\n+   * @param key The key to hash for the item\n+   * @param k The maximum number of items closest to the key\n+   * @param distCol The column to store the distance between pairs\n+   * @return A dataset containing at most k items closest to the key. A distCol is added to show\n+   *         the distance between each record and the key.\n+   */\n+  def approxNearestNeighbors(dataset: Dataset[_], key: KeyType, k: Int = 1,\n+                             distCol: String = \"distance\"): Dataset[_] = {\n+    if (k < 1) {\n+      throw new Exception(s\"Invalid number of nearest neighbors $k\")\n+    }\n+    // Get Hash Value of the key v\n+    val keyHash = hashFunction(key)\n+    val modelDataset = transform(dataset)\n+\n+    // In the origin dataset, find the hash value u that is closest to v\n+    val hashDistUDF = udf((x: Vector) => hashDistance(x, keyHash), DataTypes.DoubleType)\n+    val nearestHashDataset = modelDataset.select(min(hashDistUDF(col($(outputCol)))))\n+    val nearestHashValue = nearestHashDataset.collect()(0)(0).asInstanceOf[Double]\n+\n+    // Filter the dataset where the hash value equals to u\n+    val modelSubset = modelDataset.filter(hashDistUDF(col($(outputCol))) === nearestHashValue)\n+\n+    // Get the top k nearest neighbor by their distance to the key\n+    val keyDistUDF = udf((x: KeyType) => keyDistance(x, key), DataTypes.DoubleType)\n+    val modelSubsetWithDistCol = modelSubset.withColumn(distCol, keyDistUDF(col($(inputCol))))\n+    modelSubsetWithDistCol.sort(distCol).limit(k)\n+  }\n+\n+  /**\n+   * Preprocess step for approximate similarity join. Transform and explode the outputCol to\n+   * explodeCols.\n+   * @param dataset The dataset to transform and explode.\n+   * @param explodeCols The alias for the exploded columns, must be a seq of two strings.\n+   * @return A dataset containing idCol, inputCol and explodeCols\n+   */\n+  private[this] def processDataset(dataset: Dataset[_], explodeCols: Seq[String]): Dataset[_] = {\n+    if (explodeCols.size != 2) {\n+      throw new Exception(\"explodeCols must be two strings.\")\n+    }\n+    val vectorToMap: UserDefinedFunction = udf((x: Vector) => x.asBreeze.iterator.toMap,\n+      MapType(DataTypes.IntegerType, DataTypes.DoubleType))\n+    transform(dataset)\n+      .select(col(\"*\"), explode(vectorToMap(col($(outputCol)))).as(explodeCols))\n+  }\n+\n+  /**\n+   * Recreate a column using the same column name but different attribute id. Used in approximate\n+   * similarity join.\n+   * @param dataset The dataset where a column need to recreate\n+   * @param colName The name of the column to recreate\n+   * @param tmpColName A temporary column name which does not conflict with existing columns\n+   * @return\n+   */\n+  private[this] def recreateCol(dataset: Dataset[_], colName: String,\n+                                tmpColName: String): Dataset[_] = {\n+    dataset\n+      .withColumnRenamed(colName, tmpColName)\n+      .withColumn(colName, col(tmpColName))\n+      .drop(tmpColName)\n+  }\n+\n+  /**\n+   * Join two dataset to approximately find all pairs of records whose distance are smaller\n+   * than the threshold.\n+   * @param datasetA One of the datasets to join\n+   * @param datasetB Another dataset to join\n+   * @param threshold The threshold for the distance of record pairs\n+   * @param distCol The column to store the distance between pairs\n+   * @return A joined dataset containing pairs of records. A distCol is added to show the distance\n+   *         between each pair of records.\n+   */\n+  def approxSimilarityJoin(datasetA: Dataset[_], datasetB: Dataset[_], threshold: Double,\n+                           distCol: String = \"distance\"): Dataset[_] = {\n+\n+    val explodeCols = Seq(\"lsh#entry\", \"lsh#hashValue\")\n+    val explodedA = processDataset(datasetA, explodeCols)\n+\n+    // If this is a self join, we need to recreate the inputCol of datasetB to avoid ambiguity."
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Got it. You want to access inputCol from both left and right sides.\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-19T08:04:50Z",
    "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import scala.util.Random\n+\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg.{Vector, VectorUDT}\n+import org.apache.spark.ml.param.{IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Params for [[LSH]].\n+ */\n+private[ml] trait LSHParams extends HasInputCol with HasOutputCol {\n+  /**\n+   * Param for output dimension.\n+   *\n+   * @group param\n+   */\n+  final val outputDim: IntParam = new IntParam(this, \"outputDim\", \"output dimension\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  final def getOutputDim: Int = $(outputDim)\n+\n+  setDefault(outputDim -> 1)\n+\n+  setDefault(outputCol -> \"lsh_output\")\n+\n+  /**\n+   * Transform the Schema for LSH\n+   * @param schema The schema of the input dataset without outputCol\n+   * @return A derived schema with outputCol added\n+   */\n+  final def transformLSHSchema(schema: StructType): StructType = {\n+    val outputFields = schema.fields :+\n+      StructField($(outputCol), new VectorUDT, nullable = false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+/**\n+ * Model produced by [[LSH]].\n+ */\n+abstract class LSHModel[KeyType, T <: LSHModel[KeyType, T]] private[ml]\n+  extends Model[T] with LSHParams {\n+  override def copy(extra: ParamMap): T = defaultCopy(extra)\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * The hash function of LSH, mapping a predefined KeyType to a Vector\n+   * @return The mapping of LSH function.\n+   */\n+  protected[this] val hashFunction: KeyType => Vector\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different keys using the distance metric corresponding\n+   * to the hashFunction\n+   * @param x One of the point in the metric space\n+   * @param y Another the point in the metric space\n+   * @return The distance between x and y in double\n+   */\n+  protected[ml] def keyDistance(x: KeyType, y: KeyType): Double\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different hash Vectors. By default, the distance is the\n+   * minimum distance of two hash values in any dimension.\n+   *\n+   * @param x One of the hash vector\n+   * @param y Another hash vector\n+   * @return The distance between hash vectors x and y in double\n+   */\n+  protected[ml] def hashDistance(x: Vector, y: Vector): Double = {\n+    (x.asBreeze - y.asBreeze).toArray.map(math.abs).min\n+  }\n+\n+  /**\n+   * Transforms the input dataset.\n+   */\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    val transformUDF = udf(hashFunction, new VectorUDT)\n+    dataset.withColumn($(outputCol), transformUDF(dataset($(inputCol))))\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Check transform validity and derive the output schema from the input schema.\n+   *\n+   * Typical implementation should first conduct verification on schema change and parameter\n+   * validity, including complex parameter interaction checks.\n+   */\n+  override def transformSchema(schema: StructType): StructType = {\n+    transformLSHSchema(schema)\n+  }\n+\n+  /**\n+   * Given a large dataset and an item, approximately find at most k items which have the closest\n+   * distance to the item.\n+   * @param dataset the dataset to look for the key\n+   * @param key The key to hash for the item\n+   * @param k The maximum number of items closest to the key\n+   * @param distCol The column to store the distance between pairs\n+   * @return A dataset containing at most k items closest to the key. A distCol is added to show\n+   *         the distance between each record and the key.\n+   */\n+  def approxNearestNeighbors(dataset: Dataset[_], key: KeyType, k: Int = 1,\n+                             distCol: String = \"distance\"): Dataset[_] = {\n+    if (k < 1) {\n+      throw new Exception(s\"Invalid number of nearest neighbors $k\")\n+    }\n+    // Get Hash Value of the key v\n+    val keyHash = hashFunction(key)\n+    val modelDataset = transform(dataset)\n+\n+    // In the origin dataset, find the hash value u that is closest to v\n+    val hashDistUDF = udf((x: Vector) => hashDistance(x, keyHash), DataTypes.DoubleType)\n+    val nearestHashDataset = modelDataset.select(min(hashDistUDF(col($(outputCol)))))\n+    val nearestHashValue = nearestHashDataset.collect()(0)(0).asInstanceOf[Double]\n+\n+    // Filter the dataset where the hash value equals to u\n+    val modelSubset = modelDataset.filter(hashDistUDF(col($(outputCol))) === nearestHashValue)\n+\n+    // Get the top k nearest neighbor by their distance to the key\n+    val keyDistUDF = udf((x: KeyType) => keyDistance(x, key), DataTypes.DoubleType)\n+    val modelSubsetWithDistCol = modelSubset.withColumn(distCol, keyDistUDF(col($(inputCol))))\n+    modelSubsetWithDistCol.sort(distCol).limit(k)\n+  }\n+\n+  /**\n+   * Preprocess step for approximate similarity join. Transform and explode the outputCol to\n+   * explodeCols.\n+   * @param dataset The dataset to transform and explode.\n+   * @param explodeCols The alias for the exploded columns, must be a seq of two strings.\n+   * @return A dataset containing idCol, inputCol and explodeCols\n+   */\n+  private[this] def processDataset(dataset: Dataset[_], explodeCols: Seq[String]): Dataset[_] = {\n+    if (explodeCols.size != 2) {\n+      throw new Exception(\"explodeCols must be two strings.\")\n+    }\n+    val vectorToMap: UserDefinedFunction = udf((x: Vector) => x.asBreeze.iterator.toMap,\n+      MapType(DataTypes.IntegerType, DataTypes.DoubleType))\n+    transform(dataset)\n+      .select(col(\"*\"), explode(vectorToMap(col($(outputCol)))).as(explodeCols))\n+  }\n+\n+  /**\n+   * Recreate a column using the same column name but different attribute id. Used in approximate\n+   * similarity join.\n+   * @param dataset The dataset where a column need to recreate\n+   * @param colName The name of the column to recreate\n+   * @param tmpColName A temporary column name which does not conflict with existing columns\n+   * @return\n+   */\n+  private[this] def recreateCol(dataset: Dataset[_], colName: String,\n+                                tmpColName: String): Dataset[_] = {\n+    dataset\n+      .withColumnRenamed(colName, tmpColName)\n+      .withColumn(colName, col(tmpColName))\n+      .drop(tmpColName)\n+  }\n+\n+  /**\n+   * Join two dataset to approximately find all pairs of records whose distance are smaller\n+   * than the threshold.\n+   * @param datasetA One of the datasets to join\n+   * @param datasetB Another dataset to join\n+   * @param threshold The threshold for the distance of record pairs\n+   * @param distCol The column to store the distance between pairs\n+   * @return A joined dataset containing pairs of records. A distCol is added to show the distance\n+   *         between each pair of records.\n+   */\n+  def approxSimilarityJoin(datasetA: Dataset[_], datasetB: Dataset[_], threshold: Double,\n+                           distCol: String = \"distance\"): Dataset[_] = {\n+\n+    val explodeCols = Seq(\"lsh#entry\", \"lsh#hashValue\")\n+    val explodedA = processDataset(datasetA, explodeCols)\n+\n+    // If this is a self join, we need to recreate the inputCol of datasetB to avoid ambiguity."
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Once #14719 is merged, I think we can skip this redundant operation.\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-19T08:06:22Z",
    "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import scala.util.Random\n+\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg.{Vector, VectorUDT}\n+import org.apache.spark.ml.param.{IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Params for [[LSH]].\n+ */\n+private[ml] trait LSHParams extends HasInputCol with HasOutputCol {\n+  /**\n+   * Param for output dimension.\n+   *\n+   * @group param\n+   */\n+  final val outputDim: IntParam = new IntParam(this, \"outputDim\", \"output dimension\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  final def getOutputDim: Int = $(outputDim)\n+\n+  setDefault(outputDim -> 1)\n+\n+  setDefault(outputCol -> \"lsh_output\")\n+\n+  /**\n+   * Transform the Schema for LSH\n+   * @param schema The schema of the input dataset without outputCol\n+   * @return A derived schema with outputCol added\n+   */\n+  final def transformLSHSchema(schema: StructType): StructType = {\n+    val outputFields = schema.fields :+\n+      StructField($(outputCol), new VectorUDT, nullable = false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+/**\n+ * Model produced by [[LSH]].\n+ */\n+abstract class LSHModel[KeyType, T <: LSHModel[KeyType, T]] private[ml]\n+  extends Model[T] with LSHParams {\n+  override def copy(extra: ParamMap): T = defaultCopy(extra)\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * The hash function of LSH, mapping a predefined KeyType to a Vector\n+   * @return The mapping of LSH function.\n+   */\n+  protected[this] val hashFunction: KeyType => Vector\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different keys using the distance metric corresponding\n+   * to the hashFunction\n+   * @param x One of the point in the metric space\n+   * @param y Another the point in the metric space\n+   * @return The distance between x and y in double\n+   */\n+  protected[ml] def keyDistance(x: KeyType, y: KeyType): Double\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different hash Vectors. By default, the distance is the\n+   * minimum distance of two hash values in any dimension.\n+   *\n+   * @param x One of the hash vector\n+   * @param y Another hash vector\n+   * @return The distance between hash vectors x and y in double\n+   */\n+  protected[ml] def hashDistance(x: Vector, y: Vector): Double = {\n+    (x.asBreeze - y.asBreeze).toArray.map(math.abs).min\n+  }\n+\n+  /**\n+   * Transforms the input dataset.\n+   */\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    val transformUDF = udf(hashFunction, new VectorUDT)\n+    dataset.withColumn($(outputCol), transformUDF(dataset($(inputCol))))\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Check transform validity and derive the output schema from the input schema.\n+   *\n+   * Typical implementation should first conduct verification on schema change and parameter\n+   * validity, including complex parameter interaction checks.\n+   */\n+  override def transformSchema(schema: StructType): StructType = {\n+    transformLSHSchema(schema)\n+  }\n+\n+  /**\n+   * Given a large dataset and an item, approximately find at most k items which have the closest\n+   * distance to the item.\n+   * @param dataset the dataset to look for the key\n+   * @param key The key to hash for the item\n+   * @param k The maximum number of items closest to the key\n+   * @param distCol The column to store the distance between pairs\n+   * @return A dataset containing at most k items closest to the key. A distCol is added to show\n+   *         the distance between each record and the key.\n+   */\n+  def approxNearestNeighbors(dataset: Dataset[_], key: KeyType, k: Int = 1,\n+                             distCol: String = \"distance\"): Dataset[_] = {\n+    if (k < 1) {\n+      throw new Exception(s\"Invalid number of nearest neighbors $k\")\n+    }\n+    // Get Hash Value of the key v\n+    val keyHash = hashFunction(key)\n+    val modelDataset = transform(dataset)\n+\n+    // In the origin dataset, find the hash value u that is closest to v\n+    val hashDistUDF = udf((x: Vector) => hashDistance(x, keyHash), DataTypes.DoubleType)\n+    val nearestHashDataset = modelDataset.select(min(hashDistUDF(col($(outputCol)))))\n+    val nearestHashValue = nearestHashDataset.collect()(0)(0).asInstanceOf[Double]\n+\n+    // Filter the dataset where the hash value equals to u\n+    val modelSubset = modelDataset.filter(hashDistUDF(col($(outputCol))) === nearestHashValue)\n+\n+    // Get the top k nearest neighbor by their distance to the key\n+    val keyDistUDF = udf((x: KeyType) => keyDistance(x, key), DataTypes.DoubleType)\n+    val modelSubsetWithDistCol = modelSubset.withColumn(distCol, keyDistUDF(col($(inputCol))))\n+    modelSubsetWithDistCol.sort(distCol).limit(k)\n+  }\n+\n+  /**\n+   * Preprocess step for approximate similarity join. Transform and explode the outputCol to\n+   * explodeCols.\n+   * @param dataset The dataset to transform and explode.\n+   * @param explodeCols The alias for the exploded columns, must be a seq of two strings.\n+   * @return A dataset containing idCol, inputCol and explodeCols\n+   */\n+  private[this] def processDataset(dataset: Dataset[_], explodeCols: Seq[String]): Dataset[_] = {\n+    if (explodeCols.size != 2) {\n+      throw new Exception(\"explodeCols must be two strings.\")\n+    }\n+    val vectorToMap: UserDefinedFunction = udf((x: Vector) => x.asBreeze.iterator.toMap,\n+      MapType(DataTypes.IntegerType, DataTypes.DoubleType))\n+    transform(dataset)\n+      .select(col(\"*\"), explode(vectorToMap(col($(outputCol)))).as(explodeCols))\n+  }\n+\n+  /**\n+   * Recreate a column using the same column name but different attribute id. Used in approximate\n+   * similarity join.\n+   * @param dataset The dataset where a column need to recreate\n+   * @param colName The name of the column to recreate\n+   * @param tmpColName A temporary column name which does not conflict with existing columns\n+   * @return\n+   */\n+  private[this] def recreateCol(dataset: Dataset[_], colName: String,\n+                                tmpColName: String): Dataset[_] = {\n+    dataset\n+      .withColumnRenamed(colName, tmpColName)\n+      .withColumn(colName, col(tmpColName))\n+      .drop(tmpColName)\n+  }\n+\n+  /**\n+   * Join two dataset to approximately find all pairs of records whose distance are smaller\n+   * than the threshold.\n+   * @param datasetA One of the datasets to join\n+   * @param datasetB Another dataset to join\n+   * @param threshold The threshold for the distance of record pairs\n+   * @param distCol The column to store the distance between pairs\n+   * @return A joined dataset containing pairs of records. A distCol is added to show the distance\n+   *         between each pair of records.\n+   */\n+  def approxSimilarityJoin(datasetA: Dataset[_], datasetB: Dataset[_], threshold: Double,\n+                           distCol: String = \"distance\"): Dataset[_] = {\n+\n+    val explodeCols = Seq(\"lsh#entry\", \"lsh#hashValue\")\n+    val explodedA = processDataset(datasetA, explodeCols)\n+\n+    // If this is a self join, we need to recreate the inputCol of datasetB to avoid ambiguity."
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Added a TODO.\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-19T22:37:37Z",
    "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import scala.util.Random\n+\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg.{Vector, VectorUDT}\n+import org.apache.spark.ml.param.{IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Params for [[LSH]].\n+ */\n+private[ml] trait LSHParams extends HasInputCol with HasOutputCol {\n+  /**\n+   * Param for output dimension.\n+   *\n+   * @group param\n+   */\n+  final val outputDim: IntParam = new IntParam(this, \"outputDim\", \"output dimension\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  final def getOutputDim: Int = $(outputDim)\n+\n+  setDefault(outputDim -> 1)\n+\n+  setDefault(outputCol -> \"lsh_output\")\n+\n+  /**\n+   * Transform the Schema for LSH\n+   * @param schema The schema of the input dataset without outputCol\n+   * @return A derived schema with outputCol added\n+   */\n+  final def transformLSHSchema(schema: StructType): StructType = {\n+    val outputFields = schema.fields :+\n+      StructField($(outputCol), new VectorUDT, nullable = false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+/**\n+ * Model produced by [[LSH]].\n+ */\n+abstract class LSHModel[KeyType, T <: LSHModel[KeyType, T]] private[ml]\n+  extends Model[T] with LSHParams {\n+  override def copy(extra: ParamMap): T = defaultCopy(extra)\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * The hash function of LSH, mapping a predefined KeyType to a Vector\n+   * @return The mapping of LSH function.\n+   */\n+  protected[this] val hashFunction: KeyType => Vector\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different keys using the distance metric corresponding\n+   * to the hashFunction\n+   * @param x One of the point in the metric space\n+   * @param y Another the point in the metric space\n+   * @return The distance between x and y in double\n+   */\n+  protected[ml] def keyDistance(x: KeyType, y: KeyType): Double\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different hash Vectors. By default, the distance is the\n+   * minimum distance of two hash values in any dimension.\n+   *\n+   * @param x One of the hash vector\n+   * @param y Another hash vector\n+   * @return The distance between hash vectors x and y in double\n+   */\n+  protected[ml] def hashDistance(x: Vector, y: Vector): Double = {\n+    (x.asBreeze - y.asBreeze).toArray.map(math.abs).min\n+  }\n+\n+  /**\n+   * Transforms the input dataset.\n+   */\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    val transformUDF = udf(hashFunction, new VectorUDT)\n+    dataset.withColumn($(outputCol), transformUDF(dataset($(inputCol))))\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Check transform validity and derive the output schema from the input schema.\n+   *\n+   * Typical implementation should first conduct verification on schema change and parameter\n+   * validity, including complex parameter interaction checks.\n+   */\n+  override def transformSchema(schema: StructType): StructType = {\n+    transformLSHSchema(schema)\n+  }\n+\n+  /**\n+   * Given a large dataset and an item, approximately find at most k items which have the closest\n+   * distance to the item.\n+   * @param dataset the dataset to look for the key\n+   * @param key The key to hash for the item\n+   * @param k The maximum number of items closest to the key\n+   * @param distCol The column to store the distance between pairs\n+   * @return A dataset containing at most k items closest to the key. A distCol is added to show\n+   *         the distance between each record and the key.\n+   */\n+  def approxNearestNeighbors(dataset: Dataset[_], key: KeyType, k: Int = 1,\n+                             distCol: String = \"distance\"): Dataset[_] = {\n+    if (k < 1) {\n+      throw new Exception(s\"Invalid number of nearest neighbors $k\")\n+    }\n+    // Get Hash Value of the key v\n+    val keyHash = hashFunction(key)\n+    val modelDataset = transform(dataset)\n+\n+    // In the origin dataset, find the hash value u that is closest to v\n+    val hashDistUDF = udf((x: Vector) => hashDistance(x, keyHash), DataTypes.DoubleType)\n+    val nearestHashDataset = modelDataset.select(min(hashDistUDF(col($(outputCol)))))\n+    val nearestHashValue = nearestHashDataset.collect()(0)(0).asInstanceOf[Double]\n+\n+    // Filter the dataset where the hash value equals to u\n+    val modelSubset = modelDataset.filter(hashDistUDF(col($(outputCol))) === nearestHashValue)\n+\n+    // Get the top k nearest neighbor by their distance to the key\n+    val keyDistUDF = udf((x: KeyType) => keyDistance(x, key), DataTypes.DoubleType)\n+    val modelSubsetWithDistCol = modelSubset.withColumn(distCol, keyDistUDF(col($(inputCol))))\n+    modelSubsetWithDistCol.sort(distCol).limit(k)\n+  }\n+\n+  /**\n+   * Preprocess step for approximate similarity join. Transform and explode the outputCol to\n+   * explodeCols.\n+   * @param dataset The dataset to transform and explode.\n+   * @param explodeCols The alias for the exploded columns, must be a seq of two strings.\n+   * @return A dataset containing idCol, inputCol and explodeCols\n+   */\n+  private[this] def processDataset(dataset: Dataset[_], explodeCols: Seq[String]): Dataset[_] = {\n+    if (explodeCols.size != 2) {\n+      throw new Exception(\"explodeCols must be two strings.\")\n+    }\n+    val vectorToMap: UserDefinedFunction = udf((x: Vector) => x.asBreeze.iterator.toMap,\n+      MapType(DataTypes.IntegerType, DataTypes.DoubleType))\n+    transform(dataset)\n+      .select(col(\"*\"), explode(vectorToMap(col($(outputCol)))).as(explodeCols))\n+  }\n+\n+  /**\n+   * Recreate a column using the same column name but different attribute id. Used in approximate\n+   * similarity join.\n+   * @param dataset The dataset where a column need to recreate\n+   * @param colName The name of the column to recreate\n+   * @param tmpColName A temporary column name which does not conflict with existing columns\n+   * @return\n+   */\n+  private[this] def recreateCol(dataset: Dataset[_], colName: String,\n+                                tmpColName: String): Dataset[_] = {\n+    dataset\n+      .withColumnRenamed(colName, tmpColName)\n+      .withColumn(colName, col(tmpColName))\n+      .drop(tmpColName)\n+  }\n+\n+  /**\n+   * Join two dataset to approximately find all pairs of records whose distance are smaller\n+   * than the threshold.\n+   * @param datasetA One of the datasets to join\n+   * @param datasetB Another dataset to join\n+   * @param threshold The threshold for the distance of record pairs\n+   * @param distCol The column to store the distance between pairs\n+   * @return A joined dataset containing pairs of records. A distCol is added to show the distance\n+   *         between each pair of records.\n+   */\n+  def approxSimilarityJoin(datasetA: Dataset[_], datasetB: Dataset[_], threshold: Double,\n+                           distCol: String = \"distance\"): Dataset[_] = {\n+\n+    val explodeCols = Seq(\"lsh#entry\", \"lsh#hashValue\")\n+    val explodedA = processDataset(datasetA, explodeCols)\n+\n+    // If this is a self join, we need to recreate the inputCol of datasetB to avoid ambiguity."
  }],
  "prId": 15148
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "I think do `distinct` after `filter` should be better as you will filter out most of records.\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-19T08:09:09Z",
    "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import scala.util.Random\n+\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg.{Vector, VectorUDT}\n+import org.apache.spark.ml.param.{IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Params for [[LSH]].\n+ */\n+private[ml] trait LSHParams extends HasInputCol with HasOutputCol {\n+  /**\n+   * Param for output dimension.\n+   *\n+   * @group param\n+   */\n+  final val outputDim: IntParam = new IntParam(this, \"outputDim\", \"output dimension\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  final def getOutputDim: Int = $(outputDim)\n+\n+  setDefault(outputDim -> 1)\n+\n+  setDefault(outputCol -> \"lsh_output\")\n+\n+  /**\n+   * Transform the Schema for LSH\n+   * @param schema The schema of the input dataset without outputCol\n+   * @return A derived schema with outputCol added\n+   */\n+  final def transformLSHSchema(schema: StructType): StructType = {\n+    val outputFields = schema.fields :+\n+      StructField($(outputCol), new VectorUDT, nullable = false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+/**\n+ * Model produced by [[LSH]].\n+ */\n+abstract class LSHModel[KeyType, T <: LSHModel[KeyType, T]] private[ml]\n+  extends Model[T] with LSHParams {\n+  override def copy(extra: ParamMap): T = defaultCopy(extra)\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * The hash function of LSH, mapping a predefined KeyType to a Vector\n+   * @return The mapping of LSH function.\n+   */\n+  protected[this] val hashFunction: KeyType => Vector\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different keys using the distance metric corresponding\n+   * to the hashFunction\n+   * @param x One of the point in the metric space\n+   * @param y Another the point in the metric space\n+   * @return The distance between x and y in double\n+   */\n+  protected[ml] def keyDistance(x: KeyType, y: KeyType): Double\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different hash Vectors. By default, the distance is the\n+   * minimum distance of two hash values in any dimension.\n+   *\n+   * @param x One of the hash vector\n+   * @param y Another hash vector\n+   * @return The distance between hash vectors x and y in double\n+   */\n+  protected[ml] def hashDistance(x: Vector, y: Vector): Double = {\n+    (x.asBreeze - y.asBreeze).toArray.map(math.abs).min\n+  }\n+\n+  /**\n+   * Transforms the input dataset.\n+   */\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    val transformUDF = udf(hashFunction, new VectorUDT)\n+    dataset.withColumn($(outputCol), transformUDF(dataset($(inputCol))))\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Check transform validity and derive the output schema from the input schema.\n+   *\n+   * Typical implementation should first conduct verification on schema change and parameter\n+   * validity, including complex parameter interaction checks.\n+   */\n+  override def transformSchema(schema: StructType): StructType = {\n+    transformLSHSchema(schema)\n+  }\n+\n+  /**\n+   * Given a large dataset and an item, approximately find at most k items which have the closest\n+   * distance to the item.\n+   * @param dataset the dataset to look for the key\n+   * @param key The key to hash for the item\n+   * @param k The maximum number of items closest to the key\n+   * @param distCol The column to store the distance between pairs\n+   * @return A dataset containing at most k items closest to the key. A distCol is added to show\n+   *         the distance between each record and the key.\n+   */\n+  def approxNearestNeighbors(dataset: Dataset[_], key: KeyType, k: Int = 1,\n+                             distCol: String = \"distance\"): Dataset[_] = {\n+    if (k < 1) {\n+      throw new Exception(s\"Invalid number of nearest neighbors $k\")\n+    }\n+    // Get Hash Value of the key v\n+    val keyHash = hashFunction(key)\n+    val modelDataset = transform(dataset)\n+\n+    // In the origin dataset, find the hash value u that is closest to v\n+    val hashDistUDF = udf((x: Vector) => hashDistance(x, keyHash), DataTypes.DoubleType)\n+    val nearestHashDataset = modelDataset.select(min(hashDistUDF(col($(outputCol)))))\n+    val nearestHashValue = nearestHashDataset.collect()(0)(0).asInstanceOf[Double]\n+\n+    // Filter the dataset where the hash value equals to u\n+    val modelSubset = modelDataset.filter(hashDistUDF(col($(outputCol))) === nearestHashValue)\n+\n+    // Get the top k nearest neighbor by their distance to the key\n+    val keyDistUDF = udf((x: KeyType) => keyDistance(x, key), DataTypes.DoubleType)\n+    val modelSubsetWithDistCol = modelSubset.withColumn(distCol, keyDistUDF(col($(inputCol))))\n+    modelSubsetWithDistCol.sort(distCol).limit(k)\n+  }\n+\n+  /**\n+   * Preprocess step for approximate similarity join. Transform and explode the outputCol to\n+   * explodeCols.\n+   * @param dataset The dataset to transform and explode.\n+   * @param explodeCols The alias for the exploded columns, must be a seq of two strings.\n+   * @return A dataset containing idCol, inputCol and explodeCols\n+   */\n+  private[this] def processDataset(dataset: Dataset[_], explodeCols: Seq[String]): Dataset[_] = {\n+    if (explodeCols.size != 2) {\n+      throw new Exception(\"explodeCols must be two strings.\")\n+    }\n+    val vectorToMap: UserDefinedFunction = udf((x: Vector) => x.asBreeze.iterator.toMap,\n+      MapType(DataTypes.IntegerType, DataTypes.DoubleType))\n+    transform(dataset)\n+      .select(col(\"*\"), explode(vectorToMap(col($(outputCol)))).as(explodeCols))\n+  }\n+\n+  /**\n+   * Recreate a column using the same column name but different attribute id. Used in approximate\n+   * similarity join.\n+   * @param dataset The dataset where a column need to recreate\n+   * @param colName The name of the column to recreate\n+   * @param tmpColName A temporary column name which does not conflict with existing columns\n+   * @return\n+   */\n+  private[this] def recreateCol(dataset: Dataset[_], colName: String,\n+                                tmpColName: String): Dataset[_] = {\n+    dataset\n+      .withColumnRenamed(colName, tmpColName)\n+      .withColumn(colName, col(tmpColName))\n+      .drop(tmpColName)\n+  }\n+\n+  /**\n+   * Join two dataset to approximately find all pairs of records whose distance are smaller\n+   * than the threshold.\n+   * @param datasetA One of the datasets to join\n+   * @param datasetB Another dataset to join\n+   * @param threshold The threshold for the distance of record pairs\n+   * @param distCol The column to store the distance between pairs\n+   * @return A joined dataset containing pairs of records. A distCol is added to show the distance\n+   *         between each pair of records.\n+   */\n+  def approxSimilarityJoin(datasetA: Dataset[_], datasetB: Dataset[_], threshold: Double,\n+                           distCol: String = \"distance\"): Dataset[_] = {\n+\n+    val explodeCols = Seq(\"lsh#entry\", \"lsh#hashValue\")\n+    val explodedA = processDataset(datasetA, explodeCols)\n+\n+    // If this is a self join, we need to recreate the inputCol of datasetB to avoid ambiguity.\n+    val explodedB = if (datasetA != datasetB) {\n+      processDataset(datasetB, explodeCols)\n+    } else {\n+      val recreatedB = recreateCol(datasetB, $(inputCol), s\"${$(inputCol)}#${Random.nextString(5)}\")\n+      processDataset(recreatedB, explodeCols)\n+    }\n+\n+    // Do a hash join on where the exploded hash values are equal.\n+    val joinedDataset = explodedA.join(explodedB, explodeCols)\n+      .drop(explodeCols: _*)\n+\n+    // Add a new column to store the distance of the two records.\n+    val distUDF = udf((x: KeyType, y: KeyType) => keyDistance(x, y), DataTypes.DoubleType)\n+    val joinedDatasetWithDist = joinedDataset.select(col(\"*\"),\n+      distUDF(explodedA($(inputCol)), explodedB($(inputCol))).as(distCol)\n+    )\n+\n+    // Filter the joined datasets where the distance are smaller than the threshold.\n+    joinedDatasetWithDist.distinct().filter(col(distCol) < threshold)"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Very good point. Done.\n",
    "commit": "35708458a0ee156c097ca604efeafaa37d3c8a6d",
    "createdAt": "2016-09-19T22:43:28Z",
    "diffHunk": "@@ -0,0 +1,270 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.ml.lsh\n+\n+import scala.util.Random\n+\n+import org.apache.spark.ml.{Estimator, Model}\n+import org.apache.spark.ml.linalg.{Vector, VectorUDT}\n+import org.apache.spark.ml.param.{IntParam, ParamMap, ParamValidators}\n+import org.apache.spark.ml.param.shared.{HasInputCol, HasOutputCol}\n+import org.apache.spark.sql._\n+import org.apache.spark.sql.expressions.UserDefinedFunction\n+import org.apache.spark.sql.functions._\n+import org.apache.spark.sql.types._\n+\n+/**\n+ * Params for [[LSH]].\n+ */\n+private[ml] trait LSHParams extends HasInputCol with HasOutputCol {\n+  /**\n+   * Param for output dimension.\n+   *\n+   * @group param\n+   */\n+  final val outputDim: IntParam = new IntParam(this, \"outputDim\", \"output dimension\",\n+    ParamValidators.gt(0))\n+\n+  /** @group getParam */\n+  final def getOutputDim: Int = $(outputDim)\n+\n+  setDefault(outputDim -> 1)\n+\n+  setDefault(outputCol -> \"lsh_output\")\n+\n+  /**\n+   * Transform the Schema for LSH\n+   * @param schema The schema of the input dataset without outputCol\n+   * @return A derived schema with outputCol added\n+   */\n+  final def transformLSHSchema(schema: StructType): StructType = {\n+    val outputFields = schema.fields :+\n+      StructField($(outputCol), new VectorUDT, nullable = false)\n+    StructType(outputFields)\n+  }\n+}\n+\n+/**\n+ * Model produced by [[LSH]].\n+ */\n+abstract class LSHModel[KeyType, T <: LSHModel[KeyType, T]] private[ml]\n+  extends Model[T] with LSHParams {\n+  override def copy(extra: ParamMap): T = defaultCopy(extra)\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * The hash function of LSH, mapping a predefined KeyType to a Vector\n+   * @return The mapping of LSH function.\n+   */\n+  protected[this] val hashFunction: KeyType => Vector\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different keys using the distance metric corresponding\n+   * to the hashFunction\n+   * @param x One of the point in the metric space\n+   * @param y Another the point in the metric space\n+   * @return The distance between x and y in double\n+   */\n+  protected[ml] def keyDistance(x: KeyType, y: KeyType): Double\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Calculate the distance between two different hash Vectors. By default, the distance is the\n+   * minimum distance of two hash values in any dimension.\n+   *\n+   * @param x One of the hash vector\n+   * @param y Another hash vector\n+   * @return The distance between hash vectors x and y in double\n+   */\n+  protected[ml] def hashDistance(x: Vector, y: Vector): Double = {\n+    (x.asBreeze - y.asBreeze).toArray.map(math.abs).min\n+  }\n+\n+  /**\n+   * Transforms the input dataset.\n+   */\n+  override def transform(dataset: Dataset[_]): DataFrame = {\n+    transformSchema(dataset.schema, logging = true)\n+    val transformUDF = udf(hashFunction, new VectorUDT)\n+    dataset.withColumn($(outputCol), transformUDF(dataset($(inputCol))))\n+  }\n+\n+  /**\n+   * :: DeveloperApi ::\n+   *\n+   * Check transform validity and derive the output schema from the input schema.\n+   *\n+   * Typical implementation should first conduct verification on schema change and parameter\n+   * validity, including complex parameter interaction checks.\n+   */\n+  override def transformSchema(schema: StructType): StructType = {\n+    transformLSHSchema(schema)\n+  }\n+\n+  /**\n+   * Given a large dataset and an item, approximately find at most k items which have the closest\n+   * distance to the item.\n+   * @param dataset the dataset to look for the key\n+   * @param key The key to hash for the item\n+   * @param k The maximum number of items closest to the key\n+   * @param distCol The column to store the distance between pairs\n+   * @return A dataset containing at most k items closest to the key. A distCol is added to show\n+   *         the distance between each record and the key.\n+   */\n+  def approxNearestNeighbors(dataset: Dataset[_], key: KeyType, k: Int = 1,\n+                             distCol: String = \"distance\"): Dataset[_] = {\n+    if (k < 1) {\n+      throw new Exception(s\"Invalid number of nearest neighbors $k\")\n+    }\n+    // Get Hash Value of the key v\n+    val keyHash = hashFunction(key)\n+    val modelDataset = transform(dataset)\n+\n+    // In the origin dataset, find the hash value u that is closest to v\n+    val hashDistUDF = udf((x: Vector) => hashDistance(x, keyHash), DataTypes.DoubleType)\n+    val nearestHashDataset = modelDataset.select(min(hashDistUDF(col($(outputCol)))))\n+    val nearestHashValue = nearestHashDataset.collect()(0)(0).asInstanceOf[Double]\n+\n+    // Filter the dataset where the hash value equals to u\n+    val modelSubset = modelDataset.filter(hashDistUDF(col($(outputCol))) === nearestHashValue)\n+\n+    // Get the top k nearest neighbor by their distance to the key\n+    val keyDistUDF = udf((x: KeyType) => keyDistance(x, key), DataTypes.DoubleType)\n+    val modelSubsetWithDistCol = modelSubset.withColumn(distCol, keyDistUDF(col($(inputCol))))\n+    modelSubsetWithDistCol.sort(distCol).limit(k)\n+  }\n+\n+  /**\n+   * Preprocess step for approximate similarity join. Transform and explode the outputCol to\n+   * explodeCols.\n+   * @param dataset The dataset to transform and explode.\n+   * @param explodeCols The alias for the exploded columns, must be a seq of two strings.\n+   * @return A dataset containing idCol, inputCol and explodeCols\n+   */\n+  private[this] def processDataset(dataset: Dataset[_], explodeCols: Seq[String]): Dataset[_] = {\n+    if (explodeCols.size != 2) {\n+      throw new Exception(\"explodeCols must be two strings.\")\n+    }\n+    val vectorToMap: UserDefinedFunction = udf((x: Vector) => x.asBreeze.iterator.toMap,\n+      MapType(DataTypes.IntegerType, DataTypes.DoubleType))\n+    transform(dataset)\n+      .select(col(\"*\"), explode(vectorToMap(col($(outputCol)))).as(explodeCols))\n+  }\n+\n+  /**\n+   * Recreate a column using the same column name but different attribute id. Used in approximate\n+   * similarity join.\n+   * @param dataset The dataset where a column need to recreate\n+   * @param colName The name of the column to recreate\n+   * @param tmpColName A temporary column name which does not conflict with existing columns\n+   * @return\n+   */\n+  private[this] def recreateCol(dataset: Dataset[_], colName: String,\n+                                tmpColName: String): Dataset[_] = {\n+    dataset\n+      .withColumnRenamed(colName, tmpColName)\n+      .withColumn(colName, col(tmpColName))\n+      .drop(tmpColName)\n+  }\n+\n+  /**\n+   * Join two dataset to approximately find all pairs of records whose distance are smaller\n+   * than the threshold.\n+   * @param datasetA One of the datasets to join\n+   * @param datasetB Another dataset to join\n+   * @param threshold The threshold for the distance of record pairs\n+   * @param distCol The column to store the distance between pairs\n+   * @return A joined dataset containing pairs of records. A distCol is added to show the distance\n+   *         between each pair of records.\n+   */\n+  def approxSimilarityJoin(datasetA: Dataset[_], datasetB: Dataset[_], threshold: Double,\n+                           distCol: String = \"distance\"): Dataset[_] = {\n+\n+    val explodeCols = Seq(\"lsh#entry\", \"lsh#hashValue\")\n+    val explodedA = processDataset(datasetA, explodeCols)\n+\n+    // If this is a self join, we need to recreate the inputCol of datasetB to avoid ambiguity.\n+    val explodedB = if (datasetA != datasetB) {\n+      processDataset(datasetB, explodeCols)\n+    } else {\n+      val recreatedB = recreateCol(datasetB, $(inputCol), s\"${$(inputCol)}#${Random.nextString(5)}\")\n+      processDataset(recreatedB, explodeCols)\n+    }\n+\n+    // Do a hash join on where the exploded hash values are equal.\n+    val joinedDataset = explodedA.join(explodedB, explodeCols)\n+      .drop(explodeCols: _*)\n+\n+    // Add a new column to store the distance of the two records.\n+    val distUDF = udf((x: KeyType, y: KeyType) => keyDistance(x, y), DataTypes.DoubleType)\n+    val joinedDatasetWithDist = joinedDataset.select(col(\"*\"),\n+      distUDF(explodedA($(inputCol)), explodedB($(inputCol))).as(distCol)\n+    )\n+\n+    // Filter the joined datasets where the distance are smaller than the threshold.\n+    joinedDatasetWithDist.distinct().filter(col(distCol) < threshold)"
  }],
  "prId": 15148
}]