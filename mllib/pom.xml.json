[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "This still feels weird to me, MLlib depending on SQL. It seems like they are both wanting to depend on a `SchemaRDD` that is specific to neither. I'm afraid of making the jar hell in Spark worse by attaching more subprojects together. That said, the SQL module itself doesn't, for instance, bring in Hive. Is this going to add much to the MLlib deps? or can the commonality not be factored out into Core?\n",
    "commit": "3a0b6e5fe8458b89cd5fb3fffc0536aa6c1b4b4d",
    "createdAt": "2014-11-03T07:32:08Z",
    "diffHunk": "@@ -46,6 +46,11 @@\n       <version>${project.version}</version>\n     </dependency>\n     <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-sql_${scala.binary.version}</artifactId>",
    "line": 5
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "@srowen Yes, it feels weird if we say ML depends on SQL, the \"query language\". Spark SQL provides RDD with schema support and execution plan optimization, both of which are need by MLlib. We need flexible table-like datasets and I/O support, and operations that \"carry over\" additional columns during the training phrase. It is natural to say that ML depends on RDD with schema support and execution plan optimization.\n\nI agree that we should factor the common part out or make SchemaRDD a first-class citizen in Core, but that definitely takes time for both design and development. This dependence change has no effect on the content we deliver to users, and UDTs are internal to Spark.\n",
    "commit": "3a0b6e5fe8458b89cd5fb3fffc0536aa6c1b4b4d",
    "createdAt": "2014-11-03T18:51:59Z",
    "diffHunk": "@@ -46,6 +46,11 @@\n       <version>${project.version}</version>\n     </dependency>\n     <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-sql_${scala.binary.version}</artifactId>",
    "line": 5
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "I think it would be pretty difficult to have a SchemaRDD that didn't at least depend on catalyst and then there still would be no way to execute the projections and structured data input/output that MLlib wants to.  I think really the problem might be in naming.  Catalyst / Spark SQL core are really more about manipulating structured data using Spark and we actually considered not even having SQL in the name (unfortunately Spark Schema doesn't have the same ring to it).\n\nThe SQL project has already been carefully factored into pieces to minimize the number of dependencies, and so I believe that the only additional dependency that we are bringing in here is Parquet (which is kind of the point of this example).\n",
    "commit": "3a0b6e5fe8458b89cd5fb3fffc0536aa6c1b4b4d",
    "createdAt": "2014-11-03T19:24:13Z",
    "diffHunk": "@@ -46,6 +46,11 @@\n       <version>${project.version}</version>\n     </dependency>\n     <dependency>\n+      <groupId>org.apache.spark</groupId>\n+      <artifactId>spark-sql_${scala.binary.version}</artifactId>",
    "line": 5
  }],
  "prId": 3070
}]