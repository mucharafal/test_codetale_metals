[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I would get rid of those details. It sounds like we have no mechanism to recover from driver failure. ",
    "commit": "80db8f25f8a118a16b5694a7064fd4772c067684",
    "createdAt": "2019-03-01T05:39:49Z",
    "diffHunk": "@@ -48,13 +48,22 @@ Some of the commonly used options are:\n * `application-jar`: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an `hdfs://` path or a `file://` path that is present on all nodes.\n * `application-arguments`: Arguments passed to the main method of your main class, if any\n \n-<b>&#8224;</b> A common deployment strategy is to submit your application from a gateway machine\n-that is\n-physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster).\n-In this setup, `client` mode is appropriate. In `client` mode, the driver is launched directly\n-within the `spark-submit` process which acts as a *client* to the cluster. The input and\n-output of the application is attached to the console. Thus, this mode is especially suitable\n-for applications that involve the REPL (e.g. Spark shell).\n+<b>&#8224;</b>In `client` mode, the driver is launched directly within the `spark-submit` process on\n+the machine which was used to submit the Spark job, and it will act as a *client* to the cluster.\n+In this mode, the input and output of the application is attached to the console, thus, this mode is\n+especially suitable for applications that involve the REPL (e.g. Spark shell).\n+However, if the process used to submit the job terminates, the machine hosting it is shut down,\n+crashes or loses network connectivity, that ends the execution of the job on the (remote) cluster as well.\n+\n+If `cluster` mode is specified, the driver program is executed on one of the cluster machines, requiring no\n+connection from the (client) machine which was used for submitting the Spark job: the Spark job will run\n+until it completes, gets terminated or the cluster service used for hosting the driver becomes unavailable\n+(e.g. the cluster service is stopped, it crashes on the cluster machine elected for the execution of the driver,\n+or the cluster machine itself crashes or loses network connectivity).",
    "line": 28
  }],
  "prId": 23919
}, {
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "eg. in client mode for k8s, driver might not be in the spark-submit process but instead in a pod in the cluster\r\nhttps://spark.apache.org/docs/latest/running-on-kubernetes.html#client-mode\r\n",
    "commit": "80db8f25f8a118a16b5694a7064fd4772c067684",
    "createdAt": "2019-03-03T01:21:41Z",
    "diffHunk": "@@ -48,13 +48,22 @@ Some of the commonly used options are:\n * `application-jar`: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an `hdfs://` path or a `file://` path that is present on all nodes.\n * `application-arguments`: Arguments passed to the main method of your main class, if any\n \n-<b>&#8224;</b> A common deployment strategy is to submit your application from a gateway machine\n-that is\n-physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster).\n-In this setup, `client` mode is appropriate. In `client` mode, the driver is launched directly\n-within the `spark-submit` process which acts as a *client* to the cluster. The input and\n-output of the application is attached to the console. Thus, this mode is especially suitable\n-for applications that involve the REPL (e.g. Spark shell).\n+<b>&#8224;</b>In `client` mode, the driver is launched directly within the `spark-submit` process on\n+the machine which was used to submit the Spark job, and it will act as a *client* to the cluster.",
    "line": 17
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "OK, maybe best to punt on this. ",
    "commit": "80db8f25f8a118a16b5694a7064fd4772c067684",
    "createdAt": "2019-03-05T14:38:34Z",
    "diffHunk": "@@ -48,13 +48,22 @@ Some of the commonly used options are:\n * `application-jar`: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an `hdfs://` path or a `file://` path that is present on all nodes.\n * `application-arguments`: Arguments passed to the main method of your main class, if any\n \n-<b>&#8224;</b> A common deployment strategy is to submit your application from a gateway machine\n-that is\n-physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster).\n-In this setup, `client` mode is appropriate. In `client` mode, the driver is launched directly\n-within the `spark-submit` process which acts as a *client* to the cluster. The input and\n-output of the application is attached to the console. Thus, this mode is especially suitable\n-for applications that involve the REPL (e.g. Spark shell).\n+<b>&#8224;</b>In `client` mode, the driver is launched directly within the `spark-submit` process on\n+the machine which was used to submit the Spark job, and it will act as a *client* to the cluster.",
    "line": 17
  }, {
    "author": {
      "login": "peter-gergely-horvath"
    },
    "body": "I have added some further description, now Kubernetes docs is also linked.",
    "commit": "80db8f25f8a118a16b5694a7064fd4772c067684",
    "createdAt": "2019-03-05T18:05:59Z",
    "diffHunk": "@@ -48,13 +48,22 @@ Some of the commonly used options are:\n * `application-jar`: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an `hdfs://` path or a `file://` path that is present on all nodes.\n * `application-arguments`: Arguments passed to the main method of your main class, if any\n \n-<b>&#8224;</b> A common deployment strategy is to submit your application from a gateway machine\n-that is\n-physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster).\n-In this setup, `client` mode is appropriate. In `client` mode, the driver is launched directly\n-within the `spark-submit` process which acts as a *client* to the cluster. The input and\n-output of the application is attached to the console. Thus, this mode is especially suitable\n-for applications that involve the REPL (e.g. Spark shell).\n+<b>&#8224;</b>In `client` mode, the driver is launched directly within the `spark-submit` process on\n+the machine which was used to submit the Spark job, and it will act as a *client* to the cluster.",
    "line": 17
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I actually think that distinction makes things less clear.\r\n\r\nclient mode = driver runs in same machine as spark-submit\r\ncluster mode = driver runs in some cluster machine\r\n\r\nIt doesn't matter what machine it is that is running spark-submit; it may be a cluster machine, a k8s pod, whatever. The distinction is whether the driver runs in the same machine as spark-submit or not.",
    "commit": "80db8f25f8a118a16b5694a7064fd4772c067684",
    "createdAt": "2019-03-05T18:23:49Z",
    "diffHunk": "@@ -48,13 +48,22 @@ Some of the commonly used options are:\n * `application-jar`: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an `hdfs://` path or a `file://` path that is present on all nodes.\n * `application-arguments`: Arguments passed to the main method of your main class, if any\n \n-<b>&#8224;</b> A common deployment strategy is to submit your application from a gateway machine\n-that is\n-physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster).\n-In this setup, `client` mode is appropriate. In `client` mode, the driver is launched directly\n-within the `spark-submit` process which acts as a *client* to the cluster. The input and\n-output of the application is attached to the console. Thus, this mode is especially suitable\n-for applications that involve the REPL (e.g. Spark shell).\n+<b>&#8224;</b>In `client` mode, the driver is launched directly within the `spark-submit` process on\n+the machine which was used to submit the Spark job, and it will act as a *client* to the cluster.",
    "line": 17
  }, {
    "author": {
      "login": "felixcheung"
    },
    "body": "well, that's terminology perhaps we need to change for k8s.",
    "commit": "80db8f25f8a118a16b5694a7064fd4772c067684",
    "createdAt": "2019-03-06T18:35:55Z",
    "diffHunk": "@@ -48,13 +48,22 @@ Some of the commonly used options are:\n * `application-jar`: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an `hdfs://` path or a `file://` path that is present on all nodes.\n * `application-arguments`: Arguments passed to the main method of your main class, if any\n \n-<b>&#8224;</b> A common deployment strategy is to submit your application from a gateway machine\n-that is\n-physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster).\n-In this setup, `client` mode is appropriate. In `client` mode, the driver is launched directly\n-within the `spark-submit` process which acts as a *client* to the cluster. The input and\n-output of the application is attached to the console. Thus, this mode is especially suitable\n-for applications that involve the REPL (e.g. Spark shell).\n+<b>&#8224;</b>In `client` mode, the driver is launched directly within the `spark-submit` process on\n+the machine which was used to submit the Spark job, and it will act as a *client* to the cluster.",
    "line": 17
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "Why so? What I wrote applies to k8s too.\r\n\r\nIn your original comment above, you mention that the driver might be in a pod in the cluster. But that pod it executing spark-submit. So it's exactly the same situation.",
    "commit": "80db8f25f8a118a16b5694a7064fd4772c067684",
    "createdAt": "2019-03-06T18:39:00Z",
    "diffHunk": "@@ -48,13 +48,22 @@ Some of the commonly used options are:\n * `application-jar`: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an `hdfs://` path or a `file://` path that is present on all nodes.\n * `application-arguments`: Arguments passed to the main method of your main class, if any\n \n-<b>&#8224;</b> A common deployment strategy is to submit your application from a gateway machine\n-that is\n-physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster).\n-In this setup, `client` mode is appropriate. In `client` mode, the driver is launched directly\n-within the `spark-submit` process which acts as a *client* to the cluster. The input and\n-output of the application is attached to the console. Thus, this mode is especially suitable\n-for applications that involve the REPL (e.g. Spark shell).\n+<b>&#8224;</b>In `client` mode, the driver is launched directly within the `spark-submit` process on\n+the machine which was used to submit the Spark job, and it will act as a *client* to the cluster.",
    "line": 17
  }, {
    "author": {
      "login": "peter-gergely-horvath"
    },
    "body": "Guys, could you please come up with an agreement on the wording? I am happy to update the PR if you put your preferred version to here as comment. \r\nThe current description is not detailed enough and apparently is not applicable to some of the cases: that should be improved...",
    "commit": "80db8f25f8a118a16b5694a7064fd4772c067684",
    "createdAt": "2019-03-06T19:10:03Z",
    "diffHunk": "@@ -48,13 +48,22 @@ Some of the commonly used options are:\n * `application-jar`: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an `hdfs://` path or a `file://` path that is present on all nodes.\n * `application-arguments`: Arguments passed to the main method of your main class, if any\n \n-<b>&#8224;</b> A common deployment strategy is to submit your application from a gateway machine\n-that is\n-physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster).\n-In this setup, `client` mode is appropriate. In `client` mode, the driver is launched directly\n-within the `spark-submit` process which acts as a *client* to the cluster. The input and\n-output of the application is attached to the console. Thus, this mode is especially suitable\n-for applications that involve the REPL (e.g. Spark shell).\n+<b>&#8224;</b>In `client` mode, the driver is launched directly within the `spark-submit` process on\n+the machine which was used to submit the Spark job, and it will act as a *client* to the cluster.",
    "line": 17
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I already explained above the different between client and cluster mode. I haven't seen anything that tells me that what I wrote is not correct in all cases.\r\n\r\nIf you run spark-submit in client mode, the driver is run by spark-submit. No cluster resources are requested for the driver.\r\n\r\nIf you run spark-submit in cluster mode, spark-submit will ask the cluster manager to run the driver. Resources will be allocated from the cluster manager to run the driver, which will then run in a different place from spark-submit itself.\r\n\r\nThere are no differences among the cluster managers here. The fact that you can run spark-submit in client mode inside a cluster node is irrelevant. From the standpoint of spark-submit, the above still applies.",
    "commit": "80db8f25f8a118a16b5694a7064fd4772c067684",
    "createdAt": "2019-03-06T19:27:40Z",
    "diffHunk": "@@ -48,13 +48,22 @@ Some of the commonly used options are:\n * `application-jar`: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an `hdfs://` path or a `file://` path that is present on all nodes.\n * `application-arguments`: Arguments passed to the main method of your main class, if any\n \n-<b>&#8224;</b> A common deployment strategy is to submit your application from a gateway machine\n-that is\n-physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster).\n-In this setup, `client` mode is appropriate. In `client` mode, the driver is launched directly\n-within the `spark-submit` process which acts as a *client* to the cluster. The input and\n-output of the application is attached to the console. Thus, this mode is especially suitable\n-for applications that involve the REPL (e.g. Spark shell).\n+<b>&#8224;</b>In `client` mode, the driver is launched directly within the `spark-submit` process on\n+the machine which was used to submit the Spark job, and it will act as a *client* to the cluster.",
    "line": 17
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "I think I agree with @vanzin here. The comment above is accurate, and probably as far as we need to distinguish it for purposes of what this comment is trying to explain. I'd propose removing the special comment about K8S.\r\n\r\nI might phrase the distinction as:\r\n\r\n- In `client` mode, the `spark-submit` process runs the driver directly, from the machine running `spark-submit`. That process is the direct 'client' to the cluster workers running the executors, and runs as long as the job does. If this `spark-submit` process is terminated, the driver terminates and thus the job does too.\r\n- In `cluster` mode, the `spark-submit` process runs the driver on the cluster. That process and machine is not involved in running the job after submission. The job continues to run even if `spark-submit` terminates, as the driver 'client' is on the cluster. The job runs as long as the driver continues to execute on the cluster.\r\n\r\nThat kind of thing.",
    "commit": "80db8f25f8a118a16b5694a7064fd4772c067684",
    "createdAt": "2019-03-09T18:49:45Z",
    "diffHunk": "@@ -48,13 +48,22 @@ Some of the commonly used options are:\n * `application-jar`: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an `hdfs://` path or a `file://` path that is present on all nodes.\n * `application-arguments`: Arguments passed to the main method of your main class, if any\n \n-<b>&#8224;</b> A common deployment strategy is to submit your application from a gateway machine\n-that is\n-physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster).\n-In this setup, `client` mode is appropriate. In `client` mode, the driver is launched directly\n-within the `spark-submit` process which acts as a *client* to the cluster. The input and\n-output of the application is attached to the console. Thus, this mode is especially suitable\n-for applications that involve the REPL (e.g. Spark shell).\n+<b>&#8224;</b>In `client` mode, the driver is launched directly within the `spark-submit` process on\n+the machine which was used to submit the Spark job, and it will act as a *client* to the cluster.",
    "line": 17
  }],
  "prId": 23919
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "I think a driver failing always kills the job -- is there any case where that isn't true?",
    "commit": "80db8f25f8a118a16b5694a7064fd4772c067684",
    "createdAt": "2019-03-13T14:12:59Z",
    "diffHunk": "@@ -48,13 +48,28 @@ Some of the commonly used options are:\n * `application-jar`: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an `hdfs://` path or a `file://` path that is present on all nodes.\n * `application-arguments`: Arguments passed to the main method of your main class, if any\n \n-<b>&#8224;</b> A common deployment strategy is to submit your application from a gateway machine\n-that is\n-physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster).\n-In this setup, `client` mode is appropriate. In `client` mode, the driver is launched directly\n-within the `spark-submit` process which acts as a *client* to the cluster. The input and\n-output of the application is attached to the console. Thus, this mode is especially suitable\n-for applications that involve the REPL (e.g. Spark shell).\n+<b>&#8224;</b> The exact behaviour depends on the resource-manager (standalone, YARN, Mesos etc.)\n+used on the cluster. While some of them offer advanced features (see for example, the\n+[support for Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html#client-mode),\n+where the driver can run inside a Kubernetes pod or on a physical host),\n+generally, the following is applicable: in `client` mode,\n+the driver is launched directly within the `spark-submit` process on\n+the machine which was used to submit the Spark job, and it will act as a *client* to the cluster.\n+In this mode, the input and output of the application is attached to the console, thus, this mode is\n+especially suitable for applications that involve the REPL (e.g. Spark shell).\n+Depending on the resource-manager used and its configuration, the handling of driver failures\n+(termination or disconnect) might be different, but in most of the cases that ends the execution",
    "line": 21
  }],
  "prId": 23919
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "The word 'client' is probably confusing here; it's just the machine that ran spark-submit. If \"client\" here is used to mean the driver's machine, then this isn't the client.",
    "commit": "80db8f25f8a118a16b5694a7064fd4772c067684",
    "createdAt": "2019-03-13T14:13:44Z",
    "diffHunk": "@@ -48,13 +48,28 @@ Some of the commonly used options are:\n * `application-jar`: Path to a bundled jar including your application and all dependencies. The URL must be globally visible inside of your cluster, for instance, an `hdfs://` path or a `file://` path that is present on all nodes.\n * `application-arguments`: Arguments passed to the main method of your main class, if any\n \n-<b>&#8224;</b> A common deployment strategy is to submit your application from a gateway machine\n-that is\n-physically co-located with your worker machines (e.g. Master node in a standalone EC2 cluster).\n-In this setup, `client` mode is appropriate. In `client` mode, the driver is launched directly\n-within the `spark-submit` process which acts as a *client* to the cluster. The input and\n-output of the application is attached to the console. Thus, this mode is especially suitable\n-for applications that involve the REPL (e.g. Spark shell).\n+<b>&#8224;</b> The exact behaviour depends on the resource-manager (standalone, YARN, Mesos etc.)\n+used on the cluster. While some of them offer advanced features (see for example, the\n+[support for Kubernetes](https://spark.apache.org/docs/latest/running-on-kubernetes.html#client-mode),\n+where the driver can run inside a Kubernetes pod or on a physical host),\n+generally, the following is applicable: in `client` mode,\n+the driver is launched directly within the `spark-submit` process on\n+the machine which was used to submit the Spark job, and it will act as a *client* to the cluster.\n+In this mode, the input and output of the application is attached to the console, thus, this mode is\n+especially suitable for applications that involve the REPL (e.g. Spark shell).\n+Depending on the resource-manager used and its configuration, the handling of driver failures\n+(termination or disconnect) might be different, but in most of the cases that ends the execution\n+of the job on the (remote) cluster as well.\n+\n+If `cluster` mode is specified, the driver program is executed on one of the cluster machines, requiring no\n+connection from the (client) machine which was used for submitting the Spark job: the Spark job will run",
    "line": 25
  }],
  "prId": 23919
}]