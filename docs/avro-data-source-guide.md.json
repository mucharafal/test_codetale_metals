[{
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "should we also mention you can include with --jars if you build the jar?",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-17T13:57:56Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+---\n+layout: global\n+title: Avro Data Source Guide\n+---\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides support for reading and writing Avro data.\n+\n+## Deploying\n+The <code>spark-avro</code> module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "Here I am following https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#deploying . \r\nUsing `--packages` ensures that this library and its dependencies will be added to the classpath, which should be good enough for general users.\r\nFor users build their jar, they are supposed to know the general option `--jars`.\r\nI can add it if you insist. ",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-17T17:01:15Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+---\n+layout: global\n+title: Avro Data Source Guide\n+---\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides support for reading and writing Avro data.\n+\n+## Deploying\n+The <code>spark-avro</code> module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,"
  }, {
    "author": {
      "login": "tgravescs"
    },
    "body": "ok.  When I see a deploying section I would expect it to tell me what my options are so perhaps just rephrasing to more indicate --packages is one way to do it. \r\n\r\n  It would be nice to at least have a general statement saying the external modules aren't including with spark by default, the user must include the necessary jars themselves. The way to do this will be deployment specific. One way of doing this is via the --packages option.   \r\n\r\n Note I think the structured-streaming-kafka section should ideally be updated to something similar as well.  And really any external module for that matter. It would be nice to tell users how they can include these without assuming they just know how to.",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-17T17:36:49Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+---\n+layout: global\n+title: Avro Data Source Guide\n+---\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides support for reading and writing Avro data.\n+\n+## Deploying\n+The <code>spark-avro</code> module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "Actually the `--jars` option is well explained in https://spark.apache.org/docs/latest/submitting-applications.html#advanced-dependency-management . And the doc url is also mentioned in both Deploying sections.\r\nI still feel it is unnecessary to have a short introduction about `--jars` option here.",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-17T19:11:11Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+---\n+layout: global\n+title: Avro Data Source Guide\n+---\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides support for reading and writing Avro data.\n+\n+## Deploying\n+The <code>spark-avro</code> module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,"
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "I think this should be higher up not in the examples section.  Perhaps in its own compatibility section.",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-17T13:58:34Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+---\n+layout: global\n+title: Avro Data Source Guide\n+---\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides support for reading and writing Avro data.\n+\n+## Deploying\n+The <code>spark-avro</code> module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Examples\n+\n+Since `spark-avro` module is external, there is not such API as <code>.avro</code> in "
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "I see. I can change the title as read/write Avro data...Thanks",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-17T17:06:53Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+---\n+layout: global\n+title: Avro Data Source Guide\n+---\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides support for reading and writing Avro data.\n+\n+## Deploying\n+The <code>spark-avro</code> module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Examples\n+\n+Since `spark-avro` module is external, there is not such API as <code>.avro</code> in "
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "the configuration here has not spark. prefix?  this is set via the .option interface?\r\nI think we should clarify that for the user vs later in the table you have the spark. configs that I assume aren't set via option but via --conf",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-17T14:04:18Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+---\n+layout: global\n+title: Avro Data Source Guide\n+---\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides support for reading and writing Avro data.\n+\n+## Deploying\n+The <code>spark-avro</code> module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Examples\n+\n+Since `spark-avro` module is external, there is not such API as <code>.avro</code> in \n+<code>DataFrameReader</code> or <code>DataFrameWriter</code>.\n+To load/save data in Avro format, you need to specify the data source option <code>format</code> as short name <code>avro</code> or full name <code>org.apache.spark.sql.avro</code>.\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## Configuration\n+<table class=\"table\">\n+  <tr><th><b>Property Name</b></th><th><b>Default</b></th><th><b>Meaning</b></th><th><b>Scope</b></th></tr>\n+  <tr>\n+    <td><code>avroSchema</code></td>"
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Call it \"Apache Avro\" in the title and first mention in the paragraph below. Afterwards, just \"Avro\" is OK.",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-17T14:12:57Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+---\n+layout: global\n+title: Avro Data Source Guide"
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "You can use back-ticks rather than `<code>` for simpler code formatting. No big deal either way.",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-17T14:13:33Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+---\n+layout: global\n+title: Avro Data Source Guide\n+---\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides support for reading and writing Avro data.\n+\n+## Deploying\n+The <code>spark-avro</code> module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Examples\n+\n+Since `spark-avro` module is external, there is not such API as <code>.avro</code> in \n+<code>DataFrameReader</code> or <code>DataFrameWriter</code>.\n+To load/save data in Avro format, you need to specify the data source option <code>format</code> as short name <code>avro</code> or full name <code>org.apache.spark.sql.avro</code>."
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Space after headings like this",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-17T14:13:56Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+---\n+layout: global\n+title: Avro Data Source Guide\n+---\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides support for reading and writing Avro data.\n+\n+## Deploying\n+The <code>spark-avro</code> module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Examples\n+\n+Since `spark-avro` module is external, there is not such API as <code>.avro</code> in \n+<code>DataFrameReader</code> or <code>DataFrameWriter</code>.\n+To load/save data in Avro format, you need to specify the data source option <code>format</code> as short name <code>avro</code> or full name <code>org.apache.spark.sql.avro</code>.\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## Configuration"
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "`support` -> `built-in support`",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-17T16:54:28Z",
    "diffHunk": "@@ -0,0 +1,267 @@\n+---\n+layout: global\n+title: Avro Data Source Guide\n+---\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides support for reading and writing Avro data."
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Could you remove the repetition, line 191 ~ 195?",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-17T19:28:37Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load/Save Functions\n+\n+Since `spark-avro` module is external, there is not such API as `.avro` in \n+`DataFrameReader` or `DataFrameWriter`.\n+To load/save data in Avro format, you need to specify the data source option `format` as short name `avro` or full name `org.apache.spark.sql.avro`.\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## Data Source Options\n+\n+Data source options of Avro can be set using the `.option` method on `DataFrameReader` or `DataFrameWriter`.\n+<table class=\"table\">\n+  <tr><th><b>Property Name</b></th><th><b>Default</b></th><th><b>Meaning</b></th><th><b>Scope</b></th></tr>\n+  <tr>\n+    <td><code>avroSchema</code></td>\n+    <td>None</td>\n+    <td>Optional Avro schema provided by an user in JSON format.</td>\n+    <td>read and write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordName</code></td>\n+    <td>topLevelRecord</td>\n+    <td>Top level record name in write result, which is required in Avro spec.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordNamespace</code></td>\n+    <td>\"\"</td>\n+    <td>Record namespace in write result.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>ignoreExtension</code></td>\n+    <td>true</td>\n+    <td>The option controls ignoring of files without <code>.avro</code> extensions in read. If the option is enabled, all files (with and without <code>.avro</code> extension) are loaded.</td>\n+    <td>read</td>\n+  </tr>\n+  <tr>\n+    <td><code>compression</code></td>\n+    <td>snappy</td>\n+    <td>The <code>compression</code> option allows to specify a compression codec used in write. Currently supported codecs are <code>uncompressed</code>, <code>snappy</code>, <code>deflate</code>, <code>bzip2</code> and <code>xz</code>. If the option is not set, the configuration <code>spark.sql.avro.compression.codec</code> config is taken into account.</td>\n+    <td>write</td>\n+  </tr>\n+</table>\n+\n+## Supported types for Avro -> Spark SQL conversion\n+Currently Spark supports reading all [primitive types](https://avro.apache.org/docs/1.8.2/spec.html#schema_primitive) and [complex types](https://avro.apache.org/docs/1.8.2/spec.html#schema_complex) of Avro.\n+<table class=\"table\">\n+  <tr><th><b>Avro type</b></th><th><b>Spark SQL type</b></th></tr>\n+  <tr>\n+    <td>boolean</td>\n+    <td>BooleanType</td>\n+  </tr>\n+  <tr>\n+    <td>int</td>\n+    <td>IntegerType</td>\n+  </tr>\n+  <tr>\n+    <td>long</td>\n+    <td>LongType</td>\n+  </tr>\n+  <tr>\n+    <td>float</td>\n+    <td>FloatType</td>\n+  </tr>\n+  <tr>\n+    <td>double</td>\n+    <td>DoubleType</td>\n+  </tr>\n+  <tr>\n+    <td>string</td>\n+    <td>StringType</td>\n+  </tr>\n+  <tr>\n+    <td>enum</td>\n+    <td>StringType</td>\n+  </tr>\n+  <tr>\n+    <td>fixed</td>\n+    <td>BinaryType</td>\n+  </tr>\n+  <tr>\n+    <td>bytes</td>\n+    <td>BinaryType</td>\n+  </tr>\n+  <tr>\n+    <td>record</td>\n+    <td>StructType</td>\n+  </tr>\n+  <tr>\n+    <td>array</td>\n+    <td>ArrayType</td>\n+  </tr>\n+  <tr>\n+    <td>map</td>\n+    <td>MapType</td>\n+  </tr>\n+  <tr>\n+    <td>union</td>\n+    <td>See below</td>\n+  </tr>\n+</table>\n+\n+In addition to the types listed above, it supports reading `union` types. The following three types are considered basic `union` types:\n+\n+1. `union(int, long)` will be mapped to LongType.\n+2. `union(float, double)` will be mapped to DoubleType.\n+3. `union(something, null)`, where something is any supported Avro type. This will be mapped to the same Spark SQL type as that of something, with nullable set to true.\n+All other union types are considered complex. They will be mapped to StructType where field names are member0, member1, etc., in accordance with members of the union. This is consistent with the behavior when converting between Avro and Parquet.\n+\n+It also supports reading the following Avro [logical types](https://avro.apache.org/docs/1.8.2/spec.html#Logical+Types):\n+\n+<table class=\"table\">\n+  <tr><th><b>Avro logical type</b></th><th><b>Avro type</b></th><th><b>Spark SQL type</b></th></tr>\n+  <tr>\n+    <td>date</td>\n+    <td>int</td>\n+    <td>DateType</td>\n+  </tr>\n+  <tr>\n+    <td>timestamp-millis</td>\n+    <td>long</td>\n+    <td>TimestampType</td>\n+  </tr>\n+  <tr>\n+    <td>timestamp-micros</td>\n+    <td>long</td>\n+    <td>TimestampType</td>\n+  </tr>\n+  <tr>\n+    <td>decimal</td>\n+    <td>bytes</td>\n+    <td>DecimalType</td>\n+  </tr>\n+  <tr>\n+    <td>decimal</td>\n+    <td>bytes</td>\n+    <td>DecimalType</td>\n+  </tr>",
    "line": 315
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "It is a mistake. Thanks for pointing out!",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-18T17:33:40Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load/Save Functions\n+\n+Since `spark-avro` module is external, there is not such API as `.avro` in \n+`DataFrameReader` or `DataFrameWriter`.\n+To load/save data in Avro format, you need to specify the data source option `format` as short name `avro` or full name `org.apache.spark.sql.avro`.\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## Data Source Options\n+\n+Data source options of Avro can be set using the `.option` method on `DataFrameReader` or `DataFrameWriter`.\n+<table class=\"table\">\n+  <tr><th><b>Property Name</b></th><th><b>Default</b></th><th><b>Meaning</b></th><th><b>Scope</b></th></tr>\n+  <tr>\n+    <td><code>avroSchema</code></td>\n+    <td>None</td>\n+    <td>Optional Avro schema provided by an user in JSON format.</td>\n+    <td>read and write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordName</code></td>\n+    <td>topLevelRecord</td>\n+    <td>Top level record name in write result, which is required in Avro spec.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordNamespace</code></td>\n+    <td>\"\"</td>\n+    <td>Record namespace in write result.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>ignoreExtension</code></td>\n+    <td>true</td>\n+    <td>The option controls ignoring of files without <code>.avro</code> extensions in read. If the option is enabled, all files (with and without <code>.avro</code> extension) are loaded.</td>\n+    <td>read</td>\n+  </tr>\n+  <tr>\n+    <td><code>compression</code></td>\n+    <td>snappy</td>\n+    <td>The <code>compression</code> option allows to specify a compression codec used in write. Currently supported codecs are <code>uncompressed</code>, <code>snappy</code>, <code>deflate</code>, <code>bzip2</code> and <code>xz</code>. If the option is not set, the configuration <code>spark.sql.avro.compression.codec</code> config is taken into account.</td>\n+    <td>write</td>\n+  </tr>\n+</table>\n+\n+## Supported types for Avro -> Spark SQL conversion\n+Currently Spark supports reading all [primitive types](https://avro.apache.org/docs/1.8.2/spec.html#schema_primitive) and [complex types](https://avro.apache.org/docs/1.8.2/spec.html#schema_complex) of Avro.\n+<table class=\"table\">\n+  <tr><th><b>Avro type</b></th><th><b>Spark SQL type</b></th></tr>\n+  <tr>\n+    <td>boolean</td>\n+    <td>BooleanType</td>\n+  </tr>\n+  <tr>\n+    <td>int</td>\n+    <td>IntegerType</td>\n+  </tr>\n+  <tr>\n+    <td>long</td>\n+    <td>LongType</td>\n+  </tr>\n+  <tr>\n+    <td>float</td>\n+    <td>FloatType</td>\n+  </tr>\n+  <tr>\n+    <td>double</td>\n+    <td>DoubleType</td>\n+  </tr>\n+  <tr>\n+    <td>string</td>\n+    <td>StringType</td>\n+  </tr>\n+  <tr>\n+    <td>enum</td>\n+    <td>StringType</td>\n+  </tr>\n+  <tr>\n+    <td>fixed</td>\n+    <td>BinaryType</td>\n+  </tr>\n+  <tr>\n+    <td>bytes</td>\n+    <td>BinaryType</td>\n+  </tr>\n+  <tr>\n+    <td>record</td>\n+    <td>StructType</td>\n+  </tr>\n+  <tr>\n+    <td>array</td>\n+    <td>ArrayType</td>\n+  </tr>\n+  <tr>\n+    <td>map</td>\n+    <td>MapType</td>\n+  </tr>\n+  <tr>\n+    <td>union</td>\n+    <td>See below</td>\n+  </tr>\n+</table>\n+\n+In addition to the types listed above, it supports reading `union` types. The following three types are considered basic `union` types:\n+\n+1. `union(int, long)` will be mapped to LongType.\n+2. `union(float, double)` will be mapped to DoubleType.\n+3. `union(something, null)`, where something is any supported Avro type. This will be mapped to the same Spark SQL type as that of something, with nullable set to true.\n+All other union types are considered complex. They will be mapped to StructType where field names are member0, member1, etc., in accordance with members of the union. This is consistent with the behavior when converting between Avro and Parquet.\n+\n+It also supports reading the following Avro [logical types](https://avro.apache.org/docs/1.8.2/spec.html#Logical+Types):\n+\n+<table class=\"table\">\n+  <tr><th><b>Avro logical type</b></th><th><b>Avro type</b></th><th><b>Spark SQL type</b></th></tr>\n+  <tr>\n+    <td>date</td>\n+    <td>int</td>\n+    <td>DateType</td>\n+  </tr>\n+  <tr>\n+    <td>timestamp-millis</td>\n+    <td>long</td>\n+    <td>TimestampType</td>\n+  </tr>\n+  <tr>\n+    <td>timestamp-micros</td>\n+    <td>long</td>\n+    <td>TimestampType</td>\n+  </tr>\n+  <tr>\n+    <td>decimal</td>\n+    <td>bytes</td>\n+    <td>DecimalType</td>\n+  </tr>\n+  <tr>\n+    <td>decimal</td>\n+    <td>bytes</td>\n+    <td>DecimalType</td>\n+  </tr>",
    "line": 315
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`is not` -> `are not applied`?",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-17T19:31:20Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load/Save Functions\n+\n+Since `spark-avro` module is external, there is not such API as `.avro` in \n+`DataFrameReader` or `DataFrameWriter`.\n+To load/save data in Avro format, you need to specify the data source option `format` as short name `avro` or full name `org.apache.spark.sql.avro`.\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## Data Source Options\n+\n+Data source options of Avro can be set using the `.option` method on `DataFrameReader` or `DataFrameWriter`.\n+<table class=\"table\">\n+  <tr><th><b>Property Name</b></th><th><b>Default</b></th><th><b>Meaning</b></th><th><b>Scope</b></th></tr>\n+  <tr>\n+    <td><code>avroSchema</code></td>\n+    <td>None</td>\n+    <td>Optional Avro schema provided by an user in JSON format.</td>\n+    <td>read and write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordName</code></td>\n+    <td>topLevelRecord</td>\n+    <td>Top level record name in write result, which is required in Avro spec.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordNamespace</code></td>\n+    <td>\"\"</td>\n+    <td>Record namespace in write result.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>ignoreExtension</code></td>\n+    <td>true</td>\n+    <td>The option controls ignoring of files without <code>.avro</code> extensions in read. If the option is enabled, all files (with and without <code>.avro</code> extension) are loaded.</td>\n+    <td>read</td>\n+  </tr>\n+  <tr>\n+    <td><code>compression</code></td>\n+    <td>snappy</td>\n+    <td>The <code>compression</code> option allows to specify a compression codec used in write. Currently supported codecs are <code>uncompressed</code>, <code>snappy</code>, <code>deflate</code>, <code>bzip2</code> and <code>xz</code>. If the option is not set, the configuration <code>spark.sql.avro.compression.codec</code> config is taken into account.</td>\n+    <td>write</td>\n+  </tr>\n+</table>\n+\n+## Supported types for Avro -> Spark SQL conversion\n+Currently Spark supports reading all [primitive types](https://avro.apache.org/docs/1.8.2/spec.html#schema_primitive) and [complex types](https://avro.apache.org/docs/1.8.2/spec.html#schema_complex) of Avro.\n+<table class=\"table\">\n+  <tr><th><b>Avro type</b></th><th><b>Spark SQL type</b></th></tr>\n+  <tr>\n+    <td>boolean</td>\n+    <td>BooleanType</td>\n+  </tr>\n+  <tr>\n+    <td>int</td>\n+    <td>IntegerType</td>\n+  </tr>\n+  <tr>\n+    <td>long</td>\n+    <td>LongType</td>\n+  </tr>\n+  <tr>\n+    <td>float</td>\n+    <td>FloatType</td>\n+  </tr>\n+  <tr>\n+    <td>double</td>\n+    <td>DoubleType</td>\n+  </tr>\n+  <tr>\n+    <td>string</td>\n+    <td>StringType</td>\n+  </tr>\n+  <tr>\n+    <td>enum</td>\n+    <td>StringType</td>\n+  </tr>\n+  <tr>\n+    <td>fixed</td>\n+    <td>BinaryType</td>\n+  </tr>\n+  <tr>\n+    <td>bytes</td>\n+    <td>BinaryType</td>\n+  </tr>\n+  <tr>\n+    <td>record</td>\n+    <td>StructType</td>\n+  </tr>\n+  <tr>\n+    <td>array</td>\n+    <td>ArrayType</td>\n+  </tr>\n+  <tr>\n+    <td>map</td>\n+    <td>MapType</td>\n+  </tr>\n+  <tr>\n+    <td>union</td>\n+    <td>See below</td>\n+  </tr>\n+</table>\n+\n+In addition to the types listed above, it supports reading `union` types. The following three types are considered basic `union` types:\n+\n+1. `union(int, long)` will be mapped to LongType.\n+2. `union(float, double)` will be mapped to DoubleType.\n+3. `union(something, null)`, where something is any supported Avro type. This will be mapped to the same Spark SQL type as that of something, with nullable set to true.\n+All other union types are considered complex. They will be mapped to StructType where field names are member0, member1, etc., in accordance with members of the union. This is consistent with the behavior when converting between Avro and Parquet.\n+\n+It also supports reading the following Avro [logical types](https://avro.apache.org/docs/1.8.2/spec.html#Logical+Types):\n+\n+<table class=\"table\">\n+  <tr><th><b>Avro logical type</b></th><th><b>Avro type</b></th><th><b>Spark SQL type</b></th></tr>\n+  <tr>\n+    <td>date</td>\n+    <td>int</td>\n+    <td>DateType</td>\n+  </tr>\n+  <tr>\n+    <td>timestamp-millis</td>\n+    <td>long</td>\n+    <td>TimestampType</td>\n+  </tr>\n+  <tr>\n+    <td>timestamp-micros</td>\n+    <td>long</td>\n+    <td>TimestampType</td>\n+  </tr>\n+  <tr>\n+    <td>decimal</td>\n+    <td>bytes</td>\n+    <td>DecimalType</td>\n+  </tr>\n+  <tr>\n+    <td>decimal</td>\n+    <td>bytes</td>\n+    <td>DecimalType</td>\n+  </tr>\n+</table>\n+At the moment, it ignores docs, aliases and other properties present in the Avro file.\n+\n+## Supported types for Spark SQL -> Avro conversion\n+Spark supports writing of all Spark SQL types into Avro. For most types, the mapping from Spark types to Avro types is straightforward (e.g. IntegerType gets converted to int); however, there are a few special cases which are listed below:\n+\n+<table class=\"table\">\n+<tr><th><b>Spark SQL type</b></th><th><b>Avro type</b></th><th><b>Avro logical type</b></th></tr>\n+  <tr>\n+    <td>ByteType</td>\n+    <td>int</td>\n+    <td></td>\n+  </tr>\n+  <tr>\n+    <td>ShortType</td>\n+    <td>int</td>\n+    <td></td>\n+  </tr>\n+  <tr>\n+    <td>BinaryType</td>\n+    <td>bytes</td>\n+    <td></td>\n+  </tr>\n+  <tr>\n+    <td>Date</td>\n+    <td>int</td>\n+    <td>date</td>\n+  </tr>\n+  <tr>\n+    <td>TimestampType</td>\n+    <td>long</td>\n+    <td>timestamp-micros</td>\n+  </tr>\n+  <tr>\n+    <td>DecimalType</td>\n+    <td>fixed</td>\n+    <td>decimal</td>\n+  </tr>\n+</table>\n+\n+You can also specify the whole output Avro schema with the option `avroSchema`, so that Spark SQL types can be converted into other Avro types. The following conversions is not by default and require user specified Avro schema:"
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "@gengliangwang, I could check it by myself but thought it's easier to ask to you. Do we now all have the options and configurations existent in spark-avro?",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-20T01:58:30Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load/Save Functions\n+\n+Since `spark-avro` module is external, there is not such API as `.avro` in \n+`DataFrameReader` or `DataFrameWriter`.\n+To load/save data in Avro format, you need to specify the data source option `format` as short name `avro` or full name `org.apache.spark.sql.avro`.\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## Data Source Options\n+\n+Data source options of Avro can be set using the `.option` method on `DataFrameReader` or `DataFrameWriter`.\n+<table class=\"table\">\n+  <tr><th><b>Property Name</b></th><th><b>Default</b></th><th><b>Meaning</b></th><th><b>Scope</b></th></tr>\n+  <tr>\n+    <td><code>avroSchema</code></td>\n+    <td>None</td>\n+    <td>Optional Avro schema provided by an user in JSON format.</td>\n+    <td>read and write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordName</code></td>\n+    <td>topLevelRecord</td>\n+    <td>Top level record name in write result, which is required in Avro spec.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordNamespace</code></td>\n+    <td>\"\"</td>\n+    <td>Record namespace in write result.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>ignoreExtension</code></td>\n+    <td>true</td>\n+    <td>The option controls ignoring of files without <code>.avro</code> extensions in read. If the option is enabled, all files (with and without <code>.avro</code> extension) are loaded.</td>\n+    <td>read</td>\n+  </tr>\n+  <tr>\n+    <td><code>compression</code></td>\n+    <td>snappy</td>\n+    <td>The <code>compression</code> option allows to specify a compression codec used in write. Currently supported codecs are <code>uncompressed</code>, <code>snappy</code>, <code>deflate</code>, <code>bzip2</code> and <code>xz</code>. If the option is not set, the configuration <code>spark.sql.avro.compression.codec</code> config is taken into account.</td>"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "For data source options, yes.\r\nFor SQL configuration, I think the only one matters is the one in https://github.com/apache/spark/pull/22133. I am thinking of a better name for that configuration.",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-20T02:16:00Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load/Save Functions\n+\n+Since `spark-avro` module is external, there is not such API as `.avro` in \n+`DataFrameReader` or `DataFrameWriter`.\n+To load/save data in Avro format, you need to specify the data source option `format` as short name `avro` or full name `org.apache.spark.sql.avro`.\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## Data Source Options\n+\n+Data source options of Avro can be set using the `.option` method on `DataFrameReader` or `DataFrameWriter`.\n+<table class=\"table\">\n+  <tr><th><b>Property Name</b></th><th><b>Default</b></th><th><b>Meaning</b></th><th><b>Scope</b></th></tr>\n+  <tr>\n+    <td><code>avroSchema</code></td>\n+    <td>None</td>\n+    <td>Optional Avro schema provided by an user in JSON format.</td>\n+    <td>read and write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordName</code></td>\n+    <td>topLevelRecord</td>\n+    <td>Top level record name in write result, which is required in Avro spec.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordNamespace</code></td>\n+    <td>\"\"</td>\n+    <td>Record namespace in write result.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>ignoreExtension</code></td>\n+    <td>true</td>\n+    <td>The option controls ignoring of files without <code>.avro</code> extensions in read. If the option is enabled, all files (with and without <code>.avro</code> extension) are loaded.</td>\n+    <td>read</td>\n+  </tr>\n+  <tr>\n+    <td><code>compression</code></td>\n+    <td>snappy</td>\n+    <td>The <code>compression</code> option allows to specify a compression codec used in write. Currently supported codecs are <code>uncompressed</code>, <code>snappy</code>, <code>deflate</code>, <code>bzip2</code> and <code>xz</code>. If the option is not set, the configuration <code>spark.sql.avro.compression.codec</code> config is taken into account.</td>"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "I will add a section for the SQL configurations.",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-20T02:27:52Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load/Save Functions\n+\n+Since `spark-avro` module is external, there is not such API as `.avro` in \n+`DataFrameReader` or `DataFrameWriter`.\n+To load/save data in Avro format, you need to specify the data source option `format` as short name `avro` or full name `org.apache.spark.sql.avro`.\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## Data Source Options\n+\n+Data source options of Avro can be set using the `.option` method on `DataFrameReader` or `DataFrameWriter`.\n+<table class=\"table\">\n+  <tr><th><b>Property Name</b></th><th><b>Default</b></th><th><b>Meaning</b></th><th><b>Scope</b></th></tr>\n+  <tr>\n+    <td><code>avroSchema</code></td>\n+    <td>None</td>\n+    <td>Optional Avro schema provided by an user in JSON format.</td>\n+    <td>read and write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordName</code></td>\n+    <td>topLevelRecord</td>\n+    <td>Top level record name in write result, which is required in Avro spec.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordNamespace</code></td>\n+    <td>\"\"</td>\n+    <td>Record namespace in write result.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>ignoreExtension</code></td>\n+    <td>true</td>\n+    <td>The option controls ignoring of files without <code>.avro</code> extensions in read. If the option is enabled, all files (with and without <code>.avro</code> extension) are loaded.</td>\n+    <td>read</td>\n+  </tr>\n+  <tr>\n+    <td><code>compression</code></td>\n+    <td>snappy</td>\n+    <td>The <code>compression</code> option allows to specify a compression codec used in write. Currently supported codecs are <code>uncompressed</code>, <code>snappy</code>, <code>deflate</code>, <code>bzip2</code> and <code>xz</code>. If the option is not set, the configuration <code>spark.sql.avro.compression.codec</code> config is taken into account.</td>"
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "note I think we should add a compatibility section for compatibliity with databricks avro version here, reference https://github.com/apache/spark/pull/22133\r\n\r\n",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-21T21:58:28Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+",
    "line": 10
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "@tgravescs I have added an independent section for it :)",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T08:47:45Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+",
    "line": 10
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "cc @arunmahadevan ",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T12:49:28Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is not such API as `.avro` in \n+`DataFrameReader` or `DataFrameWriter`.\n+\n+To load/save data in Avro format, you need to specify the data source option `format` as `avro`(or `org.apache.spark.sql.avro`).\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## to_avro() and from_avro()\n+Spark SQL provides function `to_avro` to encode a struct as a string and `from_avro()` to retrieve the struct as a complex type.\n+\n+Using Avro record as columns are useful when reading from or writing to a streaming source like Kafka. Each \n+Kafka key-value record will be augmented with some metadata, such as the ingestion timestamp into Kafka, the offset in Kafka, etc.\n+* If the \"value\" field that contains your data is in Avro, you could use `from_avro()` to extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a file.\n+* `to_avro()` can be used to turn structs into Avro records. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\n+\n+Both methods are presently only available in Scala and Java.\n+\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+import org.apache.spark.sql.avro._\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+val jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+val df = spark\n+  .readStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+val output = df\n+  .select(from_avro('value, jsonFormatSchema) as 'user)\n+  .where(\"user.favorite_color == \\\"red\\\"\")\n+  .select(to_avro($\"user.name\") as 'value)\n+\n+val ds = output\n+  .writeStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"topic\", \"topic2\")\n+  .start()\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+import org.apache.spark.sql.avro.*\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+String jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+Dataset<Row> df = spark\n+  .readStream()\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+DataFrame output = df\n+  .select(from_avro(col(\"value\"), jsonFormatSchema).as(\"user\"))"
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "tgravescs"
    },
    "body": "there is no '.avro' API in",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T13:45:26Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is not such API as `.avro` in "
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "not \"Spark SQL\", it should be \"The Avro package\"",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T14:50:07Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is no `.avro` API in \n+`DataFrameReader` or `DataFrameWriter`.\n+\n+To load/save data in Avro format, you need to specify the data source option `format` as `avro`(or `org.apache.spark.sql.avro`).\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## to_avro() and from_avro()\n+Spark SQL provides function `to_avro` to encode a struct as a string and `from_avro()` to retrieve the struct as a complex type."
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "`encode a struct as a string`, I think it's not \"string\", but \"binary\"?",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T14:51:04Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is no `.avro` API in \n+`DataFrameReader` or `DataFrameWriter`.\n+\n+To load/save data in Avro format, you need to specify the data source option `format` as `avro`(or `org.apache.spark.sql.avro`).\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## to_avro() and from_avro()\n+Spark SQL provides function `to_avro` to encode a struct as a string and `from_avro()` to retrieve the struct as a complex type."
  }, {
    "author": {
      "login": "arunmahadevan"
    },
    "body": "does it need to be a struct or any spark sql type? \r\nmaybe: `to_avro` to encode spark sql types as avro bytes and `from_avro` to retrieve avro bytes as spark sql types?",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T16:58:11Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is no `.avro` API in \n+`DataFrameReader` or `DataFrameWriter`.\n+\n+To load/save data in Avro format, you need to specify the data source option `format` as `avro`(or `org.apache.spark.sql.avro`).\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## to_avro() and from_avro()\n+Spark SQL provides function `to_avro` to encode a struct as a string and `from_avro()` to retrieve the struct as a complex type."
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Do not use `presently`, we should say `As of Spark 2.4, ...`",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T14:52:30Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is no `.avro` API in \n+`DataFrameReader` or `DataFrameWriter`.\n+\n+To load/save data in Avro format, you need to specify the data source option `format` as `avro`(or `org.apache.spark.sql.avro`).\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## to_avro() and from_avro()\n+Spark SQL provides function `to_avro` to encode a struct as a string and `from_avro()` to retrieve the struct as a complex type.\n+\n+Using Avro record as columns are useful when reading from or writing to a streaming source like Kafka. Each \n+Kafka key-value record will be augmented with some metadata, such as the ingestion timestamp into Kafka, the offset in Kafka, etc.\n+* If the \"value\" field that contains your data is in Avro, you could use `from_avro()` to extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a file.\n+* `to_avro()` can be used to turn structs into Avro records. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\n+\n+Both methods are presently only available in Scala and Java."
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "I think it should be OK. In SQL programming guid, there is a lot of \"currently\". Otherwise we have to update the `2.4` for each release.(Is there any way to get the release version in the doc?)",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T16:47:02Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is no `.avro` API in \n+`DataFrameReader` or `DataFrameWriter`.\n+\n+To load/save data in Avro format, you need to specify the data source option `format` as `avro`(or `org.apache.spark.sql.avro`).\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## to_avro() and from_avro()\n+Spark SQL provides function `to_avro` to encode a struct as a string and `from_avro()` to retrieve the struct as a complex type.\n+\n+Using Avro record as columns are useful when reading from or writing to a streaming source like Kafka. Each \n+Kafka key-value record will be augmented with some metadata, such as the ingestion timestamp into Kafka, the offset in Kafka, etc.\n+* If the \"value\" field that contains your data is in Avro, you could use `from_avro()` to extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a file.\n+* `to_avro()` can be used to turn structs into Avro records. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\n+\n+Both methods are presently only available in Scala and Java."
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "Are you sure this compiles in Java?",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T14:54:16Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is no `.avro` API in \n+`DataFrameReader` or `DataFrameWriter`.\n+\n+To load/save data in Avro format, you need to specify the data source option `format` as `avro`(or `org.apache.spark.sql.avro`).\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## to_avro() and from_avro()\n+Spark SQL provides function `to_avro` to encode a struct as a string and `from_avro()` to retrieve the struct as a complex type.\n+\n+Using Avro record as columns are useful when reading from or writing to a streaming source like Kafka. Each \n+Kafka key-value record will be augmented with some metadata, such as the ingestion timestamp into Kafka, the offset in Kafka, etc.\n+* If the \"value\" field that contains your data is in Avro, you could use `from_avro()` to extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a file.\n+* `to_avro()` can be used to turn structs into Avro records. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\n+\n+Both methods are presently only available in Scala and Java.\n+\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+import org.apache.spark.sql.avro._\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+val jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+val df = spark\n+  .readStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+val output = df\n+  .select(from_avro('value, jsonFormatSchema) as 'user)\n+  .where(\"user.favorite_color == \\\"red\\\"\")\n+  .select(to_avro($\"user.name\") as 'value)\n+\n+val ds = output\n+  .writeStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"topic\", \"topic2\")\n+  .start()\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+import org.apache.spark.sql.avro.*\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+String jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+Dataset<Row> df = spark\n+  .readStream()\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+DataFrame output = df"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Looks OK except missing a semicolon at the end of the statements.",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T14:57:47Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is no `.avro` API in \n+`DataFrameReader` or `DataFrameWriter`.\n+\n+To load/save data in Avro format, you need to specify the data source option `format` as `avro`(or `org.apache.spark.sql.avro`).\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## to_avro() and from_avro()\n+Spark SQL provides function `to_avro` to encode a struct as a string and `from_avro()` to retrieve the struct as a complex type.\n+\n+Using Avro record as columns are useful when reading from or writing to a streaming source like Kafka. Each \n+Kafka key-value record will be augmented with some metadata, such as the ingestion timestamp into Kafka, the offset in Kafka, etc.\n+* If the \"value\" field that contains your data is in Avro, you could use `from_avro()` to extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a file.\n+* `to_avro()` can be used to turn structs into Avro records. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\n+\n+Both methods are presently only available in Scala and Java.\n+\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+import org.apache.spark.sql.avro._\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+val jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+val df = spark\n+  .readStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+val output = df\n+  .select(from_avro('value, jsonFormatSchema) as 'user)\n+  .where(\"user.favorite_color == \\\"red\\\"\")\n+  .select(to_avro($\"user.name\") as 'value)\n+\n+val ds = output\n+  .writeStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"topic\", \"topic2\")\n+  .start()\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+import org.apache.spark.sql.avro.*\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+String jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+Dataset<Row> df = spark\n+  .readStream()\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+DataFrame output = df"
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "We should mention the behavior when the specified schema doesn't match the real schema.",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T14:55:16Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is no `.avro` API in \n+`DataFrameReader` or `DataFrameWriter`.\n+\n+To load/save data in Avro format, you need to specify the data source option `format` as `avro`(or `org.apache.spark.sql.avro`).\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## to_avro() and from_avro()\n+Spark SQL provides function `to_avro` to encode a struct as a string and `from_avro()` to retrieve the struct as a complex type.\n+\n+Using Avro record as columns are useful when reading from or writing to a streaming source like Kafka. Each \n+Kafka key-value record will be augmented with some metadata, such as the ingestion timestamp into Kafka, the offset in Kafka, etc.\n+* If the \"value\" field that contains your data is in Avro, you could use `from_avro()` to extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a file.\n+* `to_avro()` can be used to turn structs into Avro records. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\n+\n+Both methods are presently only available in Scala and Java.\n+\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+import org.apache.spark.sql.avro._\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+val jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+val df = spark\n+  .readStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+val output = df\n+  .select(from_avro('value, jsonFormatSchema) as 'user)\n+  .where(\"user.favorite_color == \\\"red\\\"\")\n+  .select(to_avro($\"user.name\") as 'value)\n+\n+val ds = output\n+  .writeStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"topic\", \"topic2\")\n+  .start()\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+import org.apache.spark.sql.avro.*\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+String jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+Dataset<Row> df = spark\n+  .readStream()\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+DataFrame output = df\n+  .select(from_avro(col(\"value\"), jsonFormatSchema).as(\"user\"))\n+  .where(\"user.favorite_color == \\\"red\\\"\")\n+  .select(to_avro(col(\"user.name\")).as(\"value\"))\n+\n+StreamingQuery ds = output\n+  .writeStream()\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"topic\", \"topic2\")\n+  .start()\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## Data Source Option\n+\n+Data source options of Avro can be set using the `.option` method on `DataFrameReader` or `DataFrameWriter`.\n+<table class=\"table\">\n+  <tr><th><b>Property Name</b></th><th><b>Default</b></th><th><b>Meaning</b></th><th><b>Scope</b></th></tr>\n+  <tr>\n+    <td><code>avroSchema</code></td>\n+    <td>None</td>\n+    <td>Optional Avro schema provided by an user in JSON format.</td>"
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: usually calling the `String(byte[])` constructor is a bad idea as it interprets the bytes according to whatever the platform default encoding is. Add `StandardCharsets.UTF_8` as a second arg, but, I odn't know if this is too picky to care about in the example.",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T14:58:41Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is no `.avro` API in \n+`DataFrameReader` or `DataFrameWriter`.\n+\n+To load/save data in Avro format, you need to specify the data source option `format` as `avro`(or `org.apache.spark.sql.avro`).\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## to_avro() and from_avro()\n+Spark SQL provides function `to_avro` to encode a struct as a string and `from_avro()` to retrieve the struct as a complex type.\n+\n+Using Avro record as columns are useful when reading from or writing to a streaming source like Kafka. Each \n+Kafka key-value record will be augmented with some metadata, such as the ingestion timestamp into Kafka, the offset in Kafka, etc.\n+* If the \"value\" field that contains your data is in Avro, you could use `from_avro()` to extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a file.\n+* `to_avro()` can be used to turn structs into Avro records. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\n+\n+Both methods are presently only available in Scala and Java.\n+\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+import org.apache.spark.sql.avro._\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+val jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+val df = spark\n+  .readStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+val output = df\n+  .select(from_avro('value, jsonFormatSchema) as 'user)\n+  .where(\"user.favorite_color == \\\"red\\\"\")\n+  .select(to_avro($\"user.name\") as 'value)\n+\n+val ds = output\n+  .writeStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"topic\", \"topic2\")\n+  .start()\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+import org.apache.spark.sql.avro.*\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+String jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))"
  }, {
    "author": {
      "login": "gengliangwang"
    },
    "body": "I think it should be OK to ignore `StandardCharsets.UTF_8`.\r\nThe example code can be simple and just for demonstrating.\r\nThe key part is about `to_avro` and `from_avro` here.",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T17:40:25Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is no `.avro` API in \n+`DataFrameReader` or `DataFrameWriter`.\n+\n+To load/save data in Avro format, you need to specify the data source option `format` as `avro`(or `org.apache.spark.sql.avro`).\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## to_avro() and from_avro()\n+Spark SQL provides function `to_avro` to encode a struct as a string and `from_avro()` to retrieve the struct as a complex type.\n+\n+Using Avro record as columns are useful when reading from or writing to a streaming source like Kafka. Each \n+Kafka key-value record will be augmented with some metadata, such as the ingestion timestamp into Kafka, the offset in Kafka, etc.\n+* If the \"value\" field that contains your data is in Avro, you could use `from_avro()` to extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a file.\n+* `to_avro()` can be used to turn structs into Avro records. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\n+\n+Both methods are presently only available in Scala and Java.\n+\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+import org.apache.spark.sql.avro._\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+val jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+val df = spark\n+  .readStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+val output = df\n+  .select(from_avro('value, jsonFormatSchema) as 'user)\n+  .where(\"user.favorite_color == \\\"red\\\"\")\n+  .select(to_avro($\"user.name\") as 'value)\n+\n+val ds = output\n+  .writeStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"topic\", \"topic2\")\n+  .start()\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+import org.apache.spark.sql.avro.*\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+String jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))"
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "DateType",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T14:59:18Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is no `.avro` API in \n+`DataFrameReader` or `DataFrameWriter`.\n+\n+To load/save data in Avro format, you need to specify the data source option `format` as `avro`(or `org.apache.spark.sql.avro`).\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## to_avro() and from_avro()\n+Spark SQL provides function `to_avro` to encode a struct as a string and `from_avro()` to retrieve the struct as a complex type.\n+\n+Using Avro record as columns are useful when reading from or writing to a streaming source like Kafka. Each \n+Kafka key-value record will be augmented with some metadata, such as the ingestion timestamp into Kafka, the offset in Kafka, etc.\n+* If the \"value\" field that contains your data is in Avro, you could use `from_avro()` to extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a file.\n+* `to_avro()` can be used to turn structs into Avro records. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\n+\n+Both methods are presently only available in Scala and Java.\n+\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+import org.apache.spark.sql.avro._\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+val jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+val df = spark\n+  .readStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+val output = df\n+  .select(from_avro('value, jsonFormatSchema) as 'user)\n+  .where(\"user.favorite_color == \\\"red\\\"\")\n+  .select(to_avro($\"user.name\") as 'value)\n+\n+val ds = output\n+  .writeStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"topic\", \"topic2\")\n+  .start()\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+import org.apache.spark.sql.avro.*\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+String jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+Dataset<Row> df = spark\n+  .readStream()\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+DataFrame output = df\n+  .select(from_avro(col(\"value\"), jsonFormatSchema).as(\"user\"))\n+  .where(\"user.favorite_color == \\\"red\\\"\")\n+  .select(to_avro(col(\"user.name\")).as(\"value\"))\n+\n+StreamingQuery ds = output\n+  .writeStream()\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"topic\", \"topic2\")\n+  .start()\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## Data Source Option\n+\n+Data source options of Avro can be set using the `.option` method on `DataFrameReader` or `DataFrameWriter`.\n+<table class=\"table\">\n+  <tr><th><b>Property Name</b></th><th><b>Default</b></th><th><b>Meaning</b></th><th><b>Scope</b></th></tr>\n+  <tr>\n+    <td><code>avroSchema</code></td>\n+    <td>None</td>\n+    <td>Optional Avro schema provided by an user in JSON format.</td>\n+    <td>read and write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordName</code></td>\n+    <td>topLevelRecord</td>\n+    <td>Top level record name in write result, which is required in Avro spec.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordNamespace</code></td>\n+    <td>\"\"</td>\n+    <td>Record namespace in write result.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>ignoreExtension</code></td>\n+    <td>true</td>\n+    <td>The option controls ignoring of files without <code>.avro</code> extensions in read.<br> If the option is enabled, all files (with and without <code>.avro</code> extension) are loaded.</td>\n+    <td>read</td>\n+  </tr>\n+  <tr>\n+    <td><code>compression</code></td>\n+    <td>snappy</td>\n+    <td>The <code>compression</code> option allows to specify a compression codec used in write.<br>\n+  Currently supported codecs are <code>uncompressed</code>, <code>snappy</code>, <code>deflate</code>, <code>bzip2</code> and <code>xz</code>.<br> If the option is not set, the configuration <code>spark.sql.avro.compression.codec</code> config is taken into account.</td>\n+    <td>write</td>\n+  </tr>\n+</table>\n+\n+## Configuration\n+Configuration of Avro can be done using the `setConf` method on SparkSession or by running `SET key=value` commands using SQL.\n+<table class=\"table\">\n+  <tr><th><b>Property Name</b></th><th><b>Default</b></th><th><b>Meaning</b></th></tr>\n+  <tr>\n+    <td>spark.sql.legacy.replaceDatabricksSparkAvro.enabled</td>\n+    <td>true</td>\n+    <td>If it is set to true, the data source provider <code>com.databricks.spark.avro</code> is mapped to the built-in but external Avro data source module for backward compatibility.</td>\n+  </tr>\n+  <tr>\n+    <td>spark.sql.avro.compression.codec</td>\n+    <td>snappy</td>\n+    <td>Compression codec used in writing of AVRO files. Supported codecs: uncompressed, deflate, snappy, bzip2 and xz. Default codec is snappy.</td>\n+  </tr>\n+  <tr>\n+    <td>spark.sql.avro.deflate.level</td>\n+    <td>-1</td>\n+    <td>Compression level for the deflate codec used in writing of AVRO files. Valid value must be in the range of from 1 to 9 inclusive or -1. The default value is -1 which corresponds to 6 level in the current implementation.</td>\n+  </tr>\n+</table>\n+\n+## Compatibility with Databricks spark-avro\n+This Avro data source module is originally from and compatible with Databricks's open source repository \n+[spark-avro](https://github.com/databricks/spark-avro).\n+\n+By default with the SQL configuration `spark.sql.legacy.replaceDatabricksSparkAvro.enabled` enabled, the data source provider `com.databricks.spark.avro` is \n+mapped to this built-in Avro module. For the Spark tables created with `Provider` property as `com.databricks.spark.avro` in \n+catalog meta store, the mapping is essential to load these tables if you are using this built-in Avro module. \n+\n+Note in Databricks's [spark-avro](https://github.com/databricks/spark-avro), implicit classes \n+`AvroDataFrameWriter` and `AvroDataFrameReader` were created for shortcut function `.avro()`. In this \n+built-in but external module, both implicit classes are removed. Please use `.format(\"avro\")` in \n+`DataFrameWriter` or `DataFrameReader` instead, which should be clean and good enough.\n+\n+If you prefer using your own build of `spark-avro` jar file, you can simply disable the configuration \n+`spark.sql.legacy.replaceDatabricksSparkAvro.enabled`, and use the option `--jars` on deploying your \n+applications. Read the [Advanced Dependency Management](https://spark.apache\n+.org/docs/latest/submitting-applications.html#advanced-dependency-management) section in Application \n+Submission Guide for more details. \n+\n+## Supported types for Avro -> Spark SQL conversion\n+Currently Spark supports reading all [primitive types](https://avro.apache.org/docs/1.8.2/spec.html#schema_primitive) and [complex types](https://avro.apache.org/docs/1.8.2/spec.html#schema_complex) of Avro.\n+<table class=\"table\">\n+  <tr><th><b>Avro type</b></th><th><b>Spark SQL type</b></th></tr>\n+  <tr>\n+    <td>boolean</td>\n+    <td>BooleanType</td>\n+  </tr>\n+  <tr>\n+    <td>int</td>\n+    <td>IntegerType</td>\n+  </tr>\n+  <tr>\n+    <td>long</td>\n+    <td>LongType</td>\n+  </tr>\n+  <tr>\n+    <td>float</td>\n+    <td>FloatType</td>\n+  </tr>\n+  <tr>\n+    <td>double</td>\n+    <td>DoubleType</td>\n+  </tr>\n+  <tr>\n+    <td>string</td>\n+    <td>StringType</td>\n+  </tr>\n+  <tr>\n+    <td>enum</td>\n+    <td>StringType</td>\n+  </tr>\n+  <tr>\n+    <td>fixed</td>\n+    <td>BinaryType</td>\n+  </tr>\n+  <tr>\n+    <td>bytes</td>\n+    <td>BinaryType</td>\n+  </tr>\n+  <tr>\n+    <td>record</td>\n+    <td>StructType</td>\n+  </tr>\n+  <tr>\n+    <td>array</td>\n+    <td>ArrayType</td>\n+  </tr>\n+  <tr>\n+    <td>map</td>\n+    <td>MapType</td>\n+  </tr>\n+  <tr>\n+    <td>union</td>\n+    <td>See below</td>\n+  </tr>\n+</table>\n+\n+In addition to the types listed above, it supports reading `union` types. The following three types are considered basic `union` types:\n+\n+1. `union(int, long)` will be mapped to LongType.\n+2. `union(float, double)` will be mapped to DoubleType.\n+3. `union(something, null)`, where something is any supported Avro type. This will be mapped to the same Spark SQL type as that of something, with nullable set to true.\n+All other union types are considered complex. They will be mapped to StructType where field names are member0, member1, etc., in accordance with members of the union. This is consistent with the behavior when converting between Avro and Parquet.\n+\n+It also supports reading the following Avro [logical types](https://avro.apache.org/docs/1.8.2/spec.html#Logical+Types):\n+\n+<table class=\"table\">\n+  <tr><th><b>Avro logical type</b></th><th><b>Avro type</b></th><th><b>Spark SQL type</b></th></tr>\n+  <tr>\n+    <td>date</td>\n+    <td>int</td>\n+    <td>DateType</td>\n+  </tr>\n+  <tr>\n+    <td>timestamp-millis</td>\n+    <td>long</td>\n+    <td>TimestampType</td>\n+  </tr>\n+  <tr>\n+    <td>timestamp-micros</td>\n+    <td>long</td>\n+    <td>TimestampType</td>\n+  </tr>\n+  <tr>\n+    <td>decimal</td>\n+    <td>fixed</td>\n+    <td>DecimalType</td>\n+  </tr>\n+  <tr>\n+    <td>decimal</td>\n+    <td>bytes</td>\n+    <td>DecimalType</td>\n+  </tr>\n+</table>\n+At the moment, it ignores docs, aliases and other properties present in the Avro file.\n+\n+## Supported types for Spark SQL -> Avro conversion\n+Spark supports writing of all Spark SQL types into Avro. For most types, the mapping from Spark types to Avro types is straightforward (e.g. IntegerType gets converted to int); however, there are a few special cases which are listed below:\n+\n+<table class=\"table\">\n+<tr><th><b>Spark SQL type</b></th><th><b>Avro type</b></th><th><b>Avro logical type</b></th></tr>\n+  <tr>\n+    <td>ByteType</td>\n+    <td>int</td>\n+    <td></td>\n+  </tr>\n+  <tr>\n+    <td>ShortType</td>\n+    <td>int</td>\n+    <td></td>\n+  </tr>\n+  <tr>\n+    <td>BinaryType</td>\n+    <td>bytes</td>\n+    <td></td>\n+  </tr>\n+  <tr>\n+    <td>Date</td>"
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Semicolon at end of line (all statements in java)",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T15:00:42Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is no `.avro` API in \n+`DataFrameReader` or `DataFrameWriter`.\n+\n+To load/save data in Avro format, you need to specify the data source option `format` as `avro`(or `org.apache.spark.sql.avro`).\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## to_avro() and from_avro()\n+Spark SQL provides function `to_avro` to encode a struct as a string and `from_avro()` to retrieve the struct as a complex type.\n+\n+Using Avro record as columns are useful when reading from or writing to a streaming source like Kafka. Each \n+Kafka key-value record will be augmented with some metadata, such as the ingestion timestamp into Kafka, the offset in Kafka, etc.\n+* If the \"value\" field that contains your data is in Avro, you could use `from_avro()` to extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a file.\n+* `to_avro()` can be used to turn structs into Avro records. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\n+\n+Both methods are presently only available in Scala and Java.\n+\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+import org.apache.spark.sql.avro._\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+val jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+val df = spark\n+  .readStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+val output = df\n+  .select(from_avro('value, jsonFormatSchema) as 'user)\n+  .where(\"user.favorite_color == \\\"red\\\"\")\n+  .select(to_avro($\"user.name\") as 'value)\n+\n+val ds = output\n+  .writeStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"topic\", \"topic2\")\n+  .start()\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+import org.apache.spark.sql.avro.*"
  }],
  "prId": 22121
}, {
  "comments": [{
    "author": {
      "login": "dhruve"
    },
    "body": "Hey. I know that we didn't support reading primitive types in the databricks-avro package, so I just tried to read a primitive avro file and I wasn't able to do so using the current master. \r\n\r\nHow I tried reading it => `spark.read.format(\"avro\").load(\"avroPrimitiveTypes/randomBoolean.avro\")`\r\n\r\nI think we could reword and be explicit that we support reading primitive types under records unless I am missing something here.\r\n\r\n",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T17:02:12Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is no `.avro` API in \n+`DataFrameReader` or `DataFrameWriter`.\n+\n+To load/save data in Avro format, you need to specify the data source option `format` as `avro`(or `org.apache.spark.sql.avro`).\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## to_avro() and from_avro()\n+Spark SQL provides function `to_avro` to encode a struct as a string and `from_avro()` to retrieve the struct as a complex type.\n+\n+Using Avro record as columns are useful when reading from or writing to a streaming source like Kafka. Each \n+Kafka key-value record will be augmented with some metadata, such as the ingestion timestamp into Kafka, the offset in Kafka, etc.\n+* If the \"value\" field that contains your data is in Avro, you could use `from_avro()` to extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a file.\n+* `to_avro()` can be used to turn structs into Avro records. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\n+\n+Both methods are presently only available in Scala and Java.\n+\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+import org.apache.spark.sql.avro._\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+val jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+val df = spark\n+  .readStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+val output = df\n+  .select(from_avro('value, jsonFormatSchema) as 'user)\n+  .where(\"user.favorite_color == \\\"red\\\"\")\n+  .select(to_avro($\"user.name\") as 'value)\n+\n+val ds = output\n+  .writeStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"topic\", \"topic2\")\n+  .start()\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+import org.apache.spark.sql.avro.*\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+String jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+Dataset<Row> df = spark\n+  .readStream()\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+DataFrame output = df\n+  .select(from_avro(col(\"value\"), jsonFormatSchema).as(\"user\"))\n+  .where(\"user.favorite_color == \\\"red\\\"\")\n+  .select(to_avro(col(\"user.name\")).as(\"value\"))\n+\n+StreamingQuery ds = output\n+  .writeStream()\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"topic\", \"topic2\")\n+  .start()\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## Data Source Option\n+\n+Data source options of Avro can be set using the `.option` method on `DataFrameReader` or `DataFrameWriter`.\n+<table class=\"table\">\n+  <tr><th><b>Property Name</b></th><th><b>Default</b></th><th><b>Meaning</b></th><th><b>Scope</b></th></tr>\n+  <tr>\n+    <td><code>avroSchema</code></td>\n+    <td>None</td>\n+    <td>Optional Avro schema provided by an user in JSON format.</td>\n+    <td>read and write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordName</code></td>\n+    <td>topLevelRecord</td>\n+    <td>Top level record name in write result, which is required in Avro spec.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordNamespace</code></td>\n+    <td>\"\"</td>\n+    <td>Record namespace in write result.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>ignoreExtension</code></td>\n+    <td>true</td>\n+    <td>The option controls ignoring of files without <code>.avro</code> extensions in read.<br> If the option is enabled, all files (with and without <code>.avro</code> extension) are loaded.</td>\n+    <td>read</td>\n+  </tr>\n+  <tr>\n+    <td><code>compression</code></td>\n+    <td>snappy</td>\n+    <td>The <code>compression</code> option allows to specify a compression codec used in write.<br>\n+  Currently supported codecs are <code>uncompressed</code>, <code>snappy</code>, <code>deflate</code>, <code>bzip2</code> and <code>xz</code>.<br> If the option is not set, the configuration <code>spark.sql.avro.compression.codec</code> config is taken into account.</td>\n+    <td>write</td>\n+  </tr>\n+</table>\n+\n+## Configuration\n+Configuration of Avro can be done using the `setConf` method on SparkSession or by running `SET key=value` commands using SQL.\n+<table class=\"table\">\n+  <tr><th><b>Property Name</b></th><th><b>Default</b></th><th><b>Meaning</b></th></tr>\n+  <tr>\n+    <td>spark.sql.legacy.replaceDatabricksSparkAvro.enabled</td>\n+    <td>true</td>\n+    <td>If it is set to true, the data source provider <code>com.databricks.spark.avro</code> is mapped to the built-in but external Avro data source module for backward compatibility.</td>\n+  </tr>\n+  <tr>\n+    <td>spark.sql.avro.compression.codec</td>\n+    <td>snappy</td>\n+    <td>Compression codec used in writing of AVRO files. Supported codecs: uncompressed, deflate, snappy, bzip2 and xz. Default codec is snappy.</td>\n+  </tr>\n+  <tr>\n+    <td>spark.sql.avro.deflate.level</td>\n+    <td>-1</td>\n+    <td>Compression level for the deflate codec used in writing of AVRO files. Valid value must be in the range of from 1 to 9 inclusive or -1. The default value is -1 which corresponds to 6 level in the current implementation.</td>\n+  </tr>\n+</table>\n+\n+## Compatibility with Databricks spark-avro\n+This Avro data source module is originally from and compatible with Databricks's open source repository \n+[spark-avro](https://github.com/databricks/spark-avro).\n+\n+By default with the SQL configuration `spark.sql.legacy.replaceDatabricksSparkAvro.enabled` enabled, the data source provider `com.databricks.spark.avro` is \n+mapped to this built-in Avro module. For the Spark tables created with `Provider` property as `com.databricks.spark.avro` in \n+catalog meta store, the mapping is essential to load these tables if you are using this built-in Avro module. \n+\n+Note in Databricks's [spark-avro](https://github.com/databricks/spark-avro), implicit classes \n+`AvroDataFrameWriter` and `AvroDataFrameReader` were created for shortcut function `.avro()`. In this \n+built-in but external module, both implicit classes are removed. Please use `.format(\"avro\")` in \n+`DataFrameWriter` or `DataFrameReader` instead, which should be clean and good enough.\n+\n+If you prefer using your own build of `spark-avro` jar file, you can simply disable the configuration \n+`spark.sql.legacy.replaceDatabricksSparkAvro.enabled`, and use the option `--jars` on deploying your \n+applications. Read the [Advanced Dependency Management](https://spark.apache\n+.org/docs/latest/submitting-applications.html#advanced-dependency-management) section in Application \n+Submission Guide for more details. \n+\n+## Supported types for Avro -> Spark SQL conversion\n+Currently Spark supports reading all [primitive types](https://avro.apache.org/docs/1.8.2/spec.html#schema_primitive) and [complex types](https://avro.apache.org/docs/1.8.2/spec.html#schema_complex) of Avro."
  }, {
    "author": {
      "login": "dhruve"
    },
    "body": "@gengliangwang ^^",
    "commit": "1f253bf536c3a7bd1c07ba5ea5600f661c8e106e",
    "createdAt": "2018-08-22T19:15:59Z",
    "diffHunk": "@@ -0,0 +1,377 @@\n+---\n+layout: global\n+title: Apache Avro Data Source Guide\n+---\n+\n+* This will become a table of contents (this text will be scraped).\n+{:toc}\n+\n+Since Spark 2.4 release, [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) provides built-in support for reading and writing Apache Avro data.\n+\n+## Deploying\n+The `spark-avro` module is external and not included in `spark-submit` or `spark-shell` by default.\n+\n+As with any Spark applications, `spark-submit` is used to launch your application. `spark-avro_{{site.SCALA_BINARY_VERSION}}`\n+and its dependencies can be directly added to `spark-submit` using `--packages`, such as,\n+\n+    ./bin/spark-submit --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+For experimenting on `spark-shell`, you can also use `--packages` to add `org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}` and its dependencies directly,\n+\n+    ./bin/spark-shell --packages org.apache.spark:spark-avro_{{site.SCALA_BINARY_VERSION}}:{{site.SPARK_VERSION_SHORT}} ...\n+\n+See [Application Submission Guide](submitting-applications.html) for more details about submitting applications with external dependencies.\n+\n+## Load and Save Functions\n+\n+Since `spark-avro` module is external, there is no `.avro` API in \n+`DataFrameReader` or `DataFrameWriter`.\n+\n+To load/save data in Avro format, you need to specify the data source option `format` as `avro`(or `org.apache.spark.sql.avro`).\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+\n+val usersDF = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+usersDF.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+\n+Dataset<Row> usersDF = spark.read().format(\"avro\").load(\"examples/src/main/resources/users.avro\");\n+usersDF.select(\"name\", \"favorite_color\").write().format(\"avro\").save(\"namesAndFavColors.avro\");\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+{% highlight python %}\n+\n+df = spark.read.format(\"avro\").load(\"examples/src/main/resources/users.avro\")\n+df.select(\"name\", \"favorite_color\").write.format(\"avro\").save(\"namesAndFavColors.avro\")\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"r\" markdown=\"1\">\n+{% highlight r %}\n+\n+df <- read.df(\"examples/src/main/resources/users.avro\", \"avro\")\n+write.df(select(df, \"name\", \"favorite_color\"), \"namesAndFavColors.avro\", \"avro\")\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## to_avro() and from_avro()\n+Spark SQL provides function `to_avro` to encode a struct as a string and `from_avro()` to retrieve the struct as a complex type.\n+\n+Using Avro record as columns are useful when reading from or writing to a streaming source like Kafka. Each \n+Kafka key-value record will be augmented with some metadata, such as the ingestion timestamp into Kafka, the offset in Kafka, etc.\n+* If the \"value\" field that contains your data is in Avro, you could use `from_avro()` to extract your data, enrich it, clean it, and then push it downstream to Kafka again or write it out to a file.\n+* `to_avro()` can be used to turn structs into Avro records. This method is particularly useful when you would like to re-encode multiple columns into a single one when writing data out to Kafka.\n+\n+Both methods are presently only available in Scala and Java.\n+\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+{% highlight scala %}\n+import org.apache.spark.sql.avro._\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+val jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+val df = spark\n+  .readStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+val output = df\n+  .select(from_avro('value, jsonFormatSchema) as 'user)\n+  .where(\"user.favorite_color == \\\"red\\\"\")\n+  .select(to_avro($\"user.name\") as 'value)\n+\n+val ds = output\n+  .writeStream\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"topic\", \"topic2\")\n+  .start()\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"java\" markdown=\"1\">\n+{% highlight java %}\n+import org.apache.spark.sql.avro.*\n+\n+// `from_avro` requires Avro schema in JSON string format.\n+String jsonFormatSchema = new String(Files.readAllBytes(Paths.get(\"./examples/src/main/resources/user.avsc\")))\n+\n+Dataset<Row> df = spark\n+  .readStream()\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"subscribe\", \"topic1\")\n+  .load()\n+\n+// 1. Decode the Avro data into a struct;\n+// 2. Filter by column `favorite_color`;\n+// 3. Encode the column `name` in Avro format.\n+DataFrame output = df\n+  .select(from_avro(col(\"value\"), jsonFormatSchema).as(\"user\"))\n+  .where(\"user.favorite_color == \\\"red\\\"\")\n+  .select(to_avro(col(\"user.name\")).as(\"value\"))\n+\n+StreamingQuery ds = output\n+  .writeStream()\n+  .format(\"kafka\")\n+  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\")\n+  .option(\"topic\", \"topic2\")\n+  .start()\n+\n+{% endhighlight %}\n+</div>\n+</div>\n+\n+## Data Source Option\n+\n+Data source options of Avro can be set using the `.option` method on `DataFrameReader` or `DataFrameWriter`.\n+<table class=\"table\">\n+  <tr><th><b>Property Name</b></th><th><b>Default</b></th><th><b>Meaning</b></th><th><b>Scope</b></th></tr>\n+  <tr>\n+    <td><code>avroSchema</code></td>\n+    <td>None</td>\n+    <td>Optional Avro schema provided by an user in JSON format.</td>\n+    <td>read and write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordName</code></td>\n+    <td>topLevelRecord</td>\n+    <td>Top level record name in write result, which is required in Avro spec.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>recordNamespace</code></td>\n+    <td>\"\"</td>\n+    <td>Record namespace in write result.</td>\n+    <td>write</td>\n+  </tr>\n+  <tr>\n+    <td><code>ignoreExtension</code></td>\n+    <td>true</td>\n+    <td>The option controls ignoring of files without <code>.avro</code> extensions in read.<br> If the option is enabled, all files (with and without <code>.avro</code> extension) are loaded.</td>\n+    <td>read</td>\n+  </tr>\n+  <tr>\n+    <td><code>compression</code></td>\n+    <td>snappy</td>\n+    <td>The <code>compression</code> option allows to specify a compression codec used in write.<br>\n+  Currently supported codecs are <code>uncompressed</code>, <code>snappy</code>, <code>deflate</code>, <code>bzip2</code> and <code>xz</code>.<br> If the option is not set, the configuration <code>spark.sql.avro.compression.codec</code> config is taken into account.</td>\n+    <td>write</td>\n+  </tr>\n+</table>\n+\n+## Configuration\n+Configuration of Avro can be done using the `setConf` method on SparkSession or by running `SET key=value` commands using SQL.\n+<table class=\"table\">\n+  <tr><th><b>Property Name</b></th><th><b>Default</b></th><th><b>Meaning</b></th></tr>\n+  <tr>\n+    <td>spark.sql.legacy.replaceDatabricksSparkAvro.enabled</td>\n+    <td>true</td>\n+    <td>If it is set to true, the data source provider <code>com.databricks.spark.avro</code> is mapped to the built-in but external Avro data source module for backward compatibility.</td>\n+  </tr>\n+  <tr>\n+    <td>spark.sql.avro.compression.codec</td>\n+    <td>snappy</td>\n+    <td>Compression codec used in writing of AVRO files. Supported codecs: uncompressed, deflate, snappy, bzip2 and xz. Default codec is snappy.</td>\n+  </tr>\n+  <tr>\n+    <td>spark.sql.avro.deflate.level</td>\n+    <td>-1</td>\n+    <td>Compression level for the deflate codec used in writing of AVRO files. Valid value must be in the range of from 1 to 9 inclusive or -1. The default value is -1 which corresponds to 6 level in the current implementation.</td>\n+  </tr>\n+</table>\n+\n+## Compatibility with Databricks spark-avro\n+This Avro data source module is originally from and compatible with Databricks's open source repository \n+[spark-avro](https://github.com/databricks/spark-avro).\n+\n+By default with the SQL configuration `spark.sql.legacy.replaceDatabricksSparkAvro.enabled` enabled, the data source provider `com.databricks.spark.avro` is \n+mapped to this built-in Avro module. For the Spark tables created with `Provider` property as `com.databricks.spark.avro` in \n+catalog meta store, the mapping is essential to load these tables if you are using this built-in Avro module. \n+\n+Note in Databricks's [spark-avro](https://github.com/databricks/spark-avro), implicit classes \n+`AvroDataFrameWriter` and `AvroDataFrameReader` were created for shortcut function `.avro()`. In this \n+built-in but external module, both implicit classes are removed. Please use `.format(\"avro\")` in \n+`DataFrameWriter` or `DataFrameReader` instead, which should be clean and good enough.\n+\n+If you prefer using your own build of `spark-avro` jar file, you can simply disable the configuration \n+`spark.sql.legacy.replaceDatabricksSparkAvro.enabled`, and use the option `--jars` on deploying your \n+applications. Read the [Advanced Dependency Management](https://spark.apache\n+.org/docs/latest/submitting-applications.html#advanced-dependency-management) section in Application \n+Submission Guide for more details. \n+\n+## Supported types for Avro -> Spark SQL conversion\n+Currently Spark supports reading all [primitive types](https://avro.apache.org/docs/1.8.2/spec.html#schema_primitive) and [complex types](https://avro.apache.org/docs/1.8.2/spec.html#schema_complex) of Avro."
  }],
  "prId": 22121
}]