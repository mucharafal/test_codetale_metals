[{
  "comments": [{
    "author": {
      "login": "dilipbiswal"
    },
    "body": "Nit: written ?",
    "commit": "6c7037a75e21910e3f0e73cfb9afed66d310e187",
    "createdAt": "2019-09-03T15:24:37Z",
    "diffHunk": "@@ -363,6 +363,42 @@ number of written shuffle records, total data size, etc.\n Clicking the 'Details' link on the bottom displays the logical plans and the physical plan, which\n illustrate how Spark parses, analyzes, optimizes and performs the query.\n \n+### SQL metrics\n+\n+The metrics of SQL operators show in the block of operators. The SQL metrics can be useful when\n+we want to dive into the execution details of each operator, for example, how many rows are output\n+after a Filter operator. The related metrics are different for each type of operator, for example\n+Exchange has the metrics called \"shuffle bytes writte total\" which shows the number of bytes written"
  }],
  "prId": 25658
}, {
  "comments": [{
    "author": {
      "login": "dilipbiswal"
    },
    "body": "Nit: the time spent on writing shuffle data ?",
    "commit": "6c7037a75e21910e3f0e73cfb9afed66d310e187",
    "createdAt": "2019-09-03T15:30:52Z",
    "diffHunk": "@@ -363,6 +363,42 @@ number of written shuffle records, total data size, etc.\n Clicking the 'Details' link on the bottom displays the logical plans and the physical plan, which\n illustrate how Spark parses, analyzes, optimizes and performs the query.\n \n+### SQL metrics\n+\n+The metrics of SQL operators show in the block of operators. The SQL metrics can be useful when\n+we want to dive into the execution details of each operator, for example, how many rows are output\n+after a Filter operator. The related metrics are different for each type of operator, for example\n+Exchange has the metrics called \"shuffle bytes writte total\" which shows the number of bytes written\n+by shuffle.\n+\n+Here is the list of some SQL metrics:\n+\n+<table class=\"table\">\n+<tr><th>SQL metrics</th><th>Meaning</th><th>Operators</th></tr>\n+<tr><td> <code>number of output rows</code> </td><td> the number of output rows of the operator </td><td> Aggregate operators, Join operators, Sample, Range, Scan operators, Filter, etc.</td>></tr>\n+<tr><td> <code>data size</code> </td><td> the size of broadcasted/shuffled/collected data of the operator </td><td> BroadcastExchange, ShuffleExchange, Subquery </td></tr>\n+<tr><td> <code>time to collect</code> </td><td> the time spent to collect data </td><td> BroadcastExchange, Subquery </td></tr>\n+<tr><td> <code>scan time</code> </td><td> the time spent to scan data </td><td> ColumnarBatchScan, FileSourceScan </td></tr>\n+<tr><td> <code>metadata time</code> </td><td> the time spent on getting metadata like number of partitions, number of files </td><td> FileSourceScan </td></tr>\n+<tr><td> <code>shuffle bytes written</code> </td><td> number of bytes written </td><td> CollectLimit, TakeOrderedAndProject, ShuffleExchange </td></tr>\n+<tr><td> <code>shuffle records written</code> </td><td> number of records written </td><td> CollectLimit, TakeOrderedAndProject, ShuffleExchange </td></tr>\n+<tr><td> <code>shuffle write time</code> </td><td> the time on shuffle writing </td><td> CollectLimit, TakeOrderedAndProject, ShuffleExchange </td></tr>"
  }],
  "prId": 25658
}, {
  "comments": [{
    "author": {
      "login": "dilipbiswal"
    },
    "body": "@viirya a little confused on \"show in the block of operators\" ? Is there a way to reword this ?",
    "commit": "6c7037a75e21910e3f0e73cfb9afed66d310e187",
    "createdAt": "2019-09-03T15:33:28Z",
    "diffHunk": "@@ -363,6 +363,42 @@ number of written shuffle records, total data size, etc.\n Clicking the 'Details' link on the bottom displays the logical plans and the physical plan, which\n illustrate how Spark parses, analyzes, optimizes and performs the query.\n \n+### SQL metrics\n+\n+The metrics of SQL operators show in the block of operators. The SQL metrics can be useful when"
  }],
  "prId": 25658
}]