[{
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "If you make the proposed changes this could be simplified to just saying that you can run it in yarn-client mode.\n",
    "commit": "89889d4bb55ab6e2659c90e23b179a660a2a9204",
    "createdAt": "2014-04-15T20:10:19Z",
    "diffHunk": "@@ -63,6 +63,11 @@ All of PySpark's library dependencies, including [Py4J](http://py4j.sourceforge.\n Standalone PySpark applications should be run using the `bin/pyspark` script, which automatically configures the Java and Python environment using the settings in `conf/spark-env.sh` or `.cmd`.\n The script automatically adds the `bin/pyspark` package to the `PYTHONPATH`.\n \n+# Running PySpark on YARN\n+\n+Running PySpark on a YARN-managed cluster requires a few extra steps. The client must reference a ZIP file containing PySpark and its dependencies. To create this file, run \"make\" inside the `python/` directory in the Spark source. This will generate `pyspark-assembly.zip`  under `python/build/`. Then, set the PYSPARK_ZIP environment variable to point to the location of this file. Lastly, set MASTER=yarn-client."
  }],
  "prId": 30
}]