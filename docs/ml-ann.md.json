[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Use `MulticlassClassificationEvaluator` instead.\n",
    "commit": "07ada2bda4d1c9956611a55f9d274b3227483349",
    "createdAt": "2015-08-19T15:16:58Z",
    "diffHunk": "@@ -0,0 +1,129 @@\n+---\n+layout: global\n+title: Multilayer perceptron classifier - ML\n+displayTitle: <a href=\"ml-guide.html\">ML</a> - Multilayer perceptron classifier\n+---\n+\n+\n+`\\[\n+\\newcommand{\\R}{\\mathbb{R}}\n+\\newcommand{\\E}{\\mathbb{E}}\n+\\newcommand{\\x}{\\mathbf{x}}\n+\\newcommand{\\y}{\\mathbf{y}}\n+\\newcommand{\\wv}{\\mathbf{w}}\n+\\newcommand{\\av}{\\mathbf{\\alpha}}\n+\\newcommand{\\bv}{\\mathbf{b}}\n+\\newcommand{\\N}{\\mathbb{N}}\n+\\newcommand{\\id}{\\mathbf{I}}\n+\\newcommand{\\ind}{\\mathbf{1}}\n+\\newcommand{\\0}{\\mathbf{0}}\n+\\newcommand{\\unit}{\\mathbf{e}}\n+\\newcommand{\\one}{\\mathbf{1}}\n+\\newcommand{\\zero}{\\mathbf{0}}\n+\\]`\n+\n+\n+Multilayer perceptron classifier (MLPC) is a classifier based on the [feedforward artificial neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network). \n+MLPC consists of multiple layers of nodes. \n+Each layer is fully connected to the next layer in the network. Nodes in the input layer represent the input data. All other nodes maps inputs to the outputs \n+by performing linear combination of the inputs with the node's weights `$\\wv$` and bias `$\\bv$` and applying an activation function. \n+It can be written in matrix form for MLPC with `$K+1$` layers as follows:\n+`\\[\n+\\mathrm{y}(\\x) = \\mathrm{f_K}(...\\mathrm{f_2}(\\wv_2^T\\mathrm{f_1}(\\wv_1^T \\x+b_1)+b_2)...+b_K)\n+\\]`\n+Nodes in intermediate layers use sigmoid (logistic) function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{1}{1 + e^{-z_i}}\n+\\]`\n+Nodes in the output layer use softmax function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{e^{z_i}}{\\sum_{k=1}^N e^{z_k}}\n+\\]`\n+The number of nodes `$N$` in the output layer corresponds to the number of classes. \n+\n+MLPC employes backpropagation for learning the model. We use logistic loss function for optimization and L-BFGS as optimization routine.\n+\n+**Examples**\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+{% highlight scala %}\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics"
  }],
  "prId": 8262
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "chop down setters\n",
    "commit": "07ada2bda4d1c9956611a55f9d274b3227483349",
    "createdAt": "2015-08-19T15:17:00Z",
    "diffHunk": "@@ -0,0 +1,129 @@\n+---\n+layout: global\n+title: Multilayer perceptron classifier - ML\n+displayTitle: <a href=\"ml-guide.html\">ML</a> - Multilayer perceptron classifier\n+---\n+\n+\n+`\\[\n+\\newcommand{\\R}{\\mathbb{R}}\n+\\newcommand{\\E}{\\mathbb{E}}\n+\\newcommand{\\x}{\\mathbf{x}}\n+\\newcommand{\\y}{\\mathbf{y}}\n+\\newcommand{\\wv}{\\mathbf{w}}\n+\\newcommand{\\av}{\\mathbf{\\alpha}}\n+\\newcommand{\\bv}{\\mathbf{b}}\n+\\newcommand{\\N}{\\mathbb{N}}\n+\\newcommand{\\id}{\\mathbf{I}}\n+\\newcommand{\\ind}{\\mathbf{1}}\n+\\newcommand{\\0}{\\mathbf{0}}\n+\\newcommand{\\unit}{\\mathbf{e}}\n+\\newcommand{\\one}{\\mathbf{1}}\n+\\newcommand{\\zero}{\\mathbf{0}}\n+\\]`\n+\n+\n+Multilayer perceptron classifier (MLPC) is a classifier based on the [feedforward artificial neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network). \n+MLPC consists of multiple layers of nodes. \n+Each layer is fully connected to the next layer in the network. Nodes in the input layer represent the input data. All other nodes maps inputs to the outputs \n+by performing linear combination of the inputs with the node's weights `$\\wv$` and bias `$\\bv$` and applying an activation function. \n+It can be written in matrix form for MLPC with `$K+1$` layers as follows:\n+`\\[\n+\\mathrm{y}(\\x) = \\mathrm{f_K}(...\\mathrm{f_2}(\\wv_2^T\\mathrm{f_1}(\\wv_1^T \\x+b_1)+b_2)...+b_K)\n+\\]`\n+Nodes in intermediate layers use sigmoid (logistic) function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{1}{1 + e^{-z_i}}\n+\\]`\n+Nodes in the output layer use softmax function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{e^{z_i}}{\\sum_{k=1}^N e^{z_k}}\n+\\]`\n+The number of nodes `$N$` in the output layer corresponds to the number of classes. \n+\n+MLPC employes backpropagation for learning the model. We use logistic loss function for optimization and L-BFGS as optimization routine.\n+\n+**Examples**\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+{% highlight scala %}\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql.Row\n+\n+// Load training data\n+val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_multiclass_classification_data.txt\").toDF()\n+// Split the data into train and test\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)\n+val train = splits(0)\n+val test = splits(1)\n+// specify layers for the neural network: \n+// input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)\n+val layers = Array[Int](4, 5, 4, 3)\n+// create the trainer and set its parameters\n+val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setSeed(1234L).setMaxIter(100)"
  }],
  "prId": 8262
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Same here. Use `MulticlassClassificationEvaluator` instead.\n",
    "commit": "07ada2bda4d1c9956611a55f9d274b3227483349",
    "createdAt": "2015-08-19T15:20:39Z",
    "diffHunk": "@@ -0,0 +1,129 @@\n+---\n+layout: global\n+title: Multilayer perceptron classifier - ML\n+displayTitle: <a href=\"ml-guide.html\">ML</a> - Multilayer perceptron classifier\n+---\n+\n+\n+`\\[\n+\\newcommand{\\R}{\\mathbb{R}}\n+\\newcommand{\\E}{\\mathbb{E}}\n+\\newcommand{\\x}{\\mathbf{x}}\n+\\newcommand{\\y}{\\mathbf{y}}\n+\\newcommand{\\wv}{\\mathbf{w}}\n+\\newcommand{\\av}{\\mathbf{\\alpha}}\n+\\newcommand{\\bv}{\\mathbf{b}}\n+\\newcommand{\\N}{\\mathbb{N}}\n+\\newcommand{\\id}{\\mathbf{I}}\n+\\newcommand{\\ind}{\\mathbf{1}}\n+\\newcommand{\\0}{\\mathbf{0}}\n+\\newcommand{\\unit}{\\mathbf{e}}\n+\\newcommand{\\one}{\\mathbf{1}}\n+\\newcommand{\\zero}{\\mathbf{0}}\n+\\]`\n+\n+\n+Multilayer perceptron classifier (MLPC) is a classifier based on the [feedforward artificial neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network). \n+MLPC consists of multiple layers of nodes. \n+Each layer is fully connected to the next layer in the network. Nodes in the input layer represent the input data. All other nodes maps inputs to the outputs \n+by performing linear combination of the inputs with the node's weights `$\\wv$` and bias `$\\bv$` and applying an activation function. \n+It can be written in matrix form for MLPC with `$K+1$` layers as follows:\n+`\\[\n+\\mathrm{y}(\\x) = \\mathrm{f_K}(...\\mathrm{f_2}(\\wv_2^T\\mathrm{f_1}(\\wv_1^T \\x+b_1)+b_2)...+b_K)\n+\\]`\n+Nodes in intermediate layers use sigmoid (logistic) function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{1}{1 + e^{-z_i}}\n+\\]`\n+Nodes in the output layer use softmax function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{e^{z_i}}{\\sum_{k=1}^N e^{z_k}}\n+\\]`\n+The number of nodes `$N$` in the output layer corresponds to the number of classes. \n+\n+MLPC employes backpropagation for learning the model. We use logistic loss function for optimization and L-BFGS as optimization routine.\n+\n+**Examples**\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+{% highlight scala %}\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql.Row\n+\n+// Load training data\n+val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_multiclass_classification_data.txt\").toDF()\n+// Split the data into train and test\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)\n+val train = splits(0)\n+val test = splits(1)\n+// specify layers for the neural network: \n+// input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)\n+val layers = Array[Int](4, 5, 4, 3)\n+// create the trainer and set its parameters\n+val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setSeed(1234L).setMaxIter(100)\n+// train the model\n+val model = trainer.fit(train)\n+// compute precision on the test set\n+val result = model.transform(test)\n+val predictionAndLabels = result.select(\"prediction\", \"label\").map { case Row(p: Double, l: Double) => (p, l) }\n+val metrics = new MulticlassMetrics(predictionAndLabels)\n+println(\"Precision:\" + metrics.precision)\n+{% endhighlight %}\n+\n+</div>\n+\n+<div data-lang=\"java\" markdown=\"1\">\n+\n+{% highlight java %}\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel;\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassifier;\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics;"
  }],
  "prId": 8262
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "For Java example code, we don't usually put `main`.\n",
    "commit": "07ada2bda4d1c9956611a55f9d274b3227483349",
    "createdAt": "2015-08-19T15:20:41Z",
    "diffHunk": "@@ -0,0 +1,129 @@\n+---\n+layout: global\n+title: Multilayer perceptron classifier - ML\n+displayTitle: <a href=\"ml-guide.html\">ML</a> - Multilayer perceptron classifier\n+---\n+\n+\n+`\\[\n+\\newcommand{\\R}{\\mathbb{R}}\n+\\newcommand{\\E}{\\mathbb{E}}\n+\\newcommand{\\x}{\\mathbf{x}}\n+\\newcommand{\\y}{\\mathbf{y}}\n+\\newcommand{\\wv}{\\mathbf{w}}\n+\\newcommand{\\av}{\\mathbf{\\alpha}}\n+\\newcommand{\\bv}{\\mathbf{b}}\n+\\newcommand{\\N}{\\mathbb{N}}\n+\\newcommand{\\id}{\\mathbf{I}}\n+\\newcommand{\\ind}{\\mathbf{1}}\n+\\newcommand{\\0}{\\mathbf{0}}\n+\\newcommand{\\unit}{\\mathbf{e}}\n+\\newcommand{\\one}{\\mathbf{1}}\n+\\newcommand{\\zero}{\\mathbf{0}}\n+\\]`\n+\n+\n+Multilayer perceptron classifier (MLPC) is a classifier based on the [feedforward artificial neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network). \n+MLPC consists of multiple layers of nodes. \n+Each layer is fully connected to the next layer in the network. Nodes in the input layer represent the input data. All other nodes maps inputs to the outputs \n+by performing linear combination of the inputs with the node's weights `$\\wv$` and bias `$\\bv$` and applying an activation function. \n+It can be written in matrix form for MLPC with `$K+1$` layers as follows:\n+`\\[\n+\\mathrm{y}(\\x) = \\mathrm{f_K}(...\\mathrm{f_2}(\\wv_2^T\\mathrm{f_1}(\\wv_1^T \\x+b_1)+b_2)...+b_K)\n+\\]`\n+Nodes in intermediate layers use sigmoid (logistic) function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{1}{1 + e^{-z_i}}\n+\\]`\n+Nodes in the output layer use softmax function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{e^{z_i}}{\\sum_{k=1}^N e^{z_k}}\n+\\]`\n+The number of nodes `$N$` in the output layer corresponds to the number of classes. \n+\n+MLPC employes backpropagation for learning the model. We use logistic loss function for optimization and L-BFGS as optimization routine.\n+\n+**Examples**\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+{% highlight scala %}\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql.Row\n+\n+// Load training data\n+val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_multiclass_classification_data.txt\").toDF()\n+// Split the data into train and test\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)\n+val train = splits(0)\n+val test = splits(1)\n+// specify layers for the neural network: \n+// input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)\n+val layers = Array[Int](4, 5, 4, 3)\n+// create the trainer and set its parameters\n+val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setSeed(1234L).setMaxIter(100)\n+// train the model\n+val model = trainer.fit(train)\n+// compute precision on the test set\n+val result = model.transform(test)\n+val predictionAndLabels = result.select(\"prediction\", \"label\").map { case Row(p: Double, l: Double) => (p, l) }\n+val metrics = new MulticlassMetrics(predictionAndLabels)\n+println(\"Precision:\" + metrics.precision)\n+{% endhighlight %}\n+\n+</div>\n+\n+<div data-lang=\"java\" markdown=\"1\">\n+\n+{% highlight java %}\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel;\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassifier;\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.mllib.util.MLUtils;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.SQLContext;\n+\n+public class MultilayerPerceptronClassifierTest {\n+\n+    public static void main( String[] args )\n+    {"
  }],
  "prId": 8262
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "We can start directly from here, assuming `sc` and `sqlContext` is given.\n",
    "commit": "07ada2bda4d1c9956611a55f9d274b3227483349",
    "createdAt": "2015-08-19T15:20:42Z",
    "diffHunk": "@@ -0,0 +1,129 @@\n+---\n+layout: global\n+title: Multilayer perceptron classifier - ML\n+displayTitle: <a href=\"ml-guide.html\">ML</a> - Multilayer perceptron classifier\n+---\n+\n+\n+`\\[\n+\\newcommand{\\R}{\\mathbb{R}}\n+\\newcommand{\\E}{\\mathbb{E}}\n+\\newcommand{\\x}{\\mathbf{x}}\n+\\newcommand{\\y}{\\mathbf{y}}\n+\\newcommand{\\wv}{\\mathbf{w}}\n+\\newcommand{\\av}{\\mathbf{\\alpha}}\n+\\newcommand{\\bv}{\\mathbf{b}}\n+\\newcommand{\\N}{\\mathbb{N}}\n+\\newcommand{\\id}{\\mathbf{I}}\n+\\newcommand{\\ind}{\\mathbf{1}}\n+\\newcommand{\\0}{\\mathbf{0}}\n+\\newcommand{\\unit}{\\mathbf{e}}\n+\\newcommand{\\one}{\\mathbf{1}}\n+\\newcommand{\\zero}{\\mathbf{0}}\n+\\]`\n+\n+\n+Multilayer perceptron classifier (MLPC) is a classifier based on the [feedforward artificial neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network). \n+MLPC consists of multiple layers of nodes. \n+Each layer is fully connected to the next layer in the network. Nodes in the input layer represent the input data. All other nodes maps inputs to the outputs \n+by performing linear combination of the inputs with the node's weights `$\\wv$` and bias `$\\bv$` and applying an activation function. \n+It can be written in matrix form for MLPC with `$K+1$` layers as follows:\n+`\\[\n+\\mathrm{y}(\\x) = \\mathrm{f_K}(...\\mathrm{f_2}(\\wv_2^T\\mathrm{f_1}(\\wv_1^T \\x+b_1)+b_2)...+b_K)\n+\\]`\n+Nodes in intermediate layers use sigmoid (logistic) function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{1}{1 + e^{-z_i}}\n+\\]`\n+Nodes in the output layer use softmax function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{e^{z_i}}{\\sum_{k=1}^N e^{z_k}}\n+\\]`\n+The number of nodes `$N$` in the output layer corresponds to the number of classes. \n+\n+MLPC employes backpropagation for learning the model. We use logistic loss function for optimization and L-BFGS as optimization routine.\n+\n+**Examples**\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+{% highlight scala %}\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql.Row\n+\n+// Load training data\n+val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_multiclass_classification_data.txt\").toDF()\n+// Split the data into train and test\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)\n+val train = splits(0)\n+val test = splits(1)\n+// specify layers for the neural network: \n+// input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)\n+val layers = Array[Int](4, 5, 4, 3)\n+// create the trainer and set its parameters\n+val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setSeed(1234L).setMaxIter(100)\n+// train the model\n+val model = trainer.fit(train)\n+// compute precision on the test set\n+val result = model.transform(test)\n+val predictionAndLabels = result.select(\"prediction\", \"label\").map { case Row(p: Double, l: Double) => (p, l) }\n+val metrics = new MulticlassMetrics(predictionAndLabels)\n+println(\"Precision:\" + metrics.precision)\n+{% endhighlight %}\n+\n+</div>\n+\n+<div data-lang=\"java\" markdown=\"1\">\n+\n+{% highlight java %}\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel;\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassifier;\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.mllib.util.MLUtils;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.SQLContext;\n+\n+public class MultilayerPerceptronClassifierTest {\n+\n+    public static void main( String[] args )\n+    {\n+        SparkConf conf = new SparkConf().setAppName(\"Multilayer perceptron classifier example\").setMaster(\"local\");\n+        SparkContext sc = new SparkContext(conf);\n+        SQLContext sql = new SQLContext(sc);\n+        String path = \"data/mllib/sample_multiclass_classification_data.txt\";\n+        // Load training data"
  }],
  "prId": 8262
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`sql` -> `sqlContext`. I would suggest creating `DataFrame` first and then call `randomSplit`.\n",
    "commit": "07ada2bda4d1c9956611a55f9d274b3227483349",
    "createdAt": "2015-08-19T15:20:44Z",
    "diffHunk": "@@ -0,0 +1,129 @@\n+---\n+layout: global\n+title: Multilayer perceptron classifier - ML\n+displayTitle: <a href=\"ml-guide.html\">ML</a> - Multilayer perceptron classifier\n+---\n+\n+\n+`\\[\n+\\newcommand{\\R}{\\mathbb{R}}\n+\\newcommand{\\E}{\\mathbb{E}}\n+\\newcommand{\\x}{\\mathbf{x}}\n+\\newcommand{\\y}{\\mathbf{y}}\n+\\newcommand{\\wv}{\\mathbf{w}}\n+\\newcommand{\\av}{\\mathbf{\\alpha}}\n+\\newcommand{\\bv}{\\mathbf{b}}\n+\\newcommand{\\N}{\\mathbb{N}}\n+\\newcommand{\\id}{\\mathbf{I}}\n+\\newcommand{\\ind}{\\mathbf{1}}\n+\\newcommand{\\0}{\\mathbf{0}}\n+\\newcommand{\\unit}{\\mathbf{e}}\n+\\newcommand{\\one}{\\mathbf{1}}\n+\\newcommand{\\zero}{\\mathbf{0}}\n+\\]`\n+\n+\n+Multilayer perceptron classifier (MLPC) is a classifier based on the [feedforward artificial neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network). \n+MLPC consists of multiple layers of nodes. \n+Each layer is fully connected to the next layer in the network. Nodes in the input layer represent the input data. All other nodes maps inputs to the outputs \n+by performing linear combination of the inputs with the node's weights `$\\wv$` and bias `$\\bv$` and applying an activation function. \n+It can be written in matrix form for MLPC with `$K+1$` layers as follows:\n+`\\[\n+\\mathrm{y}(\\x) = \\mathrm{f_K}(...\\mathrm{f_2}(\\wv_2^T\\mathrm{f_1}(\\wv_1^T \\x+b_1)+b_2)...+b_K)\n+\\]`\n+Nodes in intermediate layers use sigmoid (logistic) function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{1}{1 + e^{-z_i}}\n+\\]`\n+Nodes in the output layer use softmax function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{e^{z_i}}{\\sum_{k=1}^N e^{z_k}}\n+\\]`\n+The number of nodes `$N$` in the output layer corresponds to the number of classes. \n+\n+MLPC employes backpropagation for learning the model. We use logistic loss function for optimization and L-BFGS as optimization routine.\n+\n+**Examples**\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+{% highlight scala %}\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql.Row\n+\n+// Load training data\n+val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_multiclass_classification_data.txt\").toDF()\n+// Split the data into train and test\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)\n+val train = splits(0)\n+val test = splits(1)\n+// specify layers for the neural network: \n+// input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)\n+val layers = Array[Int](4, 5, 4, 3)\n+// create the trainer and set its parameters\n+val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setSeed(1234L).setMaxIter(100)\n+// train the model\n+val model = trainer.fit(train)\n+// compute precision on the test set\n+val result = model.transform(test)\n+val predictionAndLabels = result.select(\"prediction\", \"label\").map { case Row(p: Double, l: Double) => (p, l) }\n+val metrics = new MulticlassMetrics(predictionAndLabels)\n+println(\"Precision:\" + metrics.precision)\n+{% endhighlight %}\n+\n+</div>\n+\n+<div data-lang=\"java\" markdown=\"1\">\n+\n+{% highlight java %}\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel;\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassifier;\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.mllib.util.MLUtils;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.SQLContext;\n+\n+public class MultilayerPerceptronClassifierTest {\n+\n+    public static void main( String[] args )\n+    {\n+        SparkConf conf = new SparkConf().setAppName(\"Multilayer perceptron classifier example\").setMaster(\"local\");\n+        SparkContext sc = new SparkContext(conf);\n+        SQLContext sql = new SQLContext(sc);\n+        String path = \"data/mllib/sample_multiclass_classification_data.txt\";\n+        // Load training data\n+        JavaRDD<LabeledPoint> data = MLUtils.loadLibSVMFile(sc, path).toJavaRDD();\n+        // Split the data into train and test\n+        JavaRDD<LabeledPoint>[] splits = data.randomSplit(new double[]{0.6, 0.4}, 1234L);\n+        DataFrame train = sql.createDataFrame(splits[0], LabeledPoint.class);"
  }],
  "prId": 8262
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "chain and chop down setters:\n\n``` java\nMultilayerPerceptronClassifier trainer = new MultilayerPerceptronClassifier()\n  .setLayers(layers).\n  .setBlockSize(128)\n  .setSeed(1234L)\n  .setMaxIter(100);\n```\n",
    "commit": "07ada2bda4d1c9956611a55f9d274b3227483349",
    "createdAt": "2015-08-19T15:20:46Z",
    "diffHunk": "@@ -0,0 +1,129 @@\n+---\n+layout: global\n+title: Multilayer perceptron classifier - ML\n+displayTitle: <a href=\"ml-guide.html\">ML</a> - Multilayer perceptron classifier\n+---\n+\n+\n+`\\[\n+\\newcommand{\\R}{\\mathbb{R}}\n+\\newcommand{\\E}{\\mathbb{E}}\n+\\newcommand{\\x}{\\mathbf{x}}\n+\\newcommand{\\y}{\\mathbf{y}}\n+\\newcommand{\\wv}{\\mathbf{w}}\n+\\newcommand{\\av}{\\mathbf{\\alpha}}\n+\\newcommand{\\bv}{\\mathbf{b}}\n+\\newcommand{\\N}{\\mathbb{N}}\n+\\newcommand{\\id}{\\mathbf{I}}\n+\\newcommand{\\ind}{\\mathbf{1}}\n+\\newcommand{\\0}{\\mathbf{0}}\n+\\newcommand{\\unit}{\\mathbf{e}}\n+\\newcommand{\\one}{\\mathbf{1}}\n+\\newcommand{\\zero}{\\mathbf{0}}\n+\\]`\n+\n+\n+Multilayer perceptron classifier (MLPC) is a classifier based on the [feedforward artificial neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network). \n+MLPC consists of multiple layers of nodes. \n+Each layer is fully connected to the next layer in the network. Nodes in the input layer represent the input data. All other nodes maps inputs to the outputs \n+by performing linear combination of the inputs with the node's weights `$\\wv$` and bias `$\\bv$` and applying an activation function. \n+It can be written in matrix form for MLPC with `$K+1$` layers as follows:\n+`\\[\n+\\mathrm{y}(\\x) = \\mathrm{f_K}(...\\mathrm{f_2}(\\wv_2^T\\mathrm{f_1}(\\wv_1^T \\x+b_1)+b_2)...+b_K)\n+\\]`\n+Nodes in intermediate layers use sigmoid (logistic) function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{1}{1 + e^{-z_i}}\n+\\]`\n+Nodes in the output layer use softmax function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{e^{z_i}}{\\sum_{k=1}^N e^{z_k}}\n+\\]`\n+The number of nodes `$N$` in the output layer corresponds to the number of classes. \n+\n+MLPC employes backpropagation for learning the model. We use logistic loss function for optimization and L-BFGS as optimization routine.\n+\n+**Examples**\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+{% highlight scala %}\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql.Row\n+\n+// Load training data\n+val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_multiclass_classification_data.txt\").toDF()\n+// Split the data into train and test\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)\n+val train = splits(0)\n+val test = splits(1)\n+// specify layers for the neural network: \n+// input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)\n+val layers = Array[Int](4, 5, 4, 3)\n+// create the trainer and set its parameters\n+val trainer = new MultilayerPerceptronClassifier().setLayers(layers).setBlockSize(128).setSeed(1234L).setMaxIter(100)\n+// train the model\n+val model = trainer.fit(train)\n+// compute precision on the test set\n+val result = model.transform(test)\n+val predictionAndLabels = result.select(\"prediction\", \"label\").map { case Row(p: Double, l: Double) => (p, l) }\n+val metrics = new MulticlassMetrics(predictionAndLabels)\n+println(\"Precision:\" + metrics.precision)\n+{% endhighlight %}\n+\n+</div>\n+\n+<div data-lang=\"java\" markdown=\"1\">\n+\n+{% highlight java %}\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel;\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassifier;\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.mllib.util.MLUtils;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.SQLContext;\n+\n+public class MultilayerPerceptronClassifierTest {\n+\n+    public static void main( String[] args )\n+    {\n+        SparkConf conf = new SparkConf().setAppName(\"Multilayer perceptron classifier example\").setMaster(\"local\");\n+        SparkContext sc = new SparkContext(conf);\n+        SQLContext sql = new SQLContext(sc);\n+        String path = \"data/mllib/sample_multiclass_classification_data.txt\";\n+        // Load training data\n+        JavaRDD<LabeledPoint> data = MLUtils.loadLibSVMFile(sc, path).toJavaRDD();\n+        // Split the data into train and test\n+        JavaRDD<LabeledPoint>[] splits = data.randomSplit(new double[]{0.6, 0.4}, 1234L);\n+        DataFrame train = sql.createDataFrame(splits[0], LabeledPoint.class);\n+        DataFrame test = sql.createDataFrame(splits[1], LabeledPoint.class);\n+        // specify layers for the neural network:\n+        // input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)\n+        int[] layers = new int[] {4, 5, 4, 3};\n+        // create the trainer and set its parameters\n+        MultilayerPerceptronClassifier trainer = new MultilayerPerceptronClassifier();\n+        trainer.setLayers(layers).setBlockSize(128).setSeed(1234L).setMaxIter(100);"
  }],
  "prId": 8262
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "use 2-space indentation (and please fix indentation in other places)\n",
    "commit": "07ada2bda4d1c9956611a55f9d274b3227483349",
    "createdAt": "2015-08-20T22:03:51Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+---\n+layout: global\n+title: Multilayer perceptron classifier - ML\n+displayTitle: <a href=\"ml-guide.html\">ML</a> - Multilayer perceptron classifier\n+---\n+\n+\n+`\\[\n+\\newcommand{\\R}{\\mathbb{R}}\n+\\newcommand{\\E}{\\mathbb{E}}\n+\\newcommand{\\x}{\\mathbf{x}}\n+\\newcommand{\\y}{\\mathbf{y}}\n+\\newcommand{\\wv}{\\mathbf{w}}\n+\\newcommand{\\av}{\\mathbf{\\alpha}}\n+\\newcommand{\\bv}{\\mathbf{b}}\n+\\newcommand{\\N}{\\mathbb{N}}\n+\\newcommand{\\id}{\\mathbf{I}}\n+\\newcommand{\\ind}{\\mathbf{1}}\n+\\newcommand{\\0}{\\mathbf{0}}\n+\\newcommand{\\unit}{\\mathbf{e}}\n+\\newcommand{\\one}{\\mathbf{1}}\n+\\newcommand{\\zero}{\\mathbf{0}}\n+\\]`\n+\n+\n+Multilayer perceptron classifier (MLPC) is a classifier based on the [feedforward artificial neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network). \n+MLPC consists of multiple layers of nodes. \n+Each layer is fully connected to the next layer in the network. Nodes in the input layer represent the input data. All other nodes maps inputs to the outputs \n+by performing linear combination of the inputs with the node's weights `$\\wv$` and bias `$\\bv$` and applying an activation function. \n+It can be written in matrix form for MLPC with `$K+1$` layers as follows:\n+`\\[\n+\\mathrm{y}(\\x) = \\mathrm{f_K}(...\\mathrm{f_2}(\\wv_2^T\\mathrm{f_1}(\\wv_1^T \\x+b_1)+b_2)...+b_K)\n+\\]`\n+Nodes in intermediate layers use sigmoid (logistic) function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{1}{1 + e^{-z_i}}\n+\\]`\n+Nodes in the output layer use softmax function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{e^{z_i}}{\\sum_{k=1}^N e^{z_k}}\n+\\]`\n+The number of nodes `$N$` in the output layer corresponds to the number of classes. \n+\n+MLPC employes backpropagation for learning the model. We use logistic loss function for optimization and L-BFGS as optimization routine.\n+\n+**Examples**\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+{% highlight scala %}\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n+import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql.Row\n+\n+// Load training data\n+val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_multiclass_classification_data.txt\").toDF()\n+// Split the data into train and test\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)\n+val train = splits(0)\n+val test = splits(1)\n+// specify layers for the neural network: \n+// input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)\n+val layers = Array[Int](4, 5, 4, 3)\n+// create the trainer and set its parameters\n+val trainer = new MultilayerPerceptronClassifier()\n+\t.setLayers(layers)"
  }],
  "prId": 8262
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "#8342 was merged. So we can remove `trainier.`\n",
    "commit": "07ada2bda4d1c9956611a55f9d274b3227483349",
    "createdAt": "2015-08-20T22:04:45Z",
    "diffHunk": "@@ -0,0 +1,123 @@\n+---\n+layout: global\n+title: Multilayer perceptron classifier - ML\n+displayTitle: <a href=\"ml-guide.html\">ML</a> - Multilayer perceptron classifier\n+---\n+\n+\n+`\\[\n+\\newcommand{\\R}{\\mathbb{R}}\n+\\newcommand{\\E}{\\mathbb{E}}\n+\\newcommand{\\x}{\\mathbf{x}}\n+\\newcommand{\\y}{\\mathbf{y}}\n+\\newcommand{\\wv}{\\mathbf{w}}\n+\\newcommand{\\av}{\\mathbf{\\alpha}}\n+\\newcommand{\\bv}{\\mathbf{b}}\n+\\newcommand{\\N}{\\mathbb{N}}\n+\\newcommand{\\id}{\\mathbf{I}}\n+\\newcommand{\\ind}{\\mathbf{1}}\n+\\newcommand{\\0}{\\mathbf{0}}\n+\\newcommand{\\unit}{\\mathbf{e}}\n+\\newcommand{\\one}{\\mathbf{1}}\n+\\newcommand{\\zero}{\\mathbf{0}}\n+\\]`\n+\n+\n+Multilayer perceptron classifier (MLPC) is a classifier based on the [feedforward artificial neural network](https://en.wikipedia.org/wiki/Feedforward_neural_network). \n+MLPC consists of multiple layers of nodes. \n+Each layer is fully connected to the next layer in the network. Nodes in the input layer represent the input data. All other nodes maps inputs to the outputs \n+by performing linear combination of the inputs with the node's weights `$\\wv$` and bias `$\\bv$` and applying an activation function. \n+It can be written in matrix form for MLPC with `$K+1$` layers as follows:\n+`\\[\n+\\mathrm{y}(\\x) = \\mathrm{f_K}(...\\mathrm{f_2}(\\wv_2^T\\mathrm{f_1}(\\wv_1^T \\x+b_1)+b_2)...+b_K)\n+\\]`\n+Nodes in intermediate layers use sigmoid (logistic) function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{1}{1 + e^{-z_i}}\n+\\]`\n+Nodes in the output layer use softmax function:\n+`\\[\n+\\mathrm{f}(z_i) = \\frac{e^{z_i}}{\\sum_{k=1}^N e^{z_k}}\n+\\]`\n+The number of nodes `$N$` in the output layer corresponds to the number of classes. \n+\n+MLPC employes backpropagation for learning the model. We use logistic loss function for optimization and L-BFGS as optimization routine.\n+\n+**Examples**\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+{% highlight scala %}\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n+import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.sql.Row\n+\n+// Load training data\n+val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_multiclass_classification_data.txt\").toDF()\n+// Split the data into train and test\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 1234L)\n+val train = splits(0)\n+val test = splits(1)\n+// specify layers for the neural network: \n+// input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)\n+val layers = Array[Int](4, 5, 4, 3)\n+// create the trainer and set its parameters\n+val trainer = new MultilayerPerceptronClassifier()\n+\t.setLayers(layers)\n+\t.setBlockSize(128)\n+\t.setSeed(1234L)\n+\t.setMaxIter(100)\n+// train the model\n+val model = trainer.fit(train)\n+// compute precision on the test set\n+val result = model.transform(test)\n+val predictionAndLabels = result.select(\"prediction\", \"label\")\n+val evaluator = new MulticlassClassificationEvaluator()\n+\t.setMetricName(\"precision\")\n+println(\"Precision:\" + evaluator.evaluate(predictionAndLabels))\n+{% endhighlight %}\n+\n+</div>\n+\n+<div data-lang=\"java\" markdown=\"1\">\n+\n+{% highlight java %}\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel;\n+import org.apache.spark.ml.classification.MultilayerPerceptronClassifier;\n+import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.mllib.util.MLUtils;\n+\n+// Load training data\n+String path = \"data/mllib/sample_multiclass_classification_data.txt\";\n+JavaRDD<LabeledPoint> data = MLUtils.loadLibSVMFile(sc, path).toJavaRDD();\n+DataFrame dataFrame = sqlContext.createDataFrame(data, LabeledPoint.class);\n+// Split the data into train and test\n+DataFrame[] splits = dataFrame.randomSplit(new double[]{0.6, 0.4}, 1234L);\n+DataFrame train = splits[0];\n+DataFrame test = splits[1];\n+// specify layers for the neural network:\n+// input layer of size 4 (features), two intermediate of size 5 and 4 and output of size 3 (classes)\n+int[] layers = new int[] {4, 5, 4, 3};\n+// create the trainer and set its parameters\n+MultilayerPerceptronClassifier trainer = new MultilayerPerceptronClassifier();\n+trainer.setLayers(layers)"
  }],
  "prId": 8262
}]