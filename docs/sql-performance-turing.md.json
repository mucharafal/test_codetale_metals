[{
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "nit: `It is better to over estimated` -> ` It is better to over-estimate`?",
    "commit": "17995f92bf4f7b6831f129b558669346a5eafedf",
    "createdAt": "2018-10-18T10:22:10Z",
    "diffHunk": "@@ -0,0 +1,151 @@\n+---\n+layout: global\n+title: Performance Tuning\n+displayTitle: Performance Tuning\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+For some workloads, it is possible to improve performance by either caching data in memory, or by\n+turning on some experimental options.\n+\n+## Caching Data In Memory\n+\n+Spark SQL can cache tables using an in-memory columnar format by calling `spark.catalog.cacheTable(\"tableName\")` or `dataFrame.cache()`.\n+Then Spark SQL will scan only required columns and will automatically tune compression to minimize\n+memory usage and GC pressure. You can call `spark.catalog.uncacheTable(\"tableName\")` to remove the table from memory.\n+\n+Configuration of in-memory caching can be done using the `setConf` method on `SparkSession` or by running\n+`SET key=value` commands using SQL.\n+\n+<table class=\"table\">\n+<tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr>\n+<tr>\n+  <td><code>spark.sql.inMemoryColumnarStorage.compressed</code></td>\n+  <td>true</td>\n+  <td>\n+    When set to true Spark SQL will automatically select a compression codec for each column based\n+    on statistics of the data.\n+  </td>\n+</tr>\n+<tr>\n+  <td><code>spark.sql.inMemoryColumnarStorage.batchSize</code></td>\n+  <td>10000</td>\n+  <td>\n+    Controls the size of batches for columnar caching. Larger batch sizes can improve memory utilization\n+    and compression, but risk OOMs when caching data.\n+  </td>\n+</tr>\n+\n+</table>\n+\n+## Other Configuration Options\n+\n+The following options can also be used to tune the performance of query execution. It is possible\n+that these options will be deprecated in future release as more optimizations are performed automatically.\n+\n+<table class=\"table\">\n+  <tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr>\n+  <tr>\n+    <td><code>spark.sql.files.maxPartitionBytes</code></td>\n+    <td>134217728 (128 MB)</td>\n+    <td>\n+      The maximum number of bytes to pack into a single partition when reading files.\n+    </td>\n+  </tr>\n+  <tr>\n+    <td><code>spark.sql.files.openCostInBytes</code></td>\n+    <td>4194304 (4 MB)</td>\n+    <td>\n+      The estimated cost to open a file, measured by the number of bytes could be scanned in the same\n+      time. This is used when putting multiple files into a partition. It is better to over estimated,"
  }],
  "prId": 22746
}]