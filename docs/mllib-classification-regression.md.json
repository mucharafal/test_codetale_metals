[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "FYI, the decision tree guide is now in `mllib-decision-tree.md`.\n",
    "commit": "968ca9df9b86c1dd60876c00fb3c48b758ffc34b",
    "createdAt": "2014-04-29T07:57:07Z",
    "diffHunk": "@@ -294,12 +294,9 @@ The recursive tree construction is stopped at a node when one of the two conditi\n 1. The node depth is equal to the `maxDepth` training paramemter\n 2. No split candidate leads to an information gain at the node.\n \n-### Practical Limitations\n-\n-The tree implementation stores an Array[Double] of size *O(#features \\* #splits \\* 2^maxDepth)* in memory for aggregating histograms over partitions. The current implementation might not scale to very deep trees since the memory requirement grows exponentially with tree depth. \n-\n-Please drop us a line if you encounter any issues. We are planning to solve this problem in the near future and real-world examples will be great.\n+### Implementation Details"
  }, {
    "author": {
      "login": "manishamde"
    },
    "body": "Thanks.\n",
    "commit": "968ca9df9b86c1dd60876c00fb3c48b758ffc34b",
    "createdAt": "2014-04-29T18:29:54Z",
    "diffHunk": "@@ -294,12 +294,9 @@ The recursive tree construction is stopped at a node when one of the two conditi\n 1. The node depth is equal to the `maxDepth` training paramemter\n 2. No split candidate leads to an information gain at the node.\n \n-### Practical Limitations\n-\n-The tree implementation stores an Array[Double] of size *O(#features \\* #splits \\* 2^maxDepth)* in memory for aggregating histograms over partitions. The current implementation might not scale to very deep trees since the memory requirement grows exponentially with tree depth. \n-\n-Please drop us a line if you encounter any issues. We are planning to solve this problem in the near future and real-world examples will be great.\n+### Implementation Details"
  }],
  "prId": 475
}]