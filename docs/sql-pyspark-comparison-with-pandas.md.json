[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "sintance -> instance",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:03:52Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.\n+\n+    It causes some subtleties comparing to Pandas, for instance, about `NaN`, `None` and `NULL`.\n+\n+There are similarities and differences between them which might bring confusion. In this document\n+these are described and illuminated by several examples.\n+\n+\n+\n+## DataFrame APIs\n+\n+This chapter describes DataFrame APIs in both PySpark and Pandas.\n+\n+\n+### Quick References\n+\n+| PySpark                                                            | Pandas                                   |\n+| ------------------------------------------------------------------ | ---------------------------------------- |\n+| `df.limit(3)`                                                      | `df.head(3)`                             |\n+| `df.filter(\"a == 1 AND b == 2\")`                                   | `df.filter(\"(df.a == 1) & (df.b == 2)\")` |\n+| `df.filter((df.a == 1) & (df.b == 2))`                             | `df[(df.a == 1) & (df.b == 2)]`          |\n+| `df.select(\"a\", \"b\")`                                              | `df[[\"a\", \"b\"]]`                         |\n+| `df.drop_duplicates()`                                             | `df.drop_duplicates()`                   |\n+| `df.sample(fraction=0.01)`                                         | `df.sample(frac=0.01)`                   |\n+| `df.groupby(\"a\").count()`                                          | `df.groupby(\"a\").size()`                 |\n+| `df.groupby(\"a\").agg({\"b\": \"sum\"})`                                | `df.groupby(\"a\").agg({\"b\": np.sum})`     |\n+| `df1.join(df2, on=\"a\")`                                            | `pandas.merge(df1, df2, on=\"a\")`         |\n+| `df1.union(df2)`                                                   | `pandas.concat(df1, df2)`                |\n+| `df = df.select(when(df[\"a\"] < 5, df[\"a\"] * 2).otherwise(df[\"a\"]))`| `df.loc[pdf['a'] < 5, 'a'] *= 2`         |\n+\n+\n+### Create DataFrame\n+\n+In order to create DataFrame in PySpark and Pandas, you can run the codes below: \n+\n+```python\n+# PySpark\n+data = zip(['Chicago', 'San Francisco', 'New York City'], range(1, 4))\n+spark.createDataFrame(list(data), [\"city\", \"rank\"])\n+```\n+\n+```python\n+# Pandas\n+data = {'city': ['Chicago', 'San Francisco', 'New York City'], 'rank': range(1, 4)}\n+pandas.DataFrame(data)\n+```\n+\n+One notable difference when creating DataFrame is that Pandas accepts the data as below:\n+\n+```\n+data = {\n+    'city': ['Chicago', 'San Francisco', 'New York City'],\n+    'rank': range(1, 4)\n+}\n+```\n+\n+and it interprets as:\n+\n+```\n+            city  rank\n+0        Chicago     1\n+1  San Francisco     2\n+2  New York City     3\n+```\n+\n+So, a dictionary that contains key and multiple values becomes DataFrame but PySpark does\n+support this. Instead, PySpark can make DataFrame from Pandas DataFrame as below:\n+\n+```\n+createDataFrame(pandas.DataFrame(data))\n+```\n+\n+For more information see \"Interaction between PySpark and Pandas\" below.\n+\n+\n+### Load DataFrame\n+\n+To load DataFrame, there are APIs, usually, `spark.read.xxx` (or `spark.read.format(\"xxx\")`)\n+for PySpark and `pandas.read_xxx` for Pandas.\n+\n+```python\n+# PySpark\n+df = spark.read.csv(\"data.csv\")\n+```\n+\n+```python\n+# Pandas\n+df = pandas.read_csv(\"data.csv\")\n+```\n+\n+There are many sources available in both PySpark and Pandas. For example, CSV, JSON and\n+Parquet are commonly supported but each supports different set of sources. There are pretty\n+different set of options availabe as well, please see the API documentation for both sides.\n+Note that, to match behaviours for PySpark to Pandas, `header=True` and `inferSchema=True`\n+options are required.\n+\n+\n+### Save DataFrame\n+\n+To load DataFrame, likewise, the APIs are `spark.read.xxx` (or `spark.read.format(\"xxx\")`) for\n+PySpark and `pandas_to_xxx` for Pandas.\n+\n+```\n+df.to_csv(\"data.csv\")\n+```\n+\n+```\n+df.write.csv(\"data.set\")\n+```\n+\n+Likewise, different set of sources are supported. Note that, to match behaviours for PySpark\n+to Pandas, `header=True` option is required.\n+\n+\n+### Inspect DataFrame\n+\n+DataFrame in both PySpark and Pandas can be expected in many ways. One of the way is to\n+`describe()` as below:\n+\n+```\n+# PySpark\n+df.describe().show()\n+```\n+\n+```\n++-------+-------------+----+\n+|summary|         city|rank|\n++-------+-------------+----+\n+|  count|            4|   4|\n+|   mean|         null| NaN|\n+| stddev|         null| NaN|\n+|    min|      Chicago| 1.0|\n+|    max|San Francisco| NaN|\n++-------+-------------+----+\n+```\n+\n+In Pandas, the same name function `describe()` can be called:\n+\n+```\n+# Pandas\n+df.describe()\n+```\n+\n+```\n+       rank\n+count   3.0\n+mean    2.0\n+std     1.0\n+min     1.0\n+25%     1.5\n+50%     2.0\n+75%     2.5\n+max     3.0\n+```\n+\n+There are some differences, for instance, in case of PySpark, `NaN` is excluded but Pandas ignore `NaN` in some statistics. \n+\n+To inspect columns, there are `dtype` API in PySpark and Pandas.\n+\n+```\n+# PySpark\n+df.dtypes\n+```\n+\n+```\n+[('city', 'string'), ('rank', 'double')]\n+```\n+\n+```\n+# Pandas\n+df.dtypes\n+```\n+\n+```\n+0     object\n+1    float64\n+dtype: object\n+```\n+\n+PySpark types are represented as Spark SQL types whereas they are NumPy types in case of Pandas.\n+\n+Note that, PySpark supports pretty printing of schema as below:\n+\n+```\n+# PySpark\n+df.printSchema()\n+```\n+\n+```\n+root\n+ |-- city: string (nullable = true)\n+ |-- rank: double (nullable = true)\n+```\n+\n+\n+### Interaction between PySpark and Pandas\n+\n+PySpark supports conversion to/from Pandas and Pandas UDFs out of the box.\n+See [PySpark Usage Guide for Pandas with Apache Arrow](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html).\n+\n+\n+\n+## Notable Differences\n+\n+There are other notable differences for many reasons, for instance,\n+PySpark DataFrame's core is Spark SQL so some of APIs comply SQL, and this might be different\n+comparing to Pandas context which somtimes causes some subtleties. See the differences below.\n+\n+\n+### Lazy and Eager Evaluation\n+\n+PySpark DataFrame is lazy whereas Pandas DataFrame is eager. Therefore,\n+PySpark DataFrame can optimize the computation before an actual execution whereas Pandas\n+DataFrame can have each direct result immediately after each execution.\n+\n+For sintance, suppose you select columns multiple times as below:",
    "line": 268
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "comply SQL -> are SQL compliant? or are you trying to say they're mimicing SQL-like operations, not pandas",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:04:41Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.\n+\n+    It causes some subtleties comparing to Pandas, for instance, about `NaN`, `None` and `NULL`.\n+\n+There are similarities and differences between them which might bring confusion. In this document\n+these are described and illuminated by several examples.\n+\n+\n+\n+## DataFrame APIs\n+\n+This chapter describes DataFrame APIs in both PySpark and Pandas.\n+\n+\n+### Quick References\n+\n+| PySpark                                                            | Pandas                                   |\n+| ------------------------------------------------------------------ | ---------------------------------------- |\n+| `df.limit(3)`                                                      | `df.head(3)`                             |\n+| `df.filter(\"a == 1 AND b == 2\")`                                   | `df.filter(\"(df.a == 1) & (df.b == 2)\")` |\n+| `df.filter((df.a == 1) & (df.b == 2))`                             | `df[(df.a == 1) & (df.b == 2)]`          |\n+| `df.select(\"a\", \"b\")`                                              | `df[[\"a\", \"b\"]]`                         |\n+| `df.drop_duplicates()`                                             | `df.drop_duplicates()`                   |\n+| `df.sample(fraction=0.01)`                                         | `df.sample(frac=0.01)`                   |\n+| `df.groupby(\"a\").count()`                                          | `df.groupby(\"a\").size()`                 |\n+| `df.groupby(\"a\").agg({\"b\": \"sum\"})`                                | `df.groupby(\"a\").agg({\"b\": np.sum})`     |\n+| `df1.join(df2, on=\"a\")`                                            | `pandas.merge(df1, df2, on=\"a\")`         |\n+| `df1.union(df2)`                                                   | `pandas.concat(df1, df2)`                |\n+| `df = df.select(when(df[\"a\"] < 5, df[\"a\"] * 2).otherwise(df[\"a\"]))`| `df.loc[pdf['a'] < 5, 'a'] *= 2`         |\n+\n+\n+### Create DataFrame\n+\n+In order to create DataFrame in PySpark and Pandas, you can run the codes below: \n+\n+```python\n+# PySpark\n+data = zip(['Chicago', 'San Francisco', 'New York City'], range(1, 4))\n+spark.createDataFrame(list(data), [\"city\", \"rank\"])\n+```\n+\n+```python\n+# Pandas\n+data = {'city': ['Chicago', 'San Francisco', 'New York City'], 'rank': range(1, 4)}\n+pandas.DataFrame(data)\n+```\n+\n+One notable difference when creating DataFrame is that Pandas accepts the data as below:\n+\n+```\n+data = {\n+    'city': ['Chicago', 'San Francisco', 'New York City'],\n+    'rank': range(1, 4)\n+}\n+```\n+\n+and it interprets as:\n+\n+```\n+            city  rank\n+0        Chicago     1\n+1  San Francisco     2\n+2  New York City     3\n+```\n+\n+So, a dictionary that contains key and multiple values becomes DataFrame but PySpark does\n+support this. Instead, PySpark can make DataFrame from Pandas DataFrame as below:\n+\n+```\n+createDataFrame(pandas.DataFrame(data))\n+```\n+\n+For more information see \"Interaction between PySpark and Pandas\" below.\n+\n+\n+### Load DataFrame\n+\n+To load DataFrame, there are APIs, usually, `spark.read.xxx` (or `spark.read.format(\"xxx\")`)\n+for PySpark and `pandas.read_xxx` for Pandas.\n+\n+```python\n+# PySpark\n+df = spark.read.csv(\"data.csv\")\n+```\n+\n+```python\n+# Pandas\n+df = pandas.read_csv(\"data.csv\")\n+```\n+\n+There are many sources available in both PySpark and Pandas. For example, CSV, JSON and\n+Parquet are commonly supported but each supports different set of sources. There are pretty\n+different set of options availabe as well, please see the API documentation for both sides.\n+Note that, to match behaviours for PySpark to Pandas, `header=True` and `inferSchema=True`\n+options are required.\n+\n+\n+### Save DataFrame\n+\n+To load DataFrame, likewise, the APIs are `spark.read.xxx` (or `spark.read.format(\"xxx\")`) for\n+PySpark and `pandas_to_xxx` for Pandas.\n+\n+```\n+df.to_csv(\"data.csv\")\n+```\n+\n+```\n+df.write.csv(\"data.set\")\n+```\n+\n+Likewise, different set of sources are supported. Note that, to match behaviours for PySpark\n+to Pandas, `header=True` option is required.\n+\n+\n+### Inspect DataFrame\n+\n+DataFrame in both PySpark and Pandas can be expected in many ways. One of the way is to\n+`describe()` as below:\n+\n+```\n+# PySpark\n+df.describe().show()\n+```\n+\n+```\n++-------+-------------+----+\n+|summary|         city|rank|\n++-------+-------------+----+\n+|  count|            4|   4|\n+|   mean|         null| NaN|\n+| stddev|         null| NaN|\n+|    min|      Chicago| 1.0|\n+|    max|San Francisco| NaN|\n++-------+-------------+----+\n+```\n+\n+In Pandas, the same name function `describe()` can be called:\n+\n+```\n+# Pandas\n+df.describe()\n+```\n+\n+```\n+       rank\n+count   3.0\n+mean    2.0\n+std     1.0\n+min     1.0\n+25%     1.5\n+50%     2.0\n+75%     2.5\n+max     3.0\n+```\n+\n+There are some differences, for instance, in case of PySpark, `NaN` is excluded but Pandas ignore `NaN` in some statistics. \n+\n+To inspect columns, there are `dtype` API in PySpark and Pandas.\n+\n+```\n+# PySpark\n+df.dtypes\n+```\n+\n+```\n+[('city', 'string'), ('rank', 'double')]\n+```\n+\n+```\n+# Pandas\n+df.dtypes\n+```\n+\n+```\n+0     object\n+1    float64\n+dtype: object\n+```\n+\n+PySpark types are represented as Spark SQL types whereas they are NumPy types in case of Pandas.\n+\n+Note that, PySpark supports pretty printing of schema as below:\n+\n+```\n+# PySpark\n+df.printSchema()\n+```\n+\n+```\n+root\n+ |-- city: string (nullable = true)\n+ |-- rank: double (nullable = true)\n+```\n+\n+\n+### Interaction between PySpark and Pandas\n+\n+PySpark supports conversion to/from Pandas and Pandas UDFs out of the box.\n+See [PySpark Usage Guide for Pandas with Apache Arrow](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html).\n+\n+\n+\n+## Notable Differences\n+\n+There are other notable differences for many reasons, for instance,\n+PySpark DataFrame's core is Spark SQL so some of APIs comply SQL, and this might be different",
    "line": 258
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "comparing to Pandas -> compared to the Pandas\r\nsomtimes -> sometimes\r\ncauses some subtleties -> causes subtle differences?",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:05:20Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.\n+\n+    It causes some subtleties comparing to Pandas, for instance, about `NaN`, `None` and `NULL`.\n+\n+There are similarities and differences between them which might bring confusion. In this document\n+these are described and illuminated by several examples.\n+\n+\n+\n+## DataFrame APIs\n+\n+This chapter describes DataFrame APIs in both PySpark and Pandas.\n+\n+\n+### Quick References\n+\n+| PySpark                                                            | Pandas                                   |\n+| ------------------------------------------------------------------ | ---------------------------------------- |\n+| `df.limit(3)`                                                      | `df.head(3)`                             |\n+| `df.filter(\"a == 1 AND b == 2\")`                                   | `df.filter(\"(df.a == 1) & (df.b == 2)\")` |\n+| `df.filter((df.a == 1) & (df.b == 2))`                             | `df[(df.a == 1) & (df.b == 2)]`          |\n+| `df.select(\"a\", \"b\")`                                              | `df[[\"a\", \"b\"]]`                         |\n+| `df.drop_duplicates()`                                             | `df.drop_duplicates()`                   |\n+| `df.sample(fraction=0.01)`                                         | `df.sample(frac=0.01)`                   |\n+| `df.groupby(\"a\").count()`                                          | `df.groupby(\"a\").size()`                 |\n+| `df.groupby(\"a\").agg({\"b\": \"sum\"})`                                | `df.groupby(\"a\").agg({\"b\": np.sum})`     |\n+| `df1.join(df2, on=\"a\")`                                            | `pandas.merge(df1, df2, on=\"a\")`         |\n+| `df1.union(df2)`                                                   | `pandas.concat(df1, df2)`                |\n+| `df = df.select(when(df[\"a\"] < 5, df[\"a\"] * 2).otherwise(df[\"a\"]))`| `df.loc[pdf['a'] < 5, 'a'] *= 2`         |\n+\n+\n+### Create DataFrame\n+\n+In order to create DataFrame in PySpark and Pandas, you can run the codes below: \n+\n+```python\n+# PySpark\n+data = zip(['Chicago', 'San Francisco', 'New York City'], range(1, 4))\n+spark.createDataFrame(list(data), [\"city\", \"rank\"])\n+```\n+\n+```python\n+# Pandas\n+data = {'city': ['Chicago', 'San Francisco', 'New York City'], 'rank': range(1, 4)}\n+pandas.DataFrame(data)\n+```\n+\n+One notable difference when creating DataFrame is that Pandas accepts the data as below:\n+\n+```\n+data = {\n+    'city': ['Chicago', 'San Francisco', 'New York City'],\n+    'rank': range(1, 4)\n+}\n+```\n+\n+and it interprets as:\n+\n+```\n+            city  rank\n+0        Chicago     1\n+1  San Francisco     2\n+2  New York City     3\n+```\n+\n+So, a dictionary that contains key and multiple values becomes DataFrame but PySpark does\n+support this. Instead, PySpark can make DataFrame from Pandas DataFrame as below:\n+\n+```\n+createDataFrame(pandas.DataFrame(data))\n+```\n+\n+For more information see \"Interaction between PySpark and Pandas\" below.\n+\n+\n+### Load DataFrame\n+\n+To load DataFrame, there are APIs, usually, `spark.read.xxx` (or `spark.read.format(\"xxx\")`)\n+for PySpark and `pandas.read_xxx` for Pandas.\n+\n+```python\n+# PySpark\n+df = spark.read.csv(\"data.csv\")\n+```\n+\n+```python\n+# Pandas\n+df = pandas.read_csv(\"data.csv\")\n+```\n+\n+There are many sources available in both PySpark and Pandas. For example, CSV, JSON and\n+Parquet are commonly supported but each supports different set of sources. There are pretty\n+different set of options availabe as well, please see the API documentation for both sides.\n+Note that, to match behaviours for PySpark to Pandas, `header=True` and `inferSchema=True`\n+options are required.\n+\n+\n+### Save DataFrame\n+\n+To load DataFrame, likewise, the APIs are `spark.read.xxx` (or `spark.read.format(\"xxx\")`) for\n+PySpark and `pandas_to_xxx` for Pandas.\n+\n+```\n+df.to_csv(\"data.csv\")\n+```\n+\n+```\n+df.write.csv(\"data.set\")\n+```\n+\n+Likewise, different set of sources are supported. Note that, to match behaviours for PySpark\n+to Pandas, `header=True` option is required.\n+\n+\n+### Inspect DataFrame\n+\n+DataFrame in both PySpark and Pandas can be expected in many ways. One of the way is to\n+`describe()` as below:\n+\n+```\n+# PySpark\n+df.describe().show()\n+```\n+\n+```\n++-------+-------------+----+\n+|summary|         city|rank|\n++-------+-------------+----+\n+|  count|            4|   4|\n+|   mean|         null| NaN|\n+| stddev|         null| NaN|\n+|    min|      Chicago| 1.0|\n+|    max|San Francisco| NaN|\n++-------+-------------+----+\n+```\n+\n+In Pandas, the same name function `describe()` can be called:\n+\n+```\n+# Pandas\n+df.describe()\n+```\n+\n+```\n+       rank\n+count   3.0\n+mean    2.0\n+std     1.0\n+min     1.0\n+25%     1.5\n+50%     2.0\n+75%     2.5\n+max     3.0\n+```\n+\n+There are some differences, for instance, in case of PySpark, `NaN` is excluded but Pandas ignore `NaN` in some statistics. \n+\n+To inspect columns, there are `dtype` API in PySpark and Pandas.\n+\n+```\n+# PySpark\n+df.dtypes\n+```\n+\n+```\n+[('city', 'string'), ('rank', 'double')]\n+```\n+\n+```\n+# Pandas\n+df.dtypes\n+```\n+\n+```\n+0     object\n+1    float64\n+dtype: object\n+```\n+\n+PySpark types are represented as Spark SQL types whereas they are NumPy types in case of Pandas.\n+\n+Note that, PySpark supports pretty printing of schema as below:\n+\n+```\n+# PySpark\n+df.printSchema()\n+```\n+\n+```\n+root\n+ |-- city: string (nullable = true)\n+ |-- rank: double (nullable = true)\n+```\n+\n+\n+### Interaction between PySpark and Pandas\n+\n+PySpark supports conversion to/from Pandas and Pandas UDFs out of the box.\n+See [PySpark Usage Guide for Pandas with Apache Arrow](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html).\n+\n+\n+\n+## Notable Differences\n+\n+There are other notable differences for many reasons, for instance,\n+PySpark DataFrame's core is Spark SQL so some of APIs comply SQL, and this might be different\n+comparing to Pandas context which somtimes causes some subtleties. See the differences below.",
    "line": 259
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "there are -> there is a ?",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:05:49Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.\n+\n+    It causes some subtleties comparing to Pandas, for instance, about `NaN`, `None` and `NULL`.\n+\n+There are similarities and differences between them which might bring confusion. In this document\n+these are described and illuminated by several examples.\n+\n+\n+\n+## DataFrame APIs\n+\n+This chapter describes DataFrame APIs in both PySpark and Pandas.\n+\n+\n+### Quick References\n+\n+| PySpark                                                            | Pandas                                   |\n+| ------------------------------------------------------------------ | ---------------------------------------- |\n+| `df.limit(3)`                                                      | `df.head(3)`                             |\n+| `df.filter(\"a == 1 AND b == 2\")`                                   | `df.filter(\"(df.a == 1) & (df.b == 2)\")` |\n+| `df.filter((df.a == 1) & (df.b == 2))`                             | `df[(df.a == 1) & (df.b == 2)]`          |\n+| `df.select(\"a\", \"b\")`                                              | `df[[\"a\", \"b\"]]`                         |\n+| `df.drop_duplicates()`                                             | `df.drop_duplicates()`                   |\n+| `df.sample(fraction=0.01)`                                         | `df.sample(frac=0.01)`                   |\n+| `df.groupby(\"a\").count()`                                          | `df.groupby(\"a\").size()`                 |\n+| `df.groupby(\"a\").agg({\"b\": \"sum\"})`                                | `df.groupby(\"a\").agg({\"b\": np.sum})`     |\n+| `df1.join(df2, on=\"a\")`                                            | `pandas.merge(df1, df2, on=\"a\")`         |\n+| `df1.union(df2)`                                                   | `pandas.concat(df1, df2)`                |\n+| `df = df.select(when(df[\"a\"] < 5, df[\"a\"] * 2).otherwise(df[\"a\"]))`| `df.loc[pdf['a'] < 5, 'a'] *= 2`         |\n+\n+\n+### Create DataFrame\n+\n+In order to create DataFrame in PySpark and Pandas, you can run the codes below: \n+\n+```python\n+# PySpark\n+data = zip(['Chicago', 'San Francisco', 'New York City'], range(1, 4))\n+spark.createDataFrame(list(data), [\"city\", \"rank\"])\n+```\n+\n+```python\n+# Pandas\n+data = {'city': ['Chicago', 'San Francisco', 'New York City'], 'rank': range(1, 4)}\n+pandas.DataFrame(data)\n+```\n+\n+One notable difference when creating DataFrame is that Pandas accepts the data as below:\n+\n+```\n+data = {\n+    'city': ['Chicago', 'San Francisco', 'New York City'],\n+    'rank': range(1, 4)\n+}\n+```\n+\n+and it interprets as:\n+\n+```\n+            city  rank\n+0        Chicago     1\n+1  San Francisco     2\n+2  New York City     3\n+```\n+\n+So, a dictionary that contains key and multiple values becomes DataFrame but PySpark does\n+support this. Instead, PySpark can make DataFrame from Pandas DataFrame as below:\n+\n+```\n+createDataFrame(pandas.DataFrame(data))\n+```\n+\n+For more information see \"Interaction between PySpark and Pandas\" below.\n+\n+\n+### Load DataFrame\n+\n+To load DataFrame, there are APIs, usually, `spark.read.xxx` (or `spark.read.format(\"xxx\")`)\n+for PySpark and `pandas.read_xxx` for Pandas.\n+\n+```python\n+# PySpark\n+df = spark.read.csv(\"data.csv\")\n+```\n+\n+```python\n+# Pandas\n+df = pandas.read_csv(\"data.csv\")\n+```\n+\n+There are many sources available in both PySpark and Pandas. For example, CSV, JSON and\n+Parquet are commonly supported but each supports different set of sources. There are pretty\n+different set of options availabe as well, please see the API documentation for both sides.\n+Note that, to match behaviours for PySpark to Pandas, `header=True` and `inferSchema=True`\n+options are required.\n+\n+\n+### Save DataFrame\n+\n+To load DataFrame, likewise, the APIs are `spark.read.xxx` (or `spark.read.format(\"xxx\")`) for\n+PySpark and `pandas_to_xxx` for Pandas.\n+\n+```\n+df.to_csv(\"data.csv\")\n+```\n+\n+```\n+df.write.csv(\"data.set\")\n+```\n+\n+Likewise, different set of sources are supported. Note that, to match behaviours for PySpark\n+to Pandas, `header=True` option is required.\n+\n+\n+### Inspect DataFrame\n+\n+DataFrame in both PySpark and Pandas can be expected in many ways. One of the way is to\n+`describe()` as below:\n+\n+```\n+# PySpark\n+df.describe().show()\n+```\n+\n+```\n++-------+-------------+----+\n+|summary|         city|rank|\n++-------+-------------+----+\n+|  count|            4|   4|\n+|   mean|         null| NaN|\n+| stddev|         null| NaN|\n+|    min|      Chicago| 1.0|\n+|    max|San Francisco| NaN|\n++-------+-------------+----+\n+```\n+\n+In Pandas, the same name function `describe()` can be called:\n+\n+```\n+# Pandas\n+df.describe()\n+```\n+\n+```\n+       rank\n+count   3.0\n+mean    2.0\n+std     1.0\n+min     1.0\n+25%     1.5\n+50%     2.0\n+75%     2.5\n+max     3.0\n+```\n+\n+There are some differences, for instance, in case of PySpark, `NaN` is excluded but Pandas ignore `NaN` in some statistics. \n+\n+To inspect columns, there are `dtype` API in PySpark and Pandas.",
    "line": 210
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "in case of -> in the case of\r\nignore -> ignores",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:06:03Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.\n+\n+    It causes some subtleties comparing to Pandas, for instance, about `NaN`, `None` and `NULL`.\n+\n+There are similarities and differences between them which might bring confusion. In this document\n+these are described and illuminated by several examples.\n+\n+\n+\n+## DataFrame APIs\n+\n+This chapter describes DataFrame APIs in both PySpark and Pandas.\n+\n+\n+### Quick References\n+\n+| PySpark                                                            | Pandas                                   |\n+| ------------------------------------------------------------------ | ---------------------------------------- |\n+| `df.limit(3)`                                                      | `df.head(3)`                             |\n+| `df.filter(\"a == 1 AND b == 2\")`                                   | `df.filter(\"(df.a == 1) & (df.b == 2)\")` |\n+| `df.filter((df.a == 1) & (df.b == 2))`                             | `df[(df.a == 1) & (df.b == 2)]`          |\n+| `df.select(\"a\", \"b\")`                                              | `df[[\"a\", \"b\"]]`                         |\n+| `df.drop_duplicates()`                                             | `df.drop_duplicates()`                   |\n+| `df.sample(fraction=0.01)`                                         | `df.sample(frac=0.01)`                   |\n+| `df.groupby(\"a\").count()`                                          | `df.groupby(\"a\").size()`                 |\n+| `df.groupby(\"a\").agg({\"b\": \"sum\"})`                                | `df.groupby(\"a\").agg({\"b\": np.sum})`     |\n+| `df1.join(df2, on=\"a\")`                                            | `pandas.merge(df1, df2, on=\"a\")`         |\n+| `df1.union(df2)`                                                   | `pandas.concat(df1, df2)`                |\n+| `df = df.select(when(df[\"a\"] < 5, df[\"a\"] * 2).otherwise(df[\"a\"]))`| `df.loc[pdf['a'] < 5, 'a'] *= 2`         |\n+\n+\n+### Create DataFrame\n+\n+In order to create DataFrame in PySpark and Pandas, you can run the codes below: \n+\n+```python\n+# PySpark\n+data = zip(['Chicago', 'San Francisco', 'New York City'], range(1, 4))\n+spark.createDataFrame(list(data), [\"city\", \"rank\"])\n+```\n+\n+```python\n+# Pandas\n+data = {'city': ['Chicago', 'San Francisco', 'New York City'], 'rank': range(1, 4)}\n+pandas.DataFrame(data)\n+```\n+\n+One notable difference when creating DataFrame is that Pandas accepts the data as below:\n+\n+```\n+data = {\n+    'city': ['Chicago', 'San Francisco', 'New York City'],\n+    'rank': range(1, 4)\n+}\n+```\n+\n+and it interprets as:\n+\n+```\n+            city  rank\n+0        Chicago     1\n+1  San Francisco     2\n+2  New York City     3\n+```\n+\n+So, a dictionary that contains key and multiple values becomes DataFrame but PySpark does\n+support this. Instead, PySpark can make DataFrame from Pandas DataFrame as below:\n+\n+```\n+createDataFrame(pandas.DataFrame(data))\n+```\n+\n+For more information see \"Interaction between PySpark and Pandas\" below.\n+\n+\n+### Load DataFrame\n+\n+To load DataFrame, there are APIs, usually, `spark.read.xxx` (or `spark.read.format(\"xxx\")`)\n+for PySpark and `pandas.read_xxx` for Pandas.\n+\n+```python\n+# PySpark\n+df = spark.read.csv(\"data.csv\")\n+```\n+\n+```python\n+# Pandas\n+df = pandas.read_csv(\"data.csv\")\n+```\n+\n+There are many sources available in both PySpark and Pandas. For example, CSV, JSON and\n+Parquet are commonly supported but each supports different set of sources. There are pretty\n+different set of options availabe as well, please see the API documentation for both sides.\n+Note that, to match behaviours for PySpark to Pandas, `header=True` and `inferSchema=True`\n+options are required.\n+\n+\n+### Save DataFrame\n+\n+To load DataFrame, likewise, the APIs are `spark.read.xxx` (or `spark.read.format(\"xxx\")`) for\n+PySpark and `pandas_to_xxx` for Pandas.\n+\n+```\n+df.to_csv(\"data.csv\")\n+```\n+\n+```\n+df.write.csv(\"data.set\")\n+```\n+\n+Likewise, different set of sources are supported. Note that, to match behaviours for PySpark\n+to Pandas, `header=True` option is required.\n+\n+\n+### Inspect DataFrame\n+\n+DataFrame in both PySpark and Pandas can be expected in many ways. One of the way is to\n+`describe()` as below:\n+\n+```\n+# PySpark\n+df.describe().show()\n+```\n+\n+```\n++-------+-------------+----+\n+|summary|         city|rank|\n++-------+-------------+----+\n+|  count|            4|   4|\n+|   mean|         null| NaN|\n+| stddev|         null| NaN|\n+|    min|      Chicago| 1.0|\n+|    max|San Francisco| NaN|\n++-------+-------------+----+\n+```\n+\n+In Pandas, the same name function `describe()` can be called:\n+\n+```\n+# Pandas\n+df.describe()\n+```\n+\n+```\n+       rank\n+count   3.0\n+mean    2.0\n+std     1.0\n+min     1.0\n+25%     1.5\n+50%     2.0\n+75%     2.5\n+max     3.0\n+```\n+\n+There are some differences, for instance, in case of PySpark, `NaN` is excluded but Pandas ignore `NaN` in some statistics. ",
    "line": 208
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "expected -> inspected",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:06:16Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.\n+\n+    It causes some subtleties comparing to Pandas, for instance, about `NaN`, `None` and `NULL`.\n+\n+There are similarities and differences between them which might bring confusion. In this document\n+these are described and illuminated by several examples.\n+\n+\n+\n+## DataFrame APIs\n+\n+This chapter describes DataFrame APIs in both PySpark and Pandas.\n+\n+\n+### Quick References\n+\n+| PySpark                                                            | Pandas                                   |\n+| ------------------------------------------------------------------ | ---------------------------------------- |\n+| `df.limit(3)`                                                      | `df.head(3)`                             |\n+| `df.filter(\"a == 1 AND b == 2\")`                                   | `df.filter(\"(df.a == 1) & (df.b == 2)\")` |\n+| `df.filter((df.a == 1) & (df.b == 2))`                             | `df[(df.a == 1) & (df.b == 2)]`          |\n+| `df.select(\"a\", \"b\")`                                              | `df[[\"a\", \"b\"]]`                         |\n+| `df.drop_duplicates()`                                             | `df.drop_duplicates()`                   |\n+| `df.sample(fraction=0.01)`                                         | `df.sample(frac=0.01)`                   |\n+| `df.groupby(\"a\").count()`                                          | `df.groupby(\"a\").size()`                 |\n+| `df.groupby(\"a\").agg({\"b\": \"sum\"})`                                | `df.groupby(\"a\").agg({\"b\": np.sum})`     |\n+| `df1.join(df2, on=\"a\")`                                            | `pandas.merge(df1, df2, on=\"a\")`         |\n+| `df1.union(df2)`                                                   | `pandas.concat(df1, df2)`                |\n+| `df = df.select(when(df[\"a\"] < 5, df[\"a\"] * 2).otherwise(df[\"a\"]))`| `df.loc[pdf['a'] < 5, 'a'] *= 2`         |\n+\n+\n+### Create DataFrame\n+\n+In order to create DataFrame in PySpark and Pandas, you can run the codes below: \n+\n+```python\n+# PySpark\n+data = zip(['Chicago', 'San Francisco', 'New York City'], range(1, 4))\n+spark.createDataFrame(list(data), [\"city\", \"rank\"])\n+```\n+\n+```python\n+# Pandas\n+data = {'city': ['Chicago', 'San Francisco', 'New York City'], 'rank': range(1, 4)}\n+pandas.DataFrame(data)\n+```\n+\n+One notable difference when creating DataFrame is that Pandas accepts the data as below:\n+\n+```\n+data = {\n+    'city': ['Chicago', 'San Francisco', 'New York City'],\n+    'rank': range(1, 4)\n+}\n+```\n+\n+and it interprets as:\n+\n+```\n+            city  rank\n+0        Chicago     1\n+1  San Francisco     2\n+2  New York City     3\n+```\n+\n+So, a dictionary that contains key and multiple values becomes DataFrame but PySpark does\n+support this. Instead, PySpark can make DataFrame from Pandas DataFrame as below:\n+\n+```\n+createDataFrame(pandas.DataFrame(data))\n+```\n+\n+For more information see \"Interaction between PySpark and Pandas\" below.\n+\n+\n+### Load DataFrame\n+\n+To load DataFrame, there are APIs, usually, `spark.read.xxx` (or `spark.read.format(\"xxx\")`)\n+for PySpark and `pandas.read_xxx` for Pandas.\n+\n+```python\n+# PySpark\n+df = spark.read.csv(\"data.csv\")\n+```\n+\n+```python\n+# Pandas\n+df = pandas.read_csv(\"data.csv\")\n+```\n+\n+There are many sources available in both PySpark and Pandas. For example, CSV, JSON and\n+Parquet are commonly supported but each supports different set of sources. There are pretty\n+different set of options availabe as well, please see the API documentation for both sides.\n+Note that, to match behaviours for PySpark to Pandas, `header=True` and `inferSchema=True`\n+options are required.\n+\n+\n+### Save DataFrame\n+\n+To load DataFrame, likewise, the APIs are `spark.read.xxx` (or `spark.read.format(\"xxx\")`) for\n+PySpark and `pandas_to_xxx` for Pandas.\n+\n+```\n+df.to_csv(\"data.csv\")\n+```\n+\n+```\n+df.write.csv(\"data.set\")\n+```\n+\n+Likewise, different set of sources are supported. Note that, to match behaviours for PySpark\n+to Pandas, `header=True` option is required.\n+\n+\n+### Inspect DataFrame\n+\n+DataFrame in both PySpark and Pandas can be expected in many ways. One of the way is to",
    "line": 169
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "set -> sets\r\nmatch behaviours for Pyspark -> match Pyspark behavior to",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:06:51Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.\n+\n+    It causes some subtleties comparing to Pandas, for instance, about `NaN`, `None` and `NULL`.\n+\n+There are similarities and differences between them which might bring confusion. In this document\n+these are described and illuminated by several examples.\n+\n+\n+\n+## DataFrame APIs\n+\n+This chapter describes DataFrame APIs in both PySpark and Pandas.\n+\n+\n+### Quick References\n+\n+| PySpark                                                            | Pandas                                   |\n+| ------------------------------------------------------------------ | ---------------------------------------- |\n+| `df.limit(3)`                                                      | `df.head(3)`                             |\n+| `df.filter(\"a == 1 AND b == 2\")`                                   | `df.filter(\"(df.a == 1) & (df.b == 2)\")` |\n+| `df.filter((df.a == 1) & (df.b == 2))`                             | `df[(df.a == 1) & (df.b == 2)]`          |\n+| `df.select(\"a\", \"b\")`                                              | `df[[\"a\", \"b\"]]`                         |\n+| `df.drop_duplicates()`                                             | `df.drop_duplicates()`                   |\n+| `df.sample(fraction=0.01)`                                         | `df.sample(frac=0.01)`                   |\n+| `df.groupby(\"a\").count()`                                          | `df.groupby(\"a\").size()`                 |\n+| `df.groupby(\"a\").agg({\"b\": \"sum\"})`                                | `df.groupby(\"a\").agg({\"b\": np.sum})`     |\n+| `df1.join(df2, on=\"a\")`                                            | `pandas.merge(df1, df2, on=\"a\")`         |\n+| `df1.union(df2)`                                                   | `pandas.concat(df1, df2)`                |\n+| `df = df.select(when(df[\"a\"] < 5, df[\"a\"] * 2).otherwise(df[\"a\"]))`| `df.loc[pdf['a'] < 5, 'a'] *= 2`         |\n+\n+\n+### Create DataFrame\n+\n+In order to create DataFrame in PySpark and Pandas, you can run the codes below: \n+\n+```python\n+# PySpark\n+data = zip(['Chicago', 'San Francisco', 'New York City'], range(1, 4))\n+spark.createDataFrame(list(data), [\"city\", \"rank\"])\n+```\n+\n+```python\n+# Pandas\n+data = {'city': ['Chicago', 'San Francisco', 'New York City'], 'rank': range(1, 4)}\n+pandas.DataFrame(data)\n+```\n+\n+One notable difference when creating DataFrame is that Pandas accepts the data as below:\n+\n+```\n+data = {\n+    'city': ['Chicago', 'San Francisco', 'New York City'],\n+    'rank': range(1, 4)\n+}\n+```\n+\n+and it interprets as:\n+\n+```\n+            city  rank\n+0        Chicago     1\n+1  San Francisco     2\n+2  New York City     3\n+```\n+\n+So, a dictionary that contains key and multiple values becomes DataFrame but PySpark does\n+support this. Instead, PySpark can make DataFrame from Pandas DataFrame as below:\n+\n+```\n+createDataFrame(pandas.DataFrame(data))\n+```\n+\n+For more information see \"Interaction between PySpark and Pandas\" below.\n+\n+\n+### Load DataFrame\n+\n+To load DataFrame, there are APIs, usually, `spark.read.xxx` (or `spark.read.format(\"xxx\")`)\n+for PySpark and `pandas.read_xxx` for Pandas.\n+\n+```python\n+# PySpark\n+df = spark.read.csv(\"data.csv\")\n+```\n+\n+```python\n+# Pandas\n+df = pandas.read_csv(\"data.csv\")\n+```\n+\n+There are many sources available in both PySpark and Pandas. For example, CSV, JSON and\n+Parquet are commonly supported but each supports different set of sources. There are pretty\n+different set of options availabe as well, please see the API documentation for both sides.\n+Note that, to match behaviours for PySpark to Pandas, `header=True` and `inferSchema=True`\n+options are required.\n+\n+\n+### Save DataFrame\n+\n+To load DataFrame, likewise, the APIs are `spark.read.xxx` (or `spark.read.format(\"xxx\")`) for\n+PySpark and `pandas_to_xxx` for Pandas.\n+\n+```\n+df.to_csv(\"data.csv\")\n+```\n+\n+```\n+df.write.csv(\"data.set\")\n+```\n+\n+Likewise, different set of sources are supported. Note that, to match behaviours for PySpark",
    "line": 163
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "load DataFrame -> load a DataFrame",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:07:05Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.\n+\n+    It causes some subtleties comparing to Pandas, for instance, about `NaN`, `None` and `NULL`.\n+\n+There are similarities and differences between them which might bring confusion. In this document\n+these are described and illuminated by several examples.\n+\n+\n+\n+## DataFrame APIs\n+\n+This chapter describes DataFrame APIs in both PySpark and Pandas.\n+\n+\n+### Quick References\n+\n+| PySpark                                                            | Pandas                                   |\n+| ------------------------------------------------------------------ | ---------------------------------------- |\n+| `df.limit(3)`                                                      | `df.head(3)`                             |\n+| `df.filter(\"a == 1 AND b == 2\")`                                   | `df.filter(\"(df.a == 1) & (df.b == 2)\")` |\n+| `df.filter((df.a == 1) & (df.b == 2))`                             | `df[(df.a == 1) & (df.b == 2)]`          |\n+| `df.select(\"a\", \"b\")`                                              | `df[[\"a\", \"b\"]]`                         |\n+| `df.drop_duplicates()`                                             | `df.drop_duplicates()`                   |\n+| `df.sample(fraction=0.01)`                                         | `df.sample(frac=0.01)`                   |\n+| `df.groupby(\"a\").count()`                                          | `df.groupby(\"a\").size()`                 |\n+| `df.groupby(\"a\").agg({\"b\": \"sum\"})`                                | `df.groupby(\"a\").agg({\"b\": np.sum})`     |\n+| `df1.join(df2, on=\"a\")`                                            | `pandas.merge(df1, df2, on=\"a\")`         |\n+| `df1.union(df2)`                                                   | `pandas.concat(df1, df2)`                |\n+| `df = df.select(when(df[\"a\"] < 5, df[\"a\"] * 2).otherwise(df[\"a\"]))`| `df.loc[pdf['a'] < 5, 'a'] *= 2`         |\n+\n+\n+### Create DataFrame\n+\n+In order to create DataFrame in PySpark and Pandas, you can run the codes below: \n+\n+```python\n+# PySpark\n+data = zip(['Chicago', 'San Francisco', 'New York City'], range(1, 4))\n+spark.createDataFrame(list(data), [\"city\", \"rank\"])\n+```\n+\n+```python\n+# Pandas\n+data = {'city': ['Chicago', 'San Francisco', 'New York City'], 'rank': range(1, 4)}\n+pandas.DataFrame(data)\n+```\n+\n+One notable difference when creating DataFrame is that Pandas accepts the data as below:\n+\n+```\n+data = {\n+    'city': ['Chicago', 'San Francisco', 'New York City'],\n+    'rank': range(1, 4)\n+}\n+```\n+\n+and it interprets as:\n+\n+```\n+            city  rank\n+0        Chicago     1\n+1  San Francisco     2\n+2  New York City     3\n+```\n+\n+So, a dictionary that contains key and multiple values becomes DataFrame but PySpark does\n+support this. Instead, PySpark can make DataFrame from Pandas DataFrame as below:\n+\n+```\n+createDataFrame(pandas.DataFrame(data))\n+```\n+\n+For more information see \"Interaction between PySpark and Pandas\" below.\n+\n+\n+### Load DataFrame\n+\n+To load DataFrame, there are APIs, usually, `spark.read.xxx` (or `spark.read.format(\"xxx\")`)\n+for PySpark and `pandas.read_xxx` for Pandas.\n+\n+```python\n+# PySpark\n+df = spark.read.csv(\"data.csv\")\n+```\n+\n+```python\n+# Pandas\n+df = pandas.read_csv(\"data.csv\")\n+```\n+\n+There are many sources available in both PySpark and Pandas. For example, CSV, JSON and\n+Parquet are commonly supported but each supports different set of sources. There are pretty\n+different set of options availabe as well, please see the API documentation for both sides.\n+Note that, to match behaviours for PySpark to Pandas, `header=True` and `inferSchema=True`\n+options are required.\n+\n+\n+### Save DataFrame\n+\n+To load DataFrame, likewise, the APIs are `spark.read.xxx` (or `spark.read.format(\"xxx\")`) for",
    "line": 152
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "are pretty -> are a pretty\r\nwell, please -> well. Please",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:07:28Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.\n+\n+    It causes some subtleties comparing to Pandas, for instance, about `NaN`, `None` and `NULL`.\n+\n+There are similarities and differences between them which might bring confusion. In this document\n+these are described and illuminated by several examples.\n+\n+\n+\n+## DataFrame APIs\n+\n+This chapter describes DataFrame APIs in both PySpark and Pandas.\n+\n+\n+### Quick References\n+\n+| PySpark                                                            | Pandas                                   |\n+| ------------------------------------------------------------------ | ---------------------------------------- |\n+| `df.limit(3)`                                                      | `df.head(3)`                             |\n+| `df.filter(\"a == 1 AND b == 2\")`                                   | `df.filter(\"(df.a == 1) & (df.b == 2)\")` |\n+| `df.filter((df.a == 1) & (df.b == 2))`                             | `df[(df.a == 1) & (df.b == 2)]`          |\n+| `df.select(\"a\", \"b\")`                                              | `df[[\"a\", \"b\"]]`                         |\n+| `df.drop_duplicates()`                                             | `df.drop_duplicates()`                   |\n+| `df.sample(fraction=0.01)`                                         | `df.sample(frac=0.01)`                   |\n+| `df.groupby(\"a\").count()`                                          | `df.groupby(\"a\").size()`                 |\n+| `df.groupby(\"a\").agg({\"b\": \"sum\"})`                                | `df.groupby(\"a\").agg({\"b\": np.sum})`     |\n+| `df1.join(df2, on=\"a\")`                                            | `pandas.merge(df1, df2, on=\"a\")`         |\n+| `df1.union(df2)`                                                   | `pandas.concat(df1, df2)`                |\n+| `df = df.select(when(df[\"a\"] < 5, df[\"a\"] * 2).otherwise(df[\"a\"]))`| `df.loc[pdf['a'] < 5, 'a'] *= 2`         |\n+\n+\n+### Create DataFrame\n+\n+In order to create DataFrame in PySpark and Pandas, you can run the codes below: \n+\n+```python\n+# PySpark\n+data = zip(['Chicago', 'San Francisco', 'New York City'], range(1, 4))\n+spark.createDataFrame(list(data), [\"city\", \"rank\"])\n+```\n+\n+```python\n+# Pandas\n+data = {'city': ['Chicago', 'San Francisco', 'New York City'], 'rank': range(1, 4)}\n+pandas.DataFrame(data)\n+```\n+\n+One notable difference when creating DataFrame is that Pandas accepts the data as below:\n+\n+```\n+data = {\n+    'city': ['Chicago', 'San Francisco', 'New York City'],\n+    'rank': range(1, 4)\n+}\n+```\n+\n+and it interprets as:\n+\n+```\n+            city  rank\n+0        Chicago     1\n+1  San Francisco     2\n+2  New York City     3\n+```\n+\n+So, a dictionary that contains key and multiple values becomes DataFrame but PySpark does\n+support this. Instead, PySpark can make DataFrame from Pandas DataFrame as below:\n+\n+```\n+createDataFrame(pandas.DataFrame(data))\n+```\n+\n+For more information see \"Interaction between PySpark and Pandas\" below.\n+\n+\n+### Load DataFrame\n+\n+To load DataFrame, there are APIs, usually, `spark.read.xxx` (or `spark.read.format(\"xxx\")`)\n+for PySpark and `pandas.read_xxx` for Pandas.\n+\n+```python\n+# PySpark\n+df = spark.read.csv(\"data.csv\")\n+```\n+\n+```python\n+# Pandas\n+df = pandas.read_csv(\"data.csv\")\n+```\n+\n+There are many sources available in both PySpark and Pandas. For example, CSV, JSON and\n+Parquet are commonly supported but each supports different set of sources. There are pretty",
    "line": 144
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "key and ... -> multiple values for a key\r\nbecomes DataFrame -> becomes a DataFrame,\r\nPyspark does -> Pyspark does not?",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:08:06Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.\n+\n+    It causes some subtleties comparing to Pandas, for instance, about `NaN`, `None` and `NULL`.\n+\n+There are similarities and differences between them which might bring confusion. In this document\n+these are described and illuminated by several examples.\n+\n+\n+\n+## DataFrame APIs\n+\n+This chapter describes DataFrame APIs in both PySpark and Pandas.\n+\n+\n+### Quick References\n+\n+| PySpark                                                            | Pandas                                   |\n+| ------------------------------------------------------------------ | ---------------------------------------- |\n+| `df.limit(3)`                                                      | `df.head(3)`                             |\n+| `df.filter(\"a == 1 AND b == 2\")`                                   | `df.filter(\"(df.a == 1) & (df.b == 2)\")` |\n+| `df.filter((df.a == 1) & (df.b == 2))`                             | `df[(df.a == 1) & (df.b == 2)]`          |\n+| `df.select(\"a\", \"b\")`                                              | `df[[\"a\", \"b\"]]`                         |\n+| `df.drop_duplicates()`                                             | `df.drop_duplicates()`                   |\n+| `df.sample(fraction=0.01)`                                         | `df.sample(frac=0.01)`                   |\n+| `df.groupby(\"a\").count()`                                          | `df.groupby(\"a\").size()`                 |\n+| `df.groupby(\"a\").agg({\"b\": \"sum\"})`                                | `df.groupby(\"a\").agg({\"b\": np.sum})`     |\n+| `df1.join(df2, on=\"a\")`                                            | `pandas.merge(df1, df2, on=\"a\")`         |\n+| `df1.union(df2)`                                                   | `pandas.concat(df1, df2)`                |\n+| `df = df.select(when(df[\"a\"] < 5, df[\"a\"] * 2).otherwise(df[\"a\"]))`| `df.loc[pdf['a'] < 5, 'a'] *= 2`         |\n+\n+\n+### Create DataFrame\n+\n+In order to create DataFrame in PySpark and Pandas, you can run the codes below: \n+\n+```python\n+# PySpark\n+data = zip(['Chicago', 'San Francisco', 'New York City'], range(1, 4))\n+spark.createDataFrame(list(data), [\"city\", \"rank\"])\n+```\n+\n+```python\n+# Pandas\n+data = {'city': ['Chicago', 'San Francisco', 'New York City'], 'rank': range(1, 4)}\n+pandas.DataFrame(data)\n+```\n+\n+One notable difference when creating DataFrame is that Pandas accepts the data as below:\n+\n+```\n+data = {\n+    'city': ['Chicago', 'San Francisco', 'New York City'],\n+    'rank': range(1, 4)\n+}\n+```\n+\n+and it interprets as:\n+\n+```\n+            city  rank\n+0        Chicago     1\n+1  San Francisco     2\n+2  New York City     3\n+```\n+\n+So, a dictionary that contains key and multiple values becomes DataFrame but PySpark does",
    "line": 118
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "codes -> code",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:08:18Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.\n+\n+    It causes some subtleties comparing to Pandas, for instance, about `NaN`, `None` and `NULL`.\n+\n+There are similarities and differences between them which might bring confusion. In this document\n+these are described and illuminated by several examples.\n+\n+\n+\n+## DataFrame APIs\n+\n+This chapter describes DataFrame APIs in both PySpark and Pandas.\n+\n+\n+### Quick References\n+\n+| PySpark                                                            | Pandas                                   |\n+| ------------------------------------------------------------------ | ---------------------------------------- |\n+| `df.limit(3)`                                                      | `df.head(3)`                             |\n+| `df.filter(\"a == 1 AND b == 2\")`                                   | `df.filter(\"(df.a == 1) & (df.b == 2)\")` |\n+| `df.filter((df.a == 1) & (df.b == 2))`                             | `df[(df.a == 1) & (df.b == 2)]`          |\n+| `df.select(\"a\", \"b\")`                                              | `df[[\"a\", \"b\"]]`                         |\n+| `df.drop_duplicates()`                                             | `df.drop_duplicates()`                   |\n+| `df.sample(fraction=0.01)`                                         | `df.sample(frac=0.01)`                   |\n+| `df.groupby(\"a\").count()`                                          | `df.groupby(\"a\").size()`                 |\n+| `df.groupby(\"a\").agg({\"b\": \"sum\"})`                                | `df.groupby(\"a\").agg({\"b\": np.sum})`     |\n+| `df1.join(df2, on=\"a\")`                                            | `pandas.merge(df1, df2, on=\"a\")`         |\n+| `df1.union(df2)`                                                   | `pandas.concat(df1, df2)`                |\n+| `df = df.select(when(df[\"a\"] < 5, df[\"a\"] * 2).otherwise(df[\"a\"]))`| `df.loc[pdf['a'] < 5, 'a'] *= 2`         |\n+\n+\n+### Create DataFrame\n+\n+In order to create DataFrame in PySpark and Pandas, you can run the codes below: ",
    "line": 86
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "comply SQL -> mimic SQL operations? or comply with SQL?",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:08:45Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.",
    "line": 53
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "PySpark DataFrames can't be changed. ...",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:09:10Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform",
    "line": 49
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "This may be lost on Python developers",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:09:22Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.",
    "line": 51
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "which allow use of Pandas APIs. (full stop)",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:09:44Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.",
    "line": 30
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "larget -> larger",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:10:00Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a",
    "line": 37
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "fix -> fit",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:10:10Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a",
    "line": 38
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "remove 'the'",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:10:33Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.\n+\n+    It causes some subtleties comparing to Pandas, for instance, about `NaN`, `None` and `NULL`.\n+\n+There are similarities and differences between them which might bring confusion. In this document\n+these are described and illuminated by several examples.\n+\n+\n+\n+## DataFrame APIs\n+\n+This chapter describes DataFrame APIs in both PySpark and Pandas.\n+\n+\n+### Quick References\n+\n+| PySpark                                                            | Pandas                                   |\n+| ------------------------------------------------------------------ | ---------------------------------------- |\n+| `df.limit(3)`                                                      | `df.head(3)`                             |\n+| `df.filter(\"a == 1 AND b == 2\")`                                   | `df.filter(\"(df.a == 1) & (df.b == 2)\")` |\n+| `df.filter((df.a == 1) & (df.b == 2))`                             | `df[(df.a == 1) & (df.b == 2)]`          |\n+| `df.select(\"a\", \"b\")`                                              | `df[[\"a\", \"b\"]]`                         |\n+| `df.drop_duplicates()`                                             | `df.drop_duplicates()`                   |\n+| `df.sample(fraction=0.01)`                                         | `df.sample(frac=0.01)`                   |\n+| `df.groupby(\"a\").count()`                                          | `df.groupby(\"a\").size()`                 |\n+| `df.groupby(\"a\").agg({\"b\": \"sum\"})`                                | `df.groupby(\"a\").agg({\"b\": np.sum})`     |\n+| `df1.join(df2, on=\"a\")`                                            | `pandas.merge(df1, df2, on=\"a\")`         |\n+| `df1.union(df2)`                                                   | `pandas.concat(df1, df2)`                |\n+| `df = df.select(when(df[\"a\"] < 5, df[\"a\"] * 2).otherwise(df[\"a\"]))`| `df.loc[pdf['a'] < 5, 'a'] *= 2`         |\n+\n+\n+### Create DataFrame\n+\n+In order to create DataFrame in PySpark and Pandas, you can run the codes below: \n+\n+```python\n+# PySpark\n+data = zip(['Chicago', 'San Francisco', 'New York City'], range(1, 4))\n+spark.createDataFrame(list(data), [\"city\", \"rank\"])\n+```\n+\n+```python\n+# Pandas\n+data = {'city': ['Chicago', 'San Francisco', 'New York City'], 'rank': range(1, 4)}\n+pandas.DataFrame(data)\n+```\n+\n+One notable difference when creating DataFrame is that Pandas accepts the data as below:\n+\n+```\n+data = {\n+    'city': ['Chicago', 'San Francisco', 'New York City'],\n+    'rank': range(1, 4)\n+}\n+```\n+\n+and it interprets as:\n+\n+```\n+            city  rank\n+0        Chicago     1\n+1  San Francisco     2\n+2  New York City     3\n+```\n+\n+So, a dictionary that contains key and multiple values becomes DataFrame but PySpark does\n+support this. Instead, PySpark can make DataFrame from Pandas DataFrame as below:\n+\n+```\n+createDataFrame(pandas.DataFrame(data))\n+```\n+\n+For more information see \"Interaction between PySpark and Pandas\" below.\n+\n+\n+### Load DataFrame\n+\n+To load DataFrame, there are APIs, usually, `spark.read.xxx` (or `spark.read.format(\"xxx\")`)\n+for PySpark and `pandas.read_xxx` for Pandas.\n+\n+```python\n+# PySpark\n+df = spark.read.csv(\"data.csv\")\n+```\n+\n+```python\n+# Pandas\n+df = pandas.read_csv(\"data.csv\")\n+```\n+\n+There are many sources available in both PySpark and Pandas. For example, CSV, JSON and\n+Parquet are commonly supported but each supports different set of sources. There are pretty\n+different set of options availabe as well, please see the API documentation for both sides.\n+Note that, to match behaviours for PySpark to Pandas, `header=True` and `inferSchema=True`\n+options are required.\n+\n+\n+### Save DataFrame\n+\n+To load DataFrame, likewise, the APIs are `spark.read.xxx` (or `spark.read.format(\"xxx\")`) for\n+PySpark and `pandas_to_xxx` for Pandas.\n+\n+```\n+df.to_csv(\"data.csv\")\n+```\n+\n+```\n+df.write.csv(\"data.set\")\n+```\n+\n+Likewise, different set of sources are supported. Note that, to match behaviours for PySpark\n+to Pandas, `header=True` option is required.\n+\n+\n+### Inspect DataFrame\n+\n+DataFrame in both PySpark and Pandas can be expected in many ways. One of the way is to\n+`describe()` as below:\n+\n+```\n+# PySpark\n+df.describe().show()\n+```\n+\n+```\n++-------+-------------+----+\n+|summary|         city|rank|\n++-------+-------------+----+\n+|  count|            4|   4|\n+|   mean|         null| NaN|\n+| stddev|         null| NaN|\n+|    min|      Chicago| 1.0|\n+|    max|San Francisco| NaN|\n++-------+-------------+----+\n+```\n+\n+In Pandas, the same name function `describe()` can be called:\n+\n+```\n+# Pandas\n+df.describe()\n+```\n+\n+```\n+       rank\n+count   3.0\n+mean    2.0\n+std     1.0\n+min     1.0\n+25%     1.5\n+50%     2.0\n+75%     2.5\n+max     3.0\n+```\n+\n+There are some differences, for instance, in case of PySpark, `NaN` is excluded but Pandas ignore `NaN` in some statistics. \n+\n+To inspect columns, there are `dtype` API in PySpark and Pandas.\n+\n+```\n+# PySpark\n+df.dtypes\n+```\n+\n+```\n+[('city', 'string'), ('rank', 'double')]\n+```\n+\n+```\n+# Pandas\n+df.dtypes\n+```\n+\n+```\n+0     object\n+1    float64\n+dtype: object\n+```\n+\n+PySpark types are represented as Spark SQL types whereas they are NumPy types in case of Pandas.\n+\n+Note that, PySpark supports pretty printing of schema as below:\n+\n+```\n+# PySpark\n+df.printSchema()\n+```\n+\n+```\n+root\n+ |-- city: string (nullable = true)\n+ |-- rank: double (nullable = true)\n+```\n+\n+\n+### Interaction between PySpark and Pandas\n+\n+PySpark supports conversion to/from Pandas and Pandas UDFs out of the box.\n+See [PySpark Usage Guide for Pandas with Apache Arrow](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html).\n+\n+\n+\n+## Notable Differences\n+\n+There are other notable differences for many reasons, for instance,\n+PySpark DataFrame's core is Spark SQL so some of APIs comply SQL, and this might be different\n+comparing to Pandas context which somtimes causes some subtleties. See the differences below.\n+\n+\n+### Lazy and Eager Evaluation\n+\n+PySpark DataFrame is lazy whereas Pandas DataFrame is eager. Therefore,\n+PySpark DataFrame can optimize the computation before an actual execution whereas Pandas\n+DataFrame can have each direct result immediately after each execution.\n+\n+For sintance, suppose you select columns multiple times as below:\n+  \n+```python\n+df = ...  # PySpark DataFrame\n+df = df.select(\"fieldA\", \"fieldB\")\n+df = df.select(\"fieldB\")\n+df.show()\n+```\n+\n+```python\n+pdf = ...  # Pandas DataFrame\n+pdf = pdf[[\"fieldA\", \"fieldB\"]]\n+pdf = pdf[[\"fieldA\"]]\n+```\n+  \n+PySpark DataFrame only selects `fieldB` alone for its execution when an action, `collect`, is\n+performed. Pandas DataFrame immediately applies the operation at each line. So, `fieldA` and\n+`fieldB` are selected, and then `fieldB` is selected with the updated result for each.\n+\n+\n+### Direct assignment\n+\n+PySpark does not allow the direct assignment whereas Pandas does.",
    "line": 290
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "remove extra spaces in first row's first column",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:11:12Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.\n+\n+    It causes some subtleties comparing to Pandas, for instance, about `NaN`, `None` and `NULL`.\n+\n+There are similarities and differences between them which might bring confusion. In this document\n+these are described and illuminated by several examples.\n+\n+\n+\n+## DataFrame APIs\n+\n+This chapter describes DataFrame APIs in both PySpark and Pandas.\n+\n+\n+### Quick References\n+\n+| PySpark                                                            | Pandas                                   |\n+| ------------------------------------------------------------------ | ---------------------------------------- |\n+| `df.limit(3)`                                                      | `df.head(3)`                             |\n+| `df.filter(\"a == 1 AND b == 2\")`                                   | `df.filter(\"(df.a == 1) & (df.b == 2)\")` |\n+| `df.filter((df.a == 1) & (df.b == 2))`                             | `df[(df.a == 1) & (df.b == 2)]`          |\n+| `df.select(\"a\", \"b\")`                                              | `df[[\"a\", \"b\"]]`                         |\n+| `df.drop_duplicates()`                                             | `df.drop_duplicates()`                   |\n+| `df.sample(fraction=0.01)`                                         | `df.sample(frac=0.01)`                   |\n+| `df.groupby(\"a\").count()`                                          | `df.groupby(\"a\").size()`                 |\n+| `df.groupby(\"a\").agg({\"b\": \"sum\"})`                                | `df.groupby(\"a\").agg({\"b\": np.sum})`     |\n+| `df1.join(df2, on=\"a\")`                                            | `pandas.merge(df1, df2, on=\"a\")`         |\n+| `df1.union(df2)`                                                   | `pandas.concat(df1, df2)`                |\n+| `df = df.select(when(df[\"a\"] < 5, df[\"a\"] * 2).otherwise(df[\"a\"]))`| `df.loc[pdf['a'] < 5, 'a'] *= 2`         |\n+\n+\n+### Create DataFrame\n+\n+In order to create DataFrame in PySpark and Pandas, you can run the codes below: \n+\n+```python\n+# PySpark\n+data = zip(['Chicago', 'San Francisco', 'New York City'], range(1, 4))\n+spark.createDataFrame(list(data), [\"city\", \"rank\"])\n+```\n+\n+```python\n+# Pandas\n+data = {'city': ['Chicago', 'San Francisco', 'New York City'], 'rank': range(1, 4)}\n+pandas.DataFrame(data)\n+```\n+\n+One notable difference when creating DataFrame is that Pandas accepts the data as below:\n+\n+```\n+data = {\n+    'city': ['Chicago', 'San Francisco', 'New York City'],\n+    'rank': range(1, 4)\n+}\n+```\n+\n+and it interprets as:\n+\n+```\n+            city  rank\n+0        Chicago     1\n+1  San Francisco     2\n+2  New York City     3\n+```\n+\n+So, a dictionary that contains key and multiple values becomes DataFrame but PySpark does\n+support this. Instead, PySpark can make DataFrame from Pandas DataFrame as below:\n+\n+```\n+createDataFrame(pandas.DataFrame(data))\n+```\n+\n+For more information see \"Interaction between PySpark and Pandas\" below.\n+\n+\n+### Load DataFrame\n+\n+To load DataFrame, there are APIs, usually, `spark.read.xxx` (or `spark.read.format(\"xxx\")`)\n+for PySpark and `pandas.read_xxx` for Pandas.\n+\n+```python\n+# PySpark\n+df = spark.read.csv(\"data.csv\")\n+```\n+\n+```python\n+# Pandas\n+df = pandas.read_csv(\"data.csv\")\n+```\n+\n+There are many sources available in both PySpark and Pandas. For example, CSV, JSON and\n+Parquet are commonly supported but each supports different set of sources. There are pretty\n+different set of options availabe as well, please see the API documentation for both sides.\n+Note that, to match behaviours for PySpark to Pandas, `header=True` and `inferSchema=True`\n+options are required.\n+\n+\n+### Save DataFrame\n+\n+To load DataFrame, likewise, the APIs are `spark.read.xxx` (or `spark.read.format(\"xxx\")`) for\n+PySpark and `pandas_to_xxx` for Pandas.\n+\n+```\n+df.to_csv(\"data.csv\")\n+```\n+\n+```\n+df.write.csv(\"data.set\")\n+```\n+\n+Likewise, different set of sources are supported. Note that, to match behaviours for PySpark\n+to Pandas, `header=True` option is required.\n+\n+\n+### Inspect DataFrame\n+\n+DataFrame in both PySpark and Pandas can be expected in many ways. One of the way is to\n+`describe()` as below:\n+\n+```\n+# PySpark\n+df.describe().show()\n+```\n+\n+```\n++-------+-------------+----+\n+|summary|         city|rank|\n++-------+-------------+----+\n+|  count|            4|   4|\n+|   mean|         null| NaN|\n+| stddev|         null| NaN|\n+|    min|      Chicago| 1.0|\n+|    max|San Francisco| NaN|\n++-------+-------------+----+\n+```\n+\n+In Pandas, the same name function `describe()` can be called:\n+\n+```\n+# Pandas\n+df.describe()\n+```\n+\n+```\n+       rank\n+count   3.0\n+mean    2.0\n+std     1.0\n+min     1.0\n+25%     1.5\n+50%     2.0\n+75%     2.5\n+max     3.0\n+```\n+\n+There are some differences, for instance, in case of PySpark, `NaN` is excluded but Pandas ignore `NaN` in some statistics. \n+\n+To inspect columns, there are `dtype` API in PySpark and Pandas.\n+\n+```\n+# PySpark\n+df.dtypes\n+```\n+\n+```\n+[('city', 'string'), ('rank', 'double')]\n+```\n+\n+```\n+# Pandas\n+df.dtypes\n+```\n+\n+```\n+0     object\n+1    float64\n+dtype: object\n+```\n+\n+PySpark types are represented as Spark SQL types whereas they are NumPy types in case of Pandas.\n+\n+Note that, PySpark supports pretty printing of schema as below:\n+\n+```\n+# PySpark\n+df.printSchema()\n+```\n+\n+```\n+root\n+ |-- city: string (nullable = true)\n+ |-- rank: double (nullable = true)\n+```\n+\n+\n+### Interaction between PySpark and Pandas\n+\n+PySpark supports conversion to/from Pandas and Pandas UDFs out of the box.\n+See [PySpark Usage Guide for Pandas with Apache Arrow](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html).\n+\n+\n+\n+## Notable Differences\n+\n+There are other notable differences for many reasons, for instance,\n+PySpark DataFrame's core is Spark SQL so some of APIs comply SQL, and this might be different\n+comparing to Pandas context which somtimes causes some subtleties. See the differences below.\n+\n+\n+### Lazy and Eager Evaluation\n+\n+PySpark DataFrame is lazy whereas Pandas DataFrame is eager. Therefore,\n+PySpark DataFrame can optimize the computation before an actual execution whereas Pandas\n+DataFrame can have each direct result immediately after each execution.\n+\n+For sintance, suppose you select columns multiple times as below:\n+  \n+```python\n+df = ...  # PySpark DataFrame\n+df = df.select(\"fieldA\", \"fieldB\")\n+df = df.select(\"fieldB\")\n+df.show()\n+```\n+\n+```python\n+pdf = ...  # Pandas DataFrame\n+pdf = pdf[[\"fieldA\", \"fieldB\"]]\n+pdf = pdf[[\"fieldA\"]]\n+```\n+  \n+PySpark DataFrame only selects `fieldB` alone for its execution when an action, `collect`, is\n+performed. Pandas DataFrame immediately applies the operation at each line. So, `fieldA` and\n+`fieldB` are selected, and then `fieldB` is selected with the updated result for each.\n+\n+\n+### Direct assignment\n+\n+PySpark does not allow the direct assignment whereas Pandas does.\n+\n+```python\n+# Pandas\n+df.city = 1\n+```\n+\n+```\n+   city  rank\n+0     1     1\n+1     1     2\n+2     1     3\n+```\n+\n+In PySpark, it should be done as:\n+\n+```\n+df.withColumn(\"city\", lit(1)).show()\n+```\n+\n+This is fundamentally because PySpark DataFrame is immutable whereas Pandas DataFrame is mutable.\n+\n+\n+### `NULL`, `None`, `NaN` and `NaT` \n+\n+`NULL`, `None`, `NaN` and `NaT` concepts sometimes bring confusions to users and developers. Here are definitions of each:\n+\n+- In Spark SQL, `NULL` represents a missing value.\n+- In Python, `None` represents a missing value.\n+- In Pandas, for float types, `NaN` represents not-a-number and also a missing value. \n+- In Pandas, for `datetime64[ns]` type, `NaT` represents a missing value.\n+\n+For `NaN` comparison, sometimes confusions happen. Usually the comparison is not only specific in Pandas but also regular Python, NumPy, etc. \n+\n+See the comparisons below:\n+\n+```python\n+>>> np.nan and True\n+True\n+>>> True and np.nan\n+nan\n+```\n+\n+```python\n+>>> np.nan and False\n+False\n+>>> False and np.nan\n+False\n+```\n+\n+```python\n+>>> np.nan or True\n+nan\n+>>> True or np.nan\n+True\n+```\n+\n+```python\n+>>> np.nan or False\n+nan\n+>>> False or np.nan\n+nan\n+```\n+\n+Python logical operators are different from SQL and some other programming languages. These comparisons are all correct as specified rules within Python. \n+\n+**Note:** Python's boolean operator is different:\n+\n+| Operation   | Result                        |\n+| ----------- | ----------------------------- |\n+| `x or y\t`   | if x is false, then y, else x |",
    "line": 360
  }],
  "prId": 24234
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "support -> supports. \r\nTo be consistent, behaviour -> behavior (UK vs US spelling)\r\ntend -> tends\r\nlook after -> follow",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T14:11:56Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.\n+\n+    It causes some subtleties comparing to Pandas, for instance, about `NaN`, `None` and `NULL`.\n+\n+There are similarities and differences between them which might bring confusion. In this document\n+these are described and illuminated by several examples.\n+\n+\n+\n+## DataFrame APIs\n+\n+This chapter describes DataFrame APIs in both PySpark and Pandas.\n+\n+\n+### Quick References\n+\n+| PySpark                                                            | Pandas                                   |\n+| ------------------------------------------------------------------ | ---------------------------------------- |\n+| `df.limit(3)`                                                      | `df.head(3)`                             |\n+| `df.filter(\"a == 1 AND b == 2\")`                                   | `df.filter(\"(df.a == 1) & (df.b == 2)\")` |\n+| `df.filter((df.a == 1) & (df.b == 2))`                             | `df[(df.a == 1) & (df.b == 2)]`          |\n+| `df.select(\"a\", \"b\")`                                              | `df[[\"a\", \"b\"]]`                         |\n+| `df.drop_duplicates()`                                             | `df.drop_duplicates()`                   |\n+| `df.sample(fraction=0.01)`                                         | `df.sample(frac=0.01)`                   |\n+| `df.groupby(\"a\").count()`                                          | `df.groupby(\"a\").size()`                 |\n+| `df.groupby(\"a\").agg({\"b\": \"sum\"})`                                | `df.groupby(\"a\").agg({\"b\": np.sum})`     |\n+| `df1.join(df2, on=\"a\")`                                            | `pandas.merge(df1, df2, on=\"a\")`         |\n+| `df1.union(df2)`                                                   | `pandas.concat(df1, df2)`                |\n+| `df = df.select(when(df[\"a\"] < 5, df[\"a\"] * 2).otherwise(df[\"a\"]))`| `df.loc[pdf['a'] < 5, 'a'] *= 2`         |\n+\n+\n+### Create DataFrame\n+\n+In order to create DataFrame in PySpark and Pandas, you can run the codes below: \n+\n+```python\n+# PySpark\n+data = zip(['Chicago', 'San Francisco', 'New York City'], range(1, 4))\n+spark.createDataFrame(list(data), [\"city\", \"rank\"])\n+```\n+\n+```python\n+# Pandas\n+data = {'city': ['Chicago', 'San Francisco', 'New York City'], 'rank': range(1, 4)}\n+pandas.DataFrame(data)\n+```\n+\n+One notable difference when creating DataFrame is that Pandas accepts the data as below:\n+\n+```\n+data = {\n+    'city': ['Chicago', 'San Francisco', 'New York City'],\n+    'rank': range(1, 4)\n+}\n+```\n+\n+and it interprets as:\n+\n+```\n+            city  rank\n+0        Chicago     1\n+1  San Francisco     2\n+2  New York City     3\n+```\n+\n+So, a dictionary that contains key and multiple values becomes DataFrame but PySpark does\n+support this. Instead, PySpark can make DataFrame from Pandas DataFrame as below:\n+\n+```\n+createDataFrame(pandas.DataFrame(data))\n+```\n+\n+For more information see \"Interaction between PySpark and Pandas\" below.\n+\n+\n+### Load DataFrame\n+\n+To load DataFrame, there are APIs, usually, `spark.read.xxx` (or `spark.read.format(\"xxx\")`)\n+for PySpark and `pandas.read_xxx` for Pandas.\n+\n+```python\n+# PySpark\n+df = spark.read.csv(\"data.csv\")\n+```\n+\n+```python\n+# Pandas\n+df = pandas.read_csv(\"data.csv\")\n+```\n+\n+There are many sources available in both PySpark and Pandas. For example, CSV, JSON and\n+Parquet are commonly supported but each supports different set of sources. There are pretty\n+different set of options availabe as well, please see the API documentation for both sides.\n+Note that, to match behaviours for PySpark to Pandas, `header=True` and `inferSchema=True`\n+options are required.\n+\n+\n+### Save DataFrame\n+\n+To load DataFrame, likewise, the APIs are `spark.read.xxx` (or `spark.read.format(\"xxx\")`) for\n+PySpark and `pandas_to_xxx` for Pandas.\n+\n+```\n+df.to_csv(\"data.csv\")\n+```\n+\n+```\n+df.write.csv(\"data.set\")\n+```\n+\n+Likewise, different set of sources are supported. Note that, to match behaviours for PySpark\n+to Pandas, `header=True` option is required.\n+\n+\n+### Inspect DataFrame\n+\n+DataFrame in both PySpark and Pandas can be expected in many ways. One of the way is to\n+`describe()` as below:\n+\n+```\n+# PySpark\n+df.describe().show()\n+```\n+\n+```\n++-------+-------------+----+\n+|summary|         city|rank|\n++-------+-------------+----+\n+|  count|            4|   4|\n+|   mean|         null| NaN|\n+| stddev|         null| NaN|\n+|    min|      Chicago| 1.0|\n+|    max|San Francisco| NaN|\n++-------+-------------+----+\n+```\n+\n+In Pandas, the same name function `describe()` can be called:\n+\n+```\n+# Pandas\n+df.describe()\n+```\n+\n+```\n+       rank\n+count   3.0\n+mean    2.0\n+std     1.0\n+min     1.0\n+25%     1.5\n+50%     2.0\n+75%     2.5\n+max     3.0\n+```\n+\n+There are some differences, for instance, in case of PySpark, `NaN` is excluded but Pandas ignore `NaN` in some statistics. \n+\n+To inspect columns, there are `dtype` API in PySpark and Pandas.\n+\n+```\n+# PySpark\n+df.dtypes\n+```\n+\n+```\n+[('city', 'string'), ('rank', 'double')]\n+```\n+\n+```\n+# Pandas\n+df.dtypes\n+```\n+\n+```\n+0     object\n+1    float64\n+dtype: object\n+```\n+\n+PySpark types are represented as Spark SQL types whereas they are NumPy types in case of Pandas.\n+\n+Note that, PySpark supports pretty printing of schema as below:\n+\n+```\n+# PySpark\n+df.printSchema()\n+```\n+\n+```\n+root\n+ |-- city: string (nullable = true)\n+ |-- rank: double (nullable = true)\n+```\n+\n+\n+### Interaction between PySpark and Pandas\n+\n+PySpark supports conversion to/from Pandas and Pandas UDFs out of the box.\n+See [PySpark Usage Guide for Pandas with Apache Arrow](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html).\n+\n+\n+\n+## Notable Differences\n+\n+There are other notable differences for many reasons, for instance,\n+PySpark DataFrame's core is Spark SQL so some of APIs comply SQL, and this might be different\n+comparing to Pandas context which somtimes causes some subtleties. See the differences below.\n+\n+\n+### Lazy and Eager Evaluation\n+\n+PySpark DataFrame is lazy whereas Pandas DataFrame is eager. Therefore,\n+PySpark DataFrame can optimize the computation before an actual execution whereas Pandas\n+DataFrame can have each direct result immediately after each execution.\n+\n+For sintance, suppose you select columns multiple times as below:\n+  \n+```python\n+df = ...  # PySpark DataFrame\n+df = df.select(\"fieldA\", \"fieldB\")\n+df = df.select(\"fieldB\")\n+df.show()\n+```\n+\n+```python\n+pdf = ...  # Pandas DataFrame\n+pdf = pdf[[\"fieldA\", \"fieldB\"]]\n+pdf = pdf[[\"fieldA\"]]\n+```\n+  \n+PySpark DataFrame only selects `fieldB` alone for its execution when an action, `collect`, is\n+performed. Pandas DataFrame immediately applies the operation at each line. So, `fieldA` and\n+`fieldB` are selected, and then `fieldB` is selected with the updated result for each.\n+\n+\n+### Direct assignment\n+\n+PySpark does not allow the direct assignment whereas Pandas does.\n+\n+```python\n+# Pandas\n+df.city = 1\n+```\n+\n+```\n+   city  rank\n+0     1     1\n+1     1     2\n+2     1     3\n+```\n+\n+In PySpark, it should be done as:\n+\n+```\n+df.withColumn(\"city\", lit(1)).show()\n+```\n+\n+This is fundamentally because PySpark DataFrame is immutable whereas Pandas DataFrame is mutable.\n+\n+\n+### `NULL`, `None`, `NaN` and `NaT` \n+\n+`NULL`, `None`, `NaN` and `NaT` concepts sometimes bring confusions to users and developers. Here are definitions of each:\n+\n+- In Spark SQL, `NULL` represents a missing value.\n+- In Python, `None` represents a missing value.\n+- In Pandas, for float types, `NaN` represents not-a-number and also a missing value. \n+- In Pandas, for `datetime64[ns]` type, `NaT` represents a missing value.\n+\n+For `NaN` comparison, sometimes confusions happen. Usually the comparison is not only specific in Pandas but also regular Python, NumPy, etc. \n+\n+See the comparisons below:\n+\n+```python\n+>>> np.nan and True\n+True\n+>>> True and np.nan\n+nan\n+```\n+\n+```python\n+>>> np.nan and False\n+False\n+>>> False and np.nan\n+False\n+```\n+\n+```python\n+>>> np.nan or True\n+nan\n+>>> True or np.nan\n+True\n+```\n+\n+```python\n+>>> np.nan or False\n+nan\n+>>> False or np.nan\n+nan\n+```\n+\n+Python logical operators are different from SQL and some other programming languages. These comparisons are all correct as specified rules within Python. \n+\n+**Note:** Python's boolean operator is different:\n+\n+| Operation   | Result                        |\n+| ----------- | ----------------------------- |\n+| `x or y\t`   | if x is false, then y, else x |\n+| `x and y`   | if x is false, then x, else y |\n+\n+See also [Boolean Operations](https://docs.python.org/3/library/stdtypes.html?highlight=short%20circuit#boolean-operations-and-or-not) in Python. \n+\n+**Note:** `bool(np.nan)` is `True`. See also [Truth Value Testing](https://docs.python.org/3/library/stdtypes.html#truth-value-testing).\n+\n+**Note:** When you switch `np.nan` to `float(\"nan\")`, it still shows the same results. Even when you switch `np.nan` to `np.datetime64('NaT')`, it also shows the same pattern (although the result `nan` becomes `np.datetime64('NaT')` instead).\n+\n+In PySpark, `None` comparison follows `NULL` comparison in Spark SQL.\n+\n+```\n++-----+-----+---------+--------+\n+|    x|    y|(x AND y)|(x OR y)|\n++-----+-----+---------+--------+\n+| null| true|     null|    true|\n+| true| null|     null|    true|\n+| null|false|    false|    null|\n+|false| null|    false|    null|\n++-----+-----+---------+--------+\n+```\n+\n+`NaN` and `boolean` type comparison is disallowed.\n+\n+\n+### Type inference, coercion and cast\n+\n+PySpark support native Python types currently. Those Python types are mapped to Spark SQL types in order to leverage Spark SQL. So, in general PySpark behaviours follow Spark SQL's behaviours whereas Pandas tend to look after Python's own behaviours and NumPy's behaviours since both types are supported in Pandas.",
    "line": 387
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "My english is completely mixed up between US and UK :) .. let me fix it.",
    "commit": "560a0473dcbf4684624dea9573efccd7c4179a51",
    "createdAt": "2019-03-28T23:28:41Z",
    "diffHunk": "@@ -0,0 +1,401 @@\n+---\n+layout: global\n+title: PySpark Comparison with Pandas \n+displayTitle: PySpark Comparison with Pandas \n+---\n+\n+Both PySpark and Pandas cover important use cases and provide a rich set of features to interact \n+with various structural and semistructral data in Python world. Often, PySpark users are used to \n+Pandas. Therefore, this document targets to document the comparison.\n+\n+* Overview\n+* DataFrame APIs\n+  * Quick References\n+  * Create DataFrame\n+  * Load DataFrame\n+  * Save DataFrame\n+  * Inspect DataFrame\n+  * Interaction between PySpark and Pandas\n+* Notable Differences\n+  * Lazy and Eager Evaluation\n+  * Direct assignment\n+  * NULL, None, NaN and NaT\n+  * Type inference, coercion and cast\n+\n+\n+## Overview\n+\n+PySpark and Pandas support common functionality to load, save, create, transform and describe \n+DataFrame. PySpark provides conversion from/to Pandas DataFrame, and PySpark introduced Pandas \n+UDFs which allow to use Pandas APIs as are for interoperability between them.\n+\n+Nevertheless, there are fundamental differences between them to note in general.\n+\n+1. PySpark DataFrame is a distributed dataset across multiple nodes whereas Pandas DataFrame is a\n+  local dataset within single node.\n+\n+    It brings a practical point. If you handle larget dataset, arguably PySpark brings arguably a\n+    better performance in general. If the dataset to process does not fix into the memory in a\n+    single node, using PySpark is probably the way. In case of small dataset, Pandas might be\n+    faster in general since there would not be overhead, for instance, network.\n+\n+2. PySpark DataFrame is lazy evaluation whereas Pandas DataFrame is eager evaluation.\n+\n+    PySpark DataFrame executes lazily whereas Pandas DataFrame executes each operation\n+    immediately against the data set.\n+\n+3. PySpark DataFrame is immutable in nature whereas Pandas DataFrame is mutable.\n+\n+    In PySpark, it creates DataFrame once which cannot be changed. Instead, it should transform\n+    it to another DataFrame whereas Pandas DataFrame is mutable which directly updates the state\n+    of it. Typical example is `String` vs `StringBuilder` in Java.\n+  \n+4. PySpark operations on DataFrame tend to comply SQL.\n+\n+    It causes some subtleties comparing to Pandas, for instance, about `NaN`, `None` and `NULL`.\n+\n+There are similarities and differences between them which might bring confusion. In this document\n+these are described and illuminated by several examples.\n+\n+\n+\n+## DataFrame APIs\n+\n+This chapter describes DataFrame APIs in both PySpark and Pandas.\n+\n+\n+### Quick References\n+\n+| PySpark                                                            | Pandas                                   |\n+| ------------------------------------------------------------------ | ---------------------------------------- |\n+| `df.limit(3)`                                                      | `df.head(3)`                             |\n+| `df.filter(\"a == 1 AND b == 2\")`                                   | `df.filter(\"(df.a == 1) & (df.b == 2)\")` |\n+| `df.filter((df.a == 1) & (df.b == 2))`                             | `df[(df.a == 1) & (df.b == 2)]`          |\n+| `df.select(\"a\", \"b\")`                                              | `df[[\"a\", \"b\"]]`                         |\n+| `df.drop_duplicates()`                                             | `df.drop_duplicates()`                   |\n+| `df.sample(fraction=0.01)`                                         | `df.sample(frac=0.01)`                   |\n+| `df.groupby(\"a\").count()`                                          | `df.groupby(\"a\").size()`                 |\n+| `df.groupby(\"a\").agg({\"b\": \"sum\"})`                                | `df.groupby(\"a\").agg({\"b\": np.sum})`     |\n+| `df1.join(df2, on=\"a\")`                                            | `pandas.merge(df1, df2, on=\"a\")`         |\n+| `df1.union(df2)`                                                   | `pandas.concat(df1, df2)`                |\n+| `df = df.select(when(df[\"a\"] < 5, df[\"a\"] * 2).otherwise(df[\"a\"]))`| `df.loc[pdf['a'] < 5, 'a'] *= 2`         |\n+\n+\n+### Create DataFrame\n+\n+In order to create DataFrame in PySpark and Pandas, you can run the codes below: \n+\n+```python\n+# PySpark\n+data = zip(['Chicago', 'San Francisco', 'New York City'], range(1, 4))\n+spark.createDataFrame(list(data), [\"city\", \"rank\"])\n+```\n+\n+```python\n+# Pandas\n+data = {'city': ['Chicago', 'San Francisco', 'New York City'], 'rank': range(1, 4)}\n+pandas.DataFrame(data)\n+```\n+\n+One notable difference when creating DataFrame is that Pandas accepts the data as below:\n+\n+```\n+data = {\n+    'city': ['Chicago', 'San Francisco', 'New York City'],\n+    'rank': range(1, 4)\n+}\n+```\n+\n+and it interprets as:\n+\n+```\n+            city  rank\n+0        Chicago     1\n+1  San Francisco     2\n+2  New York City     3\n+```\n+\n+So, a dictionary that contains key and multiple values becomes DataFrame but PySpark does\n+support this. Instead, PySpark can make DataFrame from Pandas DataFrame as below:\n+\n+```\n+createDataFrame(pandas.DataFrame(data))\n+```\n+\n+For more information see \"Interaction between PySpark and Pandas\" below.\n+\n+\n+### Load DataFrame\n+\n+To load DataFrame, there are APIs, usually, `spark.read.xxx` (or `spark.read.format(\"xxx\")`)\n+for PySpark and `pandas.read_xxx` for Pandas.\n+\n+```python\n+# PySpark\n+df = spark.read.csv(\"data.csv\")\n+```\n+\n+```python\n+# Pandas\n+df = pandas.read_csv(\"data.csv\")\n+```\n+\n+There are many sources available in both PySpark and Pandas. For example, CSV, JSON and\n+Parquet are commonly supported but each supports different set of sources. There are pretty\n+different set of options availabe as well, please see the API documentation for both sides.\n+Note that, to match behaviours for PySpark to Pandas, `header=True` and `inferSchema=True`\n+options are required.\n+\n+\n+### Save DataFrame\n+\n+To load DataFrame, likewise, the APIs are `spark.read.xxx` (or `spark.read.format(\"xxx\")`) for\n+PySpark and `pandas_to_xxx` for Pandas.\n+\n+```\n+df.to_csv(\"data.csv\")\n+```\n+\n+```\n+df.write.csv(\"data.set\")\n+```\n+\n+Likewise, different set of sources are supported. Note that, to match behaviours for PySpark\n+to Pandas, `header=True` option is required.\n+\n+\n+### Inspect DataFrame\n+\n+DataFrame in both PySpark and Pandas can be expected in many ways. One of the way is to\n+`describe()` as below:\n+\n+```\n+# PySpark\n+df.describe().show()\n+```\n+\n+```\n++-------+-------------+----+\n+|summary|         city|rank|\n++-------+-------------+----+\n+|  count|            4|   4|\n+|   mean|         null| NaN|\n+| stddev|         null| NaN|\n+|    min|      Chicago| 1.0|\n+|    max|San Francisco| NaN|\n++-------+-------------+----+\n+```\n+\n+In Pandas, the same name function `describe()` can be called:\n+\n+```\n+# Pandas\n+df.describe()\n+```\n+\n+```\n+       rank\n+count   3.0\n+mean    2.0\n+std     1.0\n+min     1.0\n+25%     1.5\n+50%     2.0\n+75%     2.5\n+max     3.0\n+```\n+\n+There are some differences, for instance, in case of PySpark, `NaN` is excluded but Pandas ignore `NaN` in some statistics. \n+\n+To inspect columns, there are `dtype` API in PySpark and Pandas.\n+\n+```\n+# PySpark\n+df.dtypes\n+```\n+\n+```\n+[('city', 'string'), ('rank', 'double')]\n+```\n+\n+```\n+# Pandas\n+df.dtypes\n+```\n+\n+```\n+0     object\n+1    float64\n+dtype: object\n+```\n+\n+PySpark types are represented as Spark SQL types whereas they are NumPy types in case of Pandas.\n+\n+Note that, PySpark supports pretty printing of schema as below:\n+\n+```\n+# PySpark\n+df.printSchema()\n+```\n+\n+```\n+root\n+ |-- city: string (nullable = true)\n+ |-- rank: double (nullable = true)\n+```\n+\n+\n+### Interaction between PySpark and Pandas\n+\n+PySpark supports conversion to/from Pandas and Pandas UDFs out of the box.\n+See [PySpark Usage Guide for Pandas with Apache Arrow](https://spark.apache.org/docs/latest/sql-pyspark-pandas-with-arrow.html).\n+\n+\n+\n+## Notable Differences\n+\n+There are other notable differences for many reasons, for instance,\n+PySpark DataFrame's core is Spark SQL so some of APIs comply SQL, and this might be different\n+comparing to Pandas context which somtimes causes some subtleties. See the differences below.\n+\n+\n+### Lazy and Eager Evaluation\n+\n+PySpark DataFrame is lazy whereas Pandas DataFrame is eager. Therefore,\n+PySpark DataFrame can optimize the computation before an actual execution whereas Pandas\n+DataFrame can have each direct result immediately after each execution.\n+\n+For sintance, suppose you select columns multiple times as below:\n+  \n+```python\n+df = ...  # PySpark DataFrame\n+df = df.select(\"fieldA\", \"fieldB\")\n+df = df.select(\"fieldB\")\n+df.show()\n+```\n+\n+```python\n+pdf = ...  # Pandas DataFrame\n+pdf = pdf[[\"fieldA\", \"fieldB\"]]\n+pdf = pdf[[\"fieldA\"]]\n+```\n+  \n+PySpark DataFrame only selects `fieldB` alone for its execution when an action, `collect`, is\n+performed. Pandas DataFrame immediately applies the operation at each line. So, `fieldA` and\n+`fieldB` are selected, and then `fieldB` is selected with the updated result for each.\n+\n+\n+### Direct assignment\n+\n+PySpark does not allow the direct assignment whereas Pandas does.\n+\n+```python\n+# Pandas\n+df.city = 1\n+```\n+\n+```\n+   city  rank\n+0     1     1\n+1     1     2\n+2     1     3\n+```\n+\n+In PySpark, it should be done as:\n+\n+```\n+df.withColumn(\"city\", lit(1)).show()\n+```\n+\n+This is fundamentally because PySpark DataFrame is immutable whereas Pandas DataFrame is mutable.\n+\n+\n+### `NULL`, `None`, `NaN` and `NaT` \n+\n+`NULL`, `None`, `NaN` and `NaT` concepts sometimes bring confusions to users and developers. Here are definitions of each:\n+\n+- In Spark SQL, `NULL` represents a missing value.\n+- In Python, `None` represents a missing value.\n+- In Pandas, for float types, `NaN` represents not-a-number and also a missing value. \n+- In Pandas, for `datetime64[ns]` type, `NaT` represents a missing value.\n+\n+For `NaN` comparison, sometimes confusions happen. Usually the comparison is not only specific in Pandas but also regular Python, NumPy, etc. \n+\n+See the comparisons below:\n+\n+```python\n+>>> np.nan and True\n+True\n+>>> True and np.nan\n+nan\n+```\n+\n+```python\n+>>> np.nan and False\n+False\n+>>> False and np.nan\n+False\n+```\n+\n+```python\n+>>> np.nan or True\n+nan\n+>>> True or np.nan\n+True\n+```\n+\n+```python\n+>>> np.nan or False\n+nan\n+>>> False or np.nan\n+nan\n+```\n+\n+Python logical operators are different from SQL and some other programming languages. These comparisons are all correct as specified rules within Python. \n+\n+**Note:** Python's boolean operator is different:\n+\n+| Operation   | Result                        |\n+| ----------- | ----------------------------- |\n+| `x or y\t`   | if x is false, then y, else x |\n+| `x and y`   | if x is false, then x, else y |\n+\n+See also [Boolean Operations](https://docs.python.org/3/library/stdtypes.html?highlight=short%20circuit#boolean-operations-and-or-not) in Python. \n+\n+**Note:** `bool(np.nan)` is `True`. See also [Truth Value Testing](https://docs.python.org/3/library/stdtypes.html#truth-value-testing).\n+\n+**Note:** When you switch `np.nan` to `float(\"nan\")`, it still shows the same results. Even when you switch `np.nan` to `np.datetime64('NaT')`, it also shows the same pattern (although the result `nan` becomes `np.datetime64('NaT')` instead).\n+\n+In PySpark, `None` comparison follows `NULL` comparison in Spark SQL.\n+\n+```\n++-----+-----+---------+--------+\n+|    x|    y|(x AND y)|(x OR y)|\n++-----+-----+---------+--------+\n+| null| true|     null|    true|\n+| true| null|     null|    true|\n+| null|false|    false|    null|\n+|false| null|    false|    null|\n++-----+-----+---------+--------+\n+```\n+\n+`NaN` and `boolean` type comparison is disallowed.\n+\n+\n+### Type inference, coercion and cast\n+\n+PySpark support native Python types currently. Those Python types are mapped to Spark SQL types in order to leverage Spark SQL. So, in general PySpark behaviours follow Spark SQL's behaviours whereas Pandas tend to look after Python's own behaviours and NumPy's behaviours since both types are supported in Pandas.",
    "line": 387
  }],
  "prId": 24234
}]