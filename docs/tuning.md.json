[{
  "comments": [{
    "author": {
      "login": "rxin"
    },
    "body": "i dont' think you'd want to reference a jira ticket, since it is mutable here ... i'd just remove this line.\n",
    "commit": "6838ba6b1ca05feaa97f1a00bc9475b37e49898d",
    "createdAt": "2015-11-13T19:18:44Z",
    "diffHunk": "@@ -88,9 +88,40 @@ than the \"raw\" data inside their fields. This is due to several reasons:\n   but also pointers (typically 8 bytes each) to the next object in the list.\n * Collections of primitive types often store them as \"boxed\" objects such as `java.lang.Integer`.\n \n-This section will discuss how to determine the memory usage of your objects, and how to improve\n-it -- either by changing your data structures, or by storing data in a serialized format.\n-We will then cover tuning Spark's cache size and the Java garbage collector.\n+This section will start with an overview of memory management in Spark, then discuss specific\n+strategies the user can take to make more efficient use of memory in her application. In\n+particular, we will describe how to determine the memory usage of your objects, and how to\n+improve it -- either by changing your data structures, or by storing data in a serialized\n+format. We will then cover tuning Spark's cache size and the Java garbage collector.\n+\n+## Memory Management Overview\n+\n+Memory usage in Spark largely falls into one of two categories: execution and storage.\n+Execution memory refers to that used for computation in shuffles, joins, sorts and aggregations,\n+while storage memory refers to that used for caching and propagating internal data across the\n+cluster. In Spark, execution and storage share a unified region (M). When no execution memory is\n+used, storage can acquire all the available memory and vice versa. Execution may evict storage\n+if necessary, but only until total storage memory usage falls under a certain threshold (R).\n+Storage may not evict execution due to complexities in implementation.\n+\n+This design ensures several desirable properties. First, applications that do not use caching\n+can use the entire space for execution, obviating unnecessary disk spills. Second, applications\n+that do use caching can reserve a minimum storage space (R) where their data blocks are immune\n+to being evicted. Lastly, this approach provides reasonable out-of-the-box performance for a\n+variety of workloads without requiring user expertise of how memory is divided internally.\n+\n+Although there are two relevant configurations, the typical user should not need to adjust them\n+as the default values are applicable to most workloads:\n+\n+* `spark.memory.fraction` expresses the size of `M` as a fraction of the total JVM heap space\n+(default 0.75). This sets aside memory for internal metadata, user data structures, and\n+imprecise size estimation in the case of sparse, unusually large records.\n+* `spark.memory.storageFraction` expresses the size of `R` as a fraction of `M` (default 0.5).\n+This is the amount of storage memory immune to being evicted by execution.\n+\n+For a more detailed description of the model, see the design doc attached to the associated"
  }],
  "prId": 9676
}, {
  "comments": [{
    "author": {
      "login": "vidaha"
    },
    "body": "her -> the\n",
    "commit": "6838ba6b1ca05feaa97f1a00bc9475b37e49898d",
    "createdAt": "2015-11-13T20:11:38Z",
    "diffHunk": "@@ -88,9 +88,40 @@ than the \"raw\" data inside their fields. This is due to several reasons:\n   but also pointers (typically 8 bytes each) to the next object in the list.\n * Collections of primitive types often store them as \"boxed\" objects such as `java.lang.Integer`.\n \n-This section will discuss how to determine the memory usage of your objects, and how to improve\n-it -- either by changing your data structures, or by storing data in a serialized format.\n-We will then cover tuning Spark's cache size and the Java garbage collector.\n+This section will start with an overview of memory management in Spark, then discuss specific\n+strategies the user can take to make more efficient use of memory in her application. In"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "I'll replace this with `his/her`\n",
    "commit": "6838ba6b1ca05feaa97f1a00bc9475b37e49898d",
    "createdAt": "2015-11-13T21:15:21Z",
    "diffHunk": "@@ -88,9 +88,40 @@ than the \"raw\" data inside their fields. This is due to several reasons:\n   but also pointers (typically 8 bytes each) to the next object in the list.\n * Collections of primitive types often store them as \"boxed\" objects such as `java.lang.Integer`.\n \n-This section will discuss how to determine the memory usage of your objects, and how to improve\n-it -- either by changing your data structures, or by storing data in a serialized format.\n-We will then cover tuning Spark's cache size and the Java garbage collector.\n+This section will start with an overview of memory management in Spark, then discuss specific\n+strategies the user can take to make more efficient use of memory in her application. In"
  }],
  "prId": 9676
}, {
  "comments": [{
    "author": {
      "login": "vidaha"
    },
    "body": "Can it be explained why \".75\" is the default value?  What is the remaining .25% of memory allocated for?\n",
    "commit": "6838ba6b1ca05feaa97f1a00bc9475b37e49898d",
    "createdAt": "2015-11-13T20:15:37Z",
    "diffHunk": "@@ -88,9 +88,40 @@ than the \"raw\" data inside their fields. This is due to several reasons:\n   but also pointers (typically 8 bytes each) to the next object in the list.\n * Collections of primitive types often store them as \"boxed\" objects such as `java.lang.Integer`.\n \n-This section will discuss how to determine the memory usage of your objects, and how to improve\n-it -- either by changing your data structures, or by storing data in a serialized format.\n-We will then cover tuning Spark's cache size and the Java garbage collector.\n+This section will start with an overview of memory management in Spark, then discuss specific\n+strategies the user can take to make more efficient use of memory in her application. In\n+particular, we will describe how to determine the memory usage of your objects, and how to\n+improve it -- either by changing your data structures, or by storing data in a serialized\n+format. We will then cover tuning Spark's cache size and the Java garbage collector.\n+\n+## Memory Management Overview\n+\n+Memory usage in Spark largely falls into one of two categories: execution and storage.\n+Execution memory refers to that used for computation in shuffles, joins, sorts and aggregations,\n+while storage memory refers to that used for caching and propagating internal data across the\n+cluster. In Spark, execution and storage share a unified region (M). When no execution memory is\n+used, storage can acquire all the available memory and vice versa. Execution may evict storage\n+if necessary, but only until total storage memory usage falls under a certain threshold (R).\n+Storage may not evict execution due to complexities in implementation.\n+\n+This design ensures several desirable properties. First, applications that do not use caching\n+can use the entire space for execution, obviating unnecessary disk spills. Second, applications\n+that do use caching can reserve a minimum storage space (R) where their data blocks are immune\n+to being evicted. Lastly, this approach provides reasonable out-of-the-box performance for a\n+variety of workloads without requiring user expertise of how memory is divided internally.\n+\n+Although there are two relevant configurations, the typical user should not need to adjust them\n+as the default values are applicable to most workloads:\n+\n+* `spark.memory.fraction` expresses the size of `M` as a fraction of the total JVM heap space\n+(default 0.75). This sets aside memory for internal metadata, user data structures, and"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "the next sentence explains it. Maybe the wording's not clear\n",
    "commit": "6838ba6b1ca05feaa97f1a00bc9475b37e49898d",
    "createdAt": "2015-11-13T21:06:31Z",
    "diffHunk": "@@ -88,9 +88,40 @@ than the \"raw\" data inside their fields. This is due to several reasons:\n   but also pointers (typically 8 bytes each) to the next object in the list.\n * Collections of primitive types often store them as \"boxed\" objects such as `java.lang.Integer`.\n \n-This section will discuss how to determine the memory usage of your objects, and how to improve\n-it -- either by changing your data structures, or by storing data in a serialized format.\n-We will then cover tuning Spark's cache size and the Java garbage collector.\n+This section will start with an overview of memory management in Spark, then discuss specific\n+strategies the user can take to make more efficient use of memory in her application. In\n+particular, we will describe how to determine the memory usage of your objects, and how to\n+improve it -- either by changing your data structures, or by storing data in a serialized\n+format. We will then cover tuning Spark's cache size and the Java garbage collector.\n+\n+## Memory Management Overview\n+\n+Memory usage in Spark largely falls into one of two categories: execution and storage.\n+Execution memory refers to that used for computation in shuffles, joins, sorts and aggregations,\n+while storage memory refers to that used for caching and propagating internal data across the\n+cluster. In Spark, execution and storage share a unified region (M). When no execution memory is\n+used, storage can acquire all the available memory and vice versa. Execution may evict storage\n+if necessary, but only until total storage memory usage falls under a certain threshold (R).\n+Storage may not evict execution due to complexities in implementation.\n+\n+This design ensures several desirable properties. First, applications that do not use caching\n+can use the entire space for execution, obviating unnecessary disk spills. Second, applications\n+that do use caching can reserve a minimum storage space (R) where their data blocks are immune\n+to being evicted. Lastly, this approach provides reasonable out-of-the-box performance for a\n+variety of workloads without requiring user expertise of how memory is divided internally.\n+\n+Although there are two relevant configurations, the typical user should not need to adjust them\n+as the default values are applicable to most workloads:\n+\n+* `spark.memory.fraction` expresses the size of `M` as a fraction of the total JVM heap space\n+(default 0.75). This sets aside memory for internal metadata, user data structures, and"
  }],
  "prId": 9676
}]