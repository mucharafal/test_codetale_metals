[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "`approach.`\n",
    "commit": "895eb4f286f4938486b9bb246065031ae47705ab",
    "createdAt": "2016-01-18T20:13:48Z",
    "diffHunk": "@@ -157,7 +166,7 @@ configuring Flume agents.\n \n \tNote that each input DStream can be configured to receive data from multiple sinks.\n \n-3. **Deploying:** Package `spark-streaming-flume_{{site.SCALA_BINARY_VERSION}}` and its dependencies (except `spark-core_{{site.SCALA_BINARY_VERSION}}` and `spark-streaming_{{site.SCALA_BINARY_VERSION}}` which are provided by `spark-submit`) into the application JAR. Then use `spark-submit` to launch your application (see [Deploying section](streaming-programming-guide.html#deploying-applications) in the main programming guide).\n+3. **Deploying:** This is same as the first approach, for Scala, Java and Python."
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "Did you mean `,` -> `.`?\n",
    "commit": "895eb4f286f4938486b9bb246065031ae47705ab",
    "createdAt": "2016-01-18T20:35:11Z",
    "diffHunk": "@@ -157,7 +166,7 @@ configuring Flume agents.\n \n \tNote that each input DStream can be configured to receive data from multiple sinks.\n \n-3. **Deploying:** Package `spark-streaming-flume_{{site.SCALA_BINARY_VERSION}}` and its dependencies (except `spark-core_{{site.SCALA_BINARY_VERSION}}` and `spark-streaming_{{site.SCALA_BINARY_VERSION}}` which are provided by `spark-submit`) into the application JAR. Then use `spark-submit` to launch your application (see [Deploying section](streaming-programming-guide.html#deploying-applications) in the main programming guide).\n+3. **Deploying:** This is same as the first approach, for Scala, Java and Python."
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "yes, and remove Scala JAva Python\n",
    "commit": "895eb4f286f4938486b9bb246065031ae47705ab",
    "createdAt": "2016-01-18T21:00:57Z",
    "diffHunk": "@@ -157,7 +166,7 @@ configuring Flume agents.\n \n \tNote that each input DStream can be configured to receive data from multiple sinks.\n \n-3. **Deploying:** Package `spark-streaming-flume_{{site.SCALA_BINARY_VERSION}}` and its dependencies (except `spark-core_{{site.SCALA_BINARY_VERSION}}` and `spark-streaming_{{site.SCALA_BINARY_VERSION}}` which are provided by `spark-submit`) into the application JAR. Then use `spark-submit` to launch your application (see [Deploying section](streaming-programming-guide.html#deploying-applications) in the main programming guide).\n+3. **Deploying:** This is same as the first approach, for Scala, Java and Python."
  }, {
    "author": {
      "login": "zsxwing"
    },
    "body": "Done.\n",
    "commit": "895eb4f286f4938486b9bb246065031ae47705ab",
    "createdAt": "2016-01-18T21:11:30Z",
    "diffHunk": "@@ -157,7 +166,7 @@ configuring Flume agents.\n \n \tNote that each input DStream can be configured to receive data from multiple sinks.\n \n-3. **Deploying:** Package `spark-streaming-flume_{{site.SCALA_BINARY_VERSION}}` and its dependencies (except `spark-core_{{site.SCALA_BINARY_VERSION}}` and `spark-streaming_{{site.SCALA_BINARY_VERSION}}` which are provided by `spark-submit`) into the application JAR. Then use `spark-submit` to launch your application (see [Deploying section](streaming-programming-guide.html#deploying-applications) in the main programming guide).\n+3. **Deploying:** This is same as the first approach, for Scala, Java and Python."
  }],
  "prId": 10746
}]