[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "You're missing some punctuation at the end of almost all paragraphs you're touching.",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-26T21:04:45Z",
    "diffHunk": "@@ -125,12 +125,13 @@ consult the relevant documentation.\n ### Recommended settings for writing to object stores\n \n For object stores whose consistency model means that rename-based commits are safe\n-use the `FileOutputCommitter` v2 algorithm for performance:\n+use the `FileOutputCommitter` v2 algorithm for performance, v1 for safety"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "will review and correct",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-28T14:12:35Z",
    "diffHunk": "@@ -125,12 +125,13 @@ consult the relevant documentation.\n ### Recommended settings for writing to object stores\n \n For object stores whose consistency model means that rename-based commits are safe\n-use the `FileOutputCommitter` v2 algorithm for performance:\n+use the `FileOutputCommitter` v2 algorithm for performance, v1 for safety"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Safe",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-26T21:05:01Z",
    "diffHunk": "@@ -143,8 +144,34 @@ job failure:\n spark.hadoop.mapreduce.fileoutputcommitter.cleanup-failures.ignored true\n ```\n \n+The original v1 commit algorithm renames the output of successful tasks\n+to a job attempt directory, and then renames all the files in that directory\n+into the final destination during the job commit phase\n+\n+```\n+spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 1\n+```\n+\n+The slow performance of mimicked renames on Amazon S3 makes this algorithm\n+very, very slow. The recommended solution to this is switch to an S3 \"Zero Rename\"\n+committer (see below).\n+\n+For reference, here are the performance and safety characteristics of\n+different stores and connectors\n+\n+\n+For the other object stores, their characteristics are\n+\n+| Store         | Connector | directory rename safety | rename performance |\n+|---------------|-----------|-------------------------|--------------------|\n+| Amazon S3     | S3A       | Unsafe                  | O(data) |\n+| Azure Storage | wasb      | Safe                    | O(files) |\n+| Azure Datalake Gen 2 | abfs | Safe                  | O(1) |\n+| Google GCS    | gs        | Saf  e                  | O(1) |"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I'd just not mention this. (They're free to contribute their committer to the community to earn the mention.)",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-26T21:07:23Z",
    "diffHunk": "@@ -190,6 +217,46 @@ while they are still being written. Applications can write straight to the monit\n atomic `rename()` operation.\n Otherwise the checkpointing may be slow and potentially unreliable.\n \n+## Committing work into cloud storage safely and fast.\n+\n+As covered earlier, commit-by-rename is dangerous on any object store which\n+exhibits eventual consistency (example: S3), and often slower than classic\n+filesystem renames.\n+\n+Some object store connectors provide custom committers to commit tasks and\n+jobs without using rename. In versions of Spark built with Hadoop-3.2 or later,\n+the S3A connector for AWS S3 is such a committer.\n+\n+Instead of writing data to a temporary directory on the store for renaming,\n+these committers write the files to the final destination, but do not issue\n+the final POST command to make a large \"multi-part\" upload visible. Those\n+operations are postponed until the job commit itself. As a result, task and\n+job commit are much faster, and task failures do not affect the result. \n+\n+To switch to the S3A committers, use a version of Spark which includes the\n+Hadoop-3.1+ binaries, and switch the committers through the following\n+options.\n+\n+```\n+spark.hadoop.fs.s3a.committer.name directory\n+spark.sql.sources.commitProtocolClass org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\n+spark.sql.parquet.output.committer.class org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\n+```\n+\n+Normal dataframe write commands will then use this committer for any format\n+which does not provide custom committers itself. Supported formats include\n+orc, parquet, csv and avro.\n+\n+```python\n+mydataframe.write.format(\"parquet\").save(\"s3a://bucket/destination\")\n+```\n+\n+More details on these committers can be found in the latest Hadoop documentation.\n+\n+Amazon EMR's S3 connector also has a zero-rename committer. Consult"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "cut",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-28T14:13:38Z",
    "diffHunk": "@@ -190,6 +217,46 @@ while they are still being written. Applications can write straight to the monit\n atomic `rename()` operation.\n Otherwise the checkpointing may be slow and potentially unreliable.\n \n+## Committing work into cloud storage safely and fast.\n+\n+As covered earlier, commit-by-rename is dangerous on any object store which\n+exhibits eventual consistency (example: S3), and often slower than classic\n+filesystem renames.\n+\n+Some object store connectors provide custom committers to commit tasks and\n+jobs without using rename. In versions of Spark built with Hadoop-3.2 or later,\n+the S3A connector for AWS S3 is such a committer.\n+\n+Instead of writing data to a temporary directory on the store for renaming,\n+these committers write the files to the final destination, but do not issue\n+the final POST command to make a large \"multi-part\" upload visible. Those\n+operations are postponed until the job commit itself. As a result, task and\n+job commit are much faster, and task failures do not affect the result. \n+\n+To switch to the S3A committers, use a version of Spark which includes the\n+Hadoop-3.1+ binaries, and switch the committers through the following\n+options.\n+\n+```\n+spark.hadoop.fs.s3a.committer.name directory\n+spark.sql.sources.commitProtocolClass org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\n+spark.sql.parquet.output.committer.class org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\n+```\n+\n+Normal dataframe write commands will then use this committer for any format\n+which does not provide custom committers itself. Supported formats include\n+orc, parquet, csv and avro.\n+\n+```python\n+mydataframe.write.format(\"parquet\").save(\"s3a://bucket/destination\")\n+```\n+\n+More details on these committers can be found in the latest Hadoop documentation.\n+\n+Amazon EMR's S3 connector also has a zero-rename committer. Consult"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This paragraph sounds a little self-contradictory.\r\n\r\n- Committer will be used for any format that doesn't have a custom one\r\n- These are the supported formats.\r\n\r\nDoes that mean that formats that aren't explicitly called out are not necessarily supported or tested or what?\r\n\r\n(Removing the list of \"supported formats\" would help in avoiding the ambiguity.)",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-26T21:10:15Z",
    "diffHunk": "@@ -190,6 +217,46 @@ while they are still being written. Applications can write straight to the monit\n atomic `rename()` operation.\n Otherwise the checkpointing may be slow and potentially unreliable.\n \n+## Committing work into cloud storage safely and fast.\n+\n+As covered earlier, commit-by-rename is dangerous on any object store which\n+exhibits eventual consistency (example: S3), and often slower than classic\n+filesystem renames.\n+\n+Some object store connectors provide custom committers to commit tasks and\n+jobs without using rename. In versions of Spark built with Hadoop-3.2 or later,\n+the S3A connector for AWS S3 is such a committer.\n+\n+Instead of writing data to a temporary directory on the store for renaming,\n+these committers write the files to the final destination, but do not issue\n+the final POST command to make a large \"multi-part\" upload visible. Those\n+operations are postponed until the job commit itself. As a result, task and\n+job commit are much faster, and task failures do not affect the result. \n+\n+To switch to the S3A committers, use a version of Spark which includes the\n+Hadoop-3.1+ binaries, and switch the committers through the following\n+options.\n+\n+```\n+spark.hadoop.fs.s3a.committer.name directory\n+spark.sql.sources.commitProtocolClass org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\n+spark.sql.parquet.output.committer.class org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\n+```\n+\n+Normal dataframe write commands will then use this committer for any format"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "changed to say : \"output formats which are known to work (i.e. the ones tested)\"",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-28T14:15:28Z",
    "diffHunk": "@@ -190,6 +217,46 @@ while they are still being written. Applications can write straight to the monit\n atomic `rename()` operation.\n Otherwise the checkpointing may be slow and potentially unreliable.\n \n+## Committing work into cloud storage safely and fast.\n+\n+As covered earlier, commit-by-rename is dangerous on any object store which\n+exhibits eventual consistency (example: S3), and often slower than classic\n+filesystem renames.\n+\n+Some object store connectors provide custom committers to commit tasks and\n+jobs without using rename. In versions of Spark built with Hadoop-3.2 or later,\n+the S3A connector for AWS S3 is such a committer.\n+\n+Instead of writing data to a temporary directory on the store for renaming,\n+these committers write the files to the final destination, but do not issue\n+the final POST command to make a large \"multi-part\" upload visible. Those\n+operations are postponed until the job commit itself. As a result, task and\n+job commit are much faster, and task failures do not affect the result. \n+\n+To switch to the S3A committers, use a version of Spark which includes the\n+Hadoop-3.1+ binaries, and switch the committers through the following\n+options.\n+\n+```\n+spark.hadoop.fs.s3a.committer.name directory\n+spark.sql.sources.commitProtocolClass org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\n+spark.sql.parquet.output.committer.class org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\n+```\n+\n+Normal dataframe write commands will then use this committer for any format"
  }, {
    "author": {
      "login": "vanzin"
    },
    "body": "I think that phrasing just raises question of \"which are these formats\". If you really insist on saying something, just say \"tested with the most common formats supported by Spark\".",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-28T15:00:42Z",
    "diffHunk": "@@ -190,6 +217,46 @@ while they are still being written. Applications can write straight to the monit\n atomic `rename()` operation.\n Otherwise the checkpointing may be slow and potentially unreliable.\n \n+## Committing work into cloud storage safely and fast.\n+\n+As covered earlier, commit-by-rename is dangerous on any object store which\n+exhibits eventual consistency (example: S3), and often slower than classic\n+filesystem renames.\n+\n+Some object store connectors provide custom committers to commit tasks and\n+jobs without using rename. In versions of Spark built with Hadoop-3.2 or later,\n+the S3A connector for AWS S3 is such a committer.\n+\n+Instead of writing data to a temporary directory on the store for renaming,\n+these committers write the files to the final destination, but do not issue\n+the final POST command to make a large \"multi-part\" upload visible. Those\n+operations are postponed until the job commit itself. As a result, task and\n+job commit are much faster, and task failures do not affect the result. \n+\n+To switch to the S3A committers, use a version of Spark which includes the\n+Hadoop-3.1+ binaries, and switch the committers through the following\n+options.\n+\n+```\n+spark.hadoop.fs.s3a.committer.name directory\n+spark.sql.sources.commitProtocolClass org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\n+spark.sql.parquet.output.committer.class org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\n+```\n+\n+Normal dataframe write commands will then use this committer for any format"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "Done",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-07-31T14:37:33Z",
    "diffHunk": "@@ -190,6 +217,46 @@ while they are still being written. Applications can write straight to the monit\n atomic `rename()` operation.\n Otherwise the checkpointing may be slow and potentially unreliable.\n \n+## Committing work into cloud storage safely and fast.\n+\n+As covered earlier, commit-by-rename is dangerous on any object store which\n+exhibits eventual consistency (example: S3), and often slower than classic\n+filesystem renames.\n+\n+Some object store connectors provide custom committers to commit tasks and\n+jobs without using rename. In versions of Spark built with Hadoop-3.2 or later,\n+the S3A connector for AWS S3 is such a committer.\n+\n+Instead of writing data to a temporary directory on the store for renaming,\n+these committers write the files to the final destination, but do not issue\n+the final POST command to make a large \"multi-part\" upload visible. Those\n+operations are postponed until the job commit itself. As a result, task and\n+job commit are much faster, and task failures do not affect the result. \n+\n+To switch to the S3A committers, use a version of Spark which includes the\n+Hadoop-3.1+ binaries, and switch the committers through the following\n+options.\n+\n+```\n+spark.hadoop.fs.s3a.committer.name directory\n+spark.sql.sources.commitProtocolClass org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\n+spark.sql.parquet.output.committer.class org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\n+```\n+\n+Normal dataframe write commands will then use this committer for any format"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "missing \"have\" or something similar (which does not have its own ...)",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-28T19:58:16Z",
    "diffHunk": "@@ -190,6 +213,42 @@ while they are still being written. Applications can write straight to the monit\n atomic `rename()` operation.\n Otherwise the checkpointing may be slow and potentially unreliable.\n \n+## Committing work into cloud storage safely and fast.\n+\n+As covered earlier, commit-by-rename is dangerous on any object store which\n+exhibits eventual consistency (example: S3), and often slower than classic\n+filesystem renames.\n+\n+Some object store connectors provide custom committers to commit tasks and\n+jobs without using rename. In versions of Spark built with Hadoop-3.2 or later,\n+the S3A connector for AWS S3 is such a committer.\n+\n+Instead of writing data to a temporary directory on the store for renaming,\n+these committers write the files to the final destination, but do not issue\n+the final POST command to make a large \"multi-part\" upload visible. Those\n+operations are postponed until the job commit itself. As a result, task and\n+job commit are much faster, and task failures do not affect the result. \n+\n+To switch to the S3A committers, use a version of Spark which includes the\n+Hadoop-3.1+ binaries, and switch the committers through the following\n+options.\n+\n+```\n+spark.hadoop.fs.s3a.committer.name directory\n+spark.sql.sources.commitProtocolClass org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\n+spark.sql.parquet.output.committer.class org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\n+```\n+\n+Normal dataframe write commands will then use this committer for any format\n+which does not its own custom committer. Output formats which are known"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "added \"have\". Also capitalized ORC and CSV & the first letters of Parquet and Avro",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-07-31T14:26:23Z",
    "diffHunk": "@@ -190,6 +213,42 @@ while they are still being written. Applications can write straight to the monit\n atomic `rename()` operation.\n Otherwise the checkpointing may be slow and potentially unreliable.\n \n+## Committing work into cloud storage safely and fast.\n+\n+As covered earlier, commit-by-rename is dangerous on any object store which\n+exhibits eventual consistency (example: S3), and often slower than classic\n+filesystem renames.\n+\n+Some object store connectors provide custom committers to commit tasks and\n+jobs without using rename. In versions of Spark built with Hadoop-3.2 or later,\n+the S3A connector for AWS S3 is such a committer.\n+\n+Instead of writing data to a temporary directory on the store for renaming,\n+these committers write the files to the final destination, but do not issue\n+the final POST command to make a large \"multi-part\" upload visible. Those\n+operations are postponed until the job commit itself. As a result, task and\n+job commit are much faster, and task failures do not affect the result. \n+\n+To switch to the S3A committers, use a version of Spark which includes the\n+Hadoop-3.1+ binaries, and switch the committers through the following\n+options.\n+\n+```\n+spark.hadoop.fs.s3a.committer.name directory\n+spark.sql.sources.commitProtocolClass org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\n+spark.sql.parquet.output.committer.class org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\n+```\n+\n+Normal dataframe write commands will then use this committer for any format\n+which does not its own custom committer. Output formats which are known"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "\"Hadoop 3.1\"",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-08-01T20:16:07Z",
    "diffHunk": "@@ -190,15 +212,50 @@ while they are still being written. Applications can write straight to the monit\n atomic `rename()` operation.\n Otherwise the checkpointing may be slow and potentially unreliable.\n \n+## Committing work into cloud storage safely and fast.\n+\n+As covered earlier, commit-by-rename is dangerous on any object store which\n+exhibits eventual consistency (example: S3), and often slower than classic\n+filesystem renames.\n+\n+Some object store connectors provide custom committers to commit tasks and\n+jobs without using rename. In versions of Spark built with Hadoop-3.1 or later,"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "\"Hadoop 3.1+\"",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-08-01T20:16:52Z",
    "diffHunk": "@@ -190,15 +212,50 @@ while they are still being written. Applications can write straight to the monit\n atomic `rename()` operation.\n Otherwise the checkpointing may be slow and potentially unreliable.\n \n+## Committing work into cloud storage safely and fast.\n+\n+As covered earlier, commit-by-rename is dangerous on any object store which\n+exhibits eventual consistency (example: S3), and often slower than classic\n+filesystem renames.\n+\n+Some object store connectors provide custom committers to commit tasks and\n+jobs without using rename. In versions of Spark built with Hadoop-3.1 or later,\n+the S3A connector for AWS S3 is such a committer.\n+\n+Instead of writing data to a temporary directory on the store for renaming,\n+these committers write the files to the final destination, but do not issue\n+the final POST command to make a large \"multi-part\" upload visible. Those\n+operations are postponed until the job commit itself. As a result, task and\n+job commit are much faster, and task failures do not affect the result. \n+\n+To switch to the S3A committers, use a version of Spark which includes the\n+Hadoop-3.1+ binaries, and switch the committers through the following"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "\"3.x\"",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-08-01T20:17:30Z",
    "diffHunk": "@@ -190,15 +212,50 @@ while they are still being written. Applications can write straight to the monit\n atomic `rename()` operation.\n Otherwise the checkpointing may be slow and potentially unreliable.\n \n+## Committing work into cloud storage safely and fast.\n+\n+As covered earlier, commit-by-rename is dangerous on any object store which\n+exhibits eventual consistency (example: S3), and often slower than classic\n+filesystem renames.\n+\n+Some object store connectors provide custom committers to commit tasks and\n+jobs without using rename. In versions of Spark built with Hadoop-3.1 or later,\n+the S3A connector for AWS S3 is such a committer.\n+\n+Instead of writing data to a temporary directory on the store for renaming,\n+these committers write the files to the final destination, but do not issue\n+the final POST command to make a large \"multi-part\" upload visible. Those\n+operations are postponed until the job commit itself. As a result, task and\n+job commit are much faster, and task failures do not affect the result. \n+\n+To switch to the S3A committers, use a version of Spark which includes the\n+Hadoop-3.1+ binaries, and switch the committers through the following\n+options.\n+\n+```\n+spark.hadoop.fs.s3a.committer.name directory\n+spark.sql.sources.commitProtocolClass org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\n+spark.sql.parquet.output.committer.class org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\n+```\n+\n+It has been tested with the most common formats supported by Spark.\n+\n+```python\n+mydataframe.write.format(\"parquet\").save(\"s3a://bucket/destination\")\n+```\n+\n+More details on these committers can be found in the latest Hadoop documentation.\n+\n ## Further Reading\n \n Here is the documentation on the standard connectors both from Apache and the cloud providers.\n \n-* [OpenStack Swift](https://hadoop.apache.org/docs/current/hadoop-openstack/index.html). Hadoop 2.6+\n-* [Azure Blob Storage](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html). Since Hadoop 2.7\n-* [Azure Data Lake](https://hadoop.apache.org/docs/current/hadoop-azure-datalake/index.html). Since Hadoop 2.8\n-* [Amazon S3 via S3A and S3N](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html). Hadoop 2.6+\n+* [OpenStack Swift](https://hadoop.apache.org/docs/current/hadoop-openstack/index.html).\n+* [Azure Blob Storage and Azure Datalake Gen 2](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).\n+* [Azure Data Lake Gen 1](https://hadoop.apache.org/docs/current/hadoop-azure-datalake/index.html).\n+* [Hadoop-AWS module (Hadoop 3-.x)](https://hadoop.apache.org/docs/current3/hadoop-aws/tools/hadoop-aws/index.html)."
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "done",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-08-02T13:07:20Z",
    "diffHunk": "@@ -190,15 +212,50 @@ while they are still being written. Applications can write straight to the monit\n atomic `rename()` operation.\n Otherwise the checkpointing may be slow and potentially unreliable.\n \n+## Committing work into cloud storage safely and fast.\n+\n+As covered earlier, commit-by-rename is dangerous on any object store which\n+exhibits eventual consistency (example: S3), and often slower than classic\n+filesystem renames.\n+\n+Some object store connectors provide custom committers to commit tasks and\n+jobs without using rename. In versions of Spark built with Hadoop-3.1 or later,\n+the S3A connector for AWS S3 is such a committer.\n+\n+Instead of writing data to a temporary directory on the store for renaming,\n+these committers write the files to the final destination, but do not issue\n+the final POST command to make a large \"multi-part\" upload visible. Those\n+operations are postponed until the job commit itself. As a result, task and\n+job commit are much faster, and task failures do not affect the result. \n+\n+To switch to the S3A committers, use a version of Spark which includes the\n+Hadoop-3.1+ binaries, and switch the committers through the following\n+options.\n+\n+```\n+spark.hadoop.fs.s3a.committer.name directory\n+spark.sql.sources.commitProtocolClass org.apache.spark.internal.io.cloud.PathOutputCommitProtocol\n+spark.sql.parquet.output.committer.class org.apache.spark.internal.io.cloud.BindingParquetOutputCommitter\n+```\n+\n+It has been tested with the most common formats supported by Spark.\n+\n+```python\n+mydataframe.write.format(\"parquet\").save(\"s3a://bucket/destination\")\n+```\n+\n+More details on these committers can be found in the latest Hadoop documentation.\n+\n ## Further Reading\n \n Here is the documentation on the standard connectors both from Apache and the cloud providers.\n \n-* [OpenStack Swift](https://hadoop.apache.org/docs/current/hadoop-openstack/index.html). Hadoop 2.6+\n-* [Azure Blob Storage](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html). Since Hadoop 2.7\n-* [Azure Data Lake](https://hadoop.apache.org/docs/current/hadoop-azure-datalake/index.html). Since Hadoop 2.8\n-* [Amazon S3 via S3A and S3N](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html). Hadoop 2.6+\n+* [OpenStack Swift](https://hadoop.apache.org/docs/current/hadoop-openstack/index.html).\n+* [Azure Blob Storage and Azure Datalake Gen 2](https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html).\n+* [Azure Data Lake Gen 1](https://hadoop.apache.org/docs/current/hadoop-azure-datalake/index.html).\n+* [Hadoop-AWS module (Hadoop 3-.x)](https://hadoop.apache.org/docs/current3/hadoop-aws/tools/hadoop-aws/index.html)."
  }],
  "prId": 24970
}]