[{
  "comments": [{
    "author": {
      "login": "manishamde"
    },
    "body": "How common is it to have different number of nodes in the hidden layer? I am wondering whether there should be support for a simpler method `def train(rdd, numNodesHiddenLayers, maxNumIterations)` and perhaps even a simpler `def train(rdd)` with good default settings to help users get started.\n\n@mengxr @jkbradley Would the upcoming MLlib API feature make this suggestion moot with support for default parameters?\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-03T02:03:59Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+---\n+layout: global\n+title: Artificial Neural Networks - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Artificial Neural Networks\n+---\n+\n+# Introduction\n+\n+This document describes the MLlib's Artificial Neural Network (ANN) implementation.\n+\n+The implementation currently consist of the following files:\n+\n+* 'ArtificialNeuralNetwork.scala': implements the ANN\n+* 'ANNSuite': implements automated tests for the ANN and its gradient\n+* 'ANNDemo': a demo that approximates three functions and shows a graphical representation of\n+the result\n+\n+# Summary of usage\n+\n+The \"ArtificialNeuralNetwork\" object is used as an interface to the neural network. It is\n+called as follows:\n+\n+```\n+val annModel = ArtificialNeuralNetwork.train(rdd, hiddenLayersTopology, maxNumIterations)",
    "line": 24
  }, {
    "author": {
      "login": "avulanov"
    },
    "body": "@manishamde given the size of PR @mengxr suggested to split it into multiple PRs. There is an implementation of a classifier that is based on this artificial neural network https://github.com/avulanov/spark/tree/annclassifier. It employes `RDD[LabeledPoint]` and implements MLlib `Classifier`. Softmax output and cross-entropy error is usually used for better classification performance and they are not yet implemented. We've discussed this issue with @bgreeven and our thinking is to have interface in this PR that allows setting different error function and optimizer. Does it make sense?\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-03T04:59:02Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+---\n+layout: global\n+title: Artificial Neural Networks - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Artificial Neural Networks\n+---\n+\n+# Introduction\n+\n+This document describes the MLlib's Artificial Neural Network (ANN) implementation.\n+\n+The implementation currently consist of the following files:\n+\n+* 'ArtificialNeuralNetwork.scala': implements the ANN\n+* 'ANNSuite': implements automated tests for the ANN and its gradient\n+* 'ANNDemo': a demo that approximates three functions and shows a graphical representation of\n+the result\n+\n+# Summary of usage\n+\n+The \"ArtificialNeuralNetwork\" object is used as an interface to the neural network. It is\n+called as follows:\n+\n+```\n+val annModel = ArtificialNeuralNetwork.train(rdd, hiddenLayersTopology, maxNumIterations)",
    "line": 24
  }, {
    "author": {
      "login": "manishamde"
    },
    "body": "@avulanov Thanks for the clarification. Sounds good to me. \n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-03T07:15:37Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+---\n+layout: global\n+title: Artificial Neural Networks - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Artificial Neural Networks\n+---\n+\n+# Introduction\n+\n+This document describes the MLlib's Artificial Neural Network (ANN) implementation.\n+\n+The implementation currently consist of the following files:\n+\n+* 'ArtificialNeuralNetwork.scala': implements the ANN\n+* 'ANNSuite': implements automated tests for the ANN and its gradient\n+* 'ANNDemo': a demo that approximates three functions and shows a graphical representation of\n+the result\n+\n+# Summary of usage\n+\n+The \"ArtificialNeuralNetwork\" object is used as an interface to the neural network. It is\n+called as follows:\n+\n+```\n+val annModel = ArtificialNeuralNetwork.train(rdd, hiddenLayersTopology, maxNumIterations)",
    "line": 24
  }, {
    "author": {
      "login": "bgreeven"
    },
    "body": "@manishamde: In most cases, one hidden layer is enough. For some special functions two hidden layers are needed. This is a nice text about the choice of number of layers and number of nodes per layer:\nhttp://www.heatonresearch.com/node/707\nEspecially the number of nodes per layer depends heavily on the particular problem.\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-03T09:12:16Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+---\n+layout: global\n+title: Artificial Neural Networks - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Artificial Neural Networks\n+---\n+\n+# Introduction\n+\n+This document describes the MLlib's Artificial Neural Network (ANN) implementation.\n+\n+The implementation currently consist of the following files:\n+\n+* 'ArtificialNeuralNetwork.scala': implements the ANN\n+* 'ANNSuite': implements automated tests for the ANN and its gradient\n+* 'ANNDemo': a demo that approximates three functions and shows a graphical representation of\n+the result\n+\n+# Summary of usage\n+\n+The \"ArtificialNeuralNetwork\" object is used as an interface to the neural network. It is\n+called as follows:\n+\n+```\n+val annModel = ArtificialNeuralNetwork.train(rdd, hiddenLayersTopology, maxNumIterations)",
    "line": 24
  }, {
    "author": {
      "login": "manishamde"
    },
    "body": "@bgreeven Thanks for the clarification.\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-06T02:21:04Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+---\n+layout: global\n+title: Artificial Neural Networks - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Artificial Neural Networks\n+---\n+\n+# Introduction\n+\n+This document describes the MLlib's Artificial Neural Network (ANN) implementation.\n+\n+The implementation currently consist of the following files:\n+\n+* 'ArtificialNeuralNetwork.scala': implements the ANN\n+* 'ANNSuite': implements automated tests for the ANN and its gradient\n+* 'ANNDemo': a demo that approximates three functions and shows a graphical representation of\n+the result\n+\n+# Summary of usage\n+\n+The \"ArtificialNeuralNetwork\" object is used as an interface to the neural network. It is\n+called as follows:\n+\n+```\n+val annModel = ArtificialNeuralNetwork.train(rdd, hiddenLayersTopology, maxNumIterations)",
    "line": 24
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "@manishamde  The upcoming API may make it a bit easier, but the current API here could support default parameters via a builder pattern for parameters.  I'll take a closer look at this PR!\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-11-11T21:16:58Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+---\n+layout: global\n+title: Artificial Neural Networks - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Artificial Neural Networks\n+---\n+\n+# Introduction\n+\n+This document describes the MLlib's Artificial Neural Network (ANN) implementation.\n+\n+The implementation currently consist of the following files:\n+\n+* 'ArtificialNeuralNetwork.scala': implements the ANN\n+* 'ANNSuite': implements automated tests for the ANN and its gradient\n+* 'ANNDemo': a demo that approximates three functions and shows a graphical representation of\n+the result\n+\n+# Summary of usage\n+\n+The \"ArtificialNeuralNetwork\" object is used as an interface to the neural network. It is\n+called as follows:\n+\n+```\n+val annModel = ArtificialNeuralNetwork.train(rdd, hiddenLayersTopology, maxNumIterations)",
    "line": 24
  }],
  "prId": 1290
}, {
  "comments": [{
    "author": {
      "login": "zhangandyx"
    },
    "body": "Is it 'Stochastic' or 'Statistical' Gradient Descent?\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-12-09T23:05:06Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+---\n+layout: global\n+title: Artificial Neural Networks - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Artificial Neural Networks\n+---\n+\n+# Introduction\n+\n+This document describes the MLlib's Artificial Neural Network (ANN) implementation.\n+\n+The implementation currently consist of the following files:\n+\n+* 'ArtificialNeuralNetwork.scala': implements the ANN\n+* 'ANNSuite': implements automated tests for the ANN and its gradient\n+* 'ANNDemo': a demo that approximates three functions and shows a graphical representation of\n+the result\n+\n+# Summary of usage\n+\n+The \"ArtificialNeuralNetwork\" object is used as an interface to the neural network. It is\n+called as follows:\n+\n+```\n+val annModel = ArtificialNeuralNetwork.train(rdd, hiddenLayersTopology, maxNumIterations)\n+```\n+\n+where\n+\n+* `rdd` is an RDD of type (Vector,Vector), the first element containing the input vector and\n+the second the associated output vector.\n+* `hiddenLayersTopology` is an array of integers (Array[Int]), which contains the number of\n+nodes per hidden layer, starting with the layer that takes inputs from the input layer, and\n+finishing with the layer that outputs to the output layer. The bias nodes are not counted.\n+* `maxNumIterations` is an upper bound to the number of iterations to be performed.\n+* `ANNmodel` contains the trained ANN parameters, and can be used to calculated the ANNs\n+approximation to arbitrary input values.\n+\n+The approximations can be calculated as follows:\n+\n+```\n+val v_out = annModel.predict(v_in)\n+```\n+\n+where v_in is either a Vector or an RDD of Vectors, and v_out respectively a Vector or RDD of\n+(Vector,Vector) pairs, corresponding to input and output values.\n+\n+Further details and other calling options will be elaborated upon below.\n+\n+# Architecture and Notation\n+\n+The file ArtificialNeuralNetwork.scala implements the ANN. The following picture shows the\n+architecture of a 3-layer ANN:\n+\n+```\n+ +-------+\n+ |       |\n+ | N_0,0 |\n+ |       | \n+ +-------+        +-------+\n+                  |       |\n+ +-------+        | N_0,1 |       +-------+\n+ |       |        |       |       |       |\n+ | N_1,0 |-       +-------+     ->| N_0,2 |\n+ |       | \\ Wij1              /  |       |\n+ +-------+  --    +-------+  --   +-------+\n+               \\  |       | / Wjk2\n+     :          ->| N_1,1 |-      +-------+\n+     :            |       |       |       |\n+     :            +-------+       | N_1,2 |\n+     :                            |       |\n+     :                :           +-------+\n+     :                :\n+     :                :                :\n+     :                : \n+     :                :           +-------+\n+     :                :           |       |\n+     :                :           |N_K-1,2|\n+     :                            |       |\n+     :            +-------+       +-------+\n+     :            |       |\n+     :            |N_J-1,1|\n+                  |       |\n+ +-------+        +-------+\n+ |       | \n+ |N_I-1,0|  \n+ |       |\n+ +-------+\n+\n+ +-------+        +--------+\n+ |       |        |        |\n+ |   -1  |        |   -1   |\n+ |       |        |        |\n+ +-------+        +--------+\n+\n+INPUT LAYER      HIDDEN LAYER    OUTPUT LAYER\n+```\n+\n+The i-th node in layer l is denoted by N_{i,l}, both i and l starting with 0. The weight\n+between node i in layer l-1 and node j in layer l is denoted by Wijl. Layer 0 is the input\n+layer, whereas layer L is the output layer.\n+\n+The ANN also implements bias units. These are nodes that always output the value -1. The bias\n+units are in all layers except the output layer. They act similar to other nodes, but do not\n+have input.\n+\n+The value of node N_{j,l} is calculated  as follows:\n+\n+`$N_{j,l} = g( \\sum_{i=0}^{topology_l} W_{i,j,l)*N_{i,l-1} )$`\n+\n+Where g is the sigmoid function\n+\n+`$g(t) = \\frac{e^{\\beta t} }{1+e^{\\beta t}}$`\n+\n+# LBFGS\n+\n+MLlib's ANN implementation uses the LBFGS optimisation algorithm for training. It minimises the\n+following error function:\n+\n+`$E = \\sum_{k=0}^{K-1} (N_{k,L} - Y_k)^2$`\n+\n+where Y_k is the target output given inputs N_{0,0} ... N_{I-1,0}.\n+\n+# Implementation Details\n+\n+## The \"ArtificialNeuralNetwork\" class\n+\n+The \"ArtificialNeuralNetwork\" class has the following constructor:\n+\n+```\n+class ArtificialNeuralNetwork private(topology: Array[Int], maxNumIterations: Int,\n+convergenceTol: Double)\n+```\n+\n+* `topology` is an array of integers indicating then number of nodes per layer. For example, if\n+\"topology\" holds (3, 5, 1), it means that there are three input nodes, five nodes in a single\n+hidden layer and 1 output node.\n+* `maxNumIterations` indicates the number of iterations after which the LBFGS algorithm must\n+have stopped.\n+* `convergenceTol` indicates the acceptable error, and if reached the LBFGS algorithm will\n+stop. A lower value of \"convergenceTol\" will give a higher precision.\n+\n+## The \"ArtificialNeuralNetwork\" object\n+\n+The object \"ArtificialNeuralNetwork\" is the interface to the \"ArtificialNeuralNetwork\" class.\n+The object contains the training function. There are six different instances of the training\n+function, each for use with different parameters. All take as the first parameter the RDD\n+\"input\", which contains pairs of input and output vectors.\n+\n+In addition, there are three functions for generating random weights. Two take a fixed seed,\n+which is useful for testing if one wants to start with the same weights in every test.\n+\n+* `def train(trainingRDD: RDD[(Vector, Vector)], hiddenLayersTopology: Array[Int], maxNumIterations:\n+Int): ArtificialNeuralNetworkModel`: starts training with random initial weights, and a default\n+convergenceTol=1e-4.\n+* `def train(trainingRDD: RDD[(Vector, Vector)], model: ArtificialNeuralNetworkModel,\n+maxNumIterations: Int): ArtificialNeuralNetworkModel`: resumes training given an earlier\n+calculated model, and a default convergenceTol=1e-4.\n+* `def train(trainingRDD: RDD[(Vector,Vector)], hiddenLayersTopology: Array[Int],\n+initialWeights: Vector, maxNumIterations: Int): ArtificialNeuralNetworkModel`: Trains an ANN\n+with given initial weights, and a default convergenceTol=1e-4.\n+* `def train(trainingRDD: RDD[(Vector, Vector)], hiddenLayersTopology: Array[Int], maxNumIterations:\n+Int, convergenceTol: Double): ArtificialNeuralNetworkModel`: starts training with random\n+initial weights. Allows setting a customised \"convergenceTol\".\n+* `def train(trainingRDD: RDD[(Vector, Vector)], model: ArtificialNeuralNetworkModel,\n+maxNumIterations: Int, convergenceTol: Double): ArtificialNeuralNetworkModel`: resumes training\n+given an earlier calculated model. Allows setting a customised \"convergenceTol\".\n+* `def train(trainingRDD: RDD[(Vector,Vector)], hiddenLayersTopology: Array[Int],\n+initialWeights: Vector, maxNumIterations: Int, convergenceTol: Double): \n+ArtificialNeuralNetworkModel`: Trains an ANN with given initial weights. Allows setting a\n+customised \"convergenceTol\".\n+* `def randomWeights(trainingRDD: RDD[(Vector,Vector)], hiddenLayersTopology: Array[Int]):\n+Vector`: Generates a random weights vector.\n+*`def randomWeights(trainingRDD: RDD[(Vector,Vector)], hiddenLayersTopology: Array[Int],\n+seed: Int): Vector`: Generates a random weights vector with given seed.\n+*`def randomWeights(inputLayerSize: Int, outputLayerSize: Int, hiddenLayersTopology: Array[Int],\n+seed: Int): Vector`: Generates a random weights vector, using given random seed, input layer\n+size, hidden layers topology and output layer size.\n+\n+Notice that the \"hiddenLayersTopology\" differs from the \"topology\" array. The\n+\"hiddenLayersTopology\" does not include the number of nodes in the input and output layers. The\n+number of nodes in input and output layers is calculated from the first element of the training\n+RDD. For example, the \"topology\" array (3, 5, 7, 1) would have a \"hiddenLayersTopology\" (5, 7),\n+the values 3 and 1 are deduced from the training data. The rationale for having these different\n+arrays is that future methods may have a different mapping between input values and input nodes\n+or output values and output nodes.\n+\n+## The \"ArtificialNeuralNetworkModel\" class\n+\n+All training functions return the trained ANN using the class \"ArtificialNeuralNetworkModel\".\n+This class has the following function:\n+\n+* `predict(testData: Vector): Vector` calculates the output vector given input vector\n+\"testData\".\n+* `predict(testData: RDD[Vector]): RDD[(Vector,Vector)]` returns (input, output) vector pairs,\n+using input vector pairs in \"testData\".\n+\n+The weights used by \"predict\" come from the model.\n+\n+## Training\n+\n+We have chosen to implement the ANN with LBFGS as optimiser function. We compared it with\n+Statistical Gradient Descent. LBGFS was much faster, but in accordance is also earlier with"
  }, {
    "author": {
      "login": "bgreeven"
    },
    "body": "Stochastic of course. Changed it. Thanks!\n",
    "commit": "5de5badf32081f8bbd73166b705445b0cc37ebdd",
    "createdAt": "2014-12-10T13:56:01Z",
    "diffHunk": "@@ -0,0 +1,239 @@\n+---\n+layout: global\n+title: Artificial Neural Networks - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Artificial Neural Networks\n+---\n+\n+# Introduction\n+\n+This document describes the MLlib's Artificial Neural Network (ANN) implementation.\n+\n+The implementation currently consist of the following files:\n+\n+* 'ArtificialNeuralNetwork.scala': implements the ANN\n+* 'ANNSuite': implements automated tests for the ANN and its gradient\n+* 'ANNDemo': a demo that approximates three functions and shows a graphical representation of\n+the result\n+\n+# Summary of usage\n+\n+The \"ArtificialNeuralNetwork\" object is used as an interface to the neural network. It is\n+called as follows:\n+\n+```\n+val annModel = ArtificialNeuralNetwork.train(rdd, hiddenLayersTopology, maxNumIterations)\n+```\n+\n+where\n+\n+* `rdd` is an RDD of type (Vector,Vector), the first element containing the input vector and\n+the second the associated output vector.\n+* `hiddenLayersTopology` is an array of integers (Array[Int]), which contains the number of\n+nodes per hidden layer, starting with the layer that takes inputs from the input layer, and\n+finishing with the layer that outputs to the output layer. The bias nodes are not counted.\n+* `maxNumIterations` is an upper bound to the number of iterations to be performed.\n+* `ANNmodel` contains the trained ANN parameters, and can be used to calculated the ANNs\n+approximation to arbitrary input values.\n+\n+The approximations can be calculated as follows:\n+\n+```\n+val v_out = annModel.predict(v_in)\n+```\n+\n+where v_in is either a Vector or an RDD of Vectors, and v_out respectively a Vector or RDD of\n+(Vector,Vector) pairs, corresponding to input and output values.\n+\n+Further details and other calling options will be elaborated upon below.\n+\n+# Architecture and Notation\n+\n+The file ArtificialNeuralNetwork.scala implements the ANN. The following picture shows the\n+architecture of a 3-layer ANN:\n+\n+```\n+ +-------+\n+ |       |\n+ | N_0,0 |\n+ |       | \n+ +-------+        +-------+\n+                  |       |\n+ +-------+        | N_0,1 |       +-------+\n+ |       |        |       |       |       |\n+ | N_1,0 |-       +-------+     ->| N_0,2 |\n+ |       | \\ Wij1              /  |       |\n+ +-------+  --    +-------+  --   +-------+\n+               \\  |       | / Wjk2\n+     :          ->| N_1,1 |-      +-------+\n+     :            |       |       |       |\n+     :            +-------+       | N_1,2 |\n+     :                            |       |\n+     :                :           +-------+\n+     :                :\n+     :                :                :\n+     :                : \n+     :                :           +-------+\n+     :                :           |       |\n+     :                :           |N_K-1,2|\n+     :                            |       |\n+     :            +-------+       +-------+\n+     :            |       |\n+     :            |N_J-1,1|\n+                  |       |\n+ +-------+        +-------+\n+ |       | \n+ |N_I-1,0|  \n+ |       |\n+ +-------+\n+\n+ +-------+        +--------+\n+ |       |        |        |\n+ |   -1  |        |   -1   |\n+ |       |        |        |\n+ +-------+        +--------+\n+\n+INPUT LAYER      HIDDEN LAYER    OUTPUT LAYER\n+```\n+\n+The i-th node in layer l is denoted by N_{i,l}, both i and l starting with 0. The weight\n+between node i in layer l-1 and node j in layer l is denoted by Wijl. Layer 0 is the input\n+layer, whereas layer L is the output layer.\n+\n+The ANN also implements bias units. These are nodes that always output the value -1. The bias\n+units are in all layers except the output layer. They act similar to other nodes, but do not\n+have input.\n+\n+The value of node N_{j,l} is calculated  as follows:\n+\n+`$N_{j,l} = g( \\sum_{i=0}^{topology_l} W_{i,j,l)*N_{i,l-1} )$`\n+\n+Where g is the sigmoid function\n+\n+`$g(t) = \\frac{e^{\\beta t} }{1+e^{\\beta t}}$`\n+\n+# LBFGS\n+\n+MLlib's ANN implementation uses the LBFGS optimisation algorithm for training. It minimises the\n+following error function:\n+\n+`$E = \\sum_{k=0}^{K-1} (N_{k,L} - Y_k)^2$`\n+\n+where Y_k is the target output given inputs N_{0,0} ... N_{I-1,0}.\n+\n+# Implementation Details\n+\n+## The \"ArtificialNeuralNetwork\" class\n+\n+The \"ArtificialNeuralNetwork\" class has the following constructor:\n+\n+```\n+class ArtificialNeuralNetwork private(topology: Array[Int], maxNumIterations: Int,\n+convergenceTol: Double)\n+```\n+\n+* `topology` is an array of integers indicating then number of nodes per layer. For example, if\n+\"topology\" holds (3, 5, 1), it means that there are three input nodes, five nodes in a single\n+hidden layer and 1 output node.\n+* `maxNumIterations` indicates the number of iterations after which the LBFGS algorithm must\n+have stopped.\n+* `convergenceTol` indicates the acceptable error, and if reached the LBFGS algorithm will\n+stop. A lower value of \"convergenceTol\" will give a higher precision.\n+\n+## The \"ArtificialNeuralNetwork\" object\n+\n+The object \"ArtificialNeuralNetwork\" is the interface to the \"ArtificialNeuralNetwork\" class.\n+The object contains the training function. There are six different instances of the training\n+function, each for use with different parameters. All take as the first parameter the RDD\n+\"input\", which contains pairs of input and output vectors.\n+\n+In addition, there are three functions for generating random weights. Two take a fixed seed,\n+which is useful for testing if one wants to start with the same weights in every test.\n+\n+* `def train(trainingRDD: RDD[(Vector, Vector)], hiddenLayersTopology: Array[Int], maxNumIterations:\n+Int): ArtificialNeuralNetworkModel`: starts training with random initial weights, and a default\n+convergenceTol=1e-4.\n+* `def train(trainingRDD: RDD[(Vector, Vector)], model: ArtificialNeuralNetworkModel,\n+maxNumIterations: Int): ArtificialNeuralNetworkModel`: resumes training given an earlier\n+calculated model, and a default convergenceTol=1e-4.\n+* `def train(trainingRDD: RDD[(Vector,Vector)], hiddenLayersTopology: Array[Int],\n+initialWeights: Vector, maxNumIterations: Int): ArtificialNeuralNetworkModel`: Trains an ANN\n+with given initial weights, and a default convergenceTol=1e-4.\n+* `def train(trainingRDD: RDD[(Vector, Vector)], hiddenLayersTopology: Array[Int], maxNumIterations:\n+Int, convergenceTol: Double): ArtificialNeuralNetworkModel`: starts training with random\n+initial weights. Allows setting a customised \"convergenceTol\".\n+* `def train(trainingRDD: RDD[(Vector, Vector)], model: ArtificialNeuralNetworkModel,\n+maxNumIterations: Int, convergenceTol: Double): ArtificialNeuralNetworkModel`: resumes training\n+given an earlier calculated model. Allows setting a customised \"convergenceTol\".\n+* `def train(trainingRDD: RDD[(Vector,Vector)], hiddenLayersTopology: Array[Int],\n+initialWeights: Vector, maxNumIterations: Int, convergenceTol: Double): \n+ArtificialNeuralNetworkModel`: Trains an ANN with given initial weights. Allows setting a\n+customised \"convergenceTol\".\n+* `def randomWeights(trainingRDD: RDD[(Vector,Vector)], hiddenLayersTopology: Array[Int]):\n+Vector`: Generates a random weights vector.\n+*`def randomWeights(trainingRDD: RDD[(Vector,Vector)], hiddenLayersTopology: Array[Int],\n+seed: Int): Vector`: Generates a random weights vector with given seed.\n+*`def randomWeights(inputLayerSize: Int, outputLayerSize: Int, hiddenLayersTopology: Array[Int],\n+seed: Int): Vector`: Generates a random weights vector, using given random seed, input layer\n+size, hidden layers topology and output layer size.\n+\n+Notice that the \"hiddenLayersTopology\" differs from the \"topology\" array. The\n+\"hiddenLayersTopology\" does not include the number of nodes in the input and output layers. The\n+number of nodes in input and output layers is calculated from the first element of the training\n+RDD. For example, the \"topology\" array (3, 5, 7, 1) would have a \"hiddenLayersTopology\" (5, 7),\n+the values 3 and 1 are deduced from the training data. The rationale for having these different\n+arrays is that future methods may have a different mapping between input values and input nodes\n+or output values and output nodes.\n+\n+## The \"ArtificialNeuralNetworkModel\" class\n+\n+All training functions return the trained ANN using the class \"ArtificialNeuralNetworkModel\".\n+This class has the following function:\n+\n+* `predict(testData: Vector): Vector` calculates the output vector given input vector\n+\"testData\".\n+* `predict(testData: RDD[Vector]): RDD[(Vector,Vector)]` returns (input, output) vector pairs,\n+using input vector pairs in \"testData\".\n+\n+The weights used by \"predict\" come from the model.\n+\n+## Training\n+\n+We have chosen to implement the ANN with LBFGS as optimiser function. We compared it with\n+Statistical Gradient Descent. LBGFS was much faster, but in accordance is also earlier with"
  }],
  "prId": 1290
}]