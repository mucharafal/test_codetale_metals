[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Here I am just fixing some duplication of docs that relate to this change. There were 3 identical stanzas\n",
    "commit": "69c538333ba02fc806fe7e8f794f269936ebdc28",
    "createdAt": "2016-10-06T19:51:32Z",
    "diffHunk": "@@ -869,50 +869,27 @@ access data stored in Hive.\n Configuration of Hive is done by placing your `hive-site.xml`, `core-site.xml` (for security configuration),\n and `hdfs-site.xml` (for HDFS configuration) file in `conf/`.\n \n-<div class=\"codetabs\">\n-\n-<div data-lang=\"scala\"  markdown=\"1\">\n-\n When working with Hive, one must instantiate `SparkSession` with Hive support, including\n connectivity to a persistent Hive metastore, support for Hive serdes, and Hive user-defined functions.\n Users who do not have an existing Hive deployment can still enable Hive support. When not configured\n by the `hive-site.xml`, the context automatically creates `metastore_db` in the current directory and\n creates a directory configured by `spark.sql.warehouse.dir`, which defaults to the directory\n-`spark-warehouse` in the current directory that the spark application is started. Note that\n+`spark-warehouse` in the current directory that the Spark application is started. Note that\n the `hive.metastore.warehouse.dir` property in `hive-site.xml` is deprecated since Spark 2.0.0.\n Instead, use `spark.sql.warehouse.dir` to specify the default location of database in warehouse.\n-You may need to grant write privilege to the user who starts the spark application.\n+You may need to grant write privilege to the user who starts the Spark application.\n \n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\"  markdown=\"1\">\n {% include_example spark_hive scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala %}\n </div>\n \n <div data-lang=\"java\"  markdown=\"1\">\n-",
    "line": 27
  }],
  "prId": 15382
}]