[{
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "`$n$ (the number of features)` -> `$m$ (the number of features)`?",
    "commit": "49311332c81d398829925de429dc48a62a3bac0b",
    "createdAt": "2016-12-05T09:17:31Z",
    "diffHunk": "@@ -59,17 +59,22 @@ Given $n$ weighted observations $(w_i, a_i, b_i)$:\n \n The number of features for each observation is $m$. We use the following weighted least squares formulation:\n `\\[   \n-minimize_{x}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(a_i^T x -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n+\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n \\]`\n where $\\lambda$ is the regularization parameter, $\\delta$ is the population standard deviation of the label\n and $\\sigma_j$ is the population standard deviation of the j-th feature column.\n \n-This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve.\n-Unlike the original dataset which can only be stored in a distributed system,\n-these statistics can be loaded into memory on a single machine if the number of features is relatively small, and then we can solve the objective function through Cholesky factorization on the driver.\n+This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve. For an\n+$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $n$ (the number of features) is"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Yep, thanks!",
    "commit": "49311332c81d398829925de429dc48a62a3bac0b",
    "createdAt": "2016-12-05T10:02:33Z",
    "diffHunk": "@@ -59,17 +59,22 @@ Given $n$ weighted observations $(w_i, a_i, b_i)$:\n \n The number of features for each observation is $m$. We use the following weighted least squares formulation:\n `\\[   \n-minimize_{x}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(a_i^T x -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n+\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n \\]`\n where $\\lambda$ is the regularization parameter, $\\delta$ is the population standard deviation of the label\n and $\\sigma_j$ is the population standard deviation of the j-th feature column.\n \n-This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve.\n-Unlike the original dataset which can only be stored in a distributed system,\n-these statistics can be loaded into memory on a single machine if the number of features is relatively small, and then we can solve the objective function through Cholesky factorization on the driver.\n+This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve. For an\n+$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $n$ (the number of features) is"
  }],
  "prId": 16139
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "This formulation is out of date, we should update it to include L1/elasticNet regularization.",
    "commit": "49311332c81d398829925de429dc48a62a3bac0b",
    "createdAt": "2016-12-06T10:12:28Z",
    "diffHunk": "@@ -59,17 +59,22 @@ Given $n$ weighted observations $(w_i, a_i, b_i)$:\n \n The number of features for each observation is $m$. We use the following weighted least squares formulation:\n `\\[   \n-minimize_{x}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(a_i^T x -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n+\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Great catch. Done.",
    "commit": "49311332c81d398829925de429dc48a62a3bac0b",
    "createdAt": "2016-12-07T02:48:12Z",
    "diffHunk": "@@ -59,17 +59,22 @@ Given $n$ weighted observations $(w_i, a_i, b_i)$:\n \n The number of features for each observation is $m$. We use the following weighted least squares formulation:\n `\\[   \n-minimize_{x}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(a_i^T x -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n+\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2"
  }],
  "prId": 16139
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "```ML``` -> ```MLlib```, \"Spark ML\" is not an official name of the component.",
    "commit": "49311332c81d398829925de429dc48a62a3bac0b",
    "createdAt": "2016-12-06T10:14:44Z",
    "diffHunk": "@@ -59,17 +59,22 @@ Given $n$ weighted observations $(w_i, a_i, b_i)$:\n \n The number of features for each observation is $m$. We use the following weighted least squares formulation:\n `\\[   \n-minimize_{x}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(a_i^T x -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n+\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n \\]`\n where $\\lambda$ is the regularization parameter, $\\delta$ is the population standard deviation of the label\n and $\\sigma_j$ is the population standard deviation of the j-th feature column.\n \n-This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve.\n-Unlike the original dataset which can only be stored in a distributed system,\n-these statistics can be loaded into memory on a single machine if the number of features is relatively small, and then we can solve the objective function through Cholesky factorization on the driver.\n+This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve. For an\n+$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $m$ (the number of features) is\n+relatively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimization programs.\n \n-WeightedLeastSquares only supports L2 regularization and provides options to enable or disable regularization and standardization.\n-In order to make the normal equation approach efficient, WeightedLeastSquares requires that the number of features be no more than 4096. For larger problems, use L-BFGS instead.\n+Spark ML currently supports two types of solvers for the normal equations: Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN). Cholesky factorization"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Done.",
    "commit": "49311332c81d398829925de429dc48a62a3bac0b",
    "createdAt": "2016-12-07T02:48:23Z",
    "diffHunk": "@@ -59,17 +59,22 @@ Given $n$ weighted observations $(w_i, a_i, b_i)$:\n \n The number of features for each observation is $m$. We use the following weighted least squares formulation:\n `\\[   \n-minimize_{x}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(a_i^T x -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n+\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n \\]`\n where $\\lambda$ is the regularization parameter, $\\delta$ is the population standard deviation of the label\n and $\\sigma_j$ is the population standard deviation of the j-th feature column.\n \n-This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve.\n-Unlike the original dataset which can only be stored in a distributed system,\n-these statistics can be loaded into memory on a single machine if the number of features is relatively small, and then we can solve the objective function through Cholesky factorization on the driver.\n+This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve. For an\n+$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $m$ (the number of features) is\n+relatively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimization programs.\n \n-WeightedLeastSquares only supports L2 regularization and provides options to enable or disable regularization and standardization.\n-In order to make the normal equation approach efficient, WeightedLeastSquares requires that the number of features be no more than 4096. For larger problems, use L-BFGS instead.\n+Spark ML currently supports two types of solvers for the normal equations: Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN). Cholesky factorization"
  }],
  "prId": 16139
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "and ```GeneralizedLinearRegression``` estimator.",
    "commit": "49311332c81d398829925de429dc48a62a3bac0b",
    "createdAt": "2016-12-06T10:16:40Z",
    "diffHunk": "@@ -59,17 +59,22 @@ Given $n$ weighted observations $(w_i, a_i, b_i)$:\n \n The number of features for each observation is $m$. We use the following weighted least squares formulation:\n `\\[   \n-minimize_{x}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(a_i^T x -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n+\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n \\]`\n where $\\lambda$ is the regularization parameter, $\\delta$ is the population standard deviation of the label\n and $\\sigma_j$ is the population standard deviation of the j-th feature column.\n \n-This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve.\n-Unlike the original dataset which can only be stored in a distributed system,\n-these statistics can be loaded into memory on a single machine if the number of features is relatively small, and then we can solve the objective function through Cholesky factorization on the driver.\n+This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve. For an\n+$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $m$ (the number of features) is\n+relatively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimization programs.\n \n-WeightedLeastSquares only supports L2 regularization and provides options to enable or disable regularization and standardization.\n-In order to make the normal equation approach efficient, WeightedLeastSquares requires that the number of features be no more than 4096. For larger problems, use L-BFGS instead.\n+Spark ML currently supports two types of solvers for the normal equations: Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN). Cholesky factorization\n+depends on a positive definite covariance matrix (e.g. columns of the data matrix must be linearly independent) and will fail if this condition is violated. Quasi-Newton methods\n+are still capable of providing a reasonable solution even when the covariance matrix is not positive definite, so the normal equation solver can also fall back to \n+Quasi-Newton methods in this case. This fallback is currently always enabled for the `LinearRegression` estimator."
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Done.",
    "commit": "49311332c81d398829925de429dc48a62a3bac0b",
    "createdAt": "2016-12-07T02:48:29Z",
    "diffHunk": "@@ -59,17 +59,22 @@ Given $n$ weighted observations $(w_i, a_i, b_i)$:\n \n The number of features for each observation is $m$. We use the following weighted least squares formulation:\n `\\[   \n-minimize_{x}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(a_i^T x -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n+\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n \\]`\n where $\\lambda$ is the regularization parameter, $\\delta$ is the population standard deviation of the label\n and $\\sigma_j$ is the population standard deviation of the j-th feature column.\n \n-This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve.\n-Unlike the original dataset which can only be stored in a distributed system,\n-these statistics can be loaded into memory on a single machine if the number of features is relatively small, and then we can solve the objective function through Cholesky factorization on the driver.\n+This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve. For an\n+$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $m$ (the number of features) is\n+relatively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimization programs.\n \n-WeightedLeastSquares only supports L2 regularization and provides options to enable or disable regularization and standardization.\n-In order to make the normal equation approach efficient, WeightedLeastSquares requires that the number of features be no more than 4096. For larger problems, use L-BFGS instead.\n+Spark ML currently supports two types of solvers for the normal equations: Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN). Cholesky factorization\n+depends on a positive definite covariance matrix (e.g. columns of the data matrix must be linearly independent) and will fail if this condition is violated. Quasi-Newton methods\n+are still capable of providing a reasonable solution even when the covariance matrix is not positive definite, so the normal equation solver can also fall back to \n+Quasi-Newton methods in this case. This fallback is currently always enabled for the `LinearRegression` estimator."
  }],
  "prId": 16139
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "Adding following clarification should be more clear?\r\n* For L2 or no regularization, Cholesky solver is the default choice and will fall back to Quasi-Newton solver if the covariance matrix is not positive definite.\r\n* For L1/elasticNet regularization, Quasi-Newton solver is the default and only choice.",
    "commit": "49311332c81d398829925de429dc48a62a3bac0b",
    "createdAt": "2016-12-06T10:28:47Z",
    "diffHunk": "@@ -59,17 +59,22 @@ Given $n$ weighted observations $(w_i, a_i, b_i)$:\n \n The number of features for each observation is $m$. We use the following weighted least squares formulation:\n `\\[   \n-minimize_{x}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(a_i^T x -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n+\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n \\]`\n where $\\lambda$ is the regularization parameter, $\\delta$ is the population standard deviation of the label\n and $\\sigma_j$ is the population standard deviation of the j-th feature column.\n \n-This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve.\n-Unlike the original dataset which can only be stored in a distributed system,\n-these statistics can be loaded into memory on a single machine if the number of features is relatively small, and then we can solve the objective function through Cholesky factorization on the driver.\n+This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve. For an\n+$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $m$ (the number of features) is\n+relatively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimization programs.\n \n-WeightedLeastSquares only supports L2 regularization and provides options to enable or disable regularization and standardization.\n-In order to make the normal equation approach efficient, WeightedLeastSquares requires that the number of features be no more than 4096. For larger problems, use L-BFGS instead.\n+Spark ML currently supports two types of solvers for the normal equations: Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN). Cholesky factorization\n+depends on a positive definite covariance matrix (e.g. columns of the data matrix must be linearly independent) and will fail if this condition is violated. Quasi-Newton methods\n+are still capable of providing a reasonable solution even when the covariance matrix is not positive definite, so the normal equation solver can also fall back to \n+Quasi-Newton methods in this case. This fallback is currently always enabled for the `LinearRegression` estimator.\n+\n+`WeightedLeastSquares` supports L1, L2, and elastic-net regularization and provides options to enable or disable regularization and standardization."
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "I added a note about it. It's a bit unclear to me who the audience is here. Since WLS is private, this seems more informational than anything. So I just mentioned that L1 has no analytical solution and requires QN solver. Let me know what you think.",
    "commit": "49311332c81d398829925de429dc48a62a3bac0b",
    "createdAt": "2016-12-07T02:49:51Z",
    "diffHunk": "@@ -59,17 +59,22 @@ Given $n$ weighted observations $(w_i, a_i, b_i)$:\n \n The number of features for each observation is $m$. We use the following weighted least squares formulation:\n `\\[   \n-minimize_{x}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(a_i^T x -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n+\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n \\]`\n where $\\lambda$ is the regularization parameter, $\\delta$ is the population standard deviation of the label\n and $\\sigma_j$ is the population standard deviation of the j-th feature column.\n \n-This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve.\n-Unlike the original dataset which can only be stored in a distributed system,\n-these statistics can be loaded into memory on a single machine if the number of features is relatively small, and then we can solve the objective function through Cholesky factorization on the driver.\n+This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve. For an\n+$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $m$ (the number of features) is\n+relatively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimization programs.\n \n-WeightedLeastSquares only supports L2 regularization and provides options to enable or disable regularization and standardization.\n-In order to make the normal equation approach efficient, WeightedLeastSquares requires that the number of features be no more than 4096. For larger problems, use L-BFGS instead.\n+Spark ML currently supports two types of solvers for the normal equations: Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN). Cholesky factorization\n+depends on a positive definite covariance matrix (e.g. columns of the data matrix must be linearly independent) and will fail if this condition is violated. Quasi-Newton methods\n+are still capable of providing a reasonable solution even when the covariance matrix is not positive definite, so the normal equation solver can also fall back to \n+Quasi-Newton methods in this case. This fallback is currently always enabled for the `LinearRegression` estimator.\n+\n+`WeightedLeastSquares` supports L1, L2, and elastic-net regularization and provides options to enable or disable regularization and standardization."
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "I think the current note suffices.",
    "commit": "49311332c81d398829925de429dc48a62a3bac0b",
    "createdAt": "2016-12-08T03:17:30Z",
    "diffHunk": "@@ -59,17 +59,22 @@ Given $n$ weighted observations $(w_i, a_i, b_i)$:\n \n The number of features for each observation is $m$. We use the following weighted least squares formulation:\n `\\[   \n-minimize_{x}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(a_i^T x -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n+\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n \\]`\n where $\\lambda$ is the regularization parameter, $\\delta$ is the population standard deviation of the label\n and $\\sigma_j$ is the population standard deviation of the j-th feature column.\n \n-This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve.\n-Unlike the original dataset which can only be stored in a distributed system,\n-these statistics can be loaded into memory on a single machine if the number of features is relatively small, and then we can solve the objective function through Cholesky factorization on the driver.\n+This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve. For an\n+$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $m$ (the number of features) is\n+relatively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimization programs.\n \n-WeightedLeastSquares only supports L2 regularization and provides options to enable or disable regularization and standardization.\n-In order to make the normal equation approach efficient, WeightedLeastSquares requires that the number of features be no more than 4096. For larger problems, use L-BFGS instead.\n+Spark ML currently supports two types of solvers for the normal equations: Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN). Cholesky factorization\n+depends on a positive definite covariance matrix (e.g. columns of the data matrix must be linearly independent) and will fail if this condition is violated. Quasi-Newton methods\n+are still capable of providing a reasonable solution even when the covariance matrix is not positive definite, so the normal equation solver can also fall back to \n+Quasi-Newton methods in this case. This fallback is currently always enabled for the `LinearRegression` estimator.\n+\n+`WeightedLeastSquares` supports L1, L2, and elastic-net regularization and provides options to enable or disable regularization and standardization."
  }, {
    "author": {
      "login": "yanboliang"
    },
    "body": "The audience here is developers/contributors, and the current change is perfectly OK.",
    "commit": "49311332c81d398829925de429dc48a62a3bac0b",
    "createdAt": "2016-12-08T03:39:42Z",
    "diffHunk": "@@ -59,17 +59,22 @@ Given $n$ weighted observations $(w_i, a_i, b_i)$:\n \n The number of features for each observation is $m$. We use the following weighted least squares formulation:\n `\\[   \n-minimize_{x}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(a_i^T x -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n+\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n \\]`\n where $\\lambda$ is the regularization parameter, $\\delta$ is the population standard deviation of the label\n and $\\sigma_j$ is the population standard deviation of the j-th feature column.\n \n-This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve.\n-Unlike the original dataset which can only be stored in a distributed system,\n-these statistics can be loaded into memory on a single machine if the number of features is relatively small, and then we can solve the objective function through Cholesky factorization on the driver.\n+This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve. For an\n+$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $m$ (the number of features) is\n+relatively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimization programs.\n \n-WeightedLeastSquares only supports L2 regularization and provides options to enable or disable regularization and standardization.\n-In order to make the normal equation approach efficient, WeightedLeastSquares requires that the number of features be no more than 4096. For larger problems, use L-BFGS instead.\n+Spark ML currently supports two types of solvers for the normal equations: Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN). Cholesky factorization\n+depends on a positive definite covariance matrix (e.g. columns of the data matrix must be linearly independent) and will fail if this condition is violated. Quasi-Newton methods\n+are still capable of providing a reasonable solution even when the covariance matrix is not positive definite, so the normal equation solver can also fall back to \n+Quasi-Newton methods in this case. This fallback is currently always enabled for the `LinearRegression` estimator.\n+\n+`WeightedLeastSquares` supports L1, L2, and elastic-net regularization and provides options to enable or disable regularization and standardization."
  }],
  "prId": 16139
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "\"e.g.\" -> \"i.e.\"",
    "commit": "49311332c81d398829925de429dc48a62a3bac0b",
    "createdAt": "2016-12-08T01:00:19Z",
    "diffHunk": "@@ -59,17 +59,25 @@ Given $n$ weighted observations $(w_i, a_i, b_i)$:\n \n The number of features for each observation is $m$. We use the following weighted least squares formulation:\n `\\[   \n-minimize_{x}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(a_i^T x -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n+\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{\\lambda}{\\delta}\\left[\\frac{1}{2}(1 - \\alpha)\\sum_{j=1}^m(\\sigma_j x_j)^2 + \\alpha\\sum_{j=1}^m |\\sigma_j x_j|\\right]\n \\]`\n-where $\\lambda$ is the regularization parameter, $\\delta$ is the population standard deviation of the label\n+where $\\lambda$ is the regularization parameter, $\\alpha$ is the elastic-net mixing parameter, $\\delta$ is the population standard deviation of the label\n and $\\sigma_j$ is the population standard deviation of the j-th feature column.\n \n-This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve.\n-Unlike the original dataset which can only be stored in a distributed system,\n-these statistics can be loaded into memory on a single machine if the number of features is relatively small, and then we can solve the objective function through Cholesky factorization on the driver.\n+This objective function requires only one pass over the data to collect the statistics necessary to solve it. For an\n+$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $m$ (the number of features) is\n+relatively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimization programs.\n \n-WeightedLeastSquares only supports L2 regularization and provides options to enable or disable regularization and standardization.\n-In order to make the normal equation approach efficient, WeightedLeastSquares requires that the number of features be no more than 4096. For larger problems, use L-BFGS instead.\n+Spark MLlib currently supports two types of solvers for the normal equations: Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN). Cholesky factorization\n+depends on a positive definite covariance matrix (e.g. columns of the data matrix must be linearly independent) and will fail if this condition is violated. Quasi-Newton methods"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Done, thanks!",
    "commit": "49311332c81d398829925de429dc48a62a3bac0b",
    "createdAt": "2016-12-08T01:42:59Z",
    "diffHunk": "@@ -59,17 +59,25 @@ Given $n$ weighted observations $(w_i, a_i, b_i)$:\n \n The number of features for each observation is $m$. We use the following weighted least squares formulation:\n `\\[   \n-minimize_{x}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(a_i^T x -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{1}{2}\\frac{\\lambda}{\\delta}\\sum_{j=1}^m(\\sigma_{j} x_{j})^2\n+\\min_{\\mathbf{x}}\\frac{1}{2} \\sum_{i=1}^n \\frac{w_i(\\mathbf{a}_i^T \\mathbf{x} -b_i)^2}{\\sum_{k=1}^n w_k} + \\frac{\\lambda}{\\delta}\\left[\\frac{1}{2}(1 - \\alpha)\\sum_{j=1}^m(\\sigma_j x_j)^2 + \\alpha\\sum_{j=1}^m |\\sigma_j x_j|\\right]\n \\]`\n-where $\\lambda$ is the regularization parameter, $\\delta$ is the population standard deviation of the label\n+where $\\lambda$ is the regularization parameter, $\\alpha$ is the elastic-net mixing parameter, $\\delta$ is the population standard deviation of the label\n and $\\sigma_j$ is the population standard deviation of the j-th feature column.\n \n-This objective function has an analytic solution and it requires only one pass over the data to collect necessary statistics to solve.\n-Unlike the original dataset which can only be stored in a distributed system,\n-these statistics can be loaded into memory on a single machine if the number of features is relatively small, and then we can solve the objective function through Cholesky factorization on the driver.\n+This objective function requires only one pass over the data to collect the statistics necessary to solve it. For an\n+$n \\times m$ data matrix, these statistics require only $O(m^2)$ storage and so can be stored on a single machine when $m$ (the number of features) is\n+relatively small. We can then solve the normal equations on a single machine using local methods like direct Cholesky factorization or iterative optimization programs.\n \n-WeightedLeastSquares only supports L2 regularization and provides options to enable or disable regularization and standardization.\n-In order to make the normal equation approach efficient, WeightedLeastSquares requires that the number of features be no more than 4096. For larger problems, use L-BFGS instead.\n+Spark MLlib currently supports two types of solvers for the normal equations: Cholesky factorization and Quasi-Newton methods (L-BFGS/OWL-QN). Cholesky factorization\n+depends on a positive definite covariance matrix (e.g. columns of the data matrix must be linearly independent) and will fail if this condition is violated. Quasi-Newton methods"
  }],
  "prId": 16139
}]