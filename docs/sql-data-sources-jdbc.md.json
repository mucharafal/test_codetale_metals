[{
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "`spcify` -> `specify`",
    "commit": "17995f92bf4f7b6831f129b558669346a5eafedf",
    "createdAt": "2018-10-18T09:23:03Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+---\n+layout: global\n+title: JDBC To Other Databases\n+displayTitle: JDBC To Other Databases\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+Spark SQL also includes a data source that can read data from other databases using JDBC. This\n+functionality should be preferred over using [JdbcRDD](api/scala/index.html#org.apache.spark.rdd.JdbcRDD).\n+This is because the results are returned\n+as a DataFrame and they can easily be processed in Spark SQL or joined with other data sources.\n+The JDBC data source is also easier to use from Java or Python as it does not require the user to\n+provide a ClassTag.\n+(Note that this is different than the Spark SQL JDBC server, which allows other applications to\n+run queries using Spark SQL).\n+\n+To get started you will need to include the JDBC driver for your particular database on the\n+spark classpath. For example, to connect to postgres from the Spark Shell you would run the\n+following command:\n+\n+{% highlight bash %}\n+bin/spark-shell --driver-class-path postgresql-9.4.1207.jar --jars postgresql-9.4.1207.jar\n+{% endhighlight %}\n+\n+Tables from the remote database can be loaded as a DataFrame or Spark SQL temporary view using\n+the Data Sources API. Users can specify the JDBC connection properties in the data source options.\n+<code>user</code> and <code>password</code> are normally provided as connection properties for\n+logging into the data sources. In addition to the connection properties, Spark also supports\n+the following case-insensitive options:\n+\n+<table class=\"table\">\n+  <tr><th>Property Name</th><th>Meaning</th></tr>\n+  <tr>\n+    <td><code>url</code></td>\n+    <td>\n+      The JDBC URL to connect to. The source-specific connection properties may be specified in the URL. e.g., <code>jdbc:postgresql://localhost/test?user=fred&password=secret</code>\n+    </td>\n+  </tr>\n+\n+  <tr>\n+    <td><code>dbtable</code></td>\n+    <td>\n+      The JDBC table that should be read from or written into. Note that when using it in the read\n+      path anything that is valid in a <code>FROM</code> clause of a SQL query can be used.\n+      For example, instead of a full table you could also use a subquery in parentheses. It is not\n+      allowed to specify `dbtable` and `query` options at the same time.\n+    </td>\n+  </tr>\n+  <tr>\n+    <td><code>query</code></td>\n+    <td>\n+      A query that will be used to read data into Spark. The specified query will be parenthesized and used\n+      as a subquery in the <code>FROM</code> clause. Spark will also assign an alias to the subquery clause.\n+      As an example, spark will issue a query of the following form to the JDBC Source.<br><br>\n+      <code> SELECT &lt;columns&gt; FROM (&lt;user_specified_query&gt;) spark_gen_alias</code><br><br>\n+      Below are couple of restrictions while using this option.<br>\n+      <ol>\n+         <li> It is not allowed to specify `dbtable` and `query` options at the same time. </li>\n+         <li> It is not allowed to spcify `query` and `partitionColumn` options at the same time. When specifying"
  }],
  "prId": 22746
}]