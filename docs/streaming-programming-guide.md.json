[{
  "comments": [{
    "author": {
      "login": "gsemet"
    },
    "body": "just extra space removed at the end of the file (automatically done by Sublime)\n",
    "commit": "582c8221998d6fe055d066106e9ca43204f96bb9",
    "createdAt": "2016-09-05T14:31:10Z",
    "diffHunk": "@@ -1099,7 +1099,7 @@ joinedStream = stream1.join(stream2)\n {% endhighlight %}\n </div>\n </div>\n-Here, in each batch interval, the RDD generated by `stream1` will be joined with the RDD generated by `stream2`. You can also do `leftOuterJoin`, `rightOuterJoin`, `fullOuterJoin`. Furthermore, it is often very useful to do joins over windows of the streams. That is pretty easy as well. "
  }],
  "prId": 14830
}, {
  "comments": [{
    "author": {
      "login": "gsemet"
    },
    "body": "same\n",
    "commit": "582c8221998d6fe055d066106e9ca43204f96bb9",
    "createdAt": "2016-09-05T14:31:15Z",
    "diffHunk": "@@ -1585,7 +1585,7 @@ public class JavaRow implements java.io.Serializable {\n \n /** DataFrame operations inside your streaming program */\n \n-JavaDStream<String> words = ... "
  }],
  "prId": 14830
}, {
  "comments": [{
    "author": {
      "login": "gsemet"
    },
    "body": "replace backslash syntax with more elegant (and pep8-recommended) parenthesis syntax\n",
    "commit": "582c8221998d6fe055d066106e9ca43204f96bb9",
    "createdAt": "2016-09-05T14:31:42Z",
    "diffHunk": "@@ -1626,10 +1626,10 @@ See the full [source code]({{site.SPARK_GITHUB_URL}}/blob/v{{site.SPARK_VERSION_\n # Lazily instantiated global instance of SparkSession\n def getSparkSessionInstance(sparkConf):\n     if (\"sparkSessionSingletonInstance\" not in globals()):\n-        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n-            .builder \\\n-            .config(conf=sparkConf) \\\n-            .getOrCreate()\n+        globals()[\"sparkSessionSingletonInstance\"] = (SparkSession",
    "line": 8
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "OK. I don't feel so qualified to judge that, but take your word for it. However do you really want to indent this so much?",
    "commit": "582c8221998d6fe055d066106e9ca43204f96bb9",
    "createdAt": "2016-12-21T11:12:57Z",
    "diffHunk": "@@ -1626,10 +1626,10 @@ See the full [source code]({{site.SPARK_GITHUB_URL}}/blob/v{{site.SPARK_VERSION_\n # Lazily instantiated global instance of SparkSession\n def getSparkSessionInstance(sparkConf):\n     if (\"sparkSessionSingletonInstance\" not in globals()):\n-        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n-            .builder \\\n-            .config(conf=sparkConf) \\\n-            .getOrCreate()\n+        globals()[\"sparkSessionSingletonInstance\"] = (SparkSession",
    "line": 8
  }, {
    "author": {
      "login": "gsemet"
    },
    "body": "my point was on the use of parenthesis instead of thr backslash, which is recommended by pep8. I can keep the indentation.",
    "commit": "582c8221998d6fe055d066106e9ca43204f96bb9",
    "createdAt": "2016-12-21T11:49:04Z",
    "diffHunk": "@@ -1626,10 +1626,10 @@ See the full [source code]({{site.SPARK_GITHUB_URL}}/blob/v{{site.SPARK_VERSION_\n # Lazily instantiated global instance of SparkSession\n def getSparkSessionInstance(sparkConf):\n     if (\"sparkSessionSingletonInstance\" not in globals()):\n-        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n-            .builder \\\n-            .config(conf=sparkConf) \\\n-            .getOrCreate()\n+        globals()[\"sparkSessionSingletonInstance\"] = (SparkSession",
    "line": 8
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Maybe I am wrong. Could you maybe provide the reference? \r\n\r\n> recommended by pep8\r\n\r\nDo you refer this line?\r\n\r\n>The preferred way of wrapping long lines is by using Python's implied line continuation inside parentheses, \r\n>brackets and braces. Long lines can be broken over multiple lines by wrapping expressions in parentheses. \r\n>These should be used in preference to using a backslash for line continuation.\r\n\r\nI know the rule with binary operator follows this but I guess this case is not disallowed. I am not sure if it is worth sweeping all. They look preferred but not breaking pep8. I mean, it seems not discouraging this line break..",
    "commit": "582c8221998d6fe055d066106e9ca43204f96bb9",
    "createdAt": "2017-02-15T11:20:48Z",
    "diffHunk": "@@ -1626,10 +1626,10 @@ See the full [source code]({{site.SPARK_GITHUB_URL}}/blob/v{{site.SPARK_VERSION_\n # Lazily instantiated global instance of SparkSession\n def getSparkSessionInstance(sparkConf):\n     if (\"sparkSessionSingletonInstance\" not in globals()):\n-        globals()[\"sparkSessionSingletonInstance\"] = SparkSession \\\n-            .builder \\\n-            .config(conf=sparkConf) \\\n-            .getOrCreate()\n+        globals()[\"sparkSessionSingletonInstance\"] = (SparkSession",
    "line": 8
  }],
  "prId": 14830
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "I'd avoid just taking whitespace of the end of lines. It's not related to your change, and just makes it a little harder to see what your changes are.",
    "commit": "582c8221998d6fe055d066106e9ca43204f96bb9",
    "createdAt": "2016-12-21T11:12:28Z",
    "diffHunk": "@@ -1099,7 +1099,7 @@ joinedStream = stream1.join(stream2)\n {% endhighlight %}\n </div>\n </div>\n-Here, in each batch interval, the RDD generated by `stream1` will be joined with the RDD generated by `stream2`. You can also do `leftOuterJoin`, `rightOuterJoin`, `fullOuterJoin`. Furthermore, it is often very useful to do joins over windows of the streams. That is pretty easy as well. \n+Here, in each batch interval, the RDD generated by `stream1` will be joined with the RDD generated by `stream2`. You can also do `leftOuterJoin`, `rightOuterJoin`, `fullOuterJoin`. Furthermore, it is often very useful to do joins over windows of the streams. That is pretty easy as well."
  }, {
    "author": {
      "login": "gsemet"
    },
    "body": "indeed, this is my sublime text that does it automatically. I'll remove these changes",
    "commit": "582c8221998d6fe055d066106e9ca43204f96bb9",
    "createdAt": "2016-12-21T11:45:30Z",
    "diffHunk": "@@ -1099,7 +1099,7 @@ joinedStream = stream1.join(stream2)\n {% endhighlight %}\n </div>\n </div>\n-Here, in each batch interval, the RDD generated by `stream1` will be joined with the RDD generated by `stream2`. You can also do `leftOuterJoin`, `rightOuterJoin`, `fullOuterJoin`. Furthermore, it is often very useful to do joins over windows of the streams. That is pretty easy as well. \n+Here, in each batch interval, the RDD generated by `stream1` will be joined with the RDD generated by `stream2`. You can also do `leftOuterJoin`, `rightOuterJoin`, `fullOuterJoin`. Furthermore, it is often very useful to do joins over windows of the streams. That is pretty easy as well."
  }],
  "prId": 14830
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "What is the change here? I can't make it out.",
    "commit": "582c8221998d6fe055d066106e9ca43204f96bb9",
    "createdAt": "2016-12-21T11:13:10Z",
    "diffHunk": "@@ -2105,7 +2105,7 @@ documentation), or set the `spark.default.parallelism`\n {:.no_toc}\n The overheads of data serialization can be reduced by tuning the serialization formats. In the case of streaming, there are two types of data that are being serialized.\n \n-* **Input data**: By default, the input data received through Receivers is stored in the executors' memory with [StorageLevel.MEMORY_AND_DISK_SER_2](api/scala/index.html#org.apache.spark.storage.StorageLevel$). That is, the data is serialized into bytes to reduce GC overheads, and replicated for tolerating executor failures. Also, the data is kept first in memory, and spilled over to disk only if the memory is insufficient to hold all of the input data necessary for the streaming computation. This serialization obviously has overheads -- the receiver must deserialize the received data and re-serialize it using Spark's serialization format. \n+* **Input data**: By default, the input data received through Receivers is stored in the executors' memory with [StorageLevel.MEMORY_AND_DISK_SER_2](api/scala/index.html#org.apache.spark.storage.StorageLevel$). That is, the data is serialized into bytes to reduce GC overheads, and replicated for tolerating executor failures. Also, the data is kept first in memory, and spilled over to disk only if the memory is insufficient to hold all of the input data necessary for the streaming computation. This serialization obviously has overheads -- the receiver must deserialize the received data and re-serialize it using Spark's serialization format."
  }, {
    "author": {
      "login": "gsemet"
    },
    "body": "there is an extra space at the end of the line, github doesn't display it",
    "commit": "582c8221998d6fe055d066106e9ca43204f96bb9",
    "createdAt": "2016-12-21T11:49:51Z",
    "diffHunk": "@@ -2105,7 +2105,7 @@ documentation), or set the `spark.default.parallelism`\n {:.no_toc}\n The overheads of data serialization can be reduced by tuning the serialization formats. In the case of streaming, there are two types of data that are being serialized.\n \n-* **Input data**: By default, the input data received through Receivers is stored in the executors' memory with [StorageLevel.MEMORY_AND_DISK_SER_2](api/scala/index.html#org.apache.spark.storage.StorageLevel$). That is, the data is serialized into bytes to reduce GC overheads, and replicated for tolerating executor failures. Also, the data is kept first in memory, and spilled over to disk only if the memory is insufficient to hold all of the input data necessary for the streaming computation. This serialization obviously has overheads -- the receiver must deserialize the received data and re-serialize it using Spark's serialization format. \n+* **Input data**: By default, the input data received through Receivers is stored in the executors' memory with [StorageLevel.MEMORY_AND_DISK_SER_2](api/scala/index.html#org.apache.spark.storage.StorageLevel$). That is, the data is serialized into bytes to reduce GC overheads, and replicated for tolerating executor failures. Also, the data is kept first in memory, and spilled over to disk only if the memory is insufficient to hold all of the input data necessary for the streaming computation. This serialization obviously has overheads -- the receiver must deserialize the received data and re-serialize it using Spark's serialization format."
  }],
  "prId": 14830
}]