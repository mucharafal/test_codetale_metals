[{
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "These bracketed commands actually get swallowed when the docs are rendered. Also below the `--` get coalesced into an m-dash. It would be good to clean this up. You can render the docs locally by doing this:\n\n```\ncd docs\nSKIP_API=1 jekyll serve --watch\n```\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-26T00:43:52Z",
    "diffHunk": "@@ -50,6 +50,47 @@ The system currently supports three cluster managers:\n In addition, Spark's [EC2 launch scripts](ec2-scripts.html) make it easy to launch a standalone\n cluster on Amazon EC2.\n \n+# Launching Applications\n+\n+The recommended way to launch a compiled Spark application is through the spark-submit script (located in the\n+bin directory), which takes care of setting up the classpath with Spark and its dependencies, as well as\n+provides a layer over the different cluster managers and deploy modes that Spark supports.  It's usage is\n+\n+  spark-submit <jar> <options>"
  }],
  "prId": 86
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "I think \"deploy mode\" is a new term that this PR introduces. Would you mind adding it to the glossary below? I think it's something like:\n\n```\nDeploy mode: Distinguishes who is responsible for launching the driver. In \"cluster\" mode the driver is launched inside of the cluster. In \"client\" mode, the driver is launched outside of the cluster.\n```\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-26T00:52:58Z",
    "diffHunk": "@@ -50,6 +50,47 @@ The system currently supports three cluster managers:\n In addition, Spark's [EC2 launch scripts](ec2-scripts.html) make it easy to launch a standalone\n cluster on Amazon EC2.\n \n+# Launching Applications\n+\n+The recommended way to launch a compiled Spark application is through the spark-submit script (located in the\n+bin directory), which takes care of setting up the classpath with Spark and its dependencies, as well as\n+provides a layer over the different cluster managers and deploy modes that Spark supports.  It's usage is\n+\n+  spark-submit <jar> <options>\n+\n+Where options are any of:\n+\n+- **--class** - The main class to run.\n+- **--master** - The URL of the cluster manager master, e.g. spark://host:port, mesos://host:port, yarn,\n+  or local.\n+- **--deploy-mode** - \"client\" to run the driver in the client process or \"cluster\" to run the driver in\n+  a process on the cluster.  For Mesos, only \"client\" is supported.\n+- **--executor-memory** - Memory per executor (e.g. 1000M, 2G).\n+- **--executor-cores** - Number of cores per executor.\n+- **--driver-memory** - Memory for driver (e.g. 1000M, 2G)\n+- **--name** - Name of the application.\n+- **--arg** - Argument to be passed to the application's main class. This option can be specified\n+  multiple times to pass multiple arguments.\n+- **--jars** - A comma-separated list of local jars to include on the driver classpath and that\n+  SparkContext.addJar will work with. Doesn't work on standalone with 'cluster' deploy mode.\n+\n+The following currently only work for Spark standalone with cluster deploy mode:\n+- **--driver-cores** - Cores for driver (Default: 1).\n+- **--supervise** - If given, restarts the driver on failure.\n+\n+The following only works for Spark standalone and Mesos only:\n+-  **--total-executor-cores** - Total cores for all executors.\n+\n+The following currently only work for YARN:\n+\n+- **--queue** - The YARN queue to place the application in.\n+- **--files** - Comma separated list of files to be placed next to all executors\n+- **--archives** - Comma separated list of archives to be extracted next to all executors\n+- **--num-executors** - Number of executors to start.\n+\n+The master and deploy mode can also be set with the MASTER and DEPLOY_MODE environment variables."
  }],
  "prId": 86
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Would it make sense to update the documentation for YARN and the Standalone cluster to use this script. Also it would be a good test that we've covered all the bases. I could see maybe holding off on the Standalone cluster since it has some other functionality (e.g. killing), but what about the YARN one?\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-26T00:56:46Z",
    "diffHunk": "@@ -50,6 +50,47 @@ The system currently supports three cluster managers:\n In addition, Spark's [EC2 launch scripts](ec2-scripts.html) make it easy to launch a standalone\n cluster on Amazon EC2.\n \n+# Launching Applications",
    "line": 4
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "The YARN and Standalone docs say \"The spark-submit script described in the [cluster mode overview](cluster-overview.html) provides the most straightforward way to submit a compiled Spark application to the cluster\".  Do you think the full doc for it should be replicated with them as well?\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-26T16:45:06Z",
    "diffHunk": "@@ -50,6 +50,47 @@ The system currently supports three cluster managers:\n In addition, Spark's [EC2 launch scripts](ec2-scripts.html) make it easy to launch a standalone\n cluster on Amazon EC2.\n \n+# Launching Applications",
    "line": 4
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "Alright let's punt this to a broader doc clean-up for 1.0 which we can do during the QA phase. I think that ideally yes, we should replace all of the mentions of the other clients with this.\n",
    "commit": "d428d857971866aa5c8f75970a9655071bf016fd",
    "createdAt": "2014-03-29T21:30:04Z",
    "diffHunk": "@@ -50,6 +50,47 @@ The system currently supports three cluster managers:\n In addition, Spark's [EC2 launch scripts](ec2-scripts.html) make it easy to launch a standalone\n cluster on Amazon EC2.\n \n+# Launching Applications",
    "line": 4
  }],
  "prId": 86
}]