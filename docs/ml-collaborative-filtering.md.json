[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Worth giving \"ratings\" as the canonical example of explicit feedback?\n",
    "commit": "9b351e914e87012298ab773d6b76ec019a735b6f",
    "createdAt": "2016-02-12T10:57:42Z",
    "diffHunk": "@@ -0,0 +1,148 @@\n+---\n+layout: global\n+title: Collaborative Filtering - spark.ml\n+displayTitle: Collaborative Filtering - spark.ml\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+## Collaborative filtering \n+\n+[Collaborative filtering](http://en.wikipedia.org/wiki/Recommender_system#Collaborative_filtering)\n+is commonly used for recommender systems.  These techniques aim to fill in the\n+missing entries of a user-item association matrix.  `spark.ml` currently supports\n+model-based collaborative filtering, in which users and products are described\n+by a small set of latent factors that can be used to predict missing entries.\n+`spark.ml` uses the [alternating least squares\n+(ALS)](http://dl.acm.org/citation.cfm?id=1608614)\n+algorithm to learn these latent factors. The implementation in `spark.ml` has the\n+following parameters:\n+\n+* *numBlocks* is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n+* *rank* is the number of latent factors in the model (defaults to 10).\n+* *maxIter* is the maximum number of iterations to run (defaults to 10).\n+* *regParam* specifies the regularization parameter in ALS (defaults to 1.0).\n+* *implicitPrefs* specifies whether to use the *explicit feedback* ALS variant or one adapted for\n+  *implicit feedback* data (defaults to `false` which means using *explicit feedback*).\n+* *alpha* is a parameter applicable to the implicit feedback variant of ALS that governs the\n+  *baseline* confidence in preference observations (defaults to 1.0).\n+* *nonnegative* specifies whether or not to use nonnegative constraints for least squares (defaults to `false`).\n+\n+### Explicit vs. implicit feedback\n+\n+The standard approach to matrix factorization based collaborative filtering treats \n+the entries in the user-item matrix as *explicit* preferences given by the user to the item."
  }],
  "prId": 10411
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "This might just be my own way of wording it, but the input is construed as some kind of _strength_ value in implicit data. It's inherently count-like (e.g. additive) which is how it differs from ratings. The idea of confidence is pretty much an implementation detail. I would not say that \"ratings are related to..\" anything in this model; there are no rating-like quantities. It's not predicting the strength of a preference, really, but how much it's likely to exist.\n",
    "commit": "9b351e914e87012298ab773d6b76ec019a735b6f",
    "createdAt": "2016-02-12T10:59:50Z",
    "diffHunk": "@@ -0,0 +1,148 @@\n+---\n+layout: global\n+title: Collaborative Filtering - spark.ml\n+displayTitle: Collaborative Filtering - spark.ml\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+## Collaborative filtering \n+\n+[Collaborative filtering](http://en.wikipedia.org/wiki/Recommender_system#Collaborative_filtering)\n+is commonly used for recommender systems.  These techniques aim to fill in the\n+missing entries of a user-item association matrix.  `spark.ml` currently supports\n+model-based collaborative filtering, in which users and products are described\n+by a small set of latent factors that can be used to predict missing entries.\n+`spark.ml` uses the [alternating least squares\n+(ALS)](http://dl.acm.org/citation.cfm?id=1608614)\n+algorithm to learn these latent factors. The implementation in `spark.ml` has the\n+following parameters:\n+\n+* *numBlocks* is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n+* *rank* is the number of latent factors in the model (defaults to 10).\n+* *maxIter* is the maximum number of iterations to run (defaults to 10).\n+* *regParam* specifies the regularization parameter in ALS (defaults to 1.0).\n+* *implicitPrefs* specifies whether to use the *explicit feedback* ALS variant or one adapted for\n+  *implicit feedback* data (defaults to `false` which means using *explicit feedback*).\n+* *alpha* is a parameter applicable to the implicit feedback variant of ALS that governs the\n+  *baseline* confidence in preference observations (defaults to 1.0).\n+* *nonnegative* specifies whether or not to use nonnegative constraints for least squares (defaults to `false`).\n+\n+### Explicit vs. implicit feedback\n+\n+The standard approach to matrix factorization based collaborative filtering treats \n+the entries in the user-item matrix as *explicit* preferences given by the user to the item.\n+\n+It is common in many real-world use cases to only have access to *implicit feedback* (e.g. views,\n+clicks, purchases, likes, shares etc.). The approach used in `spark.ml` to deal with such data is taken\n+from\n+[Collaborative Filtering for Implicit Feedback Datasets](http://dx.doi.org/10.1109/ICDM.2008.22).\n+Essentially instead of trying to model the matrix of ratings directly, this approach treats the data\n+as a combination of binary preferences and *confidence values*. The ratings are then related to the"
  }],
  "prId": 10411
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: \"... dataset, so that we can ...\"\n",
    "commit": "9b351e914e87012298ab773d6b76ec019a735b6f",
    "createdAt": "2016-02-12T11:00:22Z",
    "diffHunk": "@@ -0,0 +1,148 @@\n+---\n+layout: global\n+title: Collaborative Filtering - spark.ml\n+displayTitle: Collaborative Filtering - spark.ml\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+## Collaborative filtering \n+\n+[Collaborative filtering](http://en.wikipedia.org/wiki/Recommender_system#Collaborative_filtering)\n+is commonly used for recommender systems.  These techniques aim to fill in the\n+missing entries of a user-item association matrix.  `spark.ml` currently supports\n+model-based collaborative filtering, in which users and products are described\n+by a small set of latent factors that can be used to predict missing entries.\n+`spark.ml` uses the [alternating least squares\n+(ALS)](http://dl.acm.org/citation.cfm?id=1608614)\n+algorithm to learn these latent factors. The implementation in `spark.ml` has the\n+following parameters:\n+\n+* *numBlocks* is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n+* *rank* is the number of latent factors in the model (defaults to 10).\n+* *maxIter* is the maximum number of iterations to run (defaults to 10).\n+* *regParam* specifies the regularization parameter in ALS (defaults to 1.0).\n+* *implicitPrefs* specifies whether to use the *explicit feedback* ALS variant or one adapted for\n+  *implicit feedback* data (defaults to `false` which means using *explicit feedback*).\n+* *alpha* is a parameter applicable to the implicit feedback variant of ALS that governs the\n+  *baseline* confidence in preference observations (defaults to 1.0).\n+* *nonnegative* specifies whether or not to use nonnegative constraints for least squares (defaults to `false`).\n+\n+### Explicit vs. implicit feedback\n+\n+The standard approach to matrix factorization based collaborative filtering treats \n+the entries in the user-item matrix as *explicit* preferences given by the user to the item.\n+\n+It is common in many real-world use cases to only have access to *implicit feedback* (e.g. views,\n+clicks, purchases, likes, shares etc.). The approach used in `spark.ml` to deal with such data is taken\n+from\n+[Collaborative Filtering for Implicit Feedback Datasets](http://dx.doi.org/10.1109/ICDM.2008.22).\n+Essentially instead of trying to model the matrix of ratings directly, this approach treats the data\n+as a combination of binary preferences and *confidence values*. The ratings are then related to the\n+level of confidence in observed user preferences, rather than explicit ratings given to items.  The\n+model then tries to find latent factors that can be used to predict the expected preference of a\n+user for an item.\n+\n+### Scaling of the regularization parameter\n+\n+We scale the regularization parameter `regParam` in solving each least squares problem by\n+the number of ratings the user generated in updating user factors,\n+or the number of ratings the product received in updating product factors.\n+This approach is named \"ALS-WR\" and discussed in the paper\n+\"[Large-Scale Parallel Collaborative Filtering for the Netflix Prize](http://dx.doi.org/10.1007/978-3-540-68880-8_32)\".\n+It makes `regParam` less dependent on the scale of the dataset.\n+So we can apply the best parameter learned from a sampled subset to the full dataset"
  }],
  "prId": 10411
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit: you changed e.g. to i.e. below. Either is arguably fine but keep it consistent\n",
    "commit": "9b351e914e87012298ab773d6b76ec019a735b6f",
    "createdAt": "2016-02-12T11:00:52Z",
    "diffHunk": "@@ -0,0 +1,148 @@\n+---\n+layout: global\n+title: Collaborative Filtering - spark.ml\n+displayTitle: Collaborative Filtering - spark.ml\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+## Collaborative filtering \n+\n+[Collaborative filtering](http://en.wikipedia.org/wiki/Recommender_system#Collaborative_filtering)\n+is commonly used for recommender systems.  These techniques aim to fill in the\n+missing entries of a user-item association matrix.  `spark.ml` currently supports\n+model-based collaborative filtering, in which users and products are described\n+by a small set of latent factors that can be used to predict missing entries.\n+`spark.ml` uses the [alternating least squares\n+(ALS)](http://dl.acm.org/citation.cfm?id=1608614)\n+algorithm to learn these latent factors. The implementation in `spark.ml` has the\n+following parameters:\n+\n+* *numBlocks* is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n+* *rank* is the number of latent factors in the model (defaults to 10).\n+* *maxIter* is the maximum number of iterations to run (defaults to 10).\n+* *regParam* specifies the regularization parameter in ALS (defaults to 1.0).\n+* *implicitPrefs* specifies whether to use the *explicit feedback* ALS variant or one adapted for\n+  *implicit feedback* data (defaults to `false` which means using *explicit feedback*).\n+* *alpha* is a parameter applicable to the implicit feedback variant of ALS that governs the\n+  *baseline* confidence in preference observations (defaults to 1.0).\n+* *nonnegative* specifies whether or not to use nonnegative constraints for least squares (defaults to `false`).\n+\n+### Explicit vs. implicit feedback\n+\n+The standard approach to matrix factorization based collaborative filtering treats \n+the entries in the user-item matrix as *explicit* preferences given by the user to the item.\n+\n+It is common in many real-world use cases to only have access to *implicit feedback* (e.g. views,\n+clicks, purchases, likes, shares etc.). The approach used in `spark.ml` to deal with such data is taken\n+from\n+[Collaborative Filtering for Implicit Feedback Datasets](http://dx.doi.org/10.1109/ICDM.2008.22).\n+Essentially instead of trying to model the matrix of ratings directly, this approach treats the data\n+as a combination of binary preferences and *confidence values*. The ratings are then related to the\n+level of confidence in observed user preferences, rather than explicit ratings given to items.  The\n+model then tries to find latent factors that can be used to predict the expected preference of a\n+user for an item.\n+\n+### Scaling of the regularization parameter\n+\n+We scale the regularization parameter `regParam` in solving each least squares problem by\n+the number of ratings the user generated in updating user factors,\n+or the number of ratings the product received in updating product factors.\n+This approach is named \"ALS-WR\" and discussed in the paper\n+\"[Large-Scale Parallel Collaborative Filtering for the Netflix Prize](http://dx.doi.org/10.1007/978-3-540-68880-8_32)\".\n+It makes `regParam` less dependent on the scale of the dataset.\n+So we can apply the best parameter learned from a sampled subset to the full dataset\n+and expect similar performance.\n+\n+## Examples\n+\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+In the following example, we load rating data from the\n+[MovieLens dataset](http://grouplens.org/datasets/movielens/), each row\n+consisting of a user, a movie, a rating and a timestamp.\n+We then train an ALS model which assumes, by default, that the ratings are\n+explicit (`implicitPrefs` is `false`).\n+We evaluate the recommendation model by measuring the root-mean-square error of\n+rating prediction.\n+\n+Refer to the [`ALS` Scala docs](api/scala/index.html#org.apache.spark.ml.recommendation.ALS)\n+for more details on the API.\n+\n+{% include_example scala/org/apache/spark/examples/ml/ALSExample.scala %}\n+\n+If the rating matrix is derived from another source of information (e.g. it is"
  }],
  "prId": 10411
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Do people need to download this now? which file?\n",
    "commit": "9b351e914e87012298ab773d6b76ec019a735b6f",
    "createdAt": "2016-02-12T11:01:14Z",
    "diffHunk": "@@ -0,0 +1,148 @@\n+---\n+layout: global\n+title: Collaborative Filtering - spark.ml\n+displayTitle: Collaborative Filtering - spark.ml\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+## Collaborative filtering \n+\n+[Collaborative filtering](http://en.wikipedia.org/wiki/Recommender_system#Collaborative_filtering)\n+is commonly used for recommender systems.  These techniques aim to fill in the\n+missing entries of a user-item association matrix.  `spark.ml` currently supports\n+model-based collaborative filtering, in which users and products are described\n+by a small set of latent factors that can be used to predict missing entries.\n+`spark.ml` uses the [alternating least squares\n+(ALS)](http://dl.acm.org/citation.cfm?id=1608614)\n+algorithm to learn these latent factors. The implementation in `spark.ml` has the\n+following parameters:\n+\n+* *numBlocks* is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n+* *rank* is the number of latent factors in the model (defaults to 10).\n+* *maxIter* is the maximum number of iterations to run (defaults to 10).\n+* *regParam* specifies the regularization parameter in ALS (defaults to 1.0).\n+* *implicitPrefs* specifies whether to use the *explicit feedback* ALS variant or one adapted for\n+  *implicit feedback* data (defaults to `false` which means using *explicit feedback*).\n+* *alpha* is a parameter applicable to the implicit feedback variant of ALS that governs the\n+  *baseline* confidence in preference observations (defaults to 1.0).\n+* *nonnegative* specifies whether or not to use nonnegative constraints for least squares (defaults to `false`).\n+\n+### Explicit vs. implicit feedback\n+\n+The standard approach to matrix factorization based collaborative filtering treats \n+the entries in the user-item matrix as *explicit* preferences given by the user to the item.\n+\n+It is common in many real-world use cases to only have access to *implicit feedback* (e.g. views,\n+clicks, purchases, likes, shares etc.). The approach used in `spark.ml` to deal with such data is taken\n+from\n+[Collaborative Filtering for Implicit Feedback Datasets](http://dx.doi.org/10.1109/ICDM.2008.22).\n+Essentially instead of trying to model the matrix of ratings directly, this approach treats the data\n+as a combination of binary preferences and *confidence values*. The ratings are then related to the\n+level of confidence in observed user preferences, rather than explicit ratings given to items.  The\n+model then tries to find latent factors that can be used to predict the expected preference of a\n+user for an item.\n+\n+### Scaling of the regularization parameter\n+\n+We scale the regularization parameter `regParam` in solving each least squares problem by\n+the number of ratings the user generated in updating user factors,\n+or the number of ratings the product received in updating product factors.\n+This approach is named \"ALS-WR\" and discussed in the paper\n+\"[Large-Scale Parallel Collaborative Filtering for the Netflix Prize](http://dx.doi.org/10.1007/978-3-540-68880-8_32)\".\n+It makes `regParam` less dependent on the scale of the dataset.\n+So we can apply the best parameter learned from a sampled subset to the full dataset\n+and expect similar performance.\n+\n+## Examples\n+\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+In the following example, we load rating data from the\n+[MovieLens dataset](http://grouplens.org/datasets/movielens/), each row",
    "line": 64
  }, {
    "author": {
      "login": "BenFradet"
    },
    "body": "Nope, it's in the `data` folder, it's just to tell people where we got the dataset from.\n",
    "commit": "9b351e914e87012298ab773d6b76ec019a735b6f",
    "createdAt": "2016-02-12T13:14:18Z",
    "diffHunk": "@@ -0,0 +1,148 @@\n+---\n+layout: global\n+title: Collaborative Filtering - spark.ml\n+displayTitle: Collaborative Filtering - spark.ml\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+## Collaborative filtering \n+\n+[Collaborative filtering](http://en.wikipedia.org/wiki/Recommender_system#Collaborative_filtering)\n+is commonly used for recommender systems.  These techniques aim to fill in the\n+missing entries of a user-item association matrix.  `spark.ml` currently supports\n+model-based collaborative filtering, in which users and products are described\n+by a small set of latent factors that can be used to predict missing entries.\n+`spark.ml` uses the [alternating least squares\n+(ALS)](http://dl.acm.org/citation.cfm?id=1608614)\n+algorithm to learn these latent factors. The implementation in `spark.ml` has the\n+following parameters:\n+\n+* *numBlocks* is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n+* *rank* is the number of latent factors in the model (defaults to 10).\n+* *maxIter* is the maximum number of iterations to run (defaults to 10).\n+* *regParam* specifies the regularization parameter in ALS (defaults to 1.0).\n+* *implicitPrefs* specifies whether to use the *explicit feedback* ALS variant or one adapted for\n+  *implicit feedback* data (defaults to `false` which means using *explicit feedback*).\n+* *alpha* is a parameter applicable to the implicit feedback variant of ALS that governs the\n+  *baseline* confidence in preference observations (defaults to 1.0).\n+* *nonnegative* specifies whether or not to use nonnegative constraints for least squares (defaults to `false`).\n+\n+### Explicit vs. implicit feedback\n+\n+The standard approach to matrix factorization based collaborative filtering treats \n+the entries in the user-item matrix as *explicit* preferences given by the user to the item.\n+\n+It is common in many real-world use cases to only have access to *implicit feedback* (e.g. views,\n+clicks, purchases, likes, shares etc.). The approach used in `spark.ml` to deal with such data is taken\n+from\n+[Collaborative Filtering for Implicit Feedback Datasets](http://dx.doi.org/10.1109/ICDM.2008.22).\n+Essentially instead of trying to model the matrix of ratings directly, this approach treats the data\n+as a combination of binary preferences and *confidence values*. The ratings are then related to the\n+level of confidence in observed user preferences, rather than explicit ratings given to items.  The\n+model then tries to find latent factors that can be used to predict the expected preference of a\n+user for an item.\n+\n+### Scaling of the regularization parameter\n+\n+We scale the regularization parameter `regParam` in solving each least squares problem by\n+the number of ratings the user generated in updating user factors,\n+or the number of ratings the product received in updating product factors.\n+This approach is named \"ALS-WR\" and discussed in the paper\n+\"[Large-Scale Parallel Collaborative Filtering for the Netflix Prize](http://dx.doi.org/10.1007/978-3-540-68880-8_32)\".\n+It makes `regParam` less dependent on the scale of the dataset.\n+So we can apply the best parameter learned from a sampled subset to the full dataset\n+and expect similar performance.\n+\n+## Examples\n+\n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+In the following example, we load rating data from the\n+[MovieLens dataset](http://grouplens.org/datasets/movielens/), each row",
    "line": 64
  }],
  "prId": 10411
}, {
  "comments": [{
    "author": {
      "login": "BenFradet"
    },
    "body": "@srowen tried to take your remarks into account, I don't know if it's clearer now though.\n",
    "commit": "9b351e914e87012298ab773d6b76ec019a735b6f",
    "createdAt": "2016-02-13T17:18:38Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+---\n+layout: global\n+title: Collaborative Filtering - spark.ml\n+displayTitle: Collaborative Filtering - spark.ml\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+## Collaborative filtering \n+\n+[Collaborative filtering](http://en.wikipedia.org/wiki/Recommender_system#Collaborative_filtering)\n+is commonly used for recommender systems.  These techniques aim to fill in the\n+missing entries of a user-item association matrix.  `spark.ml` currently supports\n+model-based collaborative filtering, in which users and products are described\n+by a small set of latent factors that can be used to predict missing entries.\n+`spark.ml` uses the [alternating least squares\n+(ALS)](http://dl.acm.org/citation.cfm?id=1608614)\n+algorithm to learn these latent factors. The implementation in `spark.ml` has the\n+following parameters:\n+\n+* *numBlocks* is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n+* *rank* is the number of latent factors in the model (defaults to 10).\n+* *maxIter* is the maximum number of iterations to run (defaults to 10).\n+* *regParam* specifies the regularization parameter in ALS (defaults to 1.0).\n+* *implicitPrefs* specifies whether to use the *explicit feedback* ALS variant or one adapted for\n+  *implicit feedback* data (defaults to `false` which means using *explicit feedback*).\n+* *alpha* is a parameter applicable to the implicit feedback variant of ALS that governs the\n+  *baseline* confidence in preference observations (defaults to 1.0).\n+* *nonnegative* specifies whether or not to use nonnegative constraints for least squares (defaults to `false`).\n+\n+### Explicit vs. implicit feedback\n+\n+The standard approach to matrix factorization based collaborative filtering treats \n+the entries in the user-item matrix as *explicit* preferences given by the user to the item.\n+For example, users giving ratings to movies.\n+\n+It is common in many real-world use cases to only have access to *implicit feedback* (e.g. views, \n+clicks, purchases, likes, shares etc.). The approach used in `spark.mllib` to deal with such data is taken\n+from [Collaborative Filtering for Implicit Feedback Datasets](http://dx.doi.org/10.1109/ICDM.2008.22).\n+Essentially, instead of trying to model the matrix of ratings directly, this approach treats the data"
  }],
  "prId": 10411
}]