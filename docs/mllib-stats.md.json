[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Maybe we should mention `sampleByKey` first and let users know this doesn't give the exact sample size.\n",
    "commit": "a54a855ee9f6ee1cfd7da2ea5f26a9f7a2d1eb81",
    "createdAt": "2014-08-26T17:21:30Z",
    "diffHunk": "@@ -99,69 +180,277 @@ v = u.map(lambda x: 1.0 + 2.0 * x)\n \n </div>\n \n-## Stratified Sampling \n+## Correlations calculation\n \n-## Summary Statistics \n+Calculating the correlation between two series of data is a common operation in Statistics. In MLlib\n+we provide the flexibility to calculate pairwise correlations among many series. The supported \n+correlation methods are currently Pearson's and Spearman's correlation.\n+ \n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+[`Statistics`](api/scala/index.html#org.apache.spark.mllib.stat.Statistics$) provides methods to \n+calculate correlations between series. Depending on the type of input, two `RDD[Double]`s or \n+an `RDD[Vector]`, the output will be a `Double` or the correlation `Matrix` respectively.\n \n-### Multivariate summary statistics\n+{% highlight scala %}\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.mllib.stat.Statistics\n+\n+val sc: SparkContext = ...\n+\n+val seriesX: RDD[Double] = ... // a series\n+val seriesY: RDD[Double] = ... // must have the same number of partitions and cardinality as seriesX\n+\n+// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a \n+// method is not specified, Pearson's method will be used by default. \n+val correlation: Double = Statistics.corr(seriesX, seriesY, \"pearson\")\n+\n+val data: RDD[Vector] = ... // note that each Vector is a row and not a column\n+\n+// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n+// If a method is not specified, Pearson's method will be used by default. \n+val correlMatrix: Matrix = Statistics.corr(data, \"pearson\")\n+\n+{% endhighlight %}\n+</div>\n+\n+<div data-lang=\"java\" markdown=\"1\">\n+[`Statistics`](api/java/org/apache/spark/mllib/stat/Statistics.html) provides methods to \n+calculate correlations between series. Depending on the type of input, two `JavaDoubleRDD`s or \n+a `JavaRDD<Vector>`, the output will be a `Double` or the correlation `Matrix` respectively.\n+\n+{% highlight java %}\n+import org.apache.spark.api.java.JavaDoubleRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.mllib.linalg.*;\n+import org.apache.spark.mllib.stat.Statistics;\n+\n+JavaSparkContext jsc = ...\n+\n+JavaDoubleRDD seriesX = ... // a series\n+JavaDoubleRDD seriesY = ... // must have the same number of partitions and cardinality as seriesX\n+\n+// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a \n+// method is not specified, Pearson's method will be used by default. \n+Double correlation = Statistics.corr(seriesX.srdd(), seriesY.srdd(), \"pearson\");\n+\n+JavaRDD<Vector> data = ... // note that each Vector is a row and not a column\n+\n+// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n+// If a method is not specified, Pearson's method will be used by default. \n+Matrix correlMatrix = Statistics.corr(data.rdd(), \"pearson\");\n+\n+{% endhighlight %}\n+</div>\n \n-We provide column summary statistics for `RowMatrix` (note: this functionality is not currently supported in `IndexedRowMatrix` or `CoordinateMatrix`). \n-If the number of columns is not large, e.g., on the order of thousands, then the \n-covariance matrix can also be computed as a local matrix, which requires $\\mathcal{O}(n^2)$ storage where $n$ is the\n-number of columns. The total CPU time is $\\mathcal{O}(m n^2)$, where $m$ is the number of rows,\n-and is faster if the rows are sparse.\n+<div data-lang=\"python\" markdown=\"1\">\n+[`Statistics`](api/python/pyspark.mllib.stat.Statistics-class.html) provides methods to \n+calculate correlations between series. Depending on the type of input, two `RDD[Double]`s or \n+an `RDD[Vector]`, the output will be a `Double` or the correlation `Matrix` respectively.\n+\n+{% highlight python %}\n+from pyspark.mllib.stat import Statistics\n+\n+sc = ... # SparkContext\n+\n+seriesX = ... # a series\n+seriesY = ... # must have the same number of partitions and cardinality as seriesX\n+\n+# Compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a \n+# method is not specified, Pearson's method will be used by default. \n+print Statistics.corr(seriesX, seriesY, method=\"pearson\")\n+\n+data = ... # an RDD of Vectors\n+# calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n+# If a method is not specified, Pearson's method will be used by default. \n+print Statistics.corr(data, method=\"pearson\")\n+\n+{% endhighlight %}\n+</div>\n+\n+</div>\n+\n+## Stratified sampling\n+\n+Unlike the other statistics functions, which reside in MLLib, stratified sampling methods, \n+`sampleByKey` and `sampleByKeyExact`, can be performed on RDD's of key-value pairs. For stratified\n+sampling, the keys can be thought of as a label and the value as a specific attribute. For example \n+the key can be man or woman, or document ids, and the respective values can be the list of ages \n+of the people in the population or the list of words in the documents. A separate method for exact "
  }],
  "prId": 2130
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "add a line for `val approxSample = data.sampleByKey(...)` so readers can understand the difference easily\n",
    "commit": "a54a855ee9f6ee1cfd7da2ea5f26a9f7a2d1eb81",
    "createdAt": "2014-08-26T17:22:13Z",
    "diffHunk": "@@ -99,69 +180,277 @@ v = u.map(lambda x: 1.0 + 2.0 * x)\n \n </div>\n \n-## Stratified Sampling \n+## Correlations calculation\n \n-## Summary Statistics \n+Calculating the correlation between two series of data is a common operation in Statistics. In MLlib\n+we provide the flexibility to calculate pairwise correlations among many series. The supported \n+correlation methods are currently Pearson's and Spearman's correlation.\n+ \n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+[`Statistics`](api/scala/index.html#org.apache.spark.mllib.stat.Statistics$) provides methods to \n+calculate correlations between series. Depending on the type of input, two `RDD[Double]`s or \n+an `RDD[Vector]`, the output will be a `Double` or the correlation `Matrix` respectively.\n \n-### Multivariate summary statistics\n+{% highlight scala %}\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.mllib.stat.Statistics\n+\n+val sc: SparkContext = ...\n+\n+val seriesX: RDD[Double] = ... // a series\n+val seriesY: RDD[Double] = ... // must have the same number of partitions and cardinality as seriesX\n+\n+// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a \n+// method is not specified, Pearson's method will be used by default. \n+val correlation: Double = Statistics.corr(seriesX, seriesY, \"pearson\")\n+\n+val data: RDD[Vector] = ... // note that each Vector is a row and not a column\n+\n+// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n+// If a method is not specified, Pearson's method will be used by default. \n+val correlMatrix: Matrix = Statistics.corr(data, \"pearson\")\n+\n+{% endhighlight %}\n+</div>\n+\n+<div data-lang=\"java\" markdown=\"1\">\n+[`Statistics`](api/java/org/apache/spark/mllib/stat/Statistics.html) provides methods to \n+calculate correlations between series. Depending on the type of input, two `JavaDoubleRDD`s or \n+a `JavaRDD<Vector>`, the output will be a `Double` or the correlation `Matrix` respectively.\n+\n+{% highlight java %}\n+import org.apache.spark.api.java.JavaDoubleRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.mllib.linalg.*;\n+import org.apache.spark.mllib.stat.Statistics;\n+\n+JavaSparkContext jsc = ...\n+\n+JavaDoubleRDD seriesX = ... // a series\n+JavaDoubleRDD seriesY = ... // must have the same number of partitions and cardinality as seriesX\n+\n+// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a \n+// method is not specified, Pearson's method will be used by default. \n+Double correlation = Statistics.corr(seriesX.srdd(), seriesY.srdd(), \"pearson\");\n+\n+JavaRDD<Vector> data = ... // note that each Vector is a row and not a column\n+\n+// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n+// If a method is not specified, Pearson's method will be used by default. \n+Matrix correlMatrix = Statistics.corr(data.rdd(), \"pearson\");\n+\n+{% endhighlight %}\n+</div>\n \n-We provide column summary statistics for `RowMatrix` (note: this functionality is not currently supported in `IndexedRowMatrix` or `CoordinateMatrix`). \n-If the number of columns is not large, e.g., on the order of thousands, then the \n-covariance matrix can also be computed as a local matrix, which requires $\\mathcal{O}(n^2)$ storage where $n$ is the\n-number of columns. The total CPU time is $\\mathcal{O}(m n^2)$, where $m$ is the number of rows,\n-and is faster if the rows are sparse.\n+<div data-lang=\"python\" markdown=\"1\">\n+[`Statistics`](api/python/pyspark.mllib.stat.Statistics-class.html) provides methods to \n+calculate correlations between series. Depending on the type of input, two `RDD[Double]`s or \n+an `RDD[Vector]`, the output will be a `Double` or the correlation `Matrix` respectively.\n+\n+{% highlight python %}\n+from pyspark.mllib.stat import Statistics\n+\n+sc = ... # SparkContext\n+\n+seriesX = ... # a series\n+seriesY = ... # must have the same number of partitions and cardinality as seriesX\n+\n+# Compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a \n+# method is not specified, Pearson's method will be used by default. \n+print Statistics.corr(seriesX, seriesY, method=\"pearson\")\n+\n+data = ... # an RDD of Vectors\n+# calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n+# If a method is not specified, Pearson's method will be used by default. \n+print Statistics.corr(data, method=\"pearson\")\n+\n+{% endhighlight %}\n+</div>\n+\n+</div>\n+\n+## Stratified sampling\n+\n+Unlike the other statistics functions, which reside in MLLib, stratified sampling methods, \n+`sampleByKey` and `sampleByKeyExact`, can be performed on RDD's of key-value pairs. For stratified\n+sampling, the keys can be thought of as a label and the value as a specific attribute. For example \n+the key can be man or woman, or document ids, and the respective values can be the list of ages \n+of the people in the population or the list of words in the documents. A separate method for exact \n+sample size support exists as it requires significant more resources than the per-stratum simple \n+random sampling used in `sampleByKey`. `sampleByKeyExact` is currently not supported in python.\n \n <div class=\"codetabs\">\n <div data-lang=\"scala\" markdown=\"1\">\n-\n-[`computeColumnSummaryStatistics()`](api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix) returns an instance of\n-[`MultivariateStatisticalSummary`](api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary),\n-which contains the column-wise max, min, mean, variance, and number of nonzeros, as well as the\n-total count.\n+[`sampleByKeyExact()`](api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions) allows users to\n+sample exactly $\\lceil f_k \\cdot n_k \\rceil \\, \\forall k \\in K$ items, where $f_k$ is the desired \n+fraction for key $k$, and $n_k$ is the number of key-value pairs for key $k$. \n+Sampling without replacement requires one additional pass over the RDD to guarantee sample \n+size, whereas sampling with replacement requires two additional passes.\n \n {% highlight scala %}\n-import org.apache.spark.mllib.linalg.Matrix\n-import org.apache.spark.mllib.linalg.distributed.RowMatrix\n-import org.apache.spark.mllib.stat.MultivariateStatisticalSummary\n+import org.apache.spark.SparkContext\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.rdd.PairRDDFunctions\n \n-val mat: RowMatrix = ... // a RowMatrix\n+val sc: SparkContext = ...\n \n-// Compute column summary statistics.\n-val summary: MultivariateStatisticalSummary = mat.computeColumnSummaryStatistics()\n-println(summary.mean) // a dense vector containing the mean value for each column\n-println(summary.variance) // column-wise variance\n-println(summary.numNonzeros) // number of nonzeros in each column\n+val data = ... // an RDD[(K, V)] of any key value pairs\n+val fractions: Map[K, Double] = ... // specify the exact fraction desired from each key\n+\n+// Get an exact sample from each stratum\n+val sample = data.sampleByKeyExact(withReplacement = false, fractions)"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "same for Java\n",
    "commit": "a54a855ee9f6ee1cfd7da2ea5f26a9f7a2d1eb81",
    "createdAt": "2014-08-26T17:22:21Z",
    "diffHunk": "@@ -99,69 +180,277 @@ v = u.map(lambda x: 1.0 + 2.0 * x)\n \n </div>\n \n-## Stratified Sampling \n+## Correlations calculation\n \n-## Summary Statistics \n+Calculating the correlation between two series of data is a common operation in Statistics. In MLlib\n+we provide the flexibility to calculate pairwise correlations among many series. The supported \n+correlation methods are currently Pearson's and Spearman's correlation.\n+ \n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+[`Statistics`](api/scala/index.html#org.apache.spark.mllib.stat.Statistics$) provides methods to \n+calculate correlations between series. Depending on the type of input, two `RDD[Double]`s or \n+an `RDD[Vector]`, the output will be a `Double` or the correlation `Matrix` respectively.\n \n-### Multivariate summary statistics\n+{% highlight scala %}\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.mllib.stat.Statistics\n+\n+val sc: SparkContext = ...\n+\n+val seriesX: RDD[Double] = ... // a series\n+val seriesY: RDD[Double] = ... // must have the same number of partitions and cardinality as seriesX\n+\n+// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a \n+// method is not specified, Pearson's method will be used by default. \n+val correlation: Double = Statistics.corr(seriesX, seriesY, \"pearson\")\n+\n+val data: RDD[Vector] = ... // note that each Vector is a row and not a column\n+\n+// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n+// If a method is not specified, Pearson's method will be used by default. \n+val correlMatrix: Matrix = Statistics.corr(data, \"pearson\")\n+\n+{% endhighlight %}\n+</div>\n+\n+<div data-lang=\"java\" markdown=\"1\">\n+[`Statistics`](api/java/org/apache/spark/mllib/stat/Statistics.html) provides methods to \n+calculate correlations between series. Depending on the type of input, two `JavaDoubleRDD`s or \n+a `JavaRDD<Vector>`, the output will be a `Double` or the correlation `Matrix` respectively.\n+\n+{% highlight java %}\n+import org.apache.spark.api.java.JavaDoubleRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.mllib.linalg.*;\n+import org.apache.spark.mllib.stat.Statistics;\n+\n+JavaSparkContext jsc = ...\n+\n+JavaDoubleRDD seriesX = ... // a series\n+JavaDoubleRDD seriesY = ... // must have the same number of partitions and cardinality as seriesX\n+\n+// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a \n+// method is not specified, Pearson's method will be used by default. \n+Double correlation = Statistics.corr(seriesX.srdd(), seriesY.srdd(), \"pearson\");\n+\n+JavaRDD<Vector> data = ... // note that each Vector is a row and not a column\n+\n+// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n+// If a method is not specified, Pearson's method will be used by default. \n+Matrix correlMatrix = Statistics.corr(data.rdd(), \"pearson\");\n+\n+{% endhighlight %}\n+</div>\n \n-We provide column summary statistics for `RowMatrix` (note: this functionality is not currently supported in `IndexedRowMatrix` or `CoordinateMatrix`). \n-If the number of columns is not large, e.g., on the order of thousands, then the \n-covariance matrix can also be computed as a local matrix, which requires $\\mathcal{O}(n^2)$ storage where $n$ is the\n-number of columns. The total CPU time is $\\mathcal{O}(m n^2)$, where $m$ is the number of rows,\n-and is faster if the rows are sparse.\n+<div data-lang=\"python\" markdown=\"1\">\n+[`Statistics`](api/python/pyspark.mllib.stat.Statistics-class.html) provides methods to \n+calculate correlations between series. Depending on the type of input, two `RDD[Double]`s or \n+an `RDD[Vector]`, the output will be a `Double` or the correlation `Matrix` respectively.\n+\n+{% highlight python %}\n+from pyspark.mllib.stat import Statistics\n+\n+sc = ... # SparkContext\n+\n+seriesX = ... # a series\n+seriesY = ... # must have the same number of partitions and cardinality as seriesX\n+\n+# Compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a \n+# method is not specified, Pearson's method will be used by default. \n+print Statistics.corr(seriesX, seriesY, method=\"pearson\")\n+\n+data = ... # an RDD of Vectors\n+# calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n+# If a method is not specified, Pearson's method will be used by default. \n+print Statistics.corr(data, method=\"pearson\")\n+\n+{% endhighlight %}\n+</div>\n+\n+</div>\n+\n+## Stratified sampling\n+\n+Unlike the other statistics functions, which reside in MLLib, stratified sampling methods, \n+`sampleByKey` and `sampleByKeyExact`, can be performed on RDD's of key-value pairs. For stratified\n+sampling, the keys can be thought of as a label and the value as a specific attribute. For example \n+the key can be man or woman, or document ids, and the respective values can be the list of ages \n+of the people in the population or the list of words in the documents. A separate method for exact \n+sample size support exists as it requires significant more resources than the per-stratum simple \n+random sampling used in `sampleByKey`. `sampleByKeyExact` is currently not supported in python.\n \n <div class=\"codetabs\">\n <div data-lang=\"scala\" markdown=\"1\">\n-\n-[`computeColumnSummaryStatistics()`](api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix) returns an instance of\n-[`MultivariateStatisticalSummary`](api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary),\n-which contains the column-wise max, min, mean, variance, and number of nonzeros, as well as the\n-total count.\n+[`sampleByKeyExact()`](api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions) allows users to\n+sample exactly $\\lceil f_k \\cdot n_k \\rceil \\, \\forall k \\in K$ items, where $f_k$ is the desired \n+fraction for key $k$, and $n_k$ is the number of key-value pairs for key $k$. \n+Sampling without replacement requires one additional pass over the RDD to guarantee sample \n+size, whereas sampling with replacement requires two additional passes.\n \n {% highlight scala %}\n-import org.apache.spark.mllib.linalg.Matrix\n-import org.apache.spark.mllib.linalg.distributed.RowMatrix\n-import org.apache.spark.mllib.stat.MultivariateStatisticalSummary\n+import org.apache.spark.SparkContext\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.rdd.PairRDDFunctions\n \n-val mat: RowMatrix = ... // a RowMatrix\n+val sc: SparkContext = ...\n \n-// Compute column summary statistics.\n-val summary: MultivariateStatisticalSummary = mat.computeColumnSummaryStatistics()\n-println(summary.mean) // a dense vector containing the mean value for each column\n-println(summary.variance) // column-wise variance\n-println(summary.numNonzeros) // number of nonzeros in each column\n+val data = ... // an RDD[(K, V)] of any key value pairs\n+val fractions: Map[K, Double] = ... // specify the exact fraction desired from each key\n+\n+// Get an exact sample from each stratum\n+val sample = data.sampleByKeyExact(withReplacement = false, fractions)"
  }],
  "prId": 2130
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`approxSample = data.sampleByKey(False, fractions)`\n",
    "commit": "a54a855ee9f6ee1cfd7da2ea5f26a9f7a2d1eb81",
    "createdAt": "2014-08-26T17:22:44Z",
    "diffHunk": "@@ -99,69 +180,277 @@ v = u.map(lambda x: 1.0 + 2.0 * x)\n \n </div>\n \n-## Stratified Sampling \n+## Correlations calculation\n \n-## Summary Statistics \n+Calculating the correlation between two series of data is a common operation in Statistics. In MLlib\n+we provide the flexibility to calculate pairwise correlations among many series. The supported \n+correlation methods are currently Pearson's and Spearman's correlation.\n+ \n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+[`Statistics`](api/scala/index.html#org.apache.spark.mllib.stat.Statistics$) provides methods to \n+calculate correlations between series. Depending on the type of input, two `RDD[Double]`s or \n+an `RDD[Vector]`, the output will be a `Double` or the correlation `Matrix` respectively.\n \n-### Multivariate summary statistics\n+{% highlight scala %}\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.mllib.stat.Statistics\n+\n+val sc: SparkContext = ...\n+\n+val seriesX: RDD[Double] = ... // a series\n+val seriesY: RDD[Double] = ... // must have the same number of partitions and cardinality as seriesX\n+\n+// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a \n+// method is not specified, Pearson's method will be used by default. \n+val correlation: Double = Statistics.corr(seriesX, seriesY, \"pearson\")\n+\n+val data: RDD[Vector] = ... // note that each Vector is a row and not a column\n+\n+// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n+// If a method is not specified, Pearson's method will be used by default. \n+val correlMatrix: Matrix = Statistics.corr(data, \"pearson\")\n+\n+{% endhighlight %}\n+</div>\n+\n+<div data-lang=\"java\" markdown=\"1\">\n+[`Statistics`](api/java/org/apache/spark/mllib/stat/Statistics.html) provides methods to \n+calculate correlations between series. Depending on the type of input, two `JavaDoubleRDD`s or \n+a `JavaRDD<Vector>`, the output will be a `Double` or the correlation `Matrix` respectively.\n+\n+{% highlight java %}\n+import org.apache.spark.api.java.JavaDoubleRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.mllib.linalg.*;\n+import org.apache.spark.mllib.stat.Statistics;\n+\n+JavaSparkContext jsc = ...\n+\n+JavaDoubleRDD seriesX = ... // a series\n+JavaDoubleRDD seriesY = ... // must have the same number of partitions and cardinality as seriesX\n+\n+// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a \n+// method is not specified, Pearson's method will be used by default. \n+Double correlation = Statistics.corr(seriesX.srdd(), seriesY.srdd(), \"pearson\");\n+\n+JavaRDD<Vector> data = ... // note that each Vector is a row and not a column\n+\n+// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n+// If a method is not specified, Pearson's method will be used by default. \n+Matrix correlMatrix = Statistics.corr(data.rdd(), \"pearson\");\n+\n+{% endhighlight %}\n+</div>\n \n-We provide column summary statistics for `RowMatrix` (note: this functionality is not currently supported in `IndexedRowMatrix` or `CoordinateMatrix`). \n-If the number of columns is not large, e.g., on the order of thousands, then the \n-covariance matrix can also be computed as a local matrix, which requires $\\mathcal{O}(n^2)$ storage where $n$ is the\n-number of columns. The total CPU time is $\\mathcal{O}(m n^2)$, where $m$ is the number of rows,\n-and is faster if the rows are sparse.\n+<div data-lang=\"python\" markdown=\"1\">\n+[`Statistics`](api/python/pyspark.mllib.stat.Statistics-class.html) provides methods to \n+calculate correlations between series. Depending on the type of input, two `RDD[Double]`s or \n+an `RDD[Vector]`, the output will be a `Double` or the correlation `Matrix` respectively.\n+\n+{% highlight python %}\n+from pyspark.mllib.stat import Statistics\n+\n+sc = ... # SparkContext\n+\n+seriesX = ... # a series\n+seriesY = ... # must have the same number of partitions and cardinality as seriesX\n+\n+# Compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a \n+# method is not specified, Pearson's method will be used by default. \n+print Statistics.corr(seriesX, seriesY, method=\"pearson\")\n+\n+data = ... # an RDD of Vectors\n+# calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n+# If a method is not specified, Pearson's method will be used by default. \n+print Statistics.corr(data, method=\"pearson\")\n+\n+{% endhighlight %}\n+</div>\n+\n+</div>\n+\n+## Stratified sampling\n+\n+Unlike the other statistics functions, which reside in MLLib, stratified sampling methods, \n+`sampleByKey` and `sampleByKeyExact`, can be performed on RDD's of key-value pairs. For stratified\n+sampling, the keys can be thought of as a label and the value as a specific attribute. For example \n+the key can be man or woman, or document ids, and the respective values can be the list of ages \n+of the people in the population or the list of words in the documents. A separate method for exact \n+sample size support exists as it requires significant more resources than the per-stratum simple \n+random sampling used in `sampleByKey`. `sampleByKeyExact` is currently not supported in python.\n \n <div class=\"codetabs\">\n <div data-lang=\"scala\" markdown=\"1\">\n-\n-[`computeColumnSummaryStatistics()`](api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix) returns an instance of\n-[`MultivariateStatisticalSummary`](api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary),\n-which contains the column-wise max, min, mean, variance, and number of nonzeros, as well as the\n-total count.\n+[`sampleByKeyExact()`](api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions) allows users to\n+sample exactly $\\lceil f_k \\cdot n_k \\rceil \\, \\forall k \\in K$ items, where $f_k$ is the desired \n+fraction for key $k$, and $n_k$ is the number of key-value pairs for key $k$. \n+Sampling without replacement requires one additional pass over the RDD to guarantee sample \n+size, whereas sampling with replacement requires two additional passes.\n \n {% highlight scala %}\n-import org.apache.spark.mllib.linalg.Matrix\n-import org.apache.spark.mllib.linalg.distributed.RowMatrix\n-import org.apache.spark.mllib.stat.MultivariateStatisticalSummary\n+import org.apache.spark.SparkContext\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.rdd.PairRDDFunctions\n \n-val mat: RowMatrix = ... // a RowMatrix\n+val sc: SparkContext = ...\n \n-// Compute column summary statistics.\n-val summary: MultivariateStatisticalSummary = mat.computeColumnSummaryStatistics()\n-println(summary.mean) // a dense vector containing the mean value for each column\n-println(summary.variance) // column-wise variance\n-println(summary.numNonzeros) // number of nonzeros in each column\n+val data = ... // an RDD[(K, V)] of any key value pairs\n+val fractions: Map[K, Double] = ... // specify the exact fraction desired from each key\n+\n+// Get an exact sample from each stratum\n+val sample = data.sampleByKeyExact(withReplacement = false, fractions)\n \n-// Compute the covariance matrix.\n-val cov: Matrix = mat.computeCovariance()\n {% endhighlight %}\n </div>\n \n <div data-lang=\"java\" markdown=\"1\">\n-\n-[`RowMatrix#computeColumnSummaryStatistics`](api/java/org/apache/spark/mllib/linalg/distributed/RowMatrix.html#computeColumnSummaryStatistics()) returns an instance of\n-[`MultivariateStatisticalSummary`](api/java/org/apache/spark/mllib/stat/MultivariateStatisticalSummary.html),\n-which contains the column-wise max, min, mean, variance, and number of nonzeros, as well as the\n-total count.\n+[`sampleByKeyExact()`](api/java/org/apache/spark/api/java/JavaPairRDD.html) allows users to\n+sample exactly $\\lceil f_k \\cdot n_k \\rceil \\, \\forall k \\in K$ items, where $f_k$ is the desired \n+fraction for key $k$, and $n_k$ is the number of key-value pairs for key $k$. \n+Sampling without replacement requires one additional pass over the RDD to guarantee sample \n+size, whereas sampling with replacement requires two additional passes.\n \n {% highlight java %}\n-import org.apache.spark.mllib.linalg.Matrix;\n-import org.apache.spark.mllib.linalg.distributed.RowMatrix;\n-import org.apache.spark.mllib.stat.MultivariateStatisticalSummary;\n+import java.util.Map;\n \n-RowMatrix mat = ... // a RowMatrix\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n \n-// Compute column summary statistics.\n-MultivariateStatisticalSummary summary = mat.computeColumnSummaryStatistics();\n-System.out.println(summary.mean()); // a dense vector containing the mean value for each column\n-System.out.println(summary.variance()); // column-wise variance\n-System.out.println(summary.numNonzeros()); // number of nonzeros in each column\n+JavaSparkContext jsc = ...\n+\n+JavaPairRDD<K, V> data = ... // an RDD of any key value pairs\n+Map<K, Object> fractions = ... // specify the exact fraction desired from each key\n+\n+// Get an exact sample from each stratum\n+JavaPairRDD<K, V> sample = data.sampleByKeyExact(false, fractions);\n+\n+{% endhighlight %}\n+</div>\n+<div data-lang=\"python\" markdown=\"1\">\n+[`sampleByKey()`](api/python/pyspark.rdd.RDD-class.html#sampleByKey) allows users to\n+sample approximately $\\lceil f_k \\cdot n_k \\rceil \\, \\forall k \\in K$ items, where $f_k$ is the \n+desired fraction for key $k$, and $n_k$ is the number of key-value pairs for key $k$. \n+Sampling without replacement requires one additional pass over the RDD to guarantee sample \n+size, whereas sampling with replacement requires two additional passes.\n+\n+*Note:* `sampleByKeyExact()` is currently not supported in Python.\n+\n+\n+{% highlight python %}\n+\n+sc = ... # SparkContext\n+\n+data = ... # an RDD of any key value pairs\n+fractions = ... # specify the exact fraction desired from each key as a dictionary\n+\n+sample = data.sampleByKeyExact(False, fractions);"
  }],
  "prId": 2130
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`K` is not defined.\n",
    "commit": "a54a855ee9f6ee1cfd7da2ea5f26a9f7a2d1eb81",
    "createdAt": "2014-08-26T17:24:34Z",
    "diffHunk": "@@ -99,69 +180,277 @@ v = u.map(lambda x: 1.0 + 2.0 * x)\n \n </div>\n \n-## Stratified Sampling \n+## Correlations calculation\n \n-## Summary Statistics \n+Calculating the correlation between two series of data is a common operation in Statistics. In MLlib\n+we provide the flexibility to calculate pairwise correlations among many series. The supported \n+correlation methods are currently Pearson's and Spearman's correlation.\n+ \n+<div class=\"codetabs\">\n+<div data-lang=\"scala\" markdown=\"1\">\n+[`Statistics`](api/scala/index.html#org.apache.spark.mllib.stat.Statistics$) provides methods to \n+calculate correlations between series. Depending on the type of input, two `RDD[Double]`s or \n+an `RDD[Vector]`, the output will be a `Double` or the correlation `Matrix` respectively.\n \n-### Multivariate summary statistics\n+{% highlight scala %}\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.linalg._\n+import org.apache.spark.mllib.stat.Statistics\n+\n+val sc: SparkContext = ...\n+\n+val seriesX: RDD[Double] = ... // a series\n+val seriesY: RDD[Double] = ... // must have the same number of partitions and cardinality as seriesX\n+\n+// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a \n+// method is not specified, Pearson's method will be used by default. \n+val correlation: Double = Statistics.corr(seriesX, seriesY, \"pearson\")\n+\n+val data: RDD[Vector] = ... // note that each Vector is a row and not a column\n+\n+// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n+// If a method is not specified, Pearson's method will be used by default. \n+val correlMatrix: Matrix = Statistics.corr(data, \"pearson\")\n+\n+{% endhighlight %}\n+</div>\n+\n+<div data-lang=\"java\" markdown=\"1\">\n+[`Statistics`](api/java/org/apache/spark/mllib/stat/Statistics.html) provides methods to \n+calculate correlations between series. Depending on the type of input, two `JavaDoubleRDD`s or \n+a `JavaRDD<Vector>`, the output will be a `Double` or the correlation `Matrix` respectively.\n+\n+{% highlight java %}\n+import org.apache.spark.api.java.JavaDoubleRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.mllib.linalg.*;\n+import org.apache.spark.mllib.stat.Statistics;\n+\n+JavaSparkContext jsc = ...\n+\n+JavaDoubleRDD seriesX = ... // a series\n+JavaDoubleRDD seriesY = ... // must have the same number of partitions and cardinality as seriesX\n+\n+// compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a \n+// method is not specified, Pearson's method will be used by default. \n+Double correlation = Statistics.corr(seriesX.srdd(), seriesY.srdd(), \"pearson\");\n+\n+JavaRDD<Vector> data = ... // note that each Vector is a row and not a column\n+\n+// calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n+// If a method is not specified, Pearson's method will be used by default. \n+Matrix correlMatrix = Statistics.corr(data.rdd(), \"pearson\");\n+\n+{% endhighlight %}\n+</div>\n \n-We provide column summary statistics for `RowMatrix` (note: this functionality is not currently supported in `IndexedRowMatrix` or `CoordinateMatrix`). \n-If the number of columns is not large, e.g., on the order of thousands, then the \n-covariance matrix can also be computed as a local matrix, which requires $\\mathcal{O}(n^2)$ storage where $n$ is the\n-number of columns. The total CPU time is $\\mathcal{O}(m n^2)$, where $m$ is the number of rows,\n-and is faster if the rows are sparse.\n+<div data-lang=\"python\" markdown=\"1\">\n+[`Statistics`](api/python/pyspark.mllib.stat.Statistics-class.html) provides methods to \n+calculate correlations between series. Depending on the type of input, two `RDD[Double]`s or \n+an `RDD[Vector]`, the output will be a `Double` or the correlation `Matrix` respectively.\n+\n+{% highlight python %}\n+from pyspark.mllib.stat import Statistics\n+\n+sc = ... # SparkContext\n+\n+seriesX = ... # a series\n+seriesY = ... # must have the same number of partitions and cardinality as seriesX\n+\n+# Compute the correlation using Pearson's method. Enter \"spearman\" for Spearman's method. If a \n+# method is not specified, Pearson's method will be used by default. \n+print Statistics.corr(seriesX, seriesY, method=\"pearson\")\n+\n+data = ... # an RDD of Vectors\n+# calculate the correlation matrix using Pearson's method. Use \"spearman\" for Spearman's method.\n+# If a method is not specified, Pearson's method will be used by default. \n+print Statistics.corr(data, method=\"pearson\")\n+\n+{% endhighlight %}\n+</div>\n+\n+</div>\n+\n+## Stratified sampling\n+\n+Unlike the other statistics functions, which reside in MLLib, stratified sampling methods, \n+`sampleByKey` and `sampleByKeyExact`, can be performed on RDD's of key-value pairs. For stratified\n+sampling, the keys can be thought of as a label and the value as a specific attribute. For example \n+the key can be man or woman, or document ids, and the respective values can be the list of ages \n+of the people in the population or the list of words in the documents. A separate method for exact \n+sample size support exists as it requires significant more resources than the per-stratum simple \n+random sampling used in `sampleByKey`. `sampleByKeyExact` is currently not supported in python.\n \n <div class=\"codetabs\">\n <div data-lang=\"scala\" markdown=\"1\">\n-\n-[`computeColumnSummaryStatistics()`](api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix) returns an instance of\n-[`MultivariateStatisticalSummary`](api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary),\n-which contains the column-wise max, min, mean, variance, and number of nonzeros, as well as the\n-total count.\n+[`sampleByKeyExact()`](api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions) allows users to\n+sample exactly $\\lceil f_k \\cdot n_k \\rceil \\, \\forall k \\in K$ items, where $f_k$ is the desired ",
    "line": 210
  }],
  "prId": 2130
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "It should be fine if we remove this line.\n",
    "commit": "a54a855ee9f6ee1cfd7da2ea5f26a9f7a2d1eb81",
    "createdAt": "2014-08-26T17:25:34Z",
    "diffHunk": "@@ -25,6 +25,87 @@ displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Statistics Functionality\n \\newcommand{\\zero}{\\mathbf{0}}\n \\]`\n \n+## Summary Statistics \n+\n+### Multivariate summary statistics"
  }],
  "prId": 2130
}]