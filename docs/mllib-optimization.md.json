[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Either `L-BFGS` or `The L-BFGS method`.\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-09T00:29:36Z",
    "diffHunk": "@@ -128,10 +128,24 @@ is sampled, i.e. `$|S|=$ miniBatchFraction $\\cdot n = 1$`, then the algorithm is\n standard SGD. In that case, the step direction depends from the uniformly random sampling of the\n point.\n \n+### Limited-memory BFGS\n+[Limited-memory BFGS (L-BFGS)](http://en.wikipedia.org/wiki/Limited-memory_BFGS) is an optimization \n+algorithm in the family of quasi-Newton methods to solve the optimization problems of the form \n+`$\\min_{\\wv \\in\\R^d} \\; f(\\wv)$`. The L-BFGS approximates the objective function locally as a quadratic"
  }],
  "prId": 702
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`Newton's method`\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-09T00:30:14Z",
    "diffHunk": "@@ -128,10 +128,24 @@ is sampled, i.e. `$|S|=$ miniBatchFraction $\\cdot n = 1$`, then the algorithm is\n standard SGD. In that case, the step direction depends from the uniformly random sampling of the\n point.\n \n+### Limited-memory BFGS\n+[Limited-memory BFGS (L-BFGS)](http://en.wikipedia.org/wiki/Limited-memory_BFGS) is an optimization \n+algorithm in the family of quasi-Newton methods to solve the optimization problems of the form \n+`$\\min_{\\wv \\in\\R^d} \\; f(\\wv)$`. The L-BFGS approximates the objective function locally as a quadratic\n+without evaluating the second partial derivatives of the objective function to construct the \n+Hessian matrix. The Hessian matrix is approximated by previous gradient evaluations, so there is no \n+vertical scalability issue (the number of training features) when computing the Hessian matrix \n+explicitly in Newton method. As a result, L-BFGS often achieves rapider convergence compared with "
  }],
  "prId": 702
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "I think it is safe to remove this paragraph or put it in a separate `Developer` section.\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-09T00:31:40Z",
    "diffHunk": "@@ -128,10 +128,24 @@ is sampled, i.e. `$|S|=$ miniBatchFraction $\\cdot n = 1$`, then the algorithm is\n standard SGD. In that case, the step direction depends from the uniformly random sampling of the\n point.\n \n+### Limited-memory BFGS\n+[Limited-memory BFGS (L-BFGS)](http://en.wikipedia.org/wiki/Limited-memory_BFGS) is an optimization \n+algorithm in the family of quasi-Newton methods to solve the optimization problems of the form \n+`$\\min_{\\wv \\in\\R^d} \\; f(\\wv)$`. The L-BFGS approximates the objective function locally as a quadratic\n+without evaluating the second partial derivatives of the objective function to construct the \n+Hessian matrix. The Hessian matrix is approximated by previous gradient evaluations, so there is no \n+vertical scalability issue (the number of training features) when computing the Hessian matrix \n+explicitly in Newton method. As a result, L-BFGS often achieves rapider convergence compared with \n+other first-order optimization. \n \n+Since the Hessian is constructed approximately from previous gradient evaluations, the objective \n+function can not be changed during the optimization process. As a result, Stochastic L-BFGS will \n+not work naively by just using miniBatch; therefore, we don't provide this until we have better \n+understanding.  "
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Do we have `Developer` section for this type of stuff?\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-09T22:50:55Z",
    "diffHunk": "@@ -128,10 +128,24 @@ is sampled, i.e. `$|S|=$ miniBatchFraction $\\cdot n = 1$`, then the algorithm is\n standard SGD. In that case, the step direction depends from the uniformly random sampling of the\n point.\n \n+### Limited-memory BFGS\n+[Limited-memory BFGS (L-BFGS)](http://en.wikipedia.org/wiki/Limited-memory_BFGS) is an optimization \n+algorithm in the family of quasi-Newton methods to solve the optimization problems of the form \n+`$\\min_{\\wv \\in\\R^d} \\; f(\\wv)$`. The L-BFGS approximates the objective function locally as a quadratic\n+without evaluating the second partial derivatives of the objective function to construct the \n+Hessian matrix. The Hessian matrix is approximated by previous gradient evaluations, so there is no \n+vertical scalability issue (the number of training features) when computing the Hessian matrix \n+explicitly in Newton method. As a result, L-BFGS often achieves rapider convergence compared with \n+other first-order optimization. \n \n+Since the Hessian is constructed approximately from previous gradient evaluations, the objective \n+function can not be changed during the optimization process. As a result, Stochastic L-BFGS will \n+not work naively by just using miniBatch; therefore, we don't provide this until we have better \n+understanding.  "
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "I decided to move those message to the code. \n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-09T22:55:16Z",
    "diffHunk": "@@ -128,10 +128,24 @@ is sampled, i.e. `$|S|=$ miniBatchFraction $\\cdot n = 1$`, then the algorithm is\n standard SGD. In that case, the step direction depends from the uniformly random sampling of the\n point.\n \n+### Limited-memory BFGS\n+[Limited-memory BFGS (L-BFGS)](http://en.wikipedia.org/wiki/Limited-memory_BFGS) is an optimization \n+algorithm in the family of quasi-Newton methods to solve the optimization problems of the form \n+`$\\min_{\\wv \\in\\R^d} \\; f(\\wv)$`. The L-BFGS approximates the objective function locally as a quadratic\n+without evaluating the second partial derivatives of the objective function to construct the \n+Hessian matrix. The Hessian matrix is approximated by previous gradient evaluations, so there is no \n+vertical scalability issue (the number of training features) when computing the Hessian matrix \n+explicitly in Newton method. As a result, L-BFGS often achieves rapider convergence compared with \n+other first-order optimization. \n \n+Since the Hessian is constructed approximately from previous gradient evaluations, the objective \n+function can not be changed during the optimization process. As a result, Stochastic L-BFGS will \n+not work naively by just using miniBatch; therefore, we don't provide this until we have better \n+understanding.  "
  }],
  "prId": 702
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "last `LogisticRegression` -> `LogisticRegressionWithSGD`\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-09T00:32:54Z",
    "diffHunk": "@@ -163,3 +177,100 @@ each iteration, to compute the gradient direction.\n Available algorithms for gradient descent:\n \n * [GradientDescent.runMiniBatchSGD](api/mllib/index.html#org.apache.spark.mllib.optimization.GradientDescent)\n+\n+### Limited-memory BFGS\n+L-BFGS is currently only a low-level optimization primitive in `MLlib`. If you want to use L-BFGS in various \n+ML algorithms such as Linear Regression, and Logistic Regression, you have to pass the gradient of objective\n+function, and updater into optimizer yourself instead of using the training APIs like \n+[LogisticRegression.LogisticRegressionWithSGD](api/mllib/index.html#org.apache.spark.mllib.classification.LogisticRegression)."
  }],
  "prId": 702
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`Stochastic` -> `stochastic`.\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-09T00:34:00Z",
    "diffHunk": "@@ -128,10 +128,24 @@ is sampled, i.e. `$|S|=$ miniBatchFraction $\\cdot n = 1$`, then the algorithm is\n standard SGD. In that case, the step direction depends from the uniformly random sampling of the\n point.\n \n+### Limited-memory BFGS\n+[Limited-memory BFGS (L-BFGS)](http://en.wikipedia.org/wiki/Limited-memory_BFGS) is an optimization \n+algorithm in the family of quasi-Newton methods to solve the optimization problems of the form \n+`$\\min_{\\wv \\in\\R^d} \\; f(\\wv)$`. The L-BFGS approximates the objective function locally as a quadratic\n+without evaluating the second partial derivatives of the objective function to construct the \n+Hessian matrix. The Hessian matrix is approximated by previous gradient evaluations, so there is no \n+vertical scalability issue (the number of training features) when computing the Hessian matrix \n+explicitly in Newton method. As a result, L-BFGS often achieves rapider convergence compared with \n+other first-order optimization. \n \n+Since the Hessian is constructed approximately from previous gradient evaluations, the objective \n+function can not be changed during the optimization process. As a result, Stochastic L-BFGS will \n+not work naively by just using miniBatch; therefore, we don't provide this until we have better \n+understanding.  \n \n ## Implementation in MLlib\n \n+### Gradient descent and Stochastic gradient descent"
  }],
  "prId": 702
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`L1Updater` is not under `Updater`. Should change to\n\n```\n[L1Updater](api/mllib/index.html#org.apache.spark.mllib.optimization.L1Updater)\n```\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-09T00:35:20Z",
    "diffHunk": "@@ -163,3 +177,100 @@ each iteration, to compute the gradient direction.\n Available algorithms for gradient descent:\n \n * [GradientDescent.runMiniBatchSGD](api/mllib/index.html#org.apache.spark.mllib.optimization.GradientDescent)\n+\n+### Limited-memory BFGS\n+L-BFGS is currently only a low-level optimization primitive in `MLlib`. If you want to use L-BFGS in various \n+ML algorithms such as Linear Regression, and Logistic Regression, you have to pass the gradient of objective\n+function, and updater into optimizer yourself instead of using the training APIs like \n+[LogisticRegression.LogisticRegressionWithSGD](api/mllib/index.html#org.apache.spark.mllib.classification.LogisticRegression).\n+See the example below. It will be addressed in the next release. \n+\n+The L1 regularization by using \n+[Updater.L1Updater](api/mllib/index.html#org.apache.spark.mllib.optimization.Updater) will not work since the "
  }],
  "prId": 702
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Let's remove this paragraph or move it to a separate `Developer` section. For the guide, we better separate user-facing content from developer-facing content.\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-09T00:37:38Z",
    "diffHunk": "@@ -163,3 +177,100 @@ each iteration, to compute the gradient direction.\n Available algorithms for gradient descent:\n \n * [GradientDescent.runMiniBatchSGD](api/mllib/index.html#org.apache.spark.mllib.optimization.GradientDescent)\n+\n+### Limited-memory BFGS\n+L-BFGS is currently only a low-level optimization primitive in `MLlib`. If you want to use L-BFGS in various \n+ML algorithms such as Linear Regression, and Logistic Regression, you have to pass the gradient of objective\n+function, and updater into optimizer yourself instead of using the training APIs like \n+[LogisticRegression.LogisticRegressionWithSGD](api/mllib/index.html#org.apache.spark.mllib.classification.LogisticRegression).\n+See the example below. It will be addressed in the next release. \n+\n+The L1 regularization by using \n+[Updater.L1Updater](api/mllib/index.html#org.apache.spark.mllib.optimization.Updater) will not work since the \n+soft-thresholding logic in L1Updater is designed for gradient descent.\n+\n+The L-BFGS method\n+[LBFGS.runLBFGS](api/scala/index.html#org.apache.spark.mllib.optimization.LBFGS)\n+has the following parameters:\n+\n+* `gradient` is a class that computes the gradient of the objective function\n+being optimized, i.e., with respect to a single training example, at the\n+current parameter value. MLlib includes gradient classes for common loss\n+functions, e.g., hinge, logistic, least-squares.  The gradient class takes as\n+input a training example, its label, and the current parameter value. \n+* `updater` is a class originally designed for gradient decent which computes "
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Agree. I will move it into the comment in the code.\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-09T23:06:17Z",
    "diffHunk": "@@ -163,3 +177,100 @@ each iteration, to compute the gradient direction.\n Available algorithms for gradient descent:\n \n * [GradientDescent.runMiniBatchSGD](api/mllib/index.html#org.apache.spark.mllib.optimization.GradientDescent)\n+\n+### Limited-memory BFGS\n+L-BFGS is currently only a low-level optimization primitive in `MLlib`. If you want to use L-BFGS in various \n+ML algorithms such as Linear Regression, and Logistic Regression, you have to pass the gradient of objective\n+function, and updater into optimizer yourself instead of using the training APIs like \n+[LogisticRegression.LogisticRegressionWithSGD](api/mllib/index.html#org.apache.spark.mllib.classification.LogisticRegression).\n+See the example below. It will be addressed in the next release. \n+\n+The L1 regularization by using \n+[Updater.L1Updater](api/mllib/index.html#org.apache.spark.mllib.optimization.Updater) will not work since the \n+soft-thresholding logic in L1Updater is designed for gradient descent.\n+\n+The L-BFGS method\n+[LBFGS.runLBFGS](api/scala/index.html#org.apache.spark.mllib.optimization.LBFGS)\n+has the following parameters:\n+\n+* `gradient` is a class that computes the gradient of the objective function\n+being optimized, i.e., with respect to a single training example, at the\n+current parameter value. MLlib includes gradient classes for common loss\n+functions, e.g., hinge, logistic, least-squares.  The gradient class takes as\n+input a training example, its label, and the current parameter value. \n+* `updater` is a class originally designed for gradient decent which computes "
  }],
  "prId": 702
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Move `return` out of the list. It may look like a parameter if we put them together.\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-09T00:39:10Z",
    "diffHunk": "@@ -163,3 +177,100 @@ each iteration, to compute the gradient direction.\n Available algorithms for gradient descent:\n \n * [GradientDescent.runMiniBatchSGD](api/mllib/index.html#org.apache.spark.mllib.optimization.GradientDescent)\n+\n+### Limited-memory BFGS\n+L-BFGS is currently only a low-level optimization primitive in `MLlib`. If you want to use L-BFGS in various \n+ML algorithms such as Linear Regression, and Logistic Regression, you have to pass the gradient of objective\n+function, and updater into optimizer yourself instead of using the training APIs like \n+[LogisticRegression.LogisticRegressionWithSGD](api/mllib/index.html#org.apache.spark.mllib.classification.LogisticRegression).\n+See the example below. It will be addressed in the next release. \n+\n+The L1 regularization by using \n+[Updater.L1Updater](api/mllib/index.html#org.apache.spark.mllib.optimization.Updater) will not work since the \n+soft-thresholding logic in L1Updater is designed for gradient descent.\n+\n+The L-BFGS method\n+[LBFGS.runLBFGS](api/scala/index.html#org.apache.spark.mllib.optimization.LBFGS)\n+has the following parameters:\n+\n+* `gradient` is a class that computes the gradient of the objective function\n+being optimized, i.e., with respect to a single training example, at the\n+current parameter value. MLlib includes gradient classes for common loss\n+functions, e.g., hinge, logistic, least-squares.  The gradient class takes as\n+input a training example, its label, and the current parameter value. \n+* `updater` is a class originally designed for gradient decent which computes \n+the actual gradient descent step. However, we're able to take the gradient and \n+loss of objective function of regularization for L-BFGS by ignoring the part of logic\n+only for gradient decent such as adaptive step size stuff. We will refactorize\n+this into regularizer to replace updater to separate the logic between \n+regularization and step update later. \n+* `numCorrections` is the number of corrections used in the L-BFGS update. 10 is \n+recommended.\n+* `maxNumIterations` is the maximal number of iterations that L-BFGS can be run.\n+* `regParam` is the regularization parameter when using regularization.\n+* `return` A tuple containing two elements. The first element is a column matrix"
  }],
  "prId": 702
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "We don't need `breeze` for the example. See my next comment.\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-09T00:40:04Z",
    "diffHunk": "@@ -163,3 +177,100 @@ each iteration, to compute the gradient direction.\n Available algorithms for gradient descent:\n \n * [GradientDescent.runMiniBatchSGD](api/mllib/index.html#org.apache.spark.mllib.optimization.GradientDescent)\n+\n+### Limited-memory BFGS\n+L-BFGS is currently only a low-level optimization primitive in `MLlib`. If you want to use L-BFGS in various \n+ML algorithms such as Linear Regression, and Logistic Regression, you have to pass the gradient of objective\n+function, and updater into optimizer yourself instead of using the training APIs like \n+[LogisticRegression.LogisticRegressionWithSGD](api/mllib/index.html#org.apache.spark.mllib.classification.LogisticRegression).\n+See the example below. It will be addressed in the next release. \n+\n+The L1 regularization by using \n+[Updater.L1Updater](api/mllib/index.html#org.apache.spark.mllib.optimization.Updater) will not work since the \n+soft-thresholding logic in L1Updater is designed for gradient descent.\n+\n+The L-BFGS method\n+[LBFGS.runLBFGS](api/scala/index.html#org.apache.spark.mllib.optimization.LBFGS)\n+has the following parameters:\n+\n+* `gradient` is a class that computes the gradient of the objective function\n+being optimized, i.e., with respect to a single training example, at the\n+current parameter value. MLlib includes gradient classes for common loss\n+functions, e.g., hinge, logistic, least-squares.  The gradient class takes as\n+input a training example, its label, and the current parameter value. \n+* `updater` is a class originally designed for gradient decent which computes \n+the actual gradient descent step. However, we're able to take the gradient and \n+loss of objective function of regularization for L-BFGS by ignoring the part of logic\n+only for gradient decent such as adaptive step size stuff. We will refactorize\n+this into regularizer to replace updater to separate the logic between \n+regularization and step update later. \n+* `numCorrections` is the number of corrections used in the L-BFGS update. 10 is \n+recommended.\n+* `maxNumIterations` is the maximal number of iterations that L-BFGS can be run.\n+* `regParam` is the regularization parameter when using regularization.\n+* `return` A tuple containing two elements. The first element is a column matrix\n+containing weights for every feature, and the second element is an array containing \n+the loss computed for every iteration.\n+\n+Here is an example to train binary logistic regression with L2 regularization using\n+L-BFGS optimizer. \n+{% highlight scala %}\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n+import org.apache.spark.mllib.linalg.Vectors\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.mllib.classification.LogisticRegressionModel\n+import breeze.linalg.{DenseVector => BDV}"
  }],
  "prId": 702
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "I added `MLUtils.appendBias` recently. Could you switch to it?\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-09T00:40:44Z",
    "diffHunk": "@@ -163,3 +177,100 @@ each iteration, to compute the gradient direction.\n Available algorithms for gradient descent:\n \n * [GradientDescent.runMiniBatchSGD](api/mllib/index.html#org.apache.spark.mllib.optimization.GradientDescent)\n+\n+### Limited-memory BFGS\n+L-BFGS is currently only a low-level optimization primitive in `MLlib`. If you want to use L-BFGS in various \n+ML algorithms such as Linear Regression, and Logistic Regression, you have to pass the gradient of objective\n+function, and updater into optimizer yourself instead of using the training APIs like \n+[LogisticRegression.LogisticRegressionWithSGD](api/mllib/index.html#org.apache.spark.mllib.classification.LogisticRegression).\n+See the example below. It will be addressed in the next release. \n+\n+The L1 regularization by using \n+[Updater.L1Updater](api/mllib/index.html#org.apache.spark.mllib.optimization.Updater) will not work since the \n+soft-thresholding logic in L1Updater is designed for gradient descent.\n+\n+The L-BFGS method\n+[LBFGS.runLBFGS](api/scala/index.html#org.apache.spark.mllib.optimization.LBFGS)\n+has the following parameters:\n+\n+* `gradient` is a class that computes the gradient of the objective function\n+being optimized, i.e., with respect to a single training example, at the\n+current parameter value. MLlib includes gradient classes for common loss\n+functions, e.g., hinge, logistic, least-squares.  The gradient class takes as\n+input a training example, its label, and the current parameter value. \n+* `updater` is a class originally designed for gradient decent which computes \n+the actual gradient descent step. However, we're able to take the gradient and \n+loss of objective function of regularization for L-BFGS by ignoring the part of logic\n+only for gradient decent such as adaptive step size stuff. We will refactorize\n+this into regularizer to replace updater to separate the logic between \n+regularization and step update later. \n+* `numCorrections` is the number of corrections used in the L-BFGS update. 10 is \n+recommended.\n+* `maxNumIterations` is the maximal number of iterations that L-BFGS can be run.\n+* `regParam` is the regularization parameter when using regularization.\n+* `return` A tuple containing two elements. The first element is a column matrix\n+containing weights for every feature, and the second element is an array containing \n+the loss computed for every iteration.\n+\n+Here is an example to train binary logistic regression with L2 regularization using\n+L-BFGS optimizer. \n+{% highlight scala %}\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n+import org.apache.spark.mllib.linalg.Vectors\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.mllib.classification.LogisticRegressionModel\n+import breeze.linalg.{DenseVector => BDV}\n+\n+val data = MLUtils.loadLibSVMFile(sc, \"mllib/data/sample_libsvm_data.txt\")\n+val numFeatures = data.take(1)(0).features.size\n+\n+// Split data into training (60%) and test (40%).\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)\n+\n+// Prepend 1 into the training data as intercept.\n+val training = splits(0).map(x =>\n+  (x.label, Vectors.fromBreeze(\n+    BDV.vertcat(BDV.ones[Double](1), x.features.toBreeze.toDenseVector)))"
  }],
  "prId": 702
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`appendBias` puts `1.0` at the end of the vector. So the slicing here needs update.\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-10T00:53:43Z",
    "diffHunk": "@@ -163,3 +171,108 @@ each iteration, to compute the gradient direction.\n Available algorithms for gradient descent:\n \n * [GradientDescent.runMiniBatchSGD](api/mllib/index.html#org.apache.spark.mllib.optimization.GradientDescent)\n+\n+### Limited-memory BFGS\n+L-BFGS is currently only a low-level optimization primitive in `MLlib`. If you want to use L-BFGS in various \n+ML algorithms such as Linear Regression, and Logistic Regression, you have to pass the gradient of objective\n+function, and updater into optimizer yourself instead of using the training APIs like \n+[LogisticRegression.LogisticRegressionWithSGD](api/mllib/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithSGD).\n+See the example below. It will be addressed in the next release. \n+\n+The L1 regularization by using \n+[L1Updater](api/mllib/index.html#org.apache.spark.mllib.optimization.L1Updater) will not work since the \n+soft-thresholding logic in L1Updater is designed for gradient descent. See the developer's note.\n+\n+The L-BFGS method\n+[LBFGS.runLBFGS](api/scala/index.html#org.apache.spark.mllib.optimization.LBFGS)\n+has the following parameters:\n+\n+* `gradient` is a class that computes the gradient of the objective function\n+being optimized, i.e., with respect to a single training example, at the\n+current parameter value. MLlib includes gradient classes for common loss\n+functions, e.g., hinge, logistic, least-squares.  The gradient class takes as\n+input a training example, its label, and the current parameter value. \n+* `updater` is a class that computes the gradient and loss of objective function \n+of the regularization part for L-BFGS. MLlib includes updaters for cases without \n+regularization, as well as L2 regularizer. \n+* `numCorrections` is the number of corrections used in the L-BFGS update. 10 is \n+recommended.\n+* `maxNumIterations` is the maximal number of iterations that L-BFGS can be run.\n+* `regParam` is the regularization parameter when using regularization.\n+\n+\n+The `return` is a tuple containing two elements. The first element is a column matrix\n+containing weights for every feature, and the second element is an array containing \n+the loss computed for every iteration.\n+\n+Here is an example to train binary logistic regression with L2 regularization using\n+L-BFGS optimizer. \n+{% highlight scala %}\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n+import org.apache.spark.mllib.linalg.Vectors\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.mllib.classification.LogisticRegressionModel\n+\n+val data = MLUtils.loadLibSVMFile(sc, \"mllib/data/sample_libsvm_data.txt\")\n+val numFeatures = data.take(1)(0).features.size\n+\n+// Split data into training (60%) and test (40%).\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)\n+\n+// Prepend 1 into the training data as intercept.\n+val training = splits(0).map(x => (x.label, MLUtils.appendBias(x.features))).cache()\n+\n+val test = splits(1)\n+\n+// Run training algorithm to build the model\n+val numCorrections = 10\n+val convergenceTol = 1e-4\n+val maxNumIterations = 20\n+val regParam = 0.1\n+val initialWeightsWithIntercept = Vectors.dense(new Array[Double](numFeatures + 1))\n+\n+val (weightsWithIntercept, loss) = LBFGS.runLBFGS(\n+  training,\n+  new LogisticGradient(),\n+  new SquaredL2Updater(),\n+  numCorrections,\n+  convergenceTol,\n+  maxNumIterations,\n+  regParam,\n+  initialWeightsWithIntercept)\n+\n+val model = new LogisticRegressionModel(\n+  Vectors.dense(weightsWithIntercept.toArray.slice(1, weightsWithIntercept.size)),"
  }, {
    "author": {
      "login": "dbtsai"
    },
    "body": "Why don't we have prependOne in the MLUtils as well? Due to the scope, users can not use prependOne. It's more intuitive to have intercept as first element. \n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-10T07:23:50Z",
    "diffHunk": "@@ -163,3 +171,108 @@ each iteration, to compute the gradient direction.\n Available algorithms for gradient descent:\n \n * [GradientDescent.runMiniBatchSGD](api/mllib/index.html#org.apache.spark.mllib.optimization.GradientDescent)\n+\n+### Limited-memory BFGS\n+L-BFGS is currently only a low-level optimization primitive in `MLlib`. If you want to use L-BFGS in various \n+ML algorithms such as Linear Regression, and Logistic Regression, you have to pass the gradient of objective\n+function, and updater into optimizer yourself instead of using the training APIs like \n+[LogisticRegression.LogisticRegressionWithSGD](api/mllib/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithSGD).\n+See the example below. It will be addressed in the next release. \n+\n+The L1 regularization by using \n+[L1Updater](api/mllib/index.html#org.apache.spark.mllib.optimization.L1Updater) will not work since the \n+soft-thresholding logic in L1Updater is designed for gradient descent. See the developer's note.\n+\n+The L-BFGS method\n+[LBFGS.runLBFGS](api/scala/index.html#org.apache.spark.mllib.optimization.LBFGS)\n+has the following parameters:\n+\n+* `gradient` is a class that computes the gradient of the objective function\n+being optimized, i.e., with respect to a single training example, at the\n+current parameter value. MLlib includes gradient classes for common loss\n+functions, e.g., hinge, logistic, least-squares.  The gradient class takes as\n+input a training example, its label, and the current parameter value. \n+* `updater` is a class that computes the gradient and loss of objective function \n+of the regularization part for L-BFGS. MLlib includes updaters for cases without \n+regularization, as well as L2 regularizer. \n+* `numCorrections` is the number of corrections used in the L-BFGS update. 10 is \n+recommended.\n+* `maxNumIterations` is the maximal number of iterations that L-BFGS can be run.\n+* `regParam` is the regularization parameter when using regularization.\n+\n+\n+The `return` is a tuple containing two elements. The first element is a column matrix\n+containing weights for every feature, and the second element is an array containing \n+the loss computed for every iteration.\n+\n+Here is an example to train binary logistic regression with L2 regularization using\n+L-BFGS optimizer. \n+{% highlight scala %}\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n+import org.apache.spark.mllib.linalg.Vectors\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.mllib.classification.LogisticRegressionModel\n+\n+val data = MLUtils.loadLibSVMFile(sc, \"mllib/data/sample_libsvm_data.txt\")\n+val numFeatures = data.take(1)(0).features.size\n+\n+// Split data into training (60%) and test (40%).\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)\n+\n+// Prepend 1 into the training data as intercept.\n+val training = splits(0).map(x => (x.label, MLUtils.appendBias(x.features))).cache()\n+\n+val test = splits(1)\n+\n+// Run training algorithm to build the model\n+val numCorrections = 10\n+val convergenceTol = 1e-4\n+val maxNumIterations = 20\n+val regParam = 0.1\n+val initialWeightsWithIntercept = Vectors.dense(new Array[Double](numFeatures + 1))\n+\n+val (weightsWithIntercept, loss) = LBFGS.runLBFGS(\n+  training,\n+  new LogisticGradient(),\n+  new SquaredL2Updater(),\n+  numCorrections,\n+  convergenceTol,\n+  maxNumIterations,\n+  regParam,\n+  initialWeightsWithIntercept)\n+\n+val model = new LogisticRegressionModel(\n+  Vectors.dense(weightsWithIntercept.toArray.slice(1, weightsWithIntercept.size)),"
  }],
  "prId": 702
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`updater` -> `Updater`\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-10T00:55:21Z",
    "diffHunk": "@@ -163,3 +171,108 @@ each iteration, to compute the gradient direction.\n Available algorithms for gradient descent:\n \n * [GradientDescent.runMiniBatchSGD](api/mllib/index.html#org.apache.spark.mllib.optimization.GradientDescent)\n+\n+### Limited-memory BFGS\n+L-BFGS is currently only a low-level optimization primitive in `MLlib`. If you want to use L-BFGS in various \n+ML algorithms such as Linear Regression, and Logistic Regression, you have to pass the gradient of objective\n+function, and updater into optimizer yourself instead of using the training APIs like \n+[LogisticRegression.LogisticRegressionWithSGD](api/mllib/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithSGD).\n+See the example below. It will be addressed in the next release. \n+\n+The L1 regularization by using \n+[L1Updater](api/mllib/index.html#org.apache.spark.mllib.optimization.L1Updater) will not work since the \n+soft-thresholding logic in L1Updater is designed for gradient descent. See the developer's note.\n+\n+The L-BFGS method\n+[LBFGS.runLBFGS](api/scala/index.html#org.apache.spark.mllib.optimization.LBFGS)\n+has the following parameters:\n+\n+* `gradient` is a class that computes the gradient of the objective function\n+being optimized, i.e., with respect to a single training example, at the\n+current parameter value. MLlib includes gradient classes for common loss\n+functions, e.g., hinge, logistic, least-squares.  The gradient class takes as\n+input a training example, its label, and the current parameter value. \n+* `updater` is a class that computes the gradient and loss of objective function \n+of the regularization part for L-BFGS. MLlib includes updaters for cases without \n+regularization, as well as L2 regularizer. \n+* `numCorrections` is the number of corrections used in the L-BFGS update. 10 is \n+recommended.\n+* `maxNumIterations` is the maximal number of iterations that L-BFGS can be run.\n+* `regParam` is the regularization parameter when using regularization.\n+\n+\n+The `return` is a tuple containing two elements. The first element is a column matrix\n+containing weights for every feature, and the second element is an array containing \n+the loss computed for every iteration.\n+\n+Here is an example to train binary logistic regression with L2 regularization using\n+L-BFGS optimizer. \n+{% highlight scala %}\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n+import org.apache.spark.mllib.linalg.Vectors\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.mllib.classification.LogisticRegressionModel\n+\n+val data = MLUtils.loadLibSVMFile(sc, \"mllib/data/sample_libsvm_data.txt\")\n+val numFeatures = data.take(1)(0).features.size\n+\n+// Split data into training (60%) and test (40%).\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)\n+\n+// Prepend 1 into the training data as intercept.\n+val training = splits(0).map(x => (x.label, MLUtils.appendBias(x.features))).cache()\n+\n+val test = splits(1)\n+\n+// Run training algorithm to build the model\n+val numCorrections = 10\n+val convergenceTol = 1e-4\n+val maxNumIterations = 20\n+val regParam = 0.1\n+val initialWeightsWithIntercept = Vectors.dense(new Array[Double](numFeatures + 1))\n+\n+val (weightsWithIntercept, loss) = LBFGS.runLBFGS(\n+  training,\n+  new LogisticGradient(),\n+  new SquaredL2Updater(),\n+  numCorrections,\n+  convergenceTol,\n+  maxNumIterations,\n+  regParam,\n+  initialWeightsWithIntercept)\n+\n+val model = new LogisticRegressionModel(\n+  Vectors.dense(weightsWithIntercept.toArray.slice(1, weightsWithIntercept.size)),\n+  weightsWithIntercept(0))\n+\n+// Clear the default threshold.\n+model.clearThreshold()\n+\n+// Compute raw scores on the test set.\n+val scoreAndLabels = test.map { point =>\n+  val score = model.predict(point.features)\n+  (score, point.label)\n+}\n+\n+// Get evaluation metrics.\n+val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n+val auROC = metrics.areaUnderROC()\n+\n+println(\"Loss of each step in training process\")\n+loss.foreach(println)\n+println(\"Area under ROC = \" + auROC)\n+{% endhighlight %}\n+\n+#### Developer's note\n+Since the Hessian is constructed approximately from previous gradient evaluations, \n+the objective function can not be changed during the optimization process. \n+As a result, Stochastic L-BFGS will not work naively by just using miniBatch; \n+therefore, we don't provide this until we have better understanding.\n+\n+* `updater` is a class originally designed for gradient decent which computes "
  }],
  "prId": 702
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`[LogisticRegression.LogisticRegressionWithSGD]` -> `[LogisticRegressionWithSGD]`\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-10T00:57:03Z",
    "diffHunk": "@@ -163,3 +171,108 @@ each iteration, to compute the gradient direction.\n Available algorithms for gradient descent:\n \n * [GradientDescent.runMiniBatchSGD](api/mllib/index.html#org.apache.spark.mllib.optimization.GradientDescent)\n+\n+### Limited-memory BFGS\n+L-BFGS is currently only a low-level optimization primitive in `MLlib`. If you want to use L-BFGS in various \n+ML algorithms such as Linear Regression, and Logistic Regression, you have to pass the gradient of objective\n+function, and updater into optimizer yourself instead of using the training APIs like \n+[LogisticRegression.LogisticRegressionWithSGD](api/mllib/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithSGD)."
  }],
  "prId": 702
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "Remove extra empty line.\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-10T00:57:50Z",
    "diffHunk": "@@ -163,3 +171,108 @@ each iteration, to compute the gradient direction.\n Available algorithms for gradient descent:\n \n * [GradientDescent.runMiniBatchSGD](api/mllib/index.html#org.apache.spark.mllib.optimization.GradientDescent)\n+\n+### Limited-memory BFGS\n+L-BFGS is currently only a low-level optimization primitive in `MLlib`. If you want to use L-BFGS in various \n+ML algorithms such as Linear Regression, and Logistic Regression, you have to pass the gradient of objective\n+function, and updater into optimizer yourself instead of using the training APIs like \n+[LogisticRegression.LogisticRegressionWithSGD](api/mllib/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithSGD).\n+See the example below. It will be addressed in the next release. \n+\n+The L1 regularization by using \n+[L1Updater](api/mllib/index.html#org.apache.spark.mllib.optimization.L1Updater) will not work since the \n+soft-thresholding logic in L1Updater is designed for gradient descent. See the developer's note.\n+\n+The L-BFGS method\n+[LBFGS.runLBFGS](api/scala/index.html#org.apache.spark.mllib.optimization.LBFGS)\n+has the following parameters:\n+\n+* `gradient` is a class that computes the gradient of the objective function\n+being optimized, i.e., with respect to a single training example, at the\n+current parameter value. MLlib includes gradient classes for common loss\n+functions, e.g., hinge, logistic, least-squares.  The gradient class takes as\n+input a training example, its label, and the current parameter value. \n+* `updater` is a class that computes the gradient and loss of objective function \n+of the regularization part for L-BFGS. MLlib includes updaters for cases without \n+regularization, as well as L2 regularizer. \n+* `numCorrections` is the number of corrections used in the L-BFGS update. 10 is \n+recommended.\n+* `maxNumIterations` is the maximal number of iterations that L-BFGS can be run.\n+* `regParam` is the regularization parameter when using regularization.\n+\n+"
  }],
  "prId": 702
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`Prepand` -> `Append`.\n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-10T00:58:24Z",
    "diffHunk": "@@ -163,3 +171,108 @@ each iteration, to compute the gradient direction.\n Available algorithms for gradient descent:\n \n * [GradientDescent.runMiniBatchSGD](api/mllib/index.html#org.apache.spark.mllib.optimization.GradientDescent)\n+\n+### Limited-memory BFGS\n+L-BFGS is currently only a low-level optimization primitive in `MLlib`. If you want to use L-BFGS in various \n+ML algorithms such as Linear Regression, and Logistic Regression, you have to pass the gradient of objective\n+function, and updater into optimizer yourself instead of using the training APIs like \n+[LogisticRegression.LogisticRegressionWithSGD](api/mllib/index.html#org.apache.spark.mllib.classification.LogisticRegressionWithSGD).\n+See the example below. It will be addressed in the next release. \n+\n+The L1 regularization by using \n+[L1Updater](api/mllib/index.html#org.apache.spark.mllib.optimization.L1Updater) will not work since the \n+soft-thresholding logic in L1Updater is designed for gradient descent. See the developer's note.\n+\n+The L-BFGS method\n+[LBFGS.runLBFGS](api/scala/index.html#org.apache.spark.mllib.optimization.LBFGS)\n+has the following parameters:\n+\n+* `gradient` is a class that computes the gradient of the objective function\n+being optimized, i.e., with respect to a single training example, at the\n+current parameter value. MLlib includes gradient classes for common loss\n+functions, e.g., hinge, logistic, least-squares.  The gradient class takes as\n+input a training example, its label, and the current parameter value. \n+* `updater` is a class that computes the gradient and loss of objective function \n+of the regularization part for L-BFGS. MLlib includes updaters for cases without \n+regularization, as well as L2 regularizer. \n+* `numCorrections` is the number of corrections used in the L-BFGS update. 10 is \n+recommended.\n+* `maxNumIterations` is the maximal number of iterations that L-BFGS can be run.\n+* `regParam` is the regularization parameter when using regularization.\n+\n+\n+The `return` is a tuple containing two elements. The first element is a column matrix\n+containing weights for every feature, and the second element is an array containing \n+the loss computed for every iteration.\n+\n+Here is an example to train binary logistic regression with L2 regularization using\n+L-BFGS optimizer. \n+{% highlight scala %}\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n+import org.apache.spark.mllib.linalg.Vectors\n+import org.apache.spark.mllib.util.MLUtils\n+import org.apache.spark.mllib.classification.LogisticRegressionModel\n+\n+val data = MLUtils.loadLibSVMFile(sc, \"mllib/data/sample_libsvm_data.txt\")\n+val numFeatures = data.take(1)(0).features.size\n+\n+// Split data into training (60%) and test (40%).\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)\n+\n+// Prepend 1 into the training data as intercept."
  }],
  "prId": 702
}, {
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "This is the first time `L-BFGS` appears in the guide. So let us use `Limited-memory BFGS (L-BFGS)`. Well, it is not necessary to explain `BFGS` .. \n",
    "commit": "07122155434422167b2dad27e9133b01beae6f72",
    "createdAt": "2014-05-10T01:04:15Z",
    "diffHunk": "@@ -128,10 +127,19 @@ is sampled, i.e. `$|S|=$ miniBatchFraction $\\cdot n = 1$`, then the algorithm is\n standard SGD. In that case, the step direction depends from the uniformly random sampling of the\n point.\n \n-\n+### L-BFGS"
  }],
  "prId": 702
}]