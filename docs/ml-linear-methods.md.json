[{
  "comments": [{
    "author": {
      "login": "MechCoder"
    },
    "body": "DataFrame?\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-18T18:29:57Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Datafram`s in"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Whoops, yep that's right\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-18T18:37:37Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Datafram`s in"
  }],
  "prId": 8197
}, {
  "comments": [{
    "author": {
      "login": "MechCoder"
    },
    "body": "annotated\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-18T18:30:33Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Datafram`s in\n+`BinaryLogisticRegressionSummary` are annoted `@transient` and hence"
  }, {
    "author": {
      "login": "MechCoder"
    },
    "body": "Do you think it might be useful to mention why this is done so (i.e because it is expensive to store these?)\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-18T18:34:23Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Datafram`s in\n+`BinaryLogisticRegressionSummary` are annoted `@transient` and hence"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Yep, you're right.\n\nIMO the user guide is targeted at helping users use our library as fast as possible. Hence, we should not include information that isn't relevant to someone who just wants to use the library.\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-18T18:39:45Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Datafram`s in\n+`BinaryLogisticRegressionSummary` are annoted `@transient` and hence"
  }],
  "prId": 8197
}, {
  "comments": [{
    "author": {
      "login": "MechCoder"
    },
    "body": "Actually the cast will not change. Even when multiclass classification is supported, we will have to do an explicit cast since the binary metrics will not be available in the sealed `LogisticRegressionSummary` trait. Only the multiclass metrics which will not raise errors for binary data will be common to both.\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-18T18:37:14Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Datafram`s in\n+`BinaryLogisticRegressionSummary` are annoted `@transient` and hence\n+only available on the driver.\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+[`LogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionTrainingSummary)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionModel).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary).\n+This will likely change when multiclass classification is supported.",
    "line": 99
  }, {
    "author": {
      "login": "MechCoder"
    },
    "body": "I suggest we just remove this line. What say?\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-18T18:48:11Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Datafram`s in\n+`BinaryLogisticRegressionSummary` are annoted `@transient` and hence\n+only available on the driver.\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+[`LogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionTrainingSummary)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionModel).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary).\n+This will likely change when multiclass classification is supported.",
    "line": 99
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Downcasting is almost always an indication of a poor abstraction and IMO the stabilized API should not require any explicit typecasting by the end user, [here's an explanation](http://codebetter.com/jeremymiller/2006/12/26/downcasting-is-a-code-smell/)\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-18T19:58:30Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Datafram`s in\n+`BinaryLogisticRegressionSummary` are annoted `@transient` and hence\n+only available on the driver.\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+[`LogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionTrainingSummary)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionModel).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary).\n+This will likely change when multiclass classification is supported.",
    "line": 99
  }, {
    "author": {
      "login": "MechCoder"
    },
    "body": "The other option would be just to make all metrics available in `LogisticRegressionSummary`and raise errors when multiclass data is used. In that case there would have not been a need for a `BinaryLogisticRegressionSummary`. However this was preferred.\n\nSee:\nhttps://github.com/apache/spark/pull/7538#issuecomment-128445916 and\nhttps://github.com/apache/spark/pull/7538#issuecomment-127395327 and\nhttps://github.com/apache/spark/pull/7538#issuecomment-127427628\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-18T20:17:56Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Datafram`s in\n+`BinaryLogisticRegressionSummary` are annoted `@transient` and hence\n+only available on the driver.\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+[`LogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionTrainingSummary)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionModel).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary).\n+This will likely change when multiclass classification is supported.",
    "line": 99
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Synced with @jkbradley offline. Summary:\n\nWe should not require end users to perform any sort of downcasting in the stabilized API. This is OK for now since the API is still experimental.\n\nEventually we could provide two methods, a `summary : LogisticRegressionSummary` and a `binarySummary : BInaryLogisticRegressionSummary` which errors when called on a multiclass LRModel. This will be easy to implement because `summary` is returning the base `LogisticRegressionSummary` class so will not require any public API change.\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-18T20:29:16Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Datafram`s in\n+`BinaryLogisticRegressionSummary` are annoted `@transient` and hence\n+only available on the driver.\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+[`LogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionTrainingSummary)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionModel).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary).\n+This will likely change when multiclass classification is supported.",
    "line": 99
  }],
  "prId": 8197
}, {
  "comments": [{
    "author": {
      "login": "MechCoder"
    },
    "body": "I don't know about Java style but from the other examples I think the colon should be one place to the left\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-18T18:40:28Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Datafram`s in\n+`BinaryLogisticRegressionSummary` are annoted `@transient` and hence\n+only available on the driver.\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+[`LogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionTrainingSummary)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionModel).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary).\n+This will likely change when multiclass classification is supported.\n+\n+Continuing the earlier example:\n+\n+{% highlight scala %}\n+// Extract the summary from the returned LogisticRegressionModel instance trained in the earlier example\n+val trainingSummary = lrModel.summary\n+\n+// Obtain the loss per iteration.\n+val objectiveHistory = trainingSummary.objectiveHistory\n+objectiveHistory.foreach(loss => println(loss))\n+\n+// Obtain the metrics useful to judge performance on test data.\n+// We cast the summary to a BinaryLogisticRegressionSummary since the problem is a\n+// binary classification problem.\n+val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]\n+\n+// Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n+val roc = binarySummary.roc\n+roc.show()\n+roc.select(\"FPR\").show()\n+println(binarySummary.areaUnderROC)\n+\n+// Get the threshold corresponding to the maximum F-Measure and rerun LogisticRegression with\n+// this selected threshold.\n+val fMeasure = binarySummary.fMeasureByThreshold\n+val maxFMeasure = fMeasure.select(max(\"F-Measure\")).head().getDouble(0)\n+val bestThreshold = fMeasure.where($\"F-Measure\" === maxFMeasure).\n+  select(\"threshold\").head().getDouble(0)\n+logReg.setThreshold(bestThreshold)\n+logReg.fit(logRegDataFrame)\n+{% endhighlight %}\n </div>\n \n-### Optimization\n+<div data-lang=\"java\" markdown=\"1\">\n+[`LogisticRegressionTrainingSummary`](api/java/org/apache/spark/ml/classification/LogisticRegressionTrainingSummary.html)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/java/org/apache/spark/ml/classification/LogisticRegressionModel.html).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/java/org/apache/spark/ml/classification/BinaryLogisticRegressionTrainingSummary.html).\n+This will likely change when multiclass classification is supported.\n+\n+Continuing the earlier example:\n+\n+{% highlight java %}\n+// Extract the summary from the returned LogisticRegressionModel instance trained in the earlier example\n+LogisticRegressionTrainingSummary trainingSummary = logRegModel.summary();\n+\n+// Obtain the loss per iteration.\n+double[] objectiveHistory = trainingSummary.objectiveHistory();\n+for (double lossPerIteration : objectiveHistory) {",
    "line": 151
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "I don't think so, see [Google's Java Style Guide](https://google.github.io/styleguide/javaguide.html#s4.6.2-horizontal-whitespace)\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-18T19:59:36Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Datafram`s in\n+`BinaryLogisticRegressionSummary` are annoted `@transient` and hence\n+only available on the driver.\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+[`LogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionTrainingSummary)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionModel).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary).\n+This will likely change when multiclass classification is supported.\n+\n+Continuing the earlier example:\n+\n+{% highlight scala %}\n+// Extract the summary from the returned LogisticRegressionModel instance trained in the earlier example\n+val trainingSummary = lrModel.summary\n+\n+// Obtain the loss per iteration.\n+val objectiveHistory = trainingSummary.objectiveHistory\n+objectiveHistory.foreach(loss => println(loss))\n+\n+// Obtain the metrics useful to judge performance on test data.\n+// We cast the summary to a BinaryLogisticRegressionSummary since the problem is a\n+// binary classification problem.\n+val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]\n+\n+// Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n+val roc = binarySummary.roc\n+roc.show()\n+roc.select(\"FPR\").show()\n+println(binarySummary.areaUnderROC)\n+\n+// Get the threshold corresponding to the maximum F-Measure and rerun LogisticRegression with\n+// this selected threshold.\n+val fMeasure = binarySummary.fMeasureByThreshold\n+val maxFMeasure = fMeasure.select(max(\"F-Measure\")).head().getDouble(0)\n+val bestThreshold = fMeasure.where($\"F-Measure\" === maxFMeasure).\n+  select(\"threshold\").head().getDouble(0)\n+logReg.setThreshold(bestThreshold)\n+logReg.fit(logRegDataFrame)\n+{% endhighlight %}\n </div>\n \n-### Optimization\n+<div data-lang=\"java\" markdown=\"1\">\n+[`LogisticRegressionTrainingSummary`](api/java/org/apache/spark/ml/classification/LogisticRegressionTrainingSummary.html)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/java/org/apache/spark/ml/classification/LogisticRegressionModel.html).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/java/org/apache/spark/ml/classification/BinaryLogisticRegressionTrainingSummary.html).\n+This will likely change when multiclass classification is supported.\n+\n+Continuing the earlier example:\n+\n+{% highlight java %}\n+// Extract the summary from the returned LogisticRegressionModel instance trained in the earlier example\n+LogisticRegressionTrainingSummary trainingSummary = logRegModel.summary();\n+\n+// Obtain the loss per iteration.\n+double[] objectiveHistory = trainingSummary.objectiveHistory();\n+for (double lossPerIteration : objectiveHistory) {",
    "line": 151
  }, {
    "author": {
      "login": "MechCoder"
    },
    "body": "I see then other places such as this (https://github.com/apache/spark/blob/master/mllib/src/test/java/org/apache/spark/ml/clustering/JavaKMeansSuite.java#L68) have to be changed.\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-18T20:11:57Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Datafram`s in\n+`BinaryLogisticRegressionSummary` are annoted `@transient` and hence\n+only available on the driver.\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+[`LogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionTrainingSummary)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionModel).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary).\n+This will likely change when multiclass classification is supported.\n+\n+Continuing the earlier example:\n+\n+{% highlight scala %}\n+// Extract the summary from the returned LogisticRegressionModel instance trained in the earlier example\n+val trainingSummary = lrModel.summary\n+\n+// Obtain the loss per iteration.\n+val objectiveHistory = trainingSummary.objectiveHistory\n+objectiveHistory.foreach(loss => println(loss))\n+\n+// Obtain the metrics useful to judge performance on test data.\n+// We cast the summary to a BinaryLogisticRegressionSummary since the problem is a\n+// binary classification problem.\n+val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]\n+\n+// Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n+val roc = binarySummary.roc\n+roc.show()\n+roc.select(\"FPR\").show()\n+println(binarySummary.areaUnderROC)\n+\n+// Get the threshold corresponding to the maximum F-Measure and rerun LogisticRegression with\n+// this selected threshold.\n+val fMeasure = binarySummary.fMeasureByThreshold\n+val maxFMeasure = fMeasure.select(max(\"F-Measure\")).head().getDouble(0)\n+val bestThreshold = fMeasure.where($\"F-Measure\" === maxFMeasure).\n+  select(\"threshold\").head().getDouble(0)\n+logReg.setThreshold(bestThreshold)\n+logReg.fit(logRegDataFrame)\n+{% endhighlight %}\n </div>\n \n-### Optimization\n+<div data-lang=\"java\" markdown=\"1\">\n+[`LogisticRegressionTrainingSummary`](api/java/org/apache/spark/ml/classification/LogisticRegressionTrainingSummary.html)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/java/org/apache/spark/ml/classification/LogisticRegressionModel.html).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/java/org/apache/spark/ml/classification/BinaryLogisticRegressionTrainingSummary.html).\n+This will likely change when multiclass classification is supported.\n+\n+Continuing the earlier example:\n+\n+{% highlight java %}\n+// Extract the summary from the returned LogisticRegressionModel instance trained in the earlier example\n+LogisticRegressionTrainingSummary trainingSummary = logRegModel.summary();\n+\n+// Obtain the loss per iteration.\n+double[] objectiveHistory = trainingSummary.objectiveHistory();\n+for (double lossPerIteration : objectiveHistory) {",
    "line": 151
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Perhaps... the [Spark style guide](https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide) only covers Scala and Python so I was just going by past experience on this change. We could try to get a Java style guide in if there's a community need for it\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-18T20:16:24Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Datafram`s in\n+`BinaryLogisticRegressionSummary` are annoted `@transient` and hence\n+only available on the driver.\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+[`LogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionTrainingSummary)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionModel).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary).\n+This will likely change when multiclass classification is supported.\n+\n+Continuing the earlier example:\n+\n+{% highlight scala %}\n+// Extract the summary from the returned LogisticRegressionModel instance trained in the earlier example\n+val trainingSummary = lrModel.summary\n+\n+// Obtain the loss per iteration.\n+val objectiveHistory = trainingSummary.objectiveHistory\n+objectiveHistory.foreach(loss => println(loss))\n+\n+// Obtain the metrics useful to judge performance on test data.\n+// We cast the summary to a BinaryLogisticRegressionSummary since the problem is a\n+// binary classification problem.\n+val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]\n+\n+// Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n+val roc = binarySummary.roc\n+roc.show()\n+roc.select(\"FPR\").show()\n+println(binarySummary.areaUnderROC)\n+\n+// Get the threshold corresponding to the maximum F-Measure and rerun LogisticRegression with\n+// this selected threshold.\n+val fMeasure = binarySummary.fMeasureByThreshold\n+val maxFMeasure = fMeasure.select(max(\"F-Measure\")).head().getDouble(0)\n+val bestThreshold = fMeasure.where($\"F-Measure\" === maxFMeasure).\n+  select(\"threshold\").head().getDouble(0)\n+logReg.setThreshold(bestThreshold)\n+logReg.fit(logRegDataFrame)\n+{% endhighlight %}\n </div>\n \n-### Optimization\n+<div data-lang=\"java\" markdown=\"1\">\n+[`LogisticRegressionTrainingSummary`](api/java/org/apache/spark/ml/classification/LogisticRegressionTrainingSummary.html)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/java/org/apache/spark/ml/classification/LogisticRegressionModel.html).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/java/org/apache/spark/ml/classification/BinaryLogisticRegressionTrainingSummary.html).\n+This will likely change when multiclass classification is supported.\n+\n+Continuing the earlier example:\n+\n+{% highlight java %}\n+// Extract the summary from the returned LogisticRegressionModel instance trained in the earlier example\n+LogisticRegressionTrainingSummary trainingSummary = logRegModel.summary();\n+\n+// Obtain the loss per iteration.\n+double[] objectiveHistory = trainingSummary.objectiveHistory();\n+for (double lossPerIteration : objectiveHistory) {",
    "line": 151
  }],
  "prId": 8197
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "I don't think you need to put lambda in, but if you do, then how about putting it outside of big parentheses or brackets to make the equation easier to read?\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-25T17:54:44Z",
    "diffHunk": "@@ -23,20 +23,41 @@ displayTitle: <a href=\"ml-guide.html\">ML</a> - Linear Methods\n \\]`\n \n \n-In MLlib, we implement popular linear methods such as logistic regression and linear least squares with L1 or L2 regularization. Refer to [the linear methods in mllib](mllib-linear-methods.html) for details. In `spark.ml`, we also include Pipelines API for [Elastic net](http://en.wikipedia.org/wiki/Elastic_net_regularization), a hybrid of L1 and L2 regularization proposed in [this paper](http://users.stat.umn.edu/~zouxx019/Papers/elasticnet.pdf). Mathematically it is defined as a linear combination of the L1-norm and the L2-norm:\n+In MLlib, we implement popular linear methods such as logistic\n+regression and linear least squares with $L_1$ or $L_2$ regularization.\n+Refer to [the linear methods in mllib](mllib-linear-methods.html) for\n+details.  In `spark.ml`, we also include Pipelines API for [Elastic\n+net](http://en.wikipedia.org/wiki/Elastic_net_regularization), a hybrid\n+of $L_1$ and $L_2$ regularization proposed in [Zou et al, Regularization\n+and variable selection via the elastic\n+net](http://users.stat.umn.edu/~zouxx019/Papers/elasticnet.pdf).\n+Mathematically, it is defined as a convex combination of the $L_1$ and\n+the $L_2$ regularization terms:\n `\\[\n-\\alpha \\|\\wv\\|_1 + (1-\\alpha) \\frac{1}{2}\\|\\wv\\|_2^2, \\alpha \\in [0, 1].\n+\\alpha~\\lambda \\|\\wv\\|_1 + (1-\\alpha) \\frac{\\lambda}{2}\\|\\wv\\|_2^2, \\alpha \\in [0, 1], \\lambda \\geq 0.",
    "line": 17
  }],
  "prId": 8197
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "\"loss\" --> \"objective\" (which includes regularization)\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-25T17:54:48Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Dataframe` in\n+`BinaryLogisticRegressionSummary` are annotated `@transient` and hence\n+only available on the driver.\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+[`LogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionTrainingSummary)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionModel).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary).\n+This will likely change when multiclass classification is supported.\n+\n+Continuing the earlier example:\n+\n+{% highlight scala %}\n+// Extract the summary from the returned LogisticRegressionModel instance trained in the earlier example\n+val trainingSummary = lrModel.summary\n+\n+// Obtain the loss per iteration.",
    "line": 107
  }],
  "prId": 8197
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "I'd remove this line and instead add a comment to the previous one if you want to say what the column names are.\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-25T17:54:49Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Dataframe` in\n+`BinaryLogisticRegressionSummary` are annotated `@transient` and hence\n+only available on the driver.\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+[`LogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionTrainingSummary)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionModel).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary).\n+This will likely change when multiclass classification is supported.\n+\n+Continuing the earlier example:\n+\n+{% highlight scala %}\n+// Extract the summary from the returned LogisticRegressionModel instance trained in the earlier example\n+val trainingSummary = lrModel.summary\n+\n+// Obtain the loss per iteration.\n+val objectiveHistory = trainingSummary.objectiveHistory\n+objectiveHistory.foreach(loss => println(loss))\n+\n+// Obtain the metrics useful to judge performance on test data.\n+// We cast the summary to a BinaryLogisticRegressionSummary since the problem is a\n+// binary classification problem.\n+val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]\n+\n+// Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n+val roc = binarySummary.roc\n+roc.show()\n+roc.select(\"FPR\").show()",
    "line": 119
  }],
  "prId": 8197
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "There's no need to re-fit the model since the threshold is only used during prediction.  Instead, set the threshold in lrModel.\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-25T17:54:50Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Dataframe` in\n+`BinaryLogisticRegressionSummary` are annotated `@transient` and hence\n+only available on the driver.\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+[`LogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionTrainingSummary)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionModel).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary).\n+This will likely change when multiclass classification is supported.\n+\n+Continuing the earlier example:\n+\n+{% highlight scala %}\n+// Extract the summary from the returned LogisticRegressionModel instance trained in the earlier example\n+val trainingSummary = lrModel.summary\n+\n+// Obtain the loss per iteration.\n+val objectiveHistory = trainingSummary.objectiveHistory\n+objectiveHistory.foreach(loss => println(loss))\n+\n+// Obtain the metrics useful to judge performance on test data.\n+// We cast the summary to a BinaryLogisticRegressionSummary since the problem is a\n+// binary classification problem.\n+val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]\n+\n+// Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n+val roc = binarySummary.roc\n+roc.show()\n+roc.select(\"FPR\").show()\n+println(binarySummary.areaUnderROC)\n+\n+// Get the threshold corresponding to the maximum F-Measure and rerun LogisticRegression with\n+// this selected threshold.\n+val fMeasure = binarySummary.fMeasureByThreshold\n+val maxFMeasure = fMeasure.select(max(\"F-Measure\")).head().getDouble(0)\n+val bestThreshold = fMeasure.where($\"F-Measure\" === maxFMeasure).\n+  select(\"threshold\").head().getDouble(0)\n+logReg.setThreshold(bestThreshold)",
    "line": 128
  }],
  "prId": 8197
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Please add the needed imports for this new Java section.\n",
    "commit": "7bf922c53b0e7f6e6d5304107f432b58ad7b93c7",
    "createdAt": "2015-08-25T17:54:51Z",
    "diffHunk": "@@ -118,12 +133,114 @@ lrModel = lr.fit(training)\n print(\"Weights: \" + str(lrModel.weights))\n print(\"Intercept: \" + str(lrModel.intercept))\n {% endhighlight %}\n+</div>\n \n </div>\n \n+The `spark.ml` implementation of logistic regression also supports\n+extracting a summary of the model over the training set. Note that the\n+predictions and metrics which are stored as `Dataframe` in\n+`BinaryLogisticRegressionSummary` are annotated `@transient` and hence\n+only available on the driver.\n+\n+<div class=\"codetabs\">\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+[`LogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionTrainingSummary)\n+provides a summary for a\n+[`LogisticRegressionModel`](api/scala/index.html#org.apache.spark.ml.classification.LogisticRegressionModel).\n+Currently, only binary classification is supported and the\n+summary must be explicitly cast to\n+[`BinaryLogisticRegressionTrainingSummary`](api/scala/index.html#org.apache.spark.ml.classification.BinaryLogisticRegressionTrainingSummary).\n+This will likely change when multiclass classification is supported.\n+\n+Continuing the earlier example:\n+\n+{% highlight scala %}\n+// Extract the summary from the returned LogisticRegressionModel instance trained in the earlier example\n+val trainingSummary = lrModel.summary\n+\n+// Obtain the loss per iteration.\n+val objectiveHistory = trainingSummary.objectiveHistory\n+objectiveHistory.foreach(loss => println(loss))\n+\n+// Obtain the metrics useful to judge performance on test data.\n+// We cast the summary to a BinaryLogisticRegressionSummary since the problem is a\n+// binary classification problem.\n+val binarySummary = trainingSummary.asInstanceOf[BinaryLogisticRegressionSummary]\n+\n+// Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n+val roc = binarySummary.roc\n+roc.show()\n+roc.select(\"FPR\").show()\n+println(binarySummary.areaUnderROC)\n+\n+// Get the threshold corresponding to the maximum F-Measure and rerun LogisticRegression with\n+// this selected threshold.\n+val fMeasure = binarySummary.fMeasureByThreshold\n+val maxFMeasure = fMeasure.select(max(\"F-Measure\")).head().getDouble(0)\n+val bestThreshold = fMeasure.where($\"F-Measure\" === maxFMeasure).\n+  select(\"threshold\").head().getDouble(0)\n+logReg.setThreshold(bestThreshold)\n+logReg.fit(logRegDataFrame)\n+{% endhighlight %}\n </div>\n \n-### Optimization\n+<div data-lang=\"java\" markdown=\"1\">",
    "line": 134
  }],
  "prId": 8197
}]