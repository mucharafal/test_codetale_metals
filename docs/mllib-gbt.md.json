[{
  "comments": [{
    "author": {
      "login": "manishamde"
    },
    "body": "The weight-based relabeling sounds more like AdaBoost.\n",
    "commit": "70a75f3bea208de7c0869a79734e823f1607b118",
    "createdAt": "2014-12-01T03:19:13Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+---\n+layout: global\n+title: Gradient-Boosted Trees - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Gradient-Boosted Trees\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+[Gradient-Boosted Trees (GBTs)](http://en.wikipedia.org/wiki/Gradient_boosting)\n+are ensembles of [decision trees](mllib-decision-tree.html).\n+GBTs iteratively train decision trees in order to minimize a loss function.\n+Like decision trees, GBTs handle categorical features,\n+extend to the multiclass classification setting, do not require\n+feature scaling, and are able to capture non-linearities and feature interactions.\n+\n+MLlib supports GBTs for binary classification and for regression,\n+using both continuous and categorical features.\n+MLlib implements GBTs using the existing [decision tree](mllib-decision-tree.html) implementation.  Please see the decision tree guide for more information on trees.\n+\n+*Note*: GBTs do not yet support multiclass classification.  For multiclass problems, please use\n+[decision trees](mllib-decision-tree.html) or [Random Forests](mllib-random-forest.html).\n+\n+## Basic algorithm\n+\n+Gradient boosting iteratively trains a sequence of decision trees.\n+On each iteration, the algorithm uses the current ensemble to predict the label of each training instance and then compares the prediction with the true label.  The dataset is re-labeled to put more weight on training instances with poor predictions.  Thus, in the next iteration, the decision tree will help correct for previous mistakes."
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "True, I'll change \"weight\" to \"emphasis.\"\n",
    "commit": "70a75f3bea208de7c0869a79734e823f1607b118",
    "createdAt": "2014-12-01T19:39:25Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+---\n+layout: global\n+title: Gradient-Boosted Trees - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Gradient-Boosted Trees\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+[Gradient-Boosted Trees (GBTs)](http://en.wikipedia.org/wiki/Gradient_boosting)\n+are ensembles of [decision trees](mllib-decision-tree.html).\n+GBTs iteratively train decision trees in order to minimize a loss function.\n+Like decision trees, GBTs handle categorical features,\n+extend to the multiclass classification setting, do not require\n+feature scaling, and are able to capture non-linearities and feature interactions.\n+\n+MLlib supports GBTs for binary classification and for regression,\n+using both continuous and categorical features.\n+MLlib implements GBTs using the existing [decision tree](mllib-decision-tree.html) implementation.  Please see the decision tree guide for more information on trees.\n+\n+*Note*: GBTs do not yet support multiclass classification.  For multiclass problems, please use\n+[decision trees](mllib-decision-tree.html) or [Random Forests](mllib-random-forest.html).\n+\n+## Basic algorithm\n+\n+Gradient boosting iteratively trains a sequence of decision trees.\n+On each iteration, the algorithm uses the current ensemble to predict the label of each training instance and then compares the prediction with the true label.  The dataset is re-labeled to put more weight on training instances with poor predictions.  Thus, in the next iteration, the decision tree will help correct for previous mistakes."
  }],
  "prId": 3461
}, {
  "comments": [{
    "author": {
      "login": "manishamde"
    },
    "body": "I think we should avoid this lest people start quoting us in the future. :-) There is a lot of results comparing RF and Boosting and the results are mixed.\n",
    "commit": "70a75f3bea208de7c0869a79734e823f1607b118",
    "createdAt": "2014-12-01T03:20:54Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+---\n+layout: global\n+title: Gradient-Boosted Trees - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Gradient-Boosted Trees\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+[Gradient-Boosted Trees (GBTs)](http://en.wikipedia.org/wiki/Gradient_boosting)\n+are ensembles of [decision trees](mllib-decision-tree.html).\n+GBTs iteratively train decision trees in order to minimize a loss function.\n+Like decision trees, GBTs handle categorical features,\n+extend to the multiclass classification setting, do not require\n+feature scaling, and are able to capture non-linearities and feature interactions.\n+\n+MLlib supports GBTs for binary classification and for regression,\n+using both continuous and categorical features.\n+MLlib implements GBTs using the existing [decision tree](mllib-decision-tree.html) implementation.  Please see the decision tree guide for more information on trees.\n+\n+*Note*: GBTs do not yet support multiclass classification.  For multiclass problems, please use\n+[decision trees](mllib-decision-tree.html) or [Random Forests](mllib-random-forest.html).\n+\n+## Basic algorithm\n+\n+Gradient boosting iteratively trains a sequence of decision trees.\n+On each iteration, the algorithm uses the current ensemble to predict the label of each training instance and then compares the prediction with the true label.  The dataset is re-labeled to put more weight on training instances with poor predictions.  Thus, in the next iteration, the decision tree will help correct for previous mistakes.\n+\n+The specific weight mechanism is defined by a loss function (discussed below).  With each iteration, GBTs further reduce this loss function on the training data.\n+\n+### Comparison with Random Forests\n+\n+Both GBTs and [Random Forests](mllib-random-forest.html) are algorithms for learning ensembles of trees, but the training processes are different.  There are several practical trade-offs:\n+\n+ * GBTs may be able to achieve the same accuracy using fewer trees, so the model produced may be smaller (faster for test time prediction).\n+ * GBTs train one tree at a time, so they can take longer to train than random forests.  Random Forests can train multiple trees in parallel.\n+   * On the other hand, it is often reasonable to use smaller trees with GBTs than with Random Forests, and training smaller trees takes less time.\n+ * Random Forests can be less prone to overfitting.  Training more trees in a Random Forest reduces the likelihood of overfitting, but training more trees with GBTs increases the likelihood of overfitting.\n+\n+In short, both algorithms can be effective.  GBTs may be more useful if test time prediction speed is important.  Random Forests are arguably more successful in industry."
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "I'll say less : )\n",
    "commit": "70a75f3bea208de7c0869a79734e823f1607b118",
    "createdAt": "2014-12-01T19:43:01Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+---\n+layout: global\n+title: Gradient-Boosted Trees - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Gradient-Boosted Trees\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+[Gradient-Boosted Trees (GBTs)](http://en.wikipedia.org/wiki/Gradient_boosting)\n+are ensembles of [decision trees](mllib-decision-tree.html).\n+GBTs iteratively train decision trees in order to minimize a loss function.\n+Like decision trees, GBTs handle categorical features,\n+extend to the multiclass classification setting, do not require\n+feature scaling, and are able to capture non-linearities and feature interactions.\n+\n+MLlib supports GBTs for binary classification and for regression,\n+using both continuous and categorical features.\n+MLlib implements GBTs using the existing [decision tree](mllib-decision-tree.html) implementation.  Please see the decision tree guide for more information on trees.\n+\n+*Note*: GBTs do not yet support multiclass classification.  For multiclass problems, please use\n+[decision trees](mllib-decision-tree.html) or [Random Forests](mllib-random-forest.html).\n+\n+## Basic algorithm\n+\n+Gradient boosting iteratively trains a sequence of decision trees.\n+On each iteration, the algorithm uses the current ensemble to predict the label of each training instance and then compares the prediction with the true label.  The dataset is re-labeled to put more weight on training instances with poor predictions.  Thus, in the next iteration, the decision tree will help correct for previous mistakes.\n+\n+The specific weight mechanism is defined by a loss function (discussed below).  With each iteration, GBTs further reduce this loss function on the training data.\n+\n+### Comparison with Random Forests\n+\n+Both GBTs and [Random Forests](mllib-random-forest.html) are algorithms for learning ensembles of trees, but the training processes are different.  There are several practical trade-offs:\n+\n+ * GBTs may be able to achieve the same accuracy using fewer trees, so the model produced may be smaller (faster for test time prediction).\n+ * GBTs train one tree at a time, so they can take longer to train than random forests.  Random Forests can train multiple trees in parallel.\n+   * On the other hand, it is often reasonable to use smaller trees with GBTs than with Random Forests, and training smaller trees takes less time.\n+ * Random Forests can be less prone to overfitting.  Training more trees in a Random Forest reduces the likelihood of overfitting, but training more trees with GBTs increases the likelihood of overfitting.\n+\n+In short, both algorithms can be effective.  GBTs may be more useful if test time prediction speed is important.  Random Forests are arguably more successful in industry."
  }],
  "prId": 3461
}, {
  "comments": [{
    "author": {
      "login": "manishamde"
    },
    "body": "I really like this section since this is very useful information. We should try and add some graphs here in a separate PR. However, shouldn't this be in a separate section under Ensemble comparing both RF and Boosting algorithms in terms of performance and accuracy.\n",
    "commit": "70a75f3bea208de7c0869a79734e823f1607b118",
    "createdAt": "2014-12-01T03:25:00Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+---\n+layout: global\n+title: Gradient-Boosted Trees - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Gradient-Boosted Trees\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+[Gradient-Boosted Trees (GBTs)](http://en.wikipedia.org/wiki/Gradient_boosting)\n+are ensembles of [decision trees](mllib-decision-tree.html).\n+GBTs iteratively train decision trees in order to minimize a loss function.\n+Like decision trees, GBTs handle categorical features,\n+extend to the multiclass classification setting, do not require\n+feature scaling, and are able to capture non-linearities and feature interactions.\n+\n+MLlib supports GBTs for binary classification and for regression,\n+using both continuous and categorical features.\n+MLlib implements GBTs using the existing [decision tree](mllib-decision-tree.html) implementation.  Please see the decision tree guide for more information on trees.\n+\n+*Note*: GBTs do not yet support multiclass classification.  For multiclass problems, please use\n+[decision trees](mllib-decision-tree.html) or [Random Forests](mllib-random-forest.html).\n+\n+## Basic algorithm\n+\n+Gradient boosting iteratively trains a sequence of decision trees.\n+On each iteration, the algorithm uses the current ensemble to predict the label of each training instance and then compares the prediction with the true label.  The dataset is re-labeled to put more weight on training instances with poor predictions.  Thus, in the next iteration, the decision tree will help correct for previous mistakes.\n+\n+The specific weight mechanism is defined by a loss function (discussed below).  With each iteration, GBTs further reduce this loss function on the training data.\n+\n+### Comparison with Random Forests"
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "Should we have a new Ensembles section in the guide?  It might be pretty short.  (And I haven't seen experimental results in the guide; would they belong elsewhere?)\n\nEventually, I could imagine either (a) an Ensembles section once we have more ensemble algs or (b) a section in the guide covering all algorithms and how to choose between them.\n",
    "commit": "70a75f3bea208de7c0869a79734e823f1607b118",
    "createdAt": "2014-12-01T19:45:15Z",
    "diffHunk": "@@ -0,0 +1,308 @@\n+---\n+layout: global\n+title: Gradient-Boosted Trees - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Gradient-Boosted Trees\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+[Gradient-Boosted Trees (GBTs)](http://en.wikipedia.org/wiki/Gradient_boosting)\n+are ensembles of [decision trees](mllib-decision-tree.html).\n+GBTs iteratively train decision trees in order to minimize a loss function.\n+Like decision trees, GBTs handle categorical features,\n+extend to the multiclass classification setting, do not require\n+feature scaling, and are able to capture non-linearities and feature interactions.\n+\n+MLlib supports GBTs for binary classification and for regression,\n+using both continuous and categorical features.\n+MLlib implements GBTs using the existing [decision tree](mllib-decision-tree.html) implementation.  Please see the decision tree guide for more information on trees.\n+\n+*Note*: GBTs do not yet support multiclass classification.  For multiclass problems, please use\n+[decision trees](mllib-decision-tree.html) or [Random Forests](mllib-random-forest.html).\n+\n+## Basic algorithm\n+\n+Gradient boosting iteratively trains a sequence of decision trees.\n+On each iteration, the algorithm uses the current ensemble to predict the label of each training instance and then compares the prediction with the true label.  The dataset is re-labeled to put more weight on training instances with poor predictions.  Thus, in the next iteration, the decision tree will help correct for previous mistakes.\n+\n+The specific weight mechanism is defined by a loss function (discussed below).  With each iteration, GBTs further reduce this loss function on the training data.\n+\n+### Comparison with Random Forests"
  }],
  "prId": 3461
}]