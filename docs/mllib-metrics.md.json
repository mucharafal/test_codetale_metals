[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Nit, but maybe worth saying we're evaluating the output -- the model -- not the algorithm itself. Evaluations also aren't a property of the application that consumes the model, but of the model.\n",
    "commit": "253db2d14a55ccffb12a88a8940b276f6fc25d6a",
    "createdAt": "2015-07-25T08:07:19Z",
    "diffHunk": "@@ -0,0 +1,1464 @@\n+---\n+layout: global\n+title: Evaluation Metrics - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Evaluation Metrics\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+\n+## Algorithm Metrics\n+\n+Spark's MLlib comes with a number of machine learning algorithms that can be used to learn from and make predictions\n+on data. When applying these algorithms, there is a need to evaluate their performance on certain criteria, depending"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Corrected this to say evaluation of the model.\n",
    "commit": "253db2d14a55ccffb12a88a8940b276f6fc25d6a",
    "createdAt": "2015-07-27T21:10:48Z",
    "diffHunk": "@@ -0,0 +1,1464 @@\n+---\n+layout: global\n+title: Evaluation Metrics - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Evaluation Metrics\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+\n+## Algorithm Metrics\n+\n+Spark's MLlib comes with a number of machine learning algorithms that can be used to learn from and make predictions\n+on data. When applying these algorithms, there is a need to evaluate their performance on certain criteria, depending"
  }],
  "prId": 7655
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Terms like TP aren't explained and won't be obvious to the reader that turns to this document, I think, for an explanation. The mathematical definition of things like ROC is OK but this offers no intuition about them. I don't think we need to reproduce a text on what they mean, but at least a hyperlink to wikipedia?\n",
    "commit": "253db2d14a55ccffb12a88a8940b276f6fc25d6a",
    "createdAt": "2015-07-25T08:09:05Z",
    "diffHunk": "@@ -0,0 +1,1464 @@\n+---\n+layout: global\n+title: Evaluation Metrics - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Evaluation Metrics\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+\n+## Algorithm Metrics\n+\n+Spark's MLlib comes with a number of machine learning algorithms that can be used to learn from and make predictions\n+on data. When applying these algorithms, there is a need to evaluate their performance on certain criteria, depending\n+on the application and its requirements. Spark's MLlib also provides a suite of metrics for the purpose of evaluating the\n+performance of its algorithms.\n+\n+Specific machine learning algorithms fall under broader types of machine learning applications like classification,\n+regression, clustering, etc. Each of these types have well established metrics for performance evaluation and those\n+metrics that are currently available in Spark's MLlib are detailed in this section.\n+\n+## Binary Classification\n+\n+[Binary classifiers](https://en.wikipedia.org/wiki/Binary_classification) are used to separate the elements of a given\n+dataset into one of two possible groups (e.g. fraud or not fraud) and is a special case of multiclass classification.\n+Most binary classification metrics can be generalized to multiclass classification metrics.\n+\n+<table class=\"table\">\n+  <thead>\n+    <tr><th>Metric</th><th>Definition</th></tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+      <td>Precision (Postive Predictive Value)</td>"
  }],
  "prId": 7655
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "This might be a nit, but javadoc inside example code on a web page doesn't seem necessary. This setup applies to all 3 languages right? it can be lifted out of the code and explained in prose above?\n",
    "commit": "253db2d14a55ccffb12a88a8940b276f6fc25d6a",
    "createdAt": "2015-07-25T08:11:45Z",
    "diffHunk": "@@ -0,0 +1,1464 @@\n+---\n+layout: global\n+title: Evaluation Metrics - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Evaluation Metrics\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+\n+## Algorithm Metrics\n+\n+Spark's MLlib comes with a number of machine learning algorithms that can be used to learn from and make predictions\n+on data. When applying these algorithms, there is a need to evaluate their performance on certain criteria, depending\n+on the application and its requirements. Spark's MLlib also provides a suite of metrics for the purpose of evaluating the\n+performance of its algorithms.\n+\n+Specific machine learning algorithms fall under broader types of machine learning applications like classification,\n+regression, clustering, etc. Each of these types have well established metrics for performance evaluation and those\n+metrics that are currently available in Spark's MLlib are detailed in this section.\n+\n+## Binary Classification\n+\n+[Binary classifiers](https://en.wikipedia.org/wiki/Binary_classification) are used to separate the elements of a given\n+dataset into one of two possible groups (e.g. fraud or not fraud) and is a special case of multiclass classification.\n+Most binary classification metrics can be generalized to multiclass classification metrics.\n+\n+<table class=\"table\">\n+  <thead>\n+    <tr><th>Metric</th><th>Definition</th></tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+      <td>Precision (Postive Predictive Value)</td>\n+      <td>$PPV=\\frac{TP}{TP + FP}$</td>\n+    </tr>\n+    <tr>\n+      <td>Recall (True Positive Rate)</td>\n+      <td>$TPR=\\frac{TP}{P}=\\frac{TP}{TP + FN}$</td>\n+    </tr>\n+    <tr>\n+      <td>F-measure</td>\n+      <td>$F(\\beta) = \\left(1 + \\beta^2\\right) \\cdot \\left(\\frac{PPV \\cdot TPR}\n+          {\\beta^2 \\cdot PPV + TPR}\\right)$</td>\n+    </tr>\n+    <tr>\n+      <td>Receiver Operating Characteristic (ROC)</td>\n+      <td>$FPR(T)=\\int^\\infty_{T} P_0(T)\\,dT \\\\ TPR(T)=\\int^\\infty_{T} P_1(T)\\,dT$</td>\n+    </tr>\n+    <tr>\n+      <td>Area Under ROC Curve</td>\n+      <td>$AUROC=\\int^1_{0} \\frac{TP}{P} d\\left(\\frac{FP}{N}\\right)$</td>\n+    </tr>\n+    <tr>\n+      <td>Area Under Precision-Recall Curve</td>\n+      <td>$AUPRC=\\int^1_{0} \\frac{TP}{TP+FP} d\\left(\\frac{TP}{P}\\right)$</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+\n+**Examples**\n+\n+<div class=\"codetabs\">\n+The following code snippets illustrate how to load a sample dataset, train a binary classification algorithm on the\n+data, and evaluate the performance of the algorithm by several binary evaluation metrics.\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+{% highlight scala %}\n+import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\n+import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.util.MLUtils\n+\n+// Load training data in LIBSVM format\n+val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_binary_classification_data.txt\")\n+\n+// Split data into training (60%) and test (40%)\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)\n+val training = splits(0).cache()\n+val test = splits(1)\n+\n+// Run training algorithm to build the model\n+val model = new LogisticRegressionWithLBFGS()\n+  .setNumClasses(2)\n+  .run(training)\n+\n+// Clear the prediction threshold so the model will return probabilities\n+model.clearThreshold\n+\n+// Compute raw scores on the test set\n+val predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n+  val prediction = model.predict(features)\n+  (prediction, label)\n+}\n+\n+// Instantiate metrics object\n+val metrics = new BinaryClassificationMetrics(predictionAndLabels)\n+\n+// Precision by threshold\n+val precision = metrics.precisionByThreshold\n+precision.foreach(x => printf(\"Threshold: %1.2f, Precision: %1.2f\\n\", x._1, x._2))\n+\n+// Recall by threshold\n+val recall = metrics.precisionByThreshold\n+recall.foreach(x => printf(\"Threshold: %1.2f, Recall: %1.2f\\n\", x._1, x._2))\n+\n+// Precision-Recall Curve\n+val PRC = metrics.pr\n+\n+// F-measure\n+val f1Score = metrics.fMeasureByThreshold\n+f1Score.foreach(x => printf(\"Threshold: %1.2f, F-score: %1.2f, Beta = 1\\n\", x._1, x._2))\n+\n+val beta = 0.5\n+val fScore = metrics.fMeasureByThreshold(beta)\n+fScore.foreach(x => printf(\"Threshold: %1.2f, F-score: %1.2f, Beta = 0.5\\n\", x._1, x._2))\n+\n+// AUPRC\n+val auPRC = metrics.areaUnderPR\n+println(\"Area under precision-recall curve = \" + auPRC)\n+\n+// Compute thresholds used in ROC and PR curves\n+val thresholds = precision.map(_._1)\n+\n+// ROC Curve\n+val roc = metrics.roc\n+\n+// AUROC\n+val auROC = metrics.areaUnderROC\n+println(\"Area under ROC = \" + auROC)\n+\n+{% endhighlight %}\n+\n+</div>\n+\n+<div data-lang=\"java\" markdown=\"1\">\n+\n+{% highlight java %}\n+import scala.Tuple2;\n+\n+import org.apache.spark.api.java.*;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.mllib.classification.LogisticRegressionModel;\n+import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS;\n+import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.mllib.util.MLUtils;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+\n+public class BinaryClassification {\n+  public static void main(String[] args) {\n+    SparkConf conf = new SparkConf().setAppName(\"Binary Classification Metrics\");\n+    SparkContext sc = new SparkContext(conf);\n+    String path = \"data/mllib/sample_binary_classification_data.txt\";\n+    JavaRDD<LabeledPoint> data = MLUtils.loadLibSVMFile(sc, path).toJavaRDD();\n+\n+    // Split initial RDD into two... [60% training data, 40% testing data].\n+    JavaRDD<LabeledPoint>[] splits = data.randomSplit(new double[] {0.6, 0.4}, 11L);\n+    JavaRDD<LabeledPoint> training = splits[0].cache();\n+    JavaRDD<LabeledPoint> test = splits[1];\n+\n+    // Run training algorithm to build the model.\n+    final LogisticRegressionModel model = new LogisticRegressionWithLBFGS()\n+      .setNumClasses(3)\n+      .run(training.rdd());\n+\n+    // Compute raw scores on the test set.\n+    JavaRDD<Tuple2<Object, Object>> predictionAndLabels = test.map(\n+      new Function<LabeledPoint, Tuple2<Object, Object>>() {\n+        public Tuple2<Object, Object> call(LabeledPoint p) {\n+          Double prediction = model.predict(p.features());\n+          return new Tuple2<Object, Object>(prediction, p.label());\n+        }\n+      }\n+    );\n+\n+    // Get evaluation metrics.\n+    BinaryClassificationMetrics metrics = new BinaryClassificationMetrics(predictionAndLabels.rdd());\n+\n+    // Precision by threshold\n+    JavaRDD<Tuple2<Object, Object>> precision = metrics.precisionByThreshold().toJavaRDD();\n+    System.out.println(\"Precision by threshold: \" + precision.toArray());\n+\n+    // Recall by threshold\n+    JavaRDD<Tuple2<Object, Object>> recall = metrics.recallByThreshold().toJavaRDD();\n+    System.out.println(\"Recall by threshold: \" + recall.toArray());\n+\n+    // F Score by threshold\n+    JavaRDD<Tuple2<Object, Object>> f1Score = metrics.fMeasureByThreshold().toJavaRDD();\n+    System.out.println(\"F1 Score by threshold: \" + f1Score.toArray());\n+\n+    JavaRDD<Tuple2<Object, Object>> f2Score = metrics.fMeasureByThreshold(2.0).toJavaRDD();\n+    System.out.println(\"F2 Score by threshold: \" + f2Score.toArray());\n+\n+    // Precision-recall curve\n+    JavaRDD<Tuple2<Object, Object>> prc = metrics.pr().toJavaRDD();\n+    System.out.println(\"Precision-recall curve: \" + prc.toArray());\n+\n+    // Thresholds\n+    JavaRDD<Double> thresholds = precision.map(\n+      new Function<Tuple2<Object, Object>, Double>() {\n+        public Double call (Tuple2<Object, Object> t) {\n+          return new Double(t._1().toString());\n+        }\n+      }\n+    );\n+\n+    // ROC Curve\n+    JavaRDD<Tuple2<Object, Object>> roc = metrics.roc().toJavaRDD();\n+    System.out.println(\"ROC curve: \" + roc.toArray());\n+\n+    // AUPRC\n+    System.out.println(\"Area under precision-recall curve = \" + metrics.areaUnderPR());\n+\n+    // AUROC\n+    System.out.println(\"Area under ROC = \" + metrics.areaUnderROC());\n+\n+    // Save and load model\n+    model.save(sc, \"myModelPath\");\n+    LogisticRegressionModel sameModel = LogisticRegressionModel.load(sc, \"myModelPath\");\n+  }\n+}\n+\n+{% endhighlight %}\n+\n+</div>\n+\n+<div data-lang=\"python\" markdown=\"1\">\n+\n+{% highlight python %}\n+from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n+from pyspark.mllib.evaluation import BinaryClassificationMetrics\n+from pyspark.mllib.regression import LabeledPoint\n+from pyspark.mllib.util import MLUtils\n+\n+# Several of the methods available in scala are currently missing from pyspark\n+\n+# Load training data in LIBSVM format\n+data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_binary_classification_data.txt\")\n+\n+# Split data into training (60%) and test (40%)\n+splits = data.randomSplit([0.6, 0.4], seed = 11L)\n+training = splits[0].cache()\n+test = splits[1]\n+\n+# Run training algorithm to build the model\n+model = LogisticRegressionWithLBFGS.train(training)\n+\n+# Compute raw scores on the test set\n+predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n+\n+# Instantiate metrics object\n+metrics = BinaryClassificationMetrics(predictionAndLabels)\n+\n+# Area under precision-recall curve\n+print \"Area under PR = %1.2f\" % metrics.areaUnderPR\n+\n+# Area under ROC curve\n+print \"Area under ROC = %1.2f\" % metrics.areaUnderROC\n+\n+{% endhighlight %}\n+\n+</div>\n+</div>\n+\n+\n+## Multiclass Classification\n+\n+A [multiclass classification](https://en.wikipedia.org/wiki/Multiclass_classification) describes a classification\n+problem where there are $M \\gt 2$ possible labels for each data point (the case where $M=2$ is the binary\n+classification problem). For example, classifying handwriting samples to the digits 0 to 9, having 10 possible classes.\n+\n+Define the class, or label, set as\n+\n+$$L = \\{\\ell_0, \\ell_1, \\ldots, \\ell_{M-1} \\} $$\n+\n+The true output vector $\\mathbf{y}$ consists of $N$ elements\n+\n+$$\\mathbf{y}_0, \\mathbf{y}_1, \\ldots, \\mathbf{y}_{N-1} \\in L $$\n+\n+A multiclass prediction algorithm generates a prediction vector $\\hat{\\mathbf{y}}$ of $N$ elements\n+\n+$$\\hat{\\mathbf{y}}_0, \\hat{\\mathbf{y}}_1, \\ldots, \\hat{\\mathbf{y}}_{N-1} \\in L $$\n+\n+For this section, a modified delta function $\\hat{\\delta}(x)$ will prove useful\n+\n+$$\\hat{\\delta}(x) = \\begin{cases}1 & \\text{if $x = 0$}, \\\\ 0 & \\text{otherwise}.\\end{cases}$$\n+\n+<table class=\"table\">\n+  <thead>\n+    <tr><th>Metric</th><th>Definition</th></tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+      <td>Confusion Matrix</td>\n+      <td>\n+        $C_{ij} = \\sum_{k=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_k-\\ell_i) \\cdot \\hat{\\delta}(\\hat{\\mathbf{y}}_k - \\ell_j)\\\\ \\\\\n+         \\left( \\begin{array}{ccc}\n+         \\sum_{k=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_k-\\ell_1) \\cdot \\hat{\\delta}(\\hat{\\mathbf{y}}_k - \\ell_1) & \\ldots &\n+         \\sum_{k=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_k-\\ell_1) \\cdot \\hat{\\delta}(\\hat{\\mathbf{y}}_k - \\ell_N) \\\\\n+         \\vdots & \\ddots & \\vdots \\\\\n+         \\sum_{k=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_k-\\ell_N) \\cdot \\hat{\\delta}(\\hat{\\mathbf{y}}_k - \\ell_1) & \\ldots &\n+         \\sum_{k=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_k-\\ell_N) \\cdot \\hat{\\delta}(\\hat{\\mathbf{y}}_k - \\ell_N)\n+         \\end{array} \\right)$\n+      </td>\n+    </tr>\n+    <tr>\n+      <td>Overall Precision</td>\n+      <td>$PPV = \\frac{TP}{TP + FP} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\hat{\\delta}\\left(\\hat{\\mathbf{y}}_i -\n+        \\mathbf{y}_i\\right)$</td>\n+    </tr>\n+    <tr>\n+      <td>Overall Recall</td>\n+      <td>$TPR = \\frac{TP}{TP + FN} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\hat{\\delta}\\left(\\hat{\\mathbf{y}}_i -\n+        \\mathbf{y}_i\\right)$</td>\n+    </tr>\n+    <tr>\n+      <td>Overall F1-measure</td>\n+      <td>$F1 = 2 \\cdot \\left(\\frac{PPV \\cdot TPR}\n+          {PPV + TPR}\\right)$</td>\n+    </tr>\n+    <tr>\n+      <td>Precision by label</td>\n+      <td>$PPV(\\ell) = \\frac{TP}{TP + FP} =\n+          \\frac{\\sum_{i=0}^{N-1} \\hat{\\delta}(\\hat{\\mathbf{y}}_i - \\ell) \\cdot \\hat{\\delta}(\\mathbf{y}_i - \\ell)}\n+          {\\sum_{i=0}^{N-1} \\hat{\\delta}(\\hat{\\mathbf{y}}_i - \\ell)}$</td>\n+    </tr>\n+    <tr>\n+      <td>Recall by label</td>\n+      <td>$TPR(\\ell)=\\frac{TP}{P} =\n+          \\frac{\\sum_{i=0}^{N-1} \\hat{\\delta}(\\hat{\\mathbf{y}}_i - \\ell) \\cdot \\hat{\\delta}(\\mathbf{y}_i - \\ell)}\n+          {\\sum_{i=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_i - \\ell)}$</td>\n+    </tr>\n+    <tr>\n+      <td>F-measure by label</td>\n+      <td>$F(\\beta, \\ell) = \\left(1 + \\beta^2\\right) \\cdot \\left(\\frac{PPV(\\ell) \\cdot TPR(\\ell)}\n+          {\\beta^2 \\cdot PPV(\\ell) + TPR(\\ell)}\\right)$</td>\n+    </tr>\n+    <tr>\n+      <td>Weighted precision</td>\n+      <td>$PPV_{w}= \\frac{1}{N} \\sum\\nolimits_{\\ell \\in L} PPV(\\ell)\n+          \\cdot \\sum_{i=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_i-\\ell)$</td>\n+    </tr>\n+    <tr>\n+      <td>Weighted recall</td>\n+      <td>$TPR_{w}= \\frac{1}{N} \\sum\\nolimits_{\\ell \\in L} TPR(\\ell)\n+          \\cdot \\sum_{i=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_i-\\ell)$</td>\n+    </tr>\n+    <tr>\n+      <td>Weighted F-measure</td>\n+      <td>$F_{w}(\\beta)= \\frac{1}{N} \\sum\\nolimits_{\\ell \\in L} F(\\beta, \\ell)\n+          \\cdot \\sum_{i=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_i-\\ell)$</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+**Examples**\n+\n+<div class=\"codetabs\">\n+The following code snippets illustrate how to load a sample dataset, train a multiclass classification algorithm on\n+the data, and evaluate the performance of the algorithm by several multiclass classification evaluation metrics.\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+{% highlight scala %}\n+import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.util.MLUtils\n+\n+// Load training data in LIBSVM format\n+val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_multiclass_classification_data.txt\")\n+\n+// Split data into training (60%) and test (40%)\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)\n+val training = splits(0).cache()\n+val test = splits(1)\n+\n+// Run training algorithm to build the model\n+val model = new LogisticRegressionWithLBFGS()\n+  .setNumClasses(3)\n+  .run(training)\n+\n+// Compute raw scores on the test set\n+val predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n+  val prediction = model.predict(features)\n+  (prediction, label)\n+}\n+\n+// Instantiate metrics object\n+val metrics = new MulticlassMetrics(predictionAndLabels)\n+\n+// Confusion matrix\n+println(\"Confusion matrix:\")\n+println(metrics.confusionMatrix)\n+\n+// Overall Statistics\n+val precision = metrics.precision\n+val recall = metrics.recall // same as true positive rate\n+val f1Score = metrics.fMeasure\n+println(\"Summary Statistics\")\n+printf(\"Precision = %1.2f\\n\", precision)\n+printf(\"Recall = %1.2f\\n\", recall)\n+printf(\"F1 Score = %1.2f\\n\", f1Score)\n+\n+// Precision by label\n+val labels = metrics.labels\n+labels.foreach(l => printf(\"Precision(%s): %1.2f\\n\", l, metrics.precision(l)))\n+\n+// Recall by label\n+labels.foreach(l => printf(\"Recall(%s): %1.2f\\n\", l, metrics.recall(l)))\n+\n+// False positive rate by label\n+labels.foreach(l => printf(\"FPR(%s): %1.2f\\n\", l, metrics.falsePositiveRate(l)))\n+\n+// F-measure by label\n+labels.foreach(l => printf(\"F1 Score(%s): %1.2f\\n\", l, metrics.fMeasure(l)))\n+\n+// Weighted stats\n+printf(\"Weighted precision: %1.2f\\n\", metrics.weightedPrecision)\n+printf(\"Weighted recall: %1.2f\\n\", metrics.weightedRecall)\n+printf(\"Weighted F1 score: %1.2f\\n\", metrics.weightedFMeasure)\n+printf(\"Weighted false positive rate: %1.2f\\n\", metrics.weightedFalsePositiveRate)\n+\n+{% endhighlight %}\n+\n+</div>\n+\n+<div data-lang=\"java\" markdown=\"1\">\n+\n+{% highlight java %}\n+import scala.Tuple2;\n+\n+import org.apache.spark.api.java.*;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.mllib.classification.LogisticRegressionModel;\n+import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS;\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.mllib.util.MLUtils;\n+import org.apache.spark.mllib.linalg.Matrix;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+\n+public class MulticlassClassification {\n+  public static void main(String[] args) {\n+    SparkConf conf = new SparkConf().setAppName(\"Multiclass Classification Metrics\");\n+    SparkContext sc = new SparkContext(conf);\n+    String path = \"data/mllib/sample_multiclass_classification_data.txt\";\n+    JavaRDD<LabeledPoint> data = MLUtils.loadLibSVMFile(sc, path).toJavaRDD();\n+\n+    // Split initial RDD into two... [60% training data, 40% testing data].\n+    JavaRDD<LabeledPoint>[] splits = data.randomSplit(new double[] {0.6, 0.4}, 11L);\n+    JavaRDD<LabeledPoint> training = splits[0].cache();\n+    JavaRDD<LabeledPoint> test = splits[1];\n+\n+    // Run training algorithm to build the model.\n+    final LogisticRegressionModel model = new LogisticRegressionWithLBFGS()\n+      .setNumClasses(3)\n+      .run(training.rdd());\n+\n+    // Compute raw scores on the test set.\n+    JavaRDD<Tuple2<Object, Object>> predictionAndLabels = test.map(\n+      new Function<LabeledPoint, Tuple2<Object, Object>>() {\n+        public Tuple2<Object, Object> call(LabeledPoint p) {\n+          Double prediction = model.predict(p.features());\n+          return new Tuple2<Object, Object>(prediction, p.label());\n+        }\n+      }\n+    );\n+\n+    // Get evaluation metrics.\n+    MulticlassMetrics metrics = new MulticlassMetrics(predictionAndLabels.rdd());\n+\n+    // Confusion matrix\n+    Matrix confusion = metrics.confusionMatrix();\n+    System.out.println(\"Confusion matrix: \\n\" + confusion);\n+\n+    // Overall statistics\n+    System.out.println(\"Precision = \" + metrics.precision());\n+    System.out.println(\"Recall = \" + metrics.recall());\n+    System.out.println(\"F1 Score = \" + metrics.fMeasure());\n+\n+    // Stats by labels\n+    for (int i = 0; i < metrics.labels().length; i++) {\n+        System.out.format(\"Class %1.2f precision = %1.2f\\n\", metrics.labels()[i], metrics.precision(metrics.labels()[i]));\n+        System.out.format(\"Class %1.2f recall = %1.2f\\n\", metrics.labels()[i], metrics.recall(metrics.labels()[i]));\n+        System.out.format(\"Class %1.2f F1 score = %1.2f\\n\", metrics.labels()[i], metrics.fMeasure(metrics.labels()[i]));\n+    }\n+\n+    //Weighted stats\n+    System.out.format(\"Weighted precision = %1.2f\\n\", metrics.weightedPrecision());\n+    System.out.format(\"Weighted recall = %1.2f\\n\", metrics.weightedRecall());\n+    System.out.format(\"Weighted F1 score = %1.2f\\n\", metrics.weightedFMeasure());\n+    System.out.format(\"Weighted false positive rate = %1.2f\\n\", metrics.weightedFalsePositiveRate());\n+\n+    // Save and load model\n+    model.save(sc, \"myModelPath\");\n+    LogisticRegressionModel sameModel = LogisticRegressionModel.load(sc, \"myModelPath\");\n+  }\n+}\n+\n+{% endhighlight %}\n+\n+</div>\n+\n+<div data-lang=\"python\" markdown=\"1\">\n+\n+{% highlight python %}\n+from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n+from pyspark.mllib.util import MLUtils\n+from pyspark.mllib.evaluation import MulticlassMetrics\n+\n+# Load training data in LIBSVM format\n+data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_multiclass_classification_data.txt\")\n+\n+# Split data into training (60%) and test (40%)\n+splits = data.randomSplit([0.6, 0.4], seed = 11L)\n+training = splits[0].cache()\n+test = splits[1]\n+\n+# Run training algorithm to build the model\n+model = LogisticRegressionWithLBFGS.train(training, numClasses=3)\n+\n+# Compute raw scores on the test set\n+predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n+\n+# Instantiate metrics object\n+metrics = MulticlassMetrics(predictionAndLabels)\n+\n+# Overall statistics\n+precision = metrics.precision()\n+recall = metrics.recall()\n+f1Score = metrics.fMeasure()\n+print \"Summary Stats\"\n+print \"Precision = %1.2f\" % precision\n+print \"Recall = %1.2f\" % recall\n+print \"F1 Score = %1.2f\" % f1Score\n+\n+# Statistics by class\n+labels = data.map(lambda lp: lp.label).distinct().collect()\n+for label in sorted(labels):\n+    print \"Class %s precision = %1.2f\" % (label, metrics.precision(label))\n+    print \"Class %s recall = %1.2f\" % (label, metrics.recall(label))\n+    print \"Class %s F1 Measure = %1.2f\" % (label, metrics.fMeasure(label, beta=1.0))\n+\n+# Weighted stats\n+print \"Weighted recall = %1.2f\" % metrics.weightedRecall\n+print \"Weighted precision = %1.2f\" % metrics.weightedPrecision\n+print \"Weighted F(1) Score = %1.2f\" % metrics.weightedFMeasure()\n+print \"Weighted F(0.5) Score = %1.2f\" % metrics.weightedFMeasure(beta=0.5)\n+print \"Weighted false positive rate = %1.2f\" % metrics.weightedFalsePositiveRate\n+{% endhighlight %}\n+\n+</div>\n+</div>\n+\n+## Multilabel Classification\n+\n+A [multilabel classification](https://en.wikipedia.org/wiki/Multi-label_classification) problem involves mapping\n+each sample in a dataset to a set of class labels. In this type of classification problem, the labels are not\n+mutually exclusive. For example, when classifying a set of news articles into topics, a single article might be both\n+science and politics.\n+\n+Here we define a set $D$ of $N$ documents\n+\n+$$D = \\left\\{d_0, d_1, ..., d_{N-1}\\right\\}$$\n+\n+Define $L_0, L_1, ..., L_{N-1}$ to be a family of label sets and $P_0, P_1, ..., P_{N-1}$\n+to be a family of prediction sets where $L_i$ and $P_i$ are the label set and prediction set, respectively, that\n+correspond to document $d_i$.\n+\n+The set of all unique labels is given by\n+\n+$$L = \\bigcup_{k=0}^{N-1} L_k$$\n+\n+The following definition of indicator function $I_A(x)$ on a set $A$ will be necessary\n+\n+$$I_A(x) = \\begin{cases}1 & \\text{if $x \\in A$}, \\\\ 0 & \\text{otherwise}.\\end{cases}$$\n+\n+<table class=\"table\">\n+  <thead>\n+    <tr><th>Metric</th><th>Definition</th></tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+      <td>Precision</td><td>$\\frac{1}{N} \\sum_{i=0}^{N-1} \\frac{\\left|P_i \\cap L_i\\right|}{\\left|P_i\\right|}$</td>\n+    </tr>\n+    <tr>\n+      <td>Recall</td><td>$\\frac{1}{N} \\sum_{i=0}^{N-1} \\frac{\\left|L_i \\cap P_i\\right|}{\\left|L_i\\right|}$</td>\n+    </tr>\n+    <tr>\n+      <td>Accuracy</td>\n+      <td>\n+        $\\frac{1}{N} \\sum_{i=0}^{N - 1} \\frac{\\left|L_i \\cap P_i \\right|}\n+        {\\left|L_i\\right| + \\left|P_i\\right| - \\left|L_i \\cap P_i \\right|}$\n+      </td>\n+    </tr>\n+    <tr>\n+      <td>Precision by label</td><td>$PPV(\\ell)=\\frac{TP}{TP + FP}=\n+          \\frac{\\sum_{i=0}^{N-1} I_{P_i}(\\ell) \\cdot I_{L_i}(\\ell)}\n+          {\\sum_{i=0}^{N-1} I_{P_i}(\\ell)}$</td>\n+    </tr>\n+    <tr>\n+      <td>Recall by label</td><td>$TPR(\\ell)=\\frac{TP}{P}=\n+          \\frac{\\sum_{i=0}^{N-1} I_{P_i}(\\ell) \\cdot I_{L_i}(\\ell)}\n+          {\\sum_{i=0}^{N-1} I_{L_i}(\\ell)}$</td>\n+    </tr>\n+    <tr>\n+      <td>F1-measure by label</td><td>$F1(\\ell) = 2\n+                            \\cdot \\left(\\frac{PPV(\\ell) \\cdot TPR(\\ell)}\n+                            {PPV(\\ell) + TPR(\\ell)}\\right)$</td>\n+    </tr>\n+    <tr>\n+      <td>Hamming Loss</td>\n+      <td>\n+        $\\frac{1}{N \\cdot \\left|L\\right|} \\sum_{i=0}^{N - 1} \\left|L_i\\right| + \\left|P_i\\right| - 2\\left|L_i\n+          \\cap P_i\\right|$\n+      </td>\n+    </tr>\n+    <tr>\n+      <td>Subset Accuracy</td>\n+      <td>$\\frac{1}{N} \\sum_{i=0}^{N-1} I_{\\{L_i\\}}(P_i)$</td>\n+    </tr>\n+    <tr>\n+      <td>F1 Measure</td>\n+      <td>$\\frac{1}{N} \\sum_{i=0}^{N-1} 2 \\frac{\\left|P_i \\cap L_i\\right|}{\\left|P_i\\right| \\cdot \\left|L_i\\right|}$</td>\n+    </tr>\n+    <tr>\n+      <td>Micro precision</td>\n+      <td>$\\frac{TP}{TP + FP}=\\frac{\\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right|}\n+          {\\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right| + \\sum_{i=0}^{N-1} \\left|P_i - L_i\\right|}$</td>\n+    </tr>\n+    <tr>\n+      <td>Micro recall</td>\n+      <td>$\\frac{TP}{TP + FN}=\\frac{\\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right|}\n+        {\\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right| + \\sum_{i=0}^{N-1} \\left|L_i - P_i\\right|}$</td>\n+    </tr>\n+    <tr>\n+      <td>Micro F1 Measure</td>\n+      <td>\n+        $2 \\cdot \\frac{TP}{2 \\cdot TP + FP + FN}=2 \\cdot \\frac{\\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right|}{2 \\cdot\n+        \\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right| + \\sum_{i=0}^{N-1} \\left|L_i - P_i\\right| + \\sum_{i=0}^{N-1}\n+        \\left|P_i - L_i\\right|}$\n+      </td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+**Examples**\n+\n+<div class=\"codetabs\">\n+The following code snippets illustrate how to evaluate the performance of a multilabel classifer.\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+{% highlight scala %}\n+import org.apache.spark.mllib.evaluation.MultilabelMetrics\n+import org.apache.spark.rdd.RDD;\n+\n+/**"
  }, {
    "author": {
      "login": "sethah"
    },
    "body": "Moved duplicated comments to the markdown text.\n",
    "commit": "253db2d14a55ccffb12a88a8940b276f6fc25d6a",
    "createdAt": "2015-07-27T21:11:28Z",
    "diffHunk": "@@ -0,0 +1,1464 @@\n+---\n+layout: global\n+title: Evaluation Metrics - MLlib\n+displayTitle: <a href=\"mllib-guide.html\">MLlib</a> - Evaluation Metrics\n+---\n+\n+* Table of contents\n+{:toc}\n+\n+\n+## Algorithm Metrics\n+\n+Spark's MLlib comes with a number of machine learning algorithms that can be used to learn from and make predictions\n+on data. When applying these algorithms, there is a need to evaluate their performance on certain criteria, depending\n+on the application and its requirements. Spark's MLlib also provides a suite of metrics for the purpose of evaluating the\n+performance of its algorithms.\n+\n+Specific machine learning algorithms fall under broader types of machine learning applications like classification,\n+regression, clustering, etc. Each of these types have well established metrics for performance evaluation and those\n+metrics that are currently available in Spark's MLlib are detailed in this section.\n+\n+## Binary Classification\n+\n+[Binary classifiers](https://en.wikipedia.org/wiki/Binary_classification) are used to separate the elements of a given\n+dataset into one of two possible groups (e.g. fraud or not fraud) and is a special case of multiclass classification.\n+Most binary classification metrics can be generalized to multiclass classification metrics.\n+\n+<table class=\"table\">\n+  <thead>\n+    <tr><th>Metric</th><th>Definition</th></tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+      <td>Precision (Postive Predictive Value)</td>\n+      <td>$PPV=\\frac{TP}{TP + FP}$</td>\n+    </tr>\n+    <tr>\n+      <td>Recall (True Positive Rate)</td>\n+      <td>$TPR=\\frac{TP}{P}=\\frac{TP}{TP + FN}$</td>\n+    </tr>\n+    <tr>\n+      <td>F-measure</td>\n+      <td>$F(\\beta) = \\left(1 + \\beta^2\\right) \\cdot \\left(\\frac{PPV \\cdot TPR}\n+          {\\beta^2 \\cdot PPV + TPR}\\right)$</td>\n+    </tr>\n+    <tr>\n+      <td>Receiver Operating Characteristic (ROC)</td>\n+      <td>$FPR(T)=\\int^\\infty_{T} P_0(T)\\,dT \\\\ TPR(T)=\\int^\\infty_{T} P_1(T)\\,dT$</td>\n+    </tr>\n+    <tr>\n+      <td>Area Under ROC Curve</td>\n+      <td>$AUROC=\\int^1_{0} \\frac{TP}{P} d\\left(\\frac{FP}{N}\\right)$</td>\n+    </tr>\n+    <tr>\n+      <td>Area Under Precision-Recall Curve</td>\n+      <td>$AUPRC=\\int^1_{0} \\frac{TP}{TP+FP} d\\left(\\frac{TP}{P}\\right)$</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+\n+**Examples**\n+\n+<div class=\"codetabs\">\n+The following code snippets illustrate how to load a sample dataset, train a binary classification algorithm on the\n+data, and evaluate the performance of the algorithm by several binary evaluation metrics.\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+{% highlight scala %}\n+import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\n+import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.util.MLUtils\n+\n+// Load training data in LIBSVM format\n+val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_binary_classification_data.txt\")\n+\n+// Split data into training (60%) and test (40%)\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)\n+val training = splits(0).cache()\n+val test = splits(1)\n+\n+// Run training algorithm to build the model\n+val model = new LogisticRegressionWithLBFGS()\n+  .setNumClasses(2)\n+  .run(training)\n+\n+// Clear the prediction threshold so the model will return probabilities\n+model.clearThreshold\n+\n+// Compute raw scores on the test set\n+val predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n+  val prediction = model.predict(features)\n+  (prediction, label)\n+}\n+\n+// Instantiate metrics object\n+val metrics = new BinaryClassificationMetrics(predictionAndLabels)\n+\n+// Precision by threshold\n+val precision = metrics.precisionByThreshold\n+precision.foreach(x => printf(\"Threshold: %1.2f, Precision: %1.2f\\n\", x._1, x._2))\n+\n+// Recall by threshold\n+val recall = metrics.precisionByThreshold\n+recall.foreach(x => printf(\"Threshold: %1.2f, Recall: %1.2f\\n\", x._1, x._2))\n+\n+// Precision-Recall Curve\n+val PRC = metrics.pr\n+\n+// F-measure\n+val f1Score = metrics.fMeasureByThreshold\n+f1Score.foreach(x => printf(\"Threshold: %1.2f, F-score: %1.2f, Beta = 1\\n\", x._1, x._2))\n+\n+val beta = 0.5\n+val fScore = metrics.fMeasureByThreshold(beta)\n+fScore.foreach(x => printf(\"Threshold: %1.2f, F-score: %1.2f, Beta = 0.5\\n\", x._1, x._2))\n+\n+// AUPRC\n+val auPRC = metrics.areaUnderPR\n+println(\"Area under precision-recall curve = \" + auPRC)\n+\n+// Compute thresholds used in ROC and PR curves\n+val thresholds = precision.map(_._1)\n+\n+// ROC Curve\n+val roc = metrics.roc\n+\n+// AUROC\n+val auROC = metrics.areaUnderROC\n+println(\"Area under ROC = \" + auROC)\n+\n+{% endhighlight %}\n+\n+</div>\n+\n+<div data-lang=\"java\" markdown=\"1\">\n+\n+{% highlight java %}\n+import scala.Tuple2;\n+\n+import org.apache.spark.api.java.*;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.mllib.classification.LogisticRegressionModel;\n+import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS;\n+import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.mllib.util.MLUtils;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+\n+public class BinaryClassification {\n+  public static void main(String[] args) {\n+    SparkConf conf = new SparkConf().setAppName(\"Binary Classification Metrics\");\n+    SparkContext sc = new SparkContext(conf);\n+    String path = \"data/mllib/sample_binary_classification_data.txt\";\n+    JavaRDD<LabeledPoint> data = MLUtils.loadLibSVMFile(sc, path).toJavaRDD();\n+\n+    // Split initial RDD into two... [60% training data, 40% testing data].\n+    JavaRDD<LabeledPoint>[] splits = data.randomSplit(new double[] {0.6, 0.4}, 11L);\n+    JavaRDD<LabeledPoint> training = splits[0].cache();\n+    JavaRDD<LabeledPoint> test = splits[1];\n+\n+    // Run training algorithm to build the model.\n+    final LogisticRegressionModel model = new LogisticRegressionWithLBFGS()\n+      .setNumClasses(3)\n+      .run(training.rdd());\n+\n+    // Compute raw scores on the test set.\n+    JavaRDD<Tuple2<Object, Object>> predictionAndLabels = test.map(\n+      new Function<LabeledPoint, Tuple2<Object, Object>>() {\n+        public Tuple2<Object, Object> call(LabeledPoint p) {\n+          Double prediction = model.predict(p.features());\n+          return new Tuple2<Object, Object>(prediction, p.label());\n+        }\n+      }\n+    );\n+\n+    // Get evaluation metrics.\n+    BinaryClassificationMetrics metrics = new BinaryClassificationMetrics(predictionAndLabels.rdd());\n+\n+    // Precision by threshold\n+    JavaRDD<Tuple2<Object, Object>> precision = metrics.precisionByThreshold().toJavaRDD();\n+    System.out.println(\"Precision by threshold: \" + precision.toArray());\n+\n+    // Recall by threshold\n+    JavaRDD<Tuple2<Object, Object>> recall = metrics.recallByThreshold().toJavaRDD();\n+    System.out.println(\"Recall by threshold: \" + recall.toArray());\n+\n+    // F Score by threshold\n+    JavaRDD<Tuple2<Object, Object>> f1Score = metrics.fMeasureByThreshold().toJavaRDD();\n+    System.out.println(\"F1 Score by threshold: \" + f1Score.toArray());\n+\n+    JavaRDD<Tuple2<Object, Object>> f2Score = metrics.fMeasureByThreshold(2.0).toJavaRDD();\n+    System.out.println(\"F2 Score by threshold: \" + f2Score.toArray());\n+\n+    // Precision-recall curve\n+    JavaRDD<Tuple2<Object, Object>> prc = metrics.pr().toJavaRDD();\n+    System.out.println(\"Precision-recall curve: \" + prc.toArray());\n+\n+    // Thresholds\n+    JavaRDD<Double> thresholds = precision.map(\n+      new Function<Tuple2<Object, Object>, Double>() {\n+        public Double call (Tuple2<Object, Object> t) {\n+          return new Double(t._1().toString());\n+        }\n+      }\n+    );\n+\n+    // ROC Curve\n+    JavaRDD<Tuple2<Object, Object>> roc = metrics.roc().toJavaRDD();\n+    System.out.println(\"ROC curve: \" + roc.toArray());\n+\n+    // AUPRC\n+    System.out.println(\"Area under precision-recall curve = \" + metrics.areaUnderPR());\n+\n+    // AUROC\n+    System.out.println(\"Area under ROC = \" + metrics.areaUnderROC());\n+\n+    // Save and load model\n+    model.save(sc, \"myModelPath\");\n+    LogisticRegressionModel sameModel = LogisticRegressionModel.load(sc, \"myModelPath\");\n+  }\n+}\n+\n+{% endhighlight %}\n+\n+</div>\n+\n+<div data-lang=\"python\" markdown=\"1\">\n+\n+{% highlight python %}\n+from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n+from pyspark.mllib.evaluation import BinaryClassificationMetrics\n+from pyspark.mllib.regression import LabeledPoint\n+from pyspark.mllib.util import MLUtils\n+\n+# Several of the methods available in scala are currently missing from pyspark\n+\n+# Load training data in LIBSVM format\n+data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_binary_classification_data.txt\")\n+\n+# Split data into training (60%) and test (40%)\n+splits = data.randomSplit([0.6, 0.4], seed = 11L)\n+training = splits[0].cache()\n+test = splits[1]\n+\n+# Run training algorithm to build the model\n+model = LogisticRegressionWithLBFGS.train(training)\n+\n+# Compute raw scores on the test set\n+predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n+\n+# Instantiate metrics object\n+metrics = BinaryClassificationMetrics(predictionAndLabels)\n+\n+# Area under precision-recall curve\n+print \"Area under PR = %1.2f\" % metrics.areaUnderPR\n+\n+# Area under ROC curve\n+print \"Area under ROC = %1.2f\" % metrics.areaUnderROC\n+\n+{% endhighlight %}\n+\n+</div>\n+</div>\n+\n+\n+## Multiclass Classification\n+\n+A [multiclass classification](https://en.wikipedia.org/wiki/Multiclass_classification) describes a classification\n+problem where there are $M \\gt 2$ possible labels for each data point (the case where $M=2$ is the binary\n+classification problem). For example, classifying handwriting samples to the digits 0 to 9, having 10 possible classes.\n+\n+Define the class, or label, set as\n+\n+$$L = \\{\\ell_0, \\ell_1, \\ldots, \\ell_{M-1} \\} $$\n+\n+The true output vector $\\mathbf{y}$ consists of $N$ elements\n+\n+$$\\mathbf{y}_0, \\mathbf{y}_1, \\ldots, \\mathbf{y}_{N-1} \\in L $$\n+\n+A multiclass prediction algorithm generates a prediction vector $\\hat{\\mathbf{y}}$ of $N$ elements\n+\n+$$\\hat{\\mathbf{y}}_0, \\hat{\\mathbf{y}}_1, \\ldots, \\hat{\\mathbf{y}}_{N-1} \\in L $$\n+\n+For this section, a modified delta function $\\hat{\\delta}(x)$ will prove useful\n+\n+$$\\hat{\\delta}(x) = \\begin{cases}1 & \\text{if $x = 0$}, \\\\ 0 & \\text{otherwise}.\\end{cases}$$\n+\n+<table class=\"table\">\n+  <thead>\n+    <tr><th>Metric</th><th>Definition</th></tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+      <td>Confusion Matrix</td>\n+      <td>\n+        $C_{ij} = \\sum_{k=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_k-\\ell_i) \\cdot \\hat{\\delta}(\\hat{\\mathbf{y}}_k - \\ell_j)\\\\ \\\\\n+         \\left( \\begin{array}{ccc}\n+         \\sum_{k=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_k-\\ell_1) \\cdot \\hat{\\delta}(\\hat{\\mathbf{y}}_k - \\ell_1) & \\ldots &\n+         \\sum_{k=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_k-\\ell_1) \\cdot \\hat{\\delta}(\\hat{\\mathbf{y}}_k - \\ell_N) \\\\\n+         \\vdots & \\ddots & \\vdots \\\\\n+         \\sum_{k=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_k-\\ell_N) \\cdot \\hat{\\delta}(\\hat{\\mathbf{y}}_k - \\ell_1) & \\ldots &\n+         \\sum_{k=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_k-\\ell_N) \\cdot \\hat{\\delta}(\\hat{\\mathbf{y}}_k - \\ell_N)\n+         \\end{array} \\right)$\n+      </td>\n+    </tr>\n+    <tr>\n+      <td>Overall Precision</td>\n+      <td>$PPV = \\frac{TP}{TP + FP} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\hat{\\delta}\\left(\\hat{\\mathbf{y}}_i -\n+        \\mathbf{y}_i\\right)$</td>\n+    </tr>\n+    <tr>\n+      <td>Overall Recall</td>\n+      <td>$TPR = \\frac{TP}{TP + FN} = \\frac{1}{N}\\sum_{i=0}^{N-1} \\hat{\\delta}\\left(\\hat{\\mathbf{y}}_i -\n+        \\mathbf{y}_i\\right)$</td>\n+    </tr>\n+    <tr>\n+      <td>Overall F1-measure</td>\n+      <td>$F1 = 2 \\cdot \\left(\\frac{PPV \\cdot TPR}\n+          {PPV + TPR}\\right)$</td>\n+    </tr>\n+    <tr>\n+      <td>Precision by label</td>\n+      <td>$PPV(\\ell) = \\frac{TP}{TP + FP} =\n+          \\frac{\\sum_{i=0}^{N-1} \\hat{\\delta}(\\hat{\\mathbf{y}}_i - \\ell) \\cdot \\hat{\\delta}(\\mathbf{y}_i - \\ell)}\n+          {\\sum_{i=0}^{N-1} \\hat{\\delta}(\\hat{\\mathbf{y}}_i - \\ell)}$</td>\n+    </tr>\n+    <tr>\n+      <td>Recall by label</td>\n+      <td>$TPR(\\ell)=\\frac{TP}{P} =\n+          \\frac{\\sum_{i=0}^{N-1} \\hat{\\delta}(\\hat{\\mathbf{y}}_i - \\ell) \\cdot \\hat{\\delta}(\\mathbf{y}_i - \\ell)}\n+          {\\sum_{i=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_i - \\ell)}$</td>\n+    </tr>\n+    <tr>\n+      <td>F-measure by label</td>\n+      <td>$F(\\beta, \\ell) = \\left(1 + \\beta^2\\right) \\cdot \\left(\\frac{PPV(\\ell) \\cdot TPR(\\ell)}\n+          {\\beta^2 \\cdot PPV(\\ell) + TPR(\\ell)}\\right)$</td>\n+    </tr>\n+    <tr>\n+      <td>Weighted precision</td>\n+      <td>$PPV_{w}= \\frac{1}{N} \\sum\\nolimits_{\\ell \\in L} PPV(\\ell)\n+          \\cdot \\sum_{i=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_i-\\ell)$</td>\n+    </tr>\n+    <tr>\n+      <td>Weighted recall</td>\n+      <td>$TPR_{w}= \\frac{1}{N} \\sum\\nolimits_{\\ell \\in L} TPR(\\ell)\n+          \\cdot \\sum_{i=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_i-\\ell)$</td>\n+    </tr>\n+    <tr>\n+      <td>Weighted F-measure</td>\n+      <td>$F_{w}(\\beta)= \\frac{1}{N} \\sum\\nolimits_{\\ell \\in L} F(\\beta, \\ell)\n+          \\cdot \\sum_{i=0}^{N-1} \\hat{\\delta}(\\mathbf{y}_i-\\ell)$</td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+**Examples**\n+\n+<div class=\"codetabs\">\n+The following code snippets illustrate how to load a sample dataset, train a multiclass classification algorithm on\n+the data, and evaluate the performance of the algorithm by several multiclass classification evaluation metrics.\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+{% highlight scala %}\n+import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics\n+import org.apache.spark.mllib.regression.LabeledPoint\n+import org.apache.spark.mllib.util.MLUtils\n+\n+// Load training data in LIBSVM format\n+val data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_multiclass_classification_data.txt\")\n+\n+// Split data into training (60%) and test (40%)\n+val splits = data.randomSplit(Array(0.6, 0.4), seed = 11L)\n+val training = splits(0).cache()\n+val test = splits(1)\n+\n+// Run training algorithm to build the model\n+val model = new LogisticRegressionWithLBFGS()\n+  .setNumClasses(3)\n+  .run(training)\n+\n+// Compute raw scores on the test set\n+val predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n+  val prediction = model.predict(features)\n+  (prediction, label)\n+}\n+\n+// Instantiate metrics object\n+val metrics = new MulticlassMetrics(predictionAndLabels)\n+\n+// Confusion matrix\n+println(\"Confusion matrix:\")\n+println(metrics.confusionMatrix)\n+\n+// Overall Statistics\n+val precision = metrics.precision\n+val recall = metrics.recall // same as true positive rate\n+val f1Score = metrics.fMeasure\n+println(\"Summary Statistics\")\n+printf(\"Precision = %1.2f\\n\", precision)\n+printf(\"Recall = %1.2f\\n\", recall)\n+printf(\"F1 Score = %1.2f\\n\", f1Score)\n+\n+// Precision by label\n+val labels = metrics.labels\n+labels.foreach(l => printf(\"Precision(%s): %1.2f\\n\", l, metrics.precision(l)))\n+\n+// Recall by label\n+labels.foreach(l => printf(\"Recall(%s): %1.2f\\n\", l, metrics.recall(l)))\n+\n+// False positive rate by label\n+labels.foreach(l => printf(\"FPR(%s): %1.2f\\n\", l, metrics.falsePositiveRate(l)))\n+\n+// F-measure by label\n+labels.foreach(l => printf(\"F1 Score(%s): %1.2f\\n\", l, metrics.fMeasure(l)))\n+\n+// Weighted stats\n+printf(\"Weighted precision: %1.2f\\n\", metrics.weightedPrecision)\n+printf(\"Weighted recall: %1.2f\\n\", metrics.weightedRecall)\n+printf(\"Weighted F1 score: %1.2f\\n\", metrics.weightedFMeasure)\n+printf(\"Weighted false positive rate: %1.2f\\n\", metrics.weightedFalsePositiveRate)\n+\n+{% endhighlight %}\n+\n+</div>\n+\n+<div data-lang=\"java\" markdown=\"1\">\n+\n+{% highlight java %}\n+import scala.Tuple2;\n+\n+import org.apache.spark.api.java.*;\n+import org.apache.spark.rdd.RDD;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.mllib.classification.LogisticRegressionModel;\n+import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS;\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.mllib.util.MLUtils;\n+import org.apache.spark.mllib.linalg.Matrix;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+\n+public class MulticlassClassification {\n+  public static void main(String[] args) {\n+    SparkConf conf = new SparkConf().setAppName(\"Multiclass Classification Metrics\");\n+    SparkContext sc = new SparkContext(conf);\n+    String path = \"data/mllib/sample_multiclass_classification_data.txt\";\n+    JavaRDD<LabeledPoint> data = MLUtils.loadLibSVMFile(sc, path).toJavaRDD();\n+\n+    // Split initial RDD into two... [60% training data, 40% testing data].\n+    JavaRDD<LabeledPoint>[] splits = data.randomSplit(new double[] {0.6, 0.4}, 11L);\n+    JavaRDD<LabeledPoint> training = splits[0].cache();\n+    JavaRDD<LabeledPoint> test = splits[1];\n+\n+    // Run training algorithm to build the model.\n+    final LogisticRegressionModel model = new LogisticRegressionWithLBFGS()\n+      .setNumClasses(3)\n+      .run(training.rdd());\n+\n+    // Compute raw scores on the test set.\n+    JavaRDD<Tuple2<Object, Object>> predictionAndLabels = test.map(\n+      new Function<LabeledPoint, Tuple2<Object, Object>>() {\n+        public Tuple2<Object, Object> call(LabeledPoint p) {\n+          Double prediction = model.predict(p.features());\n+          return new Tuple2<Object, Object>(prediction, p.label());\n+        }\n+      }\n+    );\n+\n+    // Get evaluation metrics.\n+    MulticlassMetrics metrics = new MulticlassMetrics(predictionAndLabels.rdd());\n+\n+    // Confusion matrix\n+    Matrix confusion = metrics.confusionMatrix();\n+    System.out.println(\"Confusion matrix: \\n\" + confusion);\n+\n+    // Overall statistics\n+    System.out.println(\"Precision = \" + metrics.precision());\n+    System.out.println(\"Recall = \" + metrics.recall());\n+    System.out.println(\"F1 Score = \" + metrics.fMeasure());\n+\n+    // Stats by labels\n+    for (int i = 0; i < metrics.labels().length; i++) {\n+        System.out.format(\"Class %1.2f precision = %1.2f\\n\", metrics.labels()[i], metrics.precision(metrics.labels()[i]));\n+        System.out.format(\"Class %1.2f recall = %1.2f\\n\", metrics.labels()[i], metrics.recall(metrics.labels()[i]));\n+        System.out.format(\"Class %1.2f F1 score = %1.2f\\n\", metrics.labels()[i], metrics.fMeasure(metrics.labels()[i]));\n+    }\n+\n+    //Weighted stats\n+    System.out.format(\"Weighted precision = %1.2f\\n\", metrics.weightedPrecision());\n+    System.out.format(\"Weighted recall = %1.2f\\n\", metrics.weightedRecall());\n+    System.out.format(\"Weighted F1 score = %1.2f\\n\", metrics.weightedFMeasure());\n+    System.out.format(\"Weighted false positive rate = %1.2f\\n\", metrics.weightedFalsePositiveRate());\n+\n+    // Save and load model\n+    model.save(sc, \"myModelPath\");\n+    LogisticRegressionModel sameModel = LogisticRegressionModel.load(sc, \"myModelPath\");\n+  }\n+}\n+\n+{% endhighlight %}\n+\n+</div>\n+\n+<div data-lang=\"python\" markdown=\"1\">\n+\n+{% highlight python %}\n+from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n+from pyspark.mllib.util import MLUtils\n+from pyspark.mllib.evaluation import MulticlassMetrics\n+\n+# Load training data in LIBSVM format\n+data = MLUtils.loadLibSVMFile(sc, \"data/mllib/sample_multiclass_classification_data.txt\")\n+\n+# Split data into training (60%) and test (40%)\n+splits = data.randomSplit([0.6, 0.4], seed = 11L)\n+training = splits[0].cache()\n+test = splits[1]\n+\n+# Run training algorithm to build the model\n+model = LogisticRegressionWithLBFGS.train(training, numClasses=3)\n+\n+# Compute raw scores on the test set\n+predictionAndLabels = test.map(lambda lp: (float(model.predict(lp.features)), lp.label))\n+\n+# Instantiate metrics object\n+metrics = MulticlassMetrics(predictionAndLabels)\n+\n+# Overall statistics\n+precision = metrics.precision()\n+recall = metrics.recall()\n+f1Score = metrics.fMeasure()\n+print \"Summary Stats\"\n+print \"Precision = %1.2f\" % precision\n+print \"Recall = %1.2f\" % recall\n+print \"F1 Score = %1.2f\" % f1Score\n+\n+# Statistics by class\n+labels = data.map(lambda lp: lp.label).distinct().collect()\n+for label in sorted(labels):\n+    print \"Class %s precision = %1.2f\" % (label, metrics.precision(label))\n+    print \"Class %s recall = %1.2f\" % (label, metrics.recall(label))\n+    print \"Class %s F1 Measure = %1.2f\" % (label, metrics.fMeasure(label, beta=1.0))\n+\n+# Weighted stats\n+print \"Weighted recall = %1.2f\" % metrics.weightedRecall\n+print \"Weighted precision = %1.2f\" % metrics.weightedPrecision\n+print \"Weighted F(1) Score = %1.2f\" % metrics.weightedFMeasure()\n+print \"Weighted F(0.5) Score = %1.2f\" % metrics.weightedFMeasure(beta=0.5)\n+print \"Weighted false positive rate = %1.2f\" % metrics.weightedFalsePositiveRate\n+{% endhighlight %}\n+\n+</div>\n+</div>\n+\n+## Multilabel Classification\n+\n+A [multilabel classification](https://en.wikipedia.org/wiki/Multi-label_classification) problem involves mapping\n+each sample in a dataset to a set of class labels. In this type of classification problem, the labels are not\n+mutually exclusive. For example, when classifying a set of news articles into topics, a single article might be both\n+science and politics.\n+\n+Here we define a set $D$ of $N$ documents\n+\n+$$D = \\left\\{d_0, d_1, ..., d_{N-1}\\right\\}$$\n+\n+Define $L_0, L_1, ..., L_{N-1}$ to be a family of label sets and $P_0, P_1, ..., P_{N-1}$\n+to be a family of prediction sets where $L_i$ and $P_i$ are the label set and prediction set, respectively, that\n+correspond to document $d_i$.\n+\n+The set of all unique labels is given by\n+\n+$$L = \\bigcup_{k=0}^{N-1} L_k$$\n+\n+The following definition of indicator function $I_A(x)$ on a set $A$ will be necessary\n+\n+$$I_A(x) = \\begin{cases}1 & \\text{if $x \\in A$}, \\\\ 0 & \\text{otherwise}.\\end{cases}$$\n+\n+<table class=\"table\">\n+  <thead>\n+    <tr><th>Metric</th><th>Definition</th></tr>\n+  </thead>\n+  <tbody>\n+    <tr>\n+      <td>Precision</td><td>$\\frac{1}{N} \\sum_{i=0}^{N-1} \\frac{\\left|P_i \\cap L_i\\right|}{\\left|P_i\\right|}$</td>\n+    </tr>\n+    <tr>\n+      <td>Recall</td><td>$\\frac{1}{N} \\sum_{i=0}^{N-1} \\frac{\\left|L_i \\cap P_i\\right|}{\\left|L_i\\right|}$</td>\n+    </tr>\n+    <tr>\n+      <td>Accuracy</td>\n+      <td>\n+        $\\frac{1}{N} \\sum_{i=0}^{N - 1} \\frac{\\left|L_i \\cap P_i \\right|}\n+        {\\left|L_i\\right| + \\left|P_i\\right| - \\left|L_i \\cap P_i \\right|}$\n+      </td>\n+    </tr>\n+    <tr>\n+      <td>Precision by label</td><td>$PPV(\\ell)=\\frac{TP}{TP + FP}=\n+          \\frac{\\sum_{i=0}^{N-1} I_{P_i}(\\ell) \\cdot I_{L_i}(\\ell)}\n+          {\\sum_{i=0}^{N-1} I_{P_i}(\\ell)}$</td>\n+    </tr>\n+    <tr>\n+      <td>Recall by label</td><td>$TPR(\\ell)=\\frac{TP}{P}=\n+          \\frac{\\sum_{i=0}^{N-1} I_{P_i}(\\ell) \\cdot I_{L_i}(\\ell)}\n+          {\\sum_{i=0}^{N-1} I_{L_i}(\\ell)}$</td>\n+    </tr>\n+    <tr>\n+      <td>F1-measure by label</td><td>$F1(\\ell) = 2\n+                            \\cdot \\left(\\frac{PPV(\\ell) \\cdot TPR(\\ell)}\n+                            {PPV(\\ell) + TPR(\\ell)}\\right)$</td>\n+    </tr>\n+    <tr>\n+      <td>Hamming Loss</td>\n+      <td>\n+        $\\frac{1}{N \\cdot \\left|L\\right|} \\sum_{i=0}^{N - 1} \\left|L_i\\right| + \\left|P_i\\right| - 2\\left|L_i\n+          \\cap P_i\\right|$\n+      </td>\n+    </tr>\n+    <tr>\n+      <td>Subset Accuracy</td>\n+      <td>$\\frac{1}{N} \\sum_{i=0}^{N-1} I_{\\{L_i\\}}(P_i)$</td>\n+    </tr>\n+    <tr>\n+      <td>F1 Measure</td>\n+      <td>$\\frac{1}{N} \\sum_{i=0}^{N-1} 2 \\frac{\\left|P_i \\cap L_i\\right|}{\\left|P_i\\right| \\cdot \\left|L_i\\right|}$</td>\n+    </tr>\n+    <tr>\n+      <td>Micro precision</td>\n+      <td>$\\frac{TP}{TP + FP}=\\frac{\\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right|}\n+          {\\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right| + \\sum_{i=0}^{N-1} \\left|P_i - L_i\\right|}$</td>\n+    </tr>\n+    <tr>\n+      <td>Micro recall</td>\n+      <td>$\\frac{TP}{TP + FN}=\\frac{\\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right|}\n+        {\\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right| + \\sum_{i=0}^{N-1} \\left|L_i - P_i\\right|}$</td>\n+    </tr>\n+    <tr>\n+      <td>Micro F1 Measure</td>\n+      <td>\n+        $2 \\cdot \\frac{TP}{2 \\cdot TP + FP + FN}=2 \\cdot \\frac{\\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right|}{2 \\cdot\n+        \\sum_{i=0}^{N-1} \\left|P_i \\cap L_i\\right| + \\sum_{i=0}^{N-1} \\left|L_i - P_i\\right| + \\sum_{i=0}^{N-1}\n+        \\left|P_i - L_i\\right|}$\n+      </td>\n+    </tr>\n+  </tbody>\n+</table>\n+\n+**Examples**\n+\n+<div class=\"codetabs\">\n+The following code snippets illustrate how to evaluate the performance of a multilabel classifer.\n+\n+<div data-lang=\"scala\" markdown=\"1\">\n+\n+{% highlight scala %}\n+import org.apache.spark.mllib.evaluation.MultilabelMetrics\n+import org.apache.spark.rdd.RDD;\n+\n+/**"
  }],
  "prId": 7655
}]