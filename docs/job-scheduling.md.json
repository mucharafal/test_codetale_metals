[{
  "comments": [{
    "author": {
      "login": "tnachen"
    },
    "body": "I'd like to add that users can run the mesos-shuffle-service.sh with Marathon, and they should start the service in the foreground running `spark-class org.apache.spark.deploy.mesos.MesosExternalShuffleService`\n",
    "commit": "8200bf50175e069bbe96003099a5391b6705eb69",
    "createdAt": "2015-11-11T22:04:17Z",
    "diffHunk": "@@ -56,36 +56,31 @@ provide another approach to share RDDs.\n \n ## Dynamic Resource Allocation\n \n-Spark 1.2 introduces the ability to dynamically scale the set of cluster resources allocated to\n-your application up and down based on the workload. This means that your application may give\n-resources back to the cluster if they are no longer used and request them again later when there\n-is demand. This feature is particularly useful if multiple applications share resources in your\n-Spark cluster. If a subset of the resources allocated to an application becomes idle, it can be\n-returned to the cluster's pool of resources and acquired by other applications. In Spark, dynamic\n-resource allocation is performed on the granularity of the executor and can be enabled through\n-`spark.dynamicAllocation.enabled`.\n-\n-This feature is currently disabled by default and available only on [YARN](running-on-yarn.html).\n-A future release will extend this to [standalone mode](spark-standalone.html) and\n-[Mesos coarse-grained mode](running-on-mesos.html#mesos-run-modes). Note that although Spark on\n-Mesos already has a similar notion of dynamic resource sharing in fine-grained mode, enabling\n-dynamic allocation allows your Mesos application to take advantage of coarse-grained low-latency\n-scheduling while sharing cluster resources efficiently.\n+Spark provides a mechanism to dynamically adjust the resources your application occupies based\n+on the workload. This means that your application may give resources back to the cluster if they\n+are no longer used and request them again later when there is demand. This feature is particularly\n+useful if multiple applications share resources in your Spark cluster.\n+\n+This feature is disabled by default and available on all coarse-grained cluster managers, i.e.\n+[standalone mode](spark-standalone.html), [YARN mode](running-on-yarn.html), and\n+[Mesos coarse-grained mode](running-on-mesos.html#mesos-run-modes).\n \n ### Configuration and Setup\n \n-All configurations used by this feature live under the `spark.dynamicAllocation.*` namespace.\n-To enable this feature, your application must set `spark.dynamicAllocation.enabled` to `true`.\n-Other relevant configurations are described on the\n-[configurations page](configuration.html#dynamic-allocation) and in the subsequent sections in\n-detail.\n+There are two requirements for using this feature. First, your application must set\n+`spark.dynamicAllocation.enabled` to `true`. Second, you must set up an *external shuffle service*\n+on each worker node in the same cluster and set `spark.shuffle.service.enabled` to true in your\n+application. The purpose of the external shuffle service is to allow executors to be removed\n+without deleting shuffle files written by them (more detail described\n+[below](job-scheduling.html#graceful-decommission-of-executors)). The way to set up this service\n+varies across cluster managers:\n+\n+In standalone mode, simply start your workers with `spark.shuffle.service.enabled` set to `true`.\n \n-Additionally, your application must use an external shuffle service. The purpose of the service is\n-to preserve the shuffle files written by executors so the executors can be safely removed (more\n-detail described [below](job-scheduling.html#graceful-decommission-of-executors)). To enable\n-this service, set `spark.shuffle.service.enabled` to `true`. In YARN, this external shuffle service\n-is implemented in `org.apache.spark.yarn.network.YarnShuffleService` that runs in each `NodeManager`\n-in your cluster. To start this service, follow these steps:\n+In Mesos coarse-grained mode, run `$SPARK_HOME/sbin/start-mesos-shuffle-service.sh` on all\n+slave nodes with `spark.shuffle.service.enabled` set to `true`."
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "not sure what you mean about the latter part. If you wish you can submit a patch against the same issue to augment the description here.\n",
    "commit": "8200bf50175e069bbe96003099a5391b6705eb69",
    "createdAt": "2015-11-12T20:35:48Z",
    "diffHunk": "@@ -56,36 +56,31 @@ provide another approach to share RDDs.\n \n ## Dynamic Resource Allocation\n \n-Spark 1.2 introduces the ability to dynamically scale the set of cluster resources allocated to\n-your application up and down based on the workload. This means that your application may give\n-resources back to the cluster if they are no longer used and request them again later when there\n-is demand. This feature is particularly useful if multiple applications share resources in your\n-Spark cluster. If a subset of the resources allocated to an application becomes idle, it can be\n-returned to the cluster's pool of resources and acquired by other applications. In Spark, dynamic\n-resource allocation is performed on the granularity of the executor and can be enabled through\n-`spark.dynamicAllocation.enabled`.\n-\n-This feature is currently disabled by default and available only on [YARN](running-on-yarn.html).\n-A future release will extend this to [standalone mode](spark-standalone.html) and\n-[Mesos coarse-grained mode](running-on-mesos.html#mesos-run-modes). Note that although Spark on\n-Mesos already has a similar notion of dynamic resource sharing in fine-grained mode, enabling\n-dynamic allocation allows your Mesos application to take advantage of coarse-grained low-latency\n-scheduling while sharing cluster resources efficiently.\n+Spark provides a mechanism to dynamically adjust the resources your application occupies based\n+on the workload. This means that your application may give resources back to the cluster if they\n+are no longer used and request them again later when there is demand. This feature is particularly\n+useful if multiple applications share resources in your Spark cluster.\n+\n+This feature is disabled by default and available on all coarse-grained cluster managers, i.e.\n+[standalone mode](spark-standalone.html), [YARN mode](running-on-yarn.html), and\n+[Mesos coarse-grained mode](running-on-mesos.html#mesos-run-modes).\n \n ### Configuration and Setup\n \n-All configurations used by this feature live under the `spark.dynamicAllocation.*` namespace.\n-To enable this feature, your application must set `spark.dynamicAllocation.enabled` to `true`.\n-Other relevant configurations are described on the\n-[configurations page](configuration.html#dynamic-allocation) and in the subsequent sections in\n-detail.\n+There are two requirements for using this feature. First, your application must set\n+`spark.dynamicAllocation.enabled` to `true`. Second, you must set up an *external shuffle service*\n+on each worker node in the same cluster and set `spark.shuffle.service.enabled` to true in your\n+application. The purpose of the external shuffle service is to allow executors to be removed\n+without deleting shuffle files written by them (more detail described\n+[below](job-scheduling.html#graceful-decommission-of-executors)). The way to set up this service\n+varies across cluster managers:\n+\n+In standalone mode, simply start your workers with `spark.shuffle.service.enabled` set to `true`.\n \n-Additionally, your application must use an external shuffle service. The purpose of the service is\n-to preserve the shuffle files written by executors so the executors can be safely removed (more\n-detail described [below](job-scheduling.html#graceful-decommission-of-executors)). To enable\n-this service, set `spark.shuffle.service.enabled` to `true`. In YARN, this external shuffle service\n-is implemented in `org.apache.spark.yarn.network.YarnShuffleService` that runs in each `NodeManager`\n-in your cluster. To start this service, follow these steps:\n+In Mesos coarse-grained mode, run `$SPARK_HOME/sbin/start-mesos-shuffle-service.sh` on all\n+slave nodes with `spark.shuffle.service.enabled` set to `true`."
  }],
  "prId": 9637
}]