[{
  "comments": [{
    "author": {
      "login": "SparkQA"
    },
    "body": "should this be  `core-site.xml`?\n",
    "commit": "ff4e3949052ec7e28f669e82c61499cd9c11a2fa",
    "createdAt": "2014-09-08T03:44:37Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+---\n+layout: global\n+title: Accessing OpenStack Swift from Spark\n+---\n+\n+Spark's support for Hadoop InputFormat allows it to process data in OpenStack Swift using the\n+same URI formats as in Hadoop. You can specify a path in Swift as input through a \n+URI of the form <code>swift://container.PROVIDER/path</code>. You will also need to set your \n+Swift security credentials, through <code>core-sites.xml</code> or via"
  }],
  "prId": 2298
}, {
  "comments": [{
    "author": {
      "login": "SparkQA"
    },
    "body": "same here...,\n",
    "commit": "ff4e3949052ec7e28f669e82c61499cd9c11a2fa",
    "createdAt": "2014-09-08T03:45:06Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+---\n+layout: global\n+title: Accessing OpenStack Swift from Spark\n+---\n+\n+Spark's support for Hadoop InputFormat allows it to process data in OpenStack Swift using the\n+same URI formats as in Hadoop. You can specify a path in Swift as input through a \n+URI of the form <code>swift://container.PROVIDER/path</code>. You will also need to set your \n+Swift security credentials, through <code>core-sites.xml</code> or via\n+<code>SparkContext.hadoopConfiguration</code>.\n+Current Swift driver requires Swift to use Keystone authentication method.\n+\n+# Configuring Swift for Better Data Locality\n+\n+Although not mandatory, it is recommended to configure the proxy server of Swift with\n+<code>list_endpoints</code> to have better data locality. More information is\n+[available here](https://github.com/openstack/swift/blob/master/swift/common/middleware/list_endpoints.py).\n+\n+\n+# Dependencies\n+\n+The Spark application should include <code>hadoop-openstack</code> dependency.\n+For example, for Maven support, add the following to the <code>pom.xml</code> file:\n+\n+{% highlight xml %}\n+<dependencyManagement>\n+  ...\n+  <dependency>\n+    <groupId>org.apache.hadoop</groupId>\n+    <artifactId>hadoop-openstack</artifactId>\n+    <version>2.3.0</version>\n+  </dependency>\n+  ...\n+</dependencyManagement>\n+{% endhighlight %}\n+\n+\n+# Configuration Parameters\n+\n+Create <code>core-sites.xml</code> and place it inside <code>/spark/conf</code> directory.\n+There are two main categories of parameters that should to be configured: declaration of the\n+Swift driver and the parameters that are required by Keystone. \n+\n+Configuration of Hadoop to use Swift File system achieved via \n+\n+<table class=\"table\">\n+<tr><th>Property Name</th><th>Value</th></tr>\n+<tr>\n+  <td>fs.swift.impl</td>\n+  <td>org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem</td>\n+</tr>\n+</table>\n+\n+Additional parameters required by Keystone (v2.0) and should be provided to the Swift driver. Those \n+parameters will be used to perform authentication in Keystone to access Swift. The following table \n+contains a list of Keystone mandatory parameters. <code>PROVIDER</code> can be any name.\n+\n+<table class=\"table\">\n+<tr><th>Property Name</th><th>Meaning</th><th>Required</th></tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.auth.url</code></td>\n+  <td>Keystone Authentication URL</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.auth.endpoint.prefix</code></td>\n+  <td>Keystone endpoints prefix</td>\n+  <td>Optional</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.tenant</code></td>\n+  <td>Tenant</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.username</code></td>\n+  <td>Username</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.password</code></td>\n+  <td>Password</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.http.port</code></td>\n+  <td>HTTP port</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.region</code></td>\n+  <td>Keystone region</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.public</code></td>\n+  <td>Indicates if all URLs are public</td>\n+  <td>Mandatory</td>\n+</tr>\n+</table>\n+\n+For example, assume <code>PROVIDER=SparkTest</code> and Keystone contains user <code>tester</code> with password <code>testing</code>\n+defined for tenant <code>test</code>. Than <code>core-sites.xml</code> should include:\n+\n+{% highlight xml %}\n+<configuration>\n+  <property>\n+    <name>fs.swift.impl</name>\n+    <value>org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem</value>\n+  </property>\n+  <property>\n+    <name>fs.swift.service.SparkTest.auth.url</name>\n+    <value>http://127.0.0.1:5000/v2.0/tokens</value>\n+  </property>\n+  <property>\n+    <name>fs.swift.service.SparkTest.auth.endpoint.prefix</name>\n+    <value>endpoints</value>\n+  </property>\n+    <name>fs.swift.service.SparkTest.http.port</name>\n+    <value>8080</value>\n+  </property>\n+  <property>\n+    <name>fs.swift.service.SparkTest.region</name>\n+    <value>RegionOne</value>\n+  </property>\n+  <property>\n+    <name>fs.swift.service.SparkTest.public</name>\n+    <value>true</value>\n+  </property>\n+  <property>\n+    <name>fs.swift.service.SparkTest.tenant</name>\n+    <value>test</value>\n+  </property>\n+  <property>\n+    <name>fs.swift.service.SparkTest.username</name>\n+    <value>tester</value>\n+  </property>\n+  <property>\n+    <name>fs.swift.service.SparkTest.password</name>\n+    <value>testing</value>\n+  </property>\n+</configuration>\n+{% endhighlight %}\n+\n+Notice that\n+<code>fs.swift.service.PROVIDER.tenant</code>,\n+<code>fs.swift.service.PROVIDER.username</code>, \n+<code>fs.swift.service.PROVIDER.password</code> contains sensitive information and keeping them in\n+<code>core-sites.xml</code> is not always a good approach."
  }],
  "prId": 2298
}, {
  "comments": [{
    "author": {
      "login": "SparkQA"
    },
    "body": "here again\n",
    "commit": "ff4e3949052ec7e28f669e82c61499cd9c11a2fa",
    "createdAt": "2014-09-08T03:46:34Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+---\n+layout: global\n+title: Accessing OpenStack Swift from Spark\n+---\n+\n+Spark's support for Hadoop InputFormat allows it to process data in OpenStack Swift using the\n+same URI formats as in Hadoop. You can specify a path in Swift as input through a \n+URI of the form <code>swift://container.PROVIDER/path</code>. You will also need to set your \n+Swift security credentials, through <code>core-sites.xml</code> or via\n+<code>SparkContext.hadoopConfiguration</code>.\n+Current Swift driver requires Swift to use Keystone authentication method.\n+\n+# Configuring Swift for Better Data Locality\n+\n+Although not mandatory, it is recommended to configure the proxy server of Swift with\n+<code>list_endpoints</code> to have better data locality. More information is\n+[available here](https://github.com/openstack/swift/blob/master/swift/common/middleware/list_endpoints.py).\n+\n+\n+# Dependencies\n+\n+The Spark application should include <code>hadoop-openstack</code> dependency.\n+For example, for Maven support, add the following to the <code>pom.xml</code> file:\n+\n+{% highlight xml %}\n+<dependencyManagement>\n+  ...\n+  <dependency>\n+    <groupId>org.apache.hadoop</groupId>\n+    <artifactId>hadoop-openstack</artifactId>\n+    <version>2.3.0</version>\n+  </dependency>\n+  ...\n+</dependencyManagement>\n+{% endhighlight %}\n+\n+\n+# Configuration Parameters\n+\n+Create <code>core-sites.xml</code> and place it inside <code>/spark/conf</code> directory.\n+There are two main categories of parameters that should to be configured: declaration of the\n+Swift driver and the parameters that are required by Keystone. \n+\n+Configuration of Hadoop to use Swift File system achieved via \n+\n+<table class=\"table\">\n+<tr><th>Property Name</th><th>Value</th></tr>\n+<tr>\n+  <td>fs.swift.impl</td>\n+  <td>org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem</td>\n+</tr>\n+</table>\n+\n+Additional parameters required by Keystone (v2.0) and should be provided to the Swift driver. Those \n+parameters will be used to perform authentication in Keystone to access Swift. The following table \n+contains a list of Keystone mandatory parameters. <code>PROVIDER</code> can be any name.\n+\n+<table class=\"table\">\n+<tr><th>Property Name</th><th>Meaning</th><th>Required</th></tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.auth.url</code></td>\n+  <td>Keystone Authentication URL</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.auth.endpoint.prefix</code></td>\n+  <td>Keystone endpoints prefix</td>\n+  <td>Optional</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.tenant</code></td>\n+  <td>Tenant</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.username</code></td>\n+  <td>Username</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.password</code></td>\n+  <td>Password</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.http.port</code></td>\n+  <td>HTTP port</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.region</code></td>\n+  <td>Keystone region</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.public</code></td>\n+  <td>Indicates if all URLs are public</td>\n+  <td>Mandatory</td>\n+</tr>\n+</table>\n+\n+For example, assume <code>PROVIDER=SparkTest</code> and Keystone contains user <code>tester</code> with password <code>testing</code>\n+defined for tenant <code>test</code>. Than <code>core-sites.xml</code> should include:"
  }],
  "prId": 2298
}, {
  "comments": [{
    "author": {
      "login": "SparkQA"
    },
    "body": "minor - but could this say `conf/` rather than `/spark/conf`... we usually don't assume the name of the root folder (in many cases it e.g. has a version)\n",
    "commit": "ff4e3949052ec7e28f669e82c61499cd9c11a2fa",
    "createdAt": "2014-09-08T03:48:52Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+---\n+layout: global\n+title: Accessing OpenStack Swift from Spark\n+---\n+\n+Spark's support for Hadoop InputFormat allows it to process data in OpenStack Swift using the\n+same URI formats as in Hadoop. You can specify a path in Swift as input through a \n+URI of the form <code>swift://container.PROVIDER/path</code>. You will also need to set your \n+Swift security credentials, through <code>core-site.xml</code> or via\n+<code>SparkContext.hadoopConfiguration</code>.\n+Current Swift driver requires Swift to use Keystone authentication method.\n+\n+# Configuring Swift for Better Data Locality\n+\n+Although not mandatory, it is recommended to configure the proxy server of Swift with\n+<code>list_endpoints</code> to have better data locality. More information is\n+[available here](https://github.com/openstack/swift/blob/master/swift/common/middleware/list_endpoints.py).\n+\n+\n+# Dependencies\n+\n+The Spark application should include <code>hadoop-openstack</code> dependency.\n+For example, for Maven support, add the following to the <code>pom.xml</code> file:\n+\n+{% highlight xml %}\n+<dependencyManagement>\n+  ...\n+  <dependency>\n+    <groupId>org.apache.hadoop</groupId>\n+    <artifactId>hadoop-openstack</artifactId>\n+    <version>2.3.0</version>\n+  </dependency>\n+  ...\n+</dependencyManagement>\n+{% endhighlight %}\n+\n+\n+# Configuration Parameters\n+\n+Create <code>core-site.xml</code> and place it inside <code>/spark/conf</code> directory."
  }, {
    "author": {
      "login": "pwendell"
    },
    "body": "i agree\n",
    "commit": "ff4e3949052ec7e28f669e82c61499cd9c11a2fa",
    "createdAt": "2014-09-08T03:50:23Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+---\n+layout: global\n+title: Accessing OpenStack Swift from Spark\n+---\n+\n+Spark's support for Hadoop InputFormat allows it to process data in OpenStack Swift using the\n+same URI formats as in Hadoop. You can specify a path in Swift as input through a \n+URI of the form <code>swift://container.PROVIDER/path</code>. You will also need to set your \n+Swift security credentials, through <code>core-site.xml</code> or via\n+<code>SparkContext.hadoopConfiguration</code>.\n+Current Swift driver requires Swift to use Keystone authentication method.\n+\n+# Configuring Swift for Better Data Locality\n+\n+Although not mandatory, it is recommended to configure the proxy server of Swift with\n+<code>list_endpoints</code> to have better data locality. More information is\n+[available here](https://github.com/openstack/swift/blob/master/swift/common/middleware/list_endpoints.py).\n+\n+\n+# Dependencies\n+\n+The Spark application should include <code>hadoop-openstack</code> dependency.\n+For example, for Maven support, add the following to the <code>pom.xml</code> file:\n+\n+{% highlight xml %}\n+<dependencyManagement>\n+  ...\n+  <dependency>\n+    <groupId>org.apache.hadoop</groupId>\n+    <artifactId>hadoop-openstack</artifactId>\n+    <version>2.3.0</version>\n+  </dependency>\n+  ...\n+</dependencyManagement>\n+{% endhighlight %}\n+\n+\n+# Configuration Parameters\n+\n+Create <code>core-site.xml</code> and place it inside <code>/spark/conf</code> directory."
  }],
  "prId": 2298
}, {
  "comments": [{
    "author": {
      "login": "pwendell"
    },
    "body": "Than --> Then\n",
    "commit": "ff4e3949052ec7e28f669e82c61499cd9c11a2fa",
    "createdAt": "2014-09-08T03:50:50Z",
    "diffHunk": "@@ -0,0 +1,152 @@\n+---\n+layout: global\n+title: Accessing OpenStack Swift from Spark\n+---\n+\n+Spark's support for Hadoop InputFormat allows it to process data in OpenStack Swift using the\n+same URI formats as in Hadoop. You can specify a path in Swift as input through a \n+URI of the form <code>swift://container.PROVIDER/path</code>. You will also need to set your \n+Swift security credentials, through <code>core-site.xml</code> or via\n+<code>SparkContext.hadoopConfiguration</code>.\n+Current Swift driver requires Swift to use Keystone authentication method.\n+\n+# Configuring Swift for Better Data Locality\n+\n+Although not mandatory, it is recommended to configure the proxy server of Swift with\n+<code>list_endpoints</code> to have better data locality. More information is\n+[available here](https://github.com/openstack/swift/blob/master/swift/common/middleware/list_endpoints.py).\n+\n+\n+# Dependencies\n+\n+The Spark application should include <code>hadoop-openstack</code> dependency.\n+For example, for Maven support, add the following to the <code>pom.xml</code> file:\n+\n+{% highlight xml %}\n+<dependencyManagement>\n+  ...\n+  <dependency>\n+    <groupId>org.apache.hadoop</groupId>\n+    <artifactId>hadoop-openstack</artifactId>\n+    <version>2.3.0</version>\n+  </dependency>\n+  ...\n+</dependencyManagement>\n+{% endhighlight %}\n+\n+\n+# Configuration Parameters\n+\n+Create <code>core-site.xml</code> and place it inside <code>/spark/conf</code> directory.\n+There are two main categories of parameters that should to be configured: declaration of the\n+Swift driver and the parameters that are required by Keystone. \n+\n+Configuration of Hadoop to use Swift File system achieved via \n+\n+<table class=\"table\">\n+<tr><th>Property Name</th><th>Value</th></tr>\n+<tr>\n+  <td>fs.swift.impl</td>\n+  <td>org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem</td>\n+</tr>\n+</table>\n+\n+Additional parameters required by Keystone (v2.0) and should be provided to the Swift driver. Those \n+parameters will be used to perform authentication in Keystone to access Swift. The following table \n+contains a list of Keystone mandatory parameters. <code>PROVIDER</code> can be any name.\n+\n+<table class=\"table\">\n+<tr><th>Property Name</th><th>Meaning</th><th>Required</th></tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.auth.url</code></td>\n+  <td>Keystone Authentication URL</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.auth.endpoint.prefix</code></td>\n+  <td>Keystone endpoints prefix</td>\n+  <td>Optional</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.tenant</code></td>\n+  <td>Tenant</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.username</code></td>\n+  <td>Username</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.password</code></td>\n+  <td>Password</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.http.port</code></td>\n+  <td>HTTP port</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.region</code></td>\n+  <td>Keystone region</td>\n+  <td>Mandatory</td>\n+</tr>\n+<tr>\n+  <td><code>fs.swift.service.PROVIDER.public</code></td>\n+  <td>Indicates if all URLs are public</td>\n+  <td>Mandatory</td>\n+</tr>\n+</table>\n+\n+For example, assume <code>PROVIDER=SparkTest</code> and Keystone contains user <code>tester</code> with password <code>testing</code>\n+defined for tenant <code>test</code>. Than <code>core-site.xml</code> should include:"
  }],
  "prId": 2298
}]