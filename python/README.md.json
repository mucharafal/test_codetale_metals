[{
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "Would it be appropriate to cut this paragraph out and just leave the stuff about packaging? If these blurbs ever change I don't think we want to have to update them in multiple places, and we already have this blurb in at least one other place, I think.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-27T14:50:48Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+# Apache Spark\n+\n+Spark is a fast and general cluster computing system for Big Data. It provides",
    "line": 3
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "So this ends up in the packaging description in PyPI (once we publish there). For people browsing on PyPI they might not be super familiar with PySpark so it probably makes sense to keep the blurb.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-27T15:20:57Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+# Apache Spark\n+\n+Spark is a fast and general cluster computing system for Big Data. It provides",
    "line": 3
  }, {
    "author": {
      "login": "nchammas"
    },
    "body": "I see. And I'm guessing we can't/don't want to somehow reference the README in the root directory? (Perhaps even with a symlink, if necessary...)\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-27T15:38:03Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+# Apache Spark\n+\n+Spark is a fast and general cluster computing system for Big Data. It provides",
    "line": 3
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "well we don't necessarily want to full Spark README for PySpark so idk. But if we would be ok with the full README we could symlink it as normal.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-27T16:13:57Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+# Apache Spark\n+\n+Spark is a fast and general cluster computing system for Big Data. It provides",
    "line": 3
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "If I am doing local development on my Mac, for example, what does pip installing Spark get me?\n\nIt sounds like from this line that even if I pip install Spark, I will still need to separately `brew install apache-spark` or something to be able to run Spark programs. Is that correct?\n\nHow does my workflow change or improve if I can pip install Spark?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-27T14:55:43Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+# Apache Spark\n+\n+Spark is a fast and general cluster computing system for Big Data. It provides\n+high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n+supports general computation graphs for data analysis. It also supports a\n+rich set of higher-level tools including Spark SQL for SQL and DataFrames,\n+MLlib for machine learning, GraphX for graph processing,\n+and Spark Streaming for stream processing.\n+\n+<http://spark.apache.org/>\n+\n+## Online Documentation\n+\n+You can find the latest Spark documentation, including a programming\n+guide, on the [project web page](http://spark.apache.org/documentation.html)\n+\n+\n+## Python Packaging\n+\n+This README file only contains basic information related to pip installed PySpark.\n+This packaging is currently experimental and may change in future versions (although we will do our best to keep compatibility).\n+Using PySpark requires the Spark JARs, and if you are building this from source please see the builder instructions at\n+[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).\n+\n+The Python packaging for Spark is not intended to replace all of the other use cases. This Python packaged version of Spark is suitable for interacting with an existing cluster (be it Spark standalone, YARN, or Mesos) - but does not contain the tools required to setup your own standalone Spark cluster. You can download the full version of Spark from the [Apache Spark downloads page](http://spark.apache.org/downloads.html).",
    "line": 25
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "No that is very much not the case. If your working on your local Mac you can run a localmode cluster just fine. You can submit to an existing Spark cluster or Yarn just fine as well. You do not need to install other components unless your trying to setup a new standalone Spark cluster.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-27T15:20:10Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+# Apache Spark\n+\n+Spark is a fast and general cluster computing system for Big Data. It provides\n+high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n+supports general computation graphs for data analysis. It also supports a\n+rich set of higher-level tools including Spark SQL for SQL and DataFrames,\n+MLlib for machine learning, GraphX for graph processing,\n+and Spark Streaming for stream processing.\n+\n+<http://spark.apache.org/>\n+\n+## Online Documentation\n+\n+You can find the latest Spark documentation, including a programming\n+guide, on the [project web page](http://spark.apache.org/documentation.html)\n+\n+\n+## Python Packaging\n+\n+This README file only contains basic information related to pip installed PySpark.\n+This packaging is currently experimental and may change in future versions (although we will do our best to keep compatibility).\n+Using PySpark requires the Spark JARs, and if you are building this from source please see the builder instructions at\n+[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).\n+\n+The Python packaging for Spark is not intended to replace all of the other use cases. This Python packaged version of Spark is suitable for interacting with an existing cluster (be it Spark standalone, YARN, or Mesos) - but does not contain the tools required to setup your own standalone Spark cluster. You can download the full version of Spark from the [Apache Spark downloads page](http://spark.apache.org/downloads.html).",
    "line": 25
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "Any suggestions on how I could improve the wording to make that clear? @nchammas ?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-27T15:30:04Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+# Apache Spark\n+\n+Spark is a fast and general cluster computing system for Big Data. It provides\n+high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n+supports general computation graphs for data analysis. It also supports a\n+rich set of higher-level tools including Spark SQL for SQL and DataFrames,\n+MLlib for machine learning, GraphX for graph processing,\n+and Spark Streaming for stream processing.\n+\n+<http://spark.apache.org/>\n+\n+## Online Documentation\n+\n+You can find the latest Spark documentation, including a programming\n+guide, on the [project web page](http://spark.apache.org/documentation.html)\n+\n+\n+## Python Packaging\n+\n+This README file only contains basic information related to pip installed PySpark.\n+This packaging is currently experimental and may change in future versions (although we will do our best to keep compatibility).\n+Using PySpark requires the Spark JARs, and if you are building this from source please see the builder instructions at\n+[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).\n+\n+The Python packaging for Spark is not intended to replace all of the other use cases. This Python packaged version of Spark is suitable for interacting with an existing cluster (be it Spark standalone, YARN, or Mesos) - but does not contain the tools required to setup your own standalone Spark cluster. You can download the full version of Spark from the [Apache Spark downloads page](http://spark.apache.org/downloads.html).",
    "line": 25
  }, {
    "author": {
      "login": "nchammas"
    },
    "body": "I see. So `pip install pyspark` can completely replace `brew install apache-spark` for local development, or for submitting from a local machine to a remote cluster.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-27T15:39:59Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+# Apache Spark\n+\n+Spark is a fast and general cluster computing system for Big Data. It provides\n+high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n+supports general computation graphs for data analysis. It also supports a\n+rich set of higher-level tools including Spark SQL for SQL and DataFrames,\n+MLlib for machine learning, GraphX for graph processing,\n+and Spark Streaming for stream processing.\n+\n+<http://spark.apache.org/>\n+\n+## Online Documentation\n+\n+You can find the latest Spark documentation, including a programming\n+guide, on the [project web page](http://spark.apache.org/documentation.html)\n+\n+\n+## Python Packaging\n+\n+This README file only contains basic information related to pip installed PySpark.\n+This packaging is currently experimental and may change in future versions (although we will do our best to keep compatibility).\n+Using PySpark requires the Spark JARs, and if you are building this from source please see the builder instructions at\n+[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).\n+\n+The Python packaging for Spark is not intended to replace all of the other use cases. This Python packaged version of Spark is suitable for interacting with an existing cluster (be it Spark standalone, YARN, or Mesos) - but does not contain the tools required to setup your own standalone Spark cluster. You can download the full version of Spark from the [Apache Spark downloads page](http://spark.apache.org/downloads.html).",
    "line": 25
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "Indeed! That is the goal, and then we can have all of the nice things of having virtualenvs, just importing PySpark rather than importing a package to find spark and then importing spark, and rainbows and kittens*.\n\n(*rainbows and kittens are not a gaurantee - see vendor for details :p)\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-27T16:12:53Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+# Apache Spark\n+\n+Spark is a fast and general cluster computing system for Big Data. It provides\n+high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n+supports general computation graphs for data analysis. It also supports a\n+rich set of higher-level tools including Spark SQL for SQL and DataFrames,\n+MLlib for machine learning, GraphX for graph processing,\n+and Spark Streaming for stream processing.\n+\n+<http://spark.apache.org/>\n+\n+## Online Documentation\n+\n+You can find the latest Spark documentation, including a programming\n+guide, on the [project web page](http://spark.apache.org/documentation.html)\n+\n+\n+## Python Packaging\n+\n+This README file only contains basic information related to pip installed PySpark.\n+This packaging is currently experimental and may change in future versions (although we will do our best to keep compatibility).\n+Using PySpark requires the Spark JARs, and if you are building this from source please see the builder instructions at\n+[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).\n+\n+The Python packaging for Spark is not intended to replace all of the other use cases. This Python packaged version of Spark is suitable for interacting with an existing cluster (be it Spark standalone, YARN, or Mesos) - but does not contain the tools required to setup your own standalone Spark cluster. You can download the full version of Spark from the [Apache Spark downloads page](http://spark.apache.org/downloads.html).",
    "line": 25
  }, {
    "author": {
      "login": "rgbkrk"
    },
    "body": "> pip install pyspark can completely replace brew install apache-spark for local development, or for submitting from a local machine to a remote cluster.\n\nThat is absolutely correct @nchammas \n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-27T16:22:45Z",
    "diffHunk": "@@ -0,0 +1,32 @@\n+# Apache Spark\n+\n+Spark is a fast and general cluster computing system for Big Data. It provides\n+high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n+supports general computation graphs for data analysis. It also supports a\n+rich set of higher-level tools including Spark SQL for SQL and DataFrames,\n+MLlib for machine learning, GraphX for graph processing,\n+and Spark Streaming for stream processing.\n+\n+<http://spark.apache.org/>\n+\n+## Online Documentation\n+\n+You can find the latest Spark documentation, including a programming\n+guide, on the [project web page](http://spark.apache.org/documentation.html)\n+\n+\n+## Python Packaging\n+\n+This README file only contains basic information related to pip installed PySpark.\n+This packaging is currently experimental and may change in future versions (although we will do our best to keep compatibility).\n+Using PySpark requires the Spark JARs, and if you are building this from source please see the builder instructions at\n+[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).\n+\n+The Python packaging for Spark is not intended to replace all of the other use cases. This Python packaged version of Spark is suitable for interacting with an existing cluster (be it Spark standalone, YARN, or Mesos) - but does not contain the tools required to setup your own standalone Spark cluster. You can download the full version of Spark from the [Apache Spark downloads page](http://spark.apache.org/downloads.html).",
    "line": 25
  }],
  "prId": 15659
}]