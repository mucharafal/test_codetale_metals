[{
  "comments": [{
    "author": {
      "login": "ahirreddy"
    },
    "body": "Would it make more sense to try to package PySpark dependencies inside a jar, instead of requiring this extra step? Since jars are just zips, Python should be able to run against them in the same way, so most of this PR wouldn't have to be changed.\n",
    "commit": "89889d4bb55ab6e2659c90e23b179a660a2a9204",
    "createdAt": "2014-03-07T22:55:20Z",
    "diffHunk": "@@ -0,0 +1,7 @@\n+assembly: clean\n+\tpython setup.py build --build-lib build/lib\n+\tunzip lib/py4j*.zip -d build/lib\n+\tcd build/lib && zip -r ../pyspark-assembly.zip .\n+"
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Are you envisioning including the PySpark dependencies in the Spark assembly jar?  I think that could work, since we need to build that jar anyways when running under YARN.\n\nI'm not sure how easy it will be to modify the Maven or SBT builds to include those files.\n",
    "commit": "89889d4bb55ab6e2659c90e23b179a660a2a9204",
    "createdAt": "2014-03-08T20:11:02Z",
    "diffHunk": "@@ -0,0 +1,7 @@\n+assembly: clean\n+\tpython setup.py build --build-lib build/lib\n+\tunzip lib/py4j*.zip -d build/lib\n+\tcd build/lib && zip -r ../pyspark-assembly.zip .\n+"
  }, {
    "author": {
      "login": "sryza"
    },
    "body": "I could probably figure out how to do this in Maven, but have no idea how to do it with SBT.\n",
    "commit": "89889d4bb55ab6e2659c90e23b179a660a2a9204",
    "createdAt": "2014-03-28T22:57:51Z",
    "diffHunk": "@@ -0,0 +1,7 @@\n+assembly: clean\n+\tpython setup.py build --build-lib build/lib\n+\tunzip lib/py4j*.zip -d build/lib\n+\tcd build/lib && zip -r ../pyspark-assembly.zip .\n+"
  }],
  "prId": 30
}]