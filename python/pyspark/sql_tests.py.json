[{
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "Could it have a short name, such as 'json' or 'JSON' ?\n",
    "commit": "f3a96f7497ae613fe4668c3ab739c1f8cb315ec6",
    "createdAt": "2015-02-10T05:30:05Z",
    "diffHunk": "@@ -285,6 +285,38 @@ def test_aggregator(self):\n         self.assertTrue(95 < g.agg(Dsl.approxCountDistinct(df.key)).first()[0])\n         self.assertEqual(100, g.agg(Dsl.countDistinct(df.value)).first()[0])\n \n+    def test_save_and_load(self):\n+        df = self.df\n+        tmpPath = tempfile.mkdtemp()\n+        shutil.rmtree(tmpPath)\n+        df.save(tmpPath, \"org.apache.spark.sql.json\", \"error\")"
  }],
  "prId": 4446
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "It it possible to use short name for 'spark.sql.sources.default' ?\n",
    "commit": "f3a96f7497ae613fe4668c3ab739c1f8cb315ec6",
    "createdAt": "2015-02-10T05:32:20Z",
    "diffHunk": "@@ -285,6 +285,38 @@ def test_aggregator(self):\n         self.assertTrue(95 < g.agg(Dsl.approxCountDistinct(df.key)).first()[0])\n         self.assertEqual(100, g.agg(Dsl.countDistinct(df.value)).first()[0])\n \n+    def test_save_and_load(self):\n+        df = self.df\n+        tmpPath = tempfile.mkdtemp()\n+        shutil.rmtree(tmpPath)\n+        df.save(tmpPath, \"org.apache.spark.sql.json\", \"error\")\n+        actual = self.sqlCtx.load(tmpPath, \"org.apache.spark.sql.json\")\n+        self.assertTrue(sorted(df.collect()) == sorted(actual.collect()))\n+\n+        from pyspark.sql import StructType, StructField, StringType\n+        schema = StructType([StructField(\"value\", StringType(), True)])\n+        actual = self.sqlCtx.load(tmpPath, \"org.apache.spark.sql.json\", schema)\n+        self.assertTrue(sorted(df.select(\"value\").collect()) == sorted(actual.collect()))\n+\n+        df.save(tmpPath, \"org.apache.spark.sql.json\", \"overwrite\")\n+        actual = self.sqlCtx.load(tmpPath, \"org.apache.spark.sql.json\")\n+        self.assertTrue(sorted(df.collect()) == sorted(actual.collect()))\n+\n+        df.save(dataSourceName=\"org.apache.spark.sql.json\", mode=\"overwrite\", path=tmpPath,\n+                noUse=\"this options will not be used in save.\")\n+        actual = self.sqlCtx.load(dataSourceName=\"org.apache.spark.sql.json\", path=tmpPath,\n+                                  noUse=\"this options will not be used in load.\")\n+        self.assertTrue(sorted(df.collect()) == sorted(actual.collect()))\n+\n+        defaultDataSourceName = self.sqlCtx._ssql_ctx.getConf(\"spark.sql.sources.default\",\n+                                                              \"org.apache.spark.sql.parquet\")\n+        self.sqlCtx.sql(\"SET spark.sql.sources.default=org.apache.spark.sql.json\")"
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "that's in a separate pr https://github.com/apache/spark/pull/4489\n",
    "commit": "f3a96f7497ae613fe4668c3ab739c1f8cb315ec6",
    "createdAt": "2015-02-10T05:34:03Z",
    "diffHunk": "@@ -285,6 +285,38 @@ def test_aggregator(self):\n         self.assertTrue(95 < g.agg(Dsl.approxCountDistinct(df.key)).first()[0])\n         self.assertEqual(100, g.agg(Dsl.countDistinct(df.value)).first()[0])\n \n+    def test_save_and_load(self):\n+        df = self.df\n+        tmpPath = tempfile.mkdtemp()\n+        shutil.rmtree(tmpPath)\n+        df.save(tmpPath, \"org.apache.spark.sql.json\", \"error\")\n+        actual = self.sqlCtx.load(tmpPath, \"org.apache.spark.sql.json\")\n+        self.assertTrue(sorted(df.collect()) == sorted(actual.collect()))\n+\n+        from pyspark.sql import StructType, StructField, StringType\n+        schema = StructType([StructField(\"value\", StringType(), True)])\n+        actual = self.sqlCtx.load(tmpPath, \"org.apache.spark.sql.json\", schema)\n+        self.assertTrue(sorted(df.select(\"value\").collect()) == sorted(actual.collect()))\n+\n+        df.save(tmpPath, \"org.apache.spark.sql.json\", \"overwrite\")\n+        actual = self.sqlCtx.load(tmpPath, \"org.apache.spark.sql.json\")\n+        self.assertTrue(sorted(df.collect()) == sorted(actual.collect()))\n+\n+        df.save(dataSourceName=\"org.apache.spark.sql.json\", mode=\"overwrite\", path=tmpPath,\n+                noUse=\"this options will not be used in save.\")\n+        actual = self.sqlCtx.load(dataSourceName=\"org.apache.spark.sql.json\", path=tmpPath,\n+                                  noUse=\"this options will not be used in load.\")\n+        self.assertTrue(sorted(df.collect()) == sorted(actual.collect()))\n+\n+        defaultDataSourceName = self.sqlCtx._ssql_ctx.getConf(\"spark.sql.sources.default\",\n+                                                              \"org.apache.spark.sql.parquet\")\n+        self.sqlCtx.sql(\"SET spark.sql.sources.default=org.apache.spark.sql.json\")"
  }, {
    "author": {
      "login": "yhuai"
    },
    "body": "@davies You meant a shorter name for the conf key \"spark.sql.sources.default\"?\n",
    "commit": "f3a96f7497ae613fe4668c3ab739c1f8cb315ec6",
    "createdAt": "2015-02-10T17:43:15Z",
    "diffHunk": "@@ -285,6 +285,38 @@ def test_aggregator(self):\n         self.assertTrue(95 < g.agg(Dsl.approxCountDistinct(df.key)).first()[0])\n         self.assertEqual(100, g.agg(Dsl.countDistinct(df.value)).first()[0])\n \n+    def test_save_and_load(self):\n+        df = self.df\n+        tmpPath = tempfile.mkdtemp()\n+        shutil.rmtree(tmpPath)\n+        df.save(tmpPath, \"org.apache.spark.sql.json\", \"error\")\n+        actual = self.sqlCtx.load(tmpPath, \"org.apache.spark.sql.json\")\n+        self.assertTrue(sorted(df.collect()) == sorted(actual.collect()))\n+\n+        from pyspark.sql import StructType, StructField, StringType\n+        schema = StructType([StructField(\"value\", StringType(), True)])\n+        actual = self.sqlCtx.load(tmpPath, \"org.apache.spark.sql.json\", schema)\n+        self.assertTrue(sorted(df.select(\"value\").collect()) == sorted(actual.collect()))\n+\n+        df.save(tmpPath, \"org.apache.spark.sql.json\", \"overwrite\")\n+        actual = self.sqlCtx.load(tmpPath, \"org.apache.spark.sql.json\")\n+        self.assertTrue(sorted(df.collect()) == sorted(actual.collect()))\n+\n+        df.save(dataSourceName=\"org.apache.spark.sql.json\", mode=\"overwrite\", path=tmpPath,\n+                noUse=\"this options will not be used in save.\")\n+        actual = self.sqlCtx.load(dataSourceName=\"org.apache.spark.sql.json\", path=tmpPath,\n+                                  noUse=\"this options will not be used in load.\")\n+        self.assertTrue(sorted(df.collect()) == sorted(actual.collect()))\n+\n+        defaultDataSourceName = self.sqlCtx._ssql_ctx.getConf(\"spark.sql.sources.default\",\n+                                                              \"org.apache.spark.sql.parquet\")\n+        self.sqlCtx.sql(\"SET spark.sql.sources.default=org.apache.spark.sql.json\")"
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "I think we should just try org.apache.spark.sql.\\* whenever what they give\nus fails to resolve on its own.\nOn Feb 10, 2015 9:43 AM, \"Yin Huai\" notifications@github.com wrote:\n\n> In python/pyspark/sql_tests.py\n> https://github.com/apache/spark/pull/4446#discussion_r24432467:\n> \n> > -        actual = self.sqlCtx.load(tmpPath, \"org.apache.spark.sql.json\", schema)\n> > -        self.assertTrue(sorted(df.select(\"value\").collect()) == sorted(actual.collect()))\n> >   +\n> > -        df.save(tmpPath, \"org.apache.spark.sql.json\", \"overwrite\")\n> > -        actual = self.sqlCtx.load(tmpPath, \"org.apache.spark.sql.json\")\n> > -        self.assertTrue(sorted(df.collect()) == sorted(actual.collect()))\n> >   +\n> > -        df.save(dataSourceName=\"org.apache.spark.sql.json\", mode=\"overwrite\", path=tmpPath,\n> > -                noUse=\"this options will not be used in save.\")\n> > -        actual = self.sqlCtx.load(dataSourceName=\"org.apache.spark.sql.json\", path=tmpPath,\n> > -                                  noUse=\"this options will not be used in load.\")\n> > -        self.assertTrue(sorted(df.collect()) == sorted(actual.collect()))\n> >   +\n> > -        defaultDataSourceName = self.sqlCtx._ssql_ctx.getConf(\"spark.sql.sources.default\",\n> > -                                                              \"org.apache.spark.sql.parquet\")\n> > -        self.sqlCtx.sql(\"SET spark.sql.sources.default=org.apache.spark.sql.json\")\n> \n> @davies https://github.com/davies You meant a shorter name for the conf\n> key \"spark.sql.sources.default\"?\n> \n> â€”\n> Reply to this email directly or view it on GitHub\n> https://github.com/apache/spark/pull/4446/files#r24432467.\n",
    "commit": "f3a96f7497ae613fe4668c3ab739c1f8cb315ec6",
    "createdAt": "2015-02-10T17:45:02Z",
    "diffHunk": "@@ -285,6 +285,38 @@ def test_aggregator(self):\n         self.assertTrue(95 < g.agg(Dsl.approxCountDistinct(df.key)).first()[0])\n         self.assertEqual(100, g.agg(Dsl.countDistinct(df.value)).first()[0])\n \n+    def test_save_and_load(self):\n+        df = self.df\n+        tmpPath = tempfile.mkdtemp()\n+        shutil.rmtree(tmpPath)\n+        df.save(tmpPath, \"org.apache.spark.sql.json\", \"error\")\n+        actual = self.sqlCtx.load(tmpPath, \"org.apache.spark.sql.json\")\n+        self.assertTrue(sorted(df.collect()) == sorted(actual.collect()))\n+\n+        from pyspark.sql import StructType, StructField, StringType\n+        schema = StructType([StructField(\"value\", StringType(), True)])\n+        actual = self.sqlCtx.load(tmpPath, \"org.apache.spark.sql.json\", schema)\n+        self.assertTrue(sorted(df.select(\"value\").collect()) == sorted(actual.collect()))\n+\n+        df.save(tmpPath, \"org.apache.spark.sql.json\", \"overwrite\")\n+        actual = self.sqlCtx.load(tmpPath, \"org.apache.spark.sql.json\")\n+        self.assertTrue(sorted(df.collect()) == sorted(actual.collect()))\n+\n+        df.save(dataSourceName=\"org.apache.spark.sql.json\", mode=\"overwrite\", path=tmpPath,\n+                noUse=\"this options will not be used in save.\")\n+        actual = self.sqlCtx.load(dataSourceName=\"org.apache.spark.sql.json\", path=tmpPath,\n+                                  noUse=\"this options will not be used in load.\")\n+        self.assertTrue(sorted(df.collect()) == sorted(actual.collect()))\n+\n+        defaultDataSourceName = self.sqlCtx._ssql_ctx.getConf(\"spark.sql.sources.default\",\n+                                                              \"org.apache.spark.sql.parquet\")\n+        self.sqlCtx.sql(\"SET spark.sql.sources.default=org.apache.spark.sql.json\")"
  }],
  "prId": 4446
}]