[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Those seems can be reverted back since the tests target HiveContext.",
    "commit": "92da887036c5fa35dbfa46c7257d52719192b4fa",
    "createdAt": "2019-09-05T09:58:40Z",
    "diffHunk": "@@ -66,7 +66,7 @@ def test_save_and_load_table(self):\n         tmpPath = tempfile.mkdtemp()\n         shutil.rmtree(tmpPath)\n         df.write.saveAsTable(\"savedJsonTable\", \"json\", \"append\", path=tmpPath)\n-        actual = self.spark.createExternalTable(\"externalJsonTable\", tmpPath, \"json\")",
    "line": 44
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "IIRC this failed, because HiveContext just inherits from SQLContext.\r\n\r\nOh, well, this is another good point: HiveContext is deprecated in 2.0! It should just be removed, no?",
    "commit": "92da887036c5fa35dbfa46c7257d52719192b4fa",
    "createdAt": "2019-09-05T14:06:24Z",
    "diffHunk": "@@ -66,7 +66,7 @@ def test_save_and_load_table(self):\n         tmpPath = tempfile.mkdtemp()\n         shutil.rmtree(tmpPath)\n         df.write.saveAsTable(\"savedJsonTable\", \"json\", \"append\", path=tmpPath)\n-        actual = self.spark.createExternalTable(\"externalJsonTable\", tmpPath, \"json\")",
    "line": 44
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Hm, wait this fails after the change, you have a point:\r\n```\r\n======================================================================\r\nERROR: test_save_and_load_table (pyspark.sql.tests.test_context.HiveContextSQLTests)\r\n----------------------------------------------------------------------\r\nTraceback (most recent call last):\r\n  File \"/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/sql/tests/test_context.py\", line 69, in test_save_and_load_table\r\n    actual = self.spark.createTable(\"externalJsonTable\", tmpPath, \"json\")\r\nAttributeError: 'HiveContext' object has no attribute 'createTable'\r\n```\r\nAlso solved by removing it",
    "commit": "92da887036c5fa35dbfa46c7257d52719192b4fa",
    "createdAt": "2019-09-05T14:07:08Z",
    "diffHunk": "@@ -66,7 +66,7 @@ def test_save_and_load_table(self):\n         tmpPath = tempfile.mkdtemp()\n         shutil.rmtree(tmpPath)\n         df.write.saveAsTable(\"savedJsonTable\", \"json\", \"append\", path=tmpPath)\n-        actual = self.spark.createExternalTable(\"externalJsonTable\", tmpPath, \"json\")",
    "line": 44
  }],
  "prId": 25684
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "@srowen, seems this code is still needed. `SparkSession.builder.enableHiveSupport().getOrCreate()` seems not able to directly check if Hive is available or not. I manually tested and just pushed the changes directly into your branch.",
    "commit": "92da887036c5fa35dbfa46c7257d52719192b4fa",
    "createdAt": "2019-09-06T04:40:55Z",
    "diffHunk": "@@ -40,15 +40,20 @@ def setUpClass(cls):\n         ReusedPySparkTestCase.setUpClass()\n         cls.tempdir = tempfile.NamedTemporaryFile(delete=False)\n         cls.hive_available = True\n+        cls.spark = None\n         try:\n             cls.sc._jvm.org.apache.hadoop.hive.conf.HiveConf()",
    "line": 15
  }],
  "prId": 25684
}]