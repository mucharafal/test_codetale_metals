[{
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "nit: we can revert this change?",
    "commit": "03702d450a0a9da7bb50057b3bab35bdec7d4584",
    "createdAt": "2018-12-06T02:12:26Z",
    "diffHunk": "@@ -44,9 +44,18 @@ def python_plus_one(self):\n \n     @property\n     def pandas_scalar_time_two(self):\n-        from pyspark.sql.functions import pandas_udf\n+        from pyspark.sql.functions import pandas_udf, PandasUDFType"
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "Sorry. Reverted",
    "commit": "03702d450a0a9da7bb50057b3bab35bdec7d4584",
    "createdAt": "2018-12-06T19:39:29Z",
    "diffHunk": "@@ -44,9 +44,18 @@ def python_plus_one(self):\n \n     @property\n     def pandas_scalar_time_two(self):\n-        from pyspark.sql.functions import pandas_udf\n+        from pyspark.sql.functions import pandas_udf, PandasUDFType"
  }],
  "prId": 22305
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "ditto.",
    "commit": "03702d450a0a9da7bb50057b3bab35bdec7d4584",
    "createdAt": "2018-12-06T02:14:21Z",
    "diffHunk": "@@ -87,8 +96,34 @@ def ordered_window(self):\n     def unpartitioned_window(self):\n         return Window.partitionBy()\n \n+    @property\n+    def sliding_row_window(self):\n+        return Window.partitionBy('id').orderBy('v').rowsBetween(-2, 1)\n+\n+    @property\n+    def sliding_range_window(self):\n+        return Window.partitionBy('id').orderBy('v').rangeBetween(-2, 4)\n+\n+    @property\n+    def growing_row_window(self):\n+        return Window.partitionBy('id').orderBy('v').rowsBetween(Window.unboundedPreceding, 3)\n+\n+    @property\n+    def growing_range_window(self):\n+        return Window.partitionBy('id').orderBy('v') \\\n+            .rangeBetween(Window.unboundedPreceding, 4)\n+\n+    @property\n+    def shrinking_row_window(self):\n+        return Window.partitionBy('id').orderBy('v').rowsBetween(-2, Window.unboundedFollowing)\n+\n+    @property\n+    def shrinking_range_window(self):\n+        return Window.partitionBy('id').orderBy('v') \\\n+            .rangeBetween(-3, Window.unboundedFollowing)\n+\n     def test_simple(self):\n-        from pyspark.sql.functions import mean\n+        from pyspark.sql.functions import pandas_udf, PandasUDFType, percent_rank, mean, max"
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "Reverted",
    "commit": "03702d450a0a9da7bb50057b3bab35bdec7d4584",
    "createdAt": "2018-12-06T19:39:36Z",
    "diffHunk": "@@ -87,8 +96,34 @@ def ordered_window(self):\n     def unpartitioned_window(self):\n         return Window.partitionBy()\n \n+    @property\n+    def sliding_row_window(self):\n+        return Window.partitionBy('id').orderBy('v').rowsBetween(-2, 1)\n+\n+    @property\n+    def sliding_range_window(self):\n+        return Window.partitionBy('id').orderBy('v').rangeBetween(-2, 4)\n+\n+    @property\n+    def growing_row_window(self):\n+        return Window.partitionBy('id').orderBy('v').rowsBetween(Window.unboundedPreceding, 3)\n+\n+    @property\n+    def growing_range_window(self):\n+        return Window.partitionBy('id').orderBy('v') \\\n+            .rangeBetween(Window.unboundedPreceding, 4)\n+\n+    @property\n+    def shrinking_row_window(self):\n+        return Window.partitionBy('id').orderBy('v').rowsBetween(-2, Window.unboundedFollowing)\n+\n+    @property\n+    def shrinking_range_window(self):\n+        return Window.partitionBy('id').orderBy('v') \\\n+            .rangeBetween(-3, Window.unboundedFollowing)\n+\n     def test_simple(self):\n-        from pyspark.sql.functions import mean\n+        from pyspark.sql.functions import pandas_udf, PandasUDFType, percent_rank, mean, max"
  }],
  "prId": 22305
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "ditto.",
    "commit": "03702d450a0a9da7bb50057b3bab35bdec7d4584",
    "createdAt": "2018-12-06T02:15:37Z",
    "diffHunk": "@@ -231,12 +266,10 @@ def test_array_type(self):\n         self.assertEquals(result1.first()['v2'], [1.0, 2.0])\n \n     def test_invalid_args(self):\n-        from pyspark.sql.functions import pandas_udf, PandasUDFType\n+        from pyspark.sql.functions import mean, pandas_udf, PandasUDFType"
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "Reverted",
    "commit": "03702d450a0a9da7bb50057b3bab35bdec7d4584",
    "createdAt": "2018-12-06T19:39:41Z",
    "diffHunk": "@@ -231,12 +266,10 @@ def test_array_type(self):\n         self.assertEquals(result1.first()['v2'], [1.0, 2.0])\n \n     def test_invalid_args(self):\n-        from pyspark.sql.functions import pandas_udf, PandasUDFType\n+        from pyspark.sql.functions import mean, pandas_udf, PandasUDFType"
  }],
  "prId": 22305
}, {
  "comments": [{
    "author": {
      "login": "ueshin"
    },
    "body": "We don't need min and count?",
    "commit": "03702d450a0a9da7bb50057b3bab35bdec7d4584",
    "createdAt": "2018-12-06T02:19:22Z",
    "diffHunk": "@@ -245,11 +278,101 @@ def test_invalid_args(self):\n                 foo_udf = pandas_udf(lambda x: x, 'v double', PandasUDFType.GROUPED_MAP)\n                 df.withColumn('v2', foo_udf(df['v']).over(w))\n \n-        with QuietTest(self.sc):\n-            with self.assertRaisesRegexp(\n-                    AnalysisException,\n-                    '.*Only unbounded window frame is supported.*'):\n-                df.withColumn('mean_v', mean_udf(df['v']).over(ow))\n+    def test_bounded_simple(self):\n+        from pyspark.sql.functions import mean, max, min, count\n+\n+        df = self.data\n+        w1 = self.sliding_row_window\n+        w2 = self.shrinking_range_window\n+\n+        plus_one = self.python_plus_one\n+        count_udf = self.pandas_agg_count_udf\n+        mean_udf = self.pandas_agg_mean_udf\n+        max_udf = self.pandas_agg_max_udf\n+        min_udf = self.pandas_agg_min_udf\n+\n+        result1 = df.withColumn('mean_v', mean_udf(plus_one(df['v'])).over(w1)) \\\n+            .withColumn('count_v', count_udf(df['v']).over(w2)) \\\n+            .withColumn('max_v',  max_udf(df['v']).over(w2)) \\\n+            .withColumn('min_v', min_udf(df['v']).over(w1))\n+\n+        expected1 = df.withColumn('mean_v', mean(plus_one(df['v'])).over(w1)) \\\n+            .withColumn('count_v', count(df['v']).over(w2)) \\\n+            .withColumn('max_v', max(df['v']).over(w2)) \\\n+            .withColumn('min_v', min(df['v']).over(w1))\n+\n+        self.assertPandasEqual(expected1.toPandas(), result1.toPandas())\n+\n+    def test_growing_window(self):\n+        from pyspark.sql.functions import mean\n+\n+        df = self.data\n+        w1 = self.growing_row_window\n+        w2 = self.growing_range_window\n+\n+        mean_udf = self.pandas_agg_mean_udf\n+\n+        result1 = df.withColumn('m1', mean_udf(df['v']).over(w1)) \\\n+            .withColumn('m2', mean_udf(df['v']).over(w2))\n+\n+        expected1 = df.withColumn('m1', mean(df['v']).over(w1)) \\\n+            .withColumn('m2', mean(df['v']).over(w2))\n+\n+        self.assertPandasEqual(expected1.toPandas(), result1.toPandas())\n+\n+    def test_sliding_window(self):\n+        from pyspark.sql.functions import mean\n+\n+        df = self.data\n+        w1 = self.sliding_row_window\n+        w2 = self.sliding_range_window\n+\n+        mean_udf = self.pandas_agg_mean_udf\n+\n+        result1 = df.withColumn('m1', mean_udf(df['v']).over(w1)) \\\n+            .withColumn('m2', mean_udf(df['v']).over(w2))\n+\n+        expected1 = df.withColumn('m1', mean(df['v']).over(w1)) \\\n+            .withColumn('m2', mean(df['v']).over(w2))\n+\n+        self.assertPandasEqual(expected1.toPandas(), result1.toPandas())\n+\n+    def test_shrinking_window(self):\n+        from pyspark.sql.functions import mean\n+\n+        df = self.data\n+        w1 = self.shrinking_row_window\n+        w2 = self.shrinking_range_window\n+\n+        mean_udf = self.pandas_agg_mean_udf\n+\n+        result1 = df.withColumn('m1', mean_udf(df['v']).over(w1)) \\\n+            .withColumn('m2', mean_udf(df['v']).over(w2))\n+\n+        expected1 = df.withColumn('m1', mean(df['v']).over(w1)) \\\n+            .withColumn('m2', mean(df['v']).over(w2))\n+\n+        self.assertPandasEqual(expected1.toPandas(), result1.toPandas())\n+\n+    def test_bounded_mixed(self):\n+        from pyspark.sql.functions import mean, max, min, count"
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "Yes - removed",
    "commit": "03702d450a0a9da7bb50057b3bab35bdec7d4584",
    "createdAt": "2018-12-06T19:39:48Z",
    "diffHunk": "@@ -245,11 +278,101 @@ def test_invalid_args(self):\n                 foo_udf = pandas_udf(lambda x: x, 'v double', PandasUDFType.GROUPED_MAP)\n                 df.withColumn('v2', foo_udf(df['v']).over(w))\n \n-        with QuietTest(self.sc):\n-            with self.assertRaisesRegexp(\n-                    AnalysisException,\n-                    '.*Only unbounded window frame is supported.*'):\n-                df.withColumn('mean_v', mean_udf(df['v']).over(ow))\n+    def test_bounded_simple(self):\n+        from pyspark.sql.functions import mean, max, min, count\n+\n+        df = self.data\n+        w1 = self.sliding_row_window\n+        w2 = self.shrinking_range_window\n+\n+        plus_one = self.python_plus_one\n+        count_udf = self.pandas_agg_count_udf\n+        mean_udf = self.pandas_agg_mean_udf\n+        max_udf = self.pandas_agg_max_udf\n+        min_udf = self.pandas_agg_min_udf\n+\n+        result1 = df.withColumn('mean_v', mean_udf(plus_one(df['v'])).over(w1)) \\\n+            .withColumn('count_v', count_udf(df['v']).over(w2)) \\\n+            .withColumn('max_v',  max_udf(df['v']).over(w2)) \\\n+            .withColumn('min_v', min_udf(df['v']).over(w1))\n+\n+        expected1 = df.withColumn('mean_v', mean(plus_one(df['v'])).over(w1)) \\\n+            .withColumn('count_v', count(df['v']).over(w2)) \\\n+            .withColumn('max_v', max(df['v']).over(w2)) \\\n+            .withColumn('min_v', min(df['v']).over(w1))\n+\n+        self.assertPandasEqual(expected1.toPandas(), result1.toPandas())\n+\n+    def test_growing_window(self):\n+        from pyspark.sql.functions import mean\n+\n+        df = self.data\n+        w1 = self.growing_row_window\n+        w2 = self.growing_range_window\n+\n+        mean_udf = self.pandas_agg_mean_udf\n+\n+        result1 = df.withColumn('m1', mean_udf(df['v']).over(w1)) \\\n+            .withColumn('m2', mean_udf(df['v']).over(w2))\n+\n+        expected1 = df.withColumn('m1', mean(df['v']).over(w1)) \\\n+            .withColumn('m2', mean(df['v']).over(w2))\n+\n+        self.assertPandasEqual(expected1.toPandas(), result1.toPandas())\n+\n+    def test_sliding_window(self):\n+        from pyspark.sql.functions import mean\n+\n+        df = self.data\n+        w1 = self.sliding_row_window\n+        w2 = self.sliding_range_window\n+\n+        mean_udf = self.pandas_agg_mean_udf\n+\n+        result1 = df.withColumn('m1', mean_udf(df['v']).over(w1)) \\\n+            .withColumn('m2', mean_udf(df['v']).over(w2))\n+\n+        expected1 = df.withColumn('m1', mean(df['v']).over(w1)) \\\n+            .withColumn('m2', mean(df['v']).over(w2))\n+\n+        self.assertPandasEqual(expected1.toPandas(), result1.toPandas())\n+\n+    def test_shrinking_window(self):\n+        from pyspark.sql.functions import mean\n+\n+        df = self.data\n+        w1 = self.shrinking_row_window\n+        w2 = self.shrinking_range_window\n+\n+        mean_udf = self.pandas_agg_mean_udf\n+\n+        result1 = df.withColumn('m1', mean_udf(df['v']).over(w1)) \\\n+            .withColumn('m2', mean_udf(df['v']).over(w2))\n+\n+        expected1 = df.withColumn('m1', mean(df['v']).over(w1)) \\\n+            .withColumn('m2', mean(df['v']).over(w2))\n+\n+        self.assertPandasEqual(expected1.toPandas(), result1.toPandas())\n+\n+    def test_bounded_mixed(self):\n+        from pyspark.sql.functions import mean, max, min, count"
  }],
  "prId": 22305
}, {
  "comments": [{
    "author": {
      "login": "BryanCutler"
    },
    "body": "this import and all the others should be moved to the top, it's repeated many times. It could done be a follow though",
    "commit": "03702d450a0a9da7bb50057b3bab35bdec7d4584",
    "createdAt": "2018-12-12T22:53:54Z",
    "diffHunk": "@@ -47,6 +47,15 @@ def pandas_scalar_time_two(self):\n         from pyspark.sql.functions import pandas_udf\n         return pandas_udf(lambda v: v * 2, 'double')\n \n+    @property\n+    def pandas_agg_count_udf(self):\n+        from pyspark.sql.functions import pandas_udf, PandasUDFType",
    "line": 6
  }, {
    "author": {
      "login": "icexelloss"
    },
    "body": "SGTM. Opened https://jira.apache.org/jira/browse/SPARK-26364",
    "commit": "03702d450a0a9da7bb50057b3bab35bdec7d4584",
    "createdAt": "2018-12-13T15:48:35Z",
    "diffHunk": "@@ -47,6 +47,15 @@ def pandas_scalar_time_two(self):\n         from pyspark.sql.functions import pandas_udf\n         return pandas_udf(lambda v: v * 2, 'double')\n \n+    @property\n+    def pandas_agg_count_udf(self):\n+        from pyspark.sql.functions import pandas_udf, PandasUDFType",
    "line": 6
  }],
  "prId": 22305
}]