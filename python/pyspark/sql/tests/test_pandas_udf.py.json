[{
  "comments": [{
    "author": {
      "login": "BryanCutler"
    },
    "body": "nit: after the unit test reorg, the `pandas_udf` import is at the top so it's not needed here or in the other test",
    "commit": "44402da2aa7e6968c0819647013f69c3e05f89d4",
    "createdAt": "2019-01-21T22:53:28Z",
    "diffHunk": "@@ -197,6 +197,66 @@ def foofoo(x, y):\n             ).collect\n         )\n \n+    def test_pandas_udf_detect_unsafe_type_conversion(self):\n+        from distutils.version import LooseVersion\n+        from pyspark.sql.functions import pandas_udf"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "Removed it. Thanks!",
    "commit": "44402da2aa7e6968c0819647013f69c3e05f89d4",
    "createdAt": "2019-01-22T00:40:26Z",
    "diffHunk": "@@ -197,6 +197,66 @@ def foofoo(x, y):\n             ).collect\n         )\n \n+    def test_pandas_udf_detect_unsafe_type_conversion(self):\n+        from distutils.version import LooseVersion\n+        from pyspark.sql.functions import pandas_udf"
  }],
  "prId": 22807
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "nit: Can we asset the result too?",
    "commit": "44402da2aa7e6968c0819647013f69c3e05f89d4",
    "createdAt": "2019-01-22T02:43:01Z",
    "diffHunk": "@@ -197,6 +197,64 @@ def foofoo(x, y):\n             ).collect\n         )\n \n+    def test_pandas_udf_detect_unsafe_type_conversion(self):\n+        from distutils.version import LooseVersion\n+        import pandas as pd\n+        import numpy as np\n+        import pyarrow as pa\n+\n+        values = [1.0] * 3\n+        pdf = pd.DataFrame({'A': values})\n+        df = self.spark.createDataFrame(pdf).repartition(1)\n+\n+        @pandas_udf(returnType=\"int\")\n+        def udf(column):\n+            return pd.Series(np.linspace(0, 1, 3))\n+\n+        # Since 0.11.0, PyArrow supports the feature to raise an error for unsafe cast.\n+        if LooseVersion(pa.__version__) >= LooseVersion(\"0.11.0\"):\n+            with self.sql_conf({\n+                    \"spark.sql.execution.pandas.arrowSafeTypeConversion\": True}):\n+                with self.assertRaisesRegexp(Exception,\n+                                             \"Exception thrown when converting pandas.Series\"):\n+                    df.select(['A']).withColumn('udf', udf('A')).collect()\n+\n+        # Disabling Arrow safe type check.\n+        with self.sql_conf({\n+                \"spark.sql.execution.pandas.arrowSafeTypeConversion\": False}):\n+            df.select(['A']).withColumn('udf', udf('A')).collect()",
    "line": 29
  }],
  "prId": 22807
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "sorry if I missed something. Why should we repartition?",
    "commit": "44402da2aa7e6968c0819647013f69c3e05f89d4",
    "createdAt": "2019-01-22T02:44:08Z",
    "diffHunk": "@@ -197,6 +197,64 @@ def foofoo(x, y):\n             ).collect\n         )\n \n+    def test_pandas_udf_detect_unsafe_type_conversion(self):\n+        from distutils.version import LooseVersion\n+        import pandas as pd\n+        import numpy as np\n+        import pyarrow as pa\n+\n+        values = [1.0] * 3\n+        pdf = pd.DataFrame({'A': values})\n+        df = self.spark.createDataFrame(pdf).repartition(1)",
    "line": 12
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "I thought I was writing this to make sure all values are in single partition so it matches the length of returned Pandas Series. This can be simplified. We can do this if we touch the code here next time.",
    "commit": "44402da2aa7e6968c0819647013f69c3e05f89d4",
    "createdAt": "2019-01-22T14:04:40Z",
    "diffHunk": "@@ -197,6 +197,64 @@ def foofoo(x, y):\n             ).collect\n         )\n \n+    def test_pandas_udf_detect_unsafe_type_conversion(self):\n+        from distutils.version import LooseVersion\n+        import pandas as pd\n+        import numpy as np\n+        import pyarrow as pa\n+\n+        values = [1.0] * 3\n+        pdf = pd.DataFrame({'A': values})\n+        df = self.spark.createDataFrame(pdf).repartition(1)",
    "line": 12
  }],
  "prId": 22807
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "nit .. : `spark.range(0)`.",
    "commit": "44402da2aa7e6968c0819647013f69c3e05f89d4",
    "createdAt": "2019-01-22T02:46:03Z",
    "diffHunk": "@@ -197,6 +197,64 @@ def foofoo(x, y):\n             ).collect\n         )\n \n+    def test_pandas_udf_detect_unsafe_type_conversion(self):\n+        from distutils.version import LooseVersion\n+        import pandas as pd\n+        import numpy as np\n+        import pyarrow as pa\n+\n+        values = [1.0] * 3\n+        pdf = pd.DataFrame({'A': values})\n+        df = self.spark.createDataFrame(pdf).repartition(1)\n+\n+        @pandas_udf(returnType=\"int\")\n+        def udf(column):\n+            return pd.Series(np.linspace(0, 1, 3))\n+\n+        # Since 0.11.0, PyArrow supports the feature to raise an error for unsafe cast.\n+        if LooseVersion(pa.__version__) >= LooseVersion(\"0.11.0\"):\n+            with self.sql_conf({\n+                    \"spark.sql.execution.pandas.arrowSafeTypeConversion\": True}):\n+                with self.assertRaisesRegexp(Exception,\n+                                             \"Exception thrown when converting pandas.Series\"):\n+                    df.select(['A']).withColumn('udf', udf('A')).collect()\n+\n+        # Disabling Arrow safe type check.\n+        with self.sql_conf({\n+                \"spark.sql.execution.pandas.arrowSafeTypeConversion\": False}):\n+            df.select(['A']).withColumn('udf', udf('A')).collect()\n+\n+    def test_pandas_udf_arrow_overflow(self):\n+        from distutils.version import LooseVersion\n+        import pandas as pd\n+        import pyarrow as pa\n+\n+        df = self.spark.range(0, 1)",
    "line": 36
  }],
  "prId": 22807
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "looks it can be inlined",
    "commit": "44402da2aa7e6968c0819647013f69c3e05f89d4",
    "createdAt": "2019-01-22T02:49:17Z",
    "diffHunk": "@@ -197,6 +197,64 @@ def foofoo(x, y):\n             ).collect\n         )\n \n+    def test_pandas_udf_detect_unsafe_type_conversion(self):\n+        from distutils.version import LooseVersion\n+        import pandas as pd\n+        import numpy as np\n+        import pyarrow as pa\n+\n+        values = [1.0] * 3\n+        pdf = pd.DataFrame({'A': values})\n+        df = self.spark.createDataFrame(pdf).repartition(1)\n+\n+        @pandas_udf(returnType=\"int\")\n+        def udf(column):\n+            return pd.Series(np.linspace(0, 1, 3))\n+\n+        # Since 0.11.0, PyArrow supports the feature to raise an error for unsafe cast.\n+        if LooseVersion(pa.__version__) >= LooseVersion(\"0.11.0\"):\n+            with self.sql_conf({\n+                    \"spark.sql.execution.pandas.arrowSafeTypeConversion\": True}):\n+                with self.assertRaisesRegexp(Exception,\n+                                             \"Exception thrown when converting pandas.Series\"):\n+                    df.select(['A']).withColumn('udf', udf('A')).collect()\n+\n+        # Disabling Arrow safe type check.\n+        with self.sql_conf({\n+                \"spark.sql.execution.pandas.arrowSafeTypeConversion\": False}):\n+            df.select(['A']).withColumn('udf', udf('A')).collect()\n+\n+    def test_pandas_udf_arrow_overflow(self):\n+        from distutils.version import LooseVersion\n+        import pandas as pd\n+        import pyarrow as pa\n+\n+        df = self.spark.range(0, 1)\n+\n+        @pandas_udf(returnType=\"byte\")\n+        def udf(column):\n+            return pd.Series([128])\n+\n+        # Arrow 0.11.0+ allows enabling or disabling safe type check.\n+        if LooseVersion(pa.__version__) >= LooseVersion(\"0.11.0\"):\n+            # When enabling safe type check, Arrow 0.11.0+ disallows overflow cast.\n+            with self.sql_conf({\n+                    \"spark.sql.execution.pandas.arrowSafeTypeConversion\": True}):\n+                with self.assertRaisesRegexp(Exception,\n+                                             \"Exception thrown when converting pandas.Series\"):\n+                    df.withColumn('udf', udf('id')).collect()\n+\n+            # Disabling safe type check, let Arrow do the cast anyway.\n+            with self.sql_conf({\"spark.sql.execution.pandas.arrowSafeTypeConversion\": False}):\n+                df.withColumn('udf', udf('id')).collect()\n+        else:\n+            # SQL config `arrowSafeTypeConversion` no matters for older Arrow.\n+            # Overflow cast causes an error.\n+            with self.sql_conf({\"spark.sql.execution.pandas.arrowSafeTypeConversion\": False}):\n+                with self.assertRaisesRegexp(Exception,\n+                                             \"Integer value out of bounds\"):",
    "line": 59
  }],
  "prId": 22807
}]