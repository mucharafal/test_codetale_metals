[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Can we avoid wlidcard import",
    "commit": "c8c8d7bdc81bc741d163c54e285828fba84371bf",
    "createdAt": "2019-05-15T02:56:42Z",
    "diffHunk": "@@ -19,6 +19,7 @@\n import sys\n \n from pyspark.sql import Row\n+from pyspark.sql.types import *"
  }],
  "prId": 24605
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "we can stick to `self.assertEqual` for a better message.",
    "commit": "c8c8d7bdc81bc741d163c54e285828fba84371bf",
    "createdAt": "2019-05-15T02:57:10Z",
    "diffHunk": "@@ -278,6 +279,22 @@ def test_sort_with_nulls_order(self):\n             df.select(df.name).orderBy(functions.desc_nulls_last('name')).collect(),\n             [Row(name=u'Tom'), Row(name=u'Alice'), Row(name=None)])\n \n+    def test_input_file_name_reset_for_rdd(self):\n+        from pyspark.sql.functions import udf, input_file_name\n+        rdd = self.sc.textFile('python/test_support/hello/hello.txt').map(lambda x: {'data': x})\n+        df = self.spark.createDataFrame(rdd, StructType([StructField('data', StringType(), True)]))\n+        df.select(input_file_name().alias('file')).collect()\n+\n+        non_file_df = self.spark.range(0, 100, 1, 100).select(input_file_name().alias('file'))\n+\n+        results = non_file_df.collect()\n+        self.assertTrue(len(results) == 100)\n+\n+        # [SC-12160]: if everything was properly reset after the last job, this should return\n+        # empty string rather than the file read in the last job.\n+        for result in results:\n+            assert(result[0] == '')"
  }],
  "prId": 24605
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "What's SC-12160?",
    "commit": "c8c8d7bdc81bc741d163c54e285828fba84371bf",
    "createdAt": "2019-05-15T02:57:18Z",
    "diffHunk": "@@ -278,6 +279,22 @@ def test_sort_with_nulls_order(self):\n             df.select(df.name).orderBy(functions.desc_nulls_last('name')).collect(),\n             [Row(name=u'Tom'), Row(name=u'Alice'), Row(name=None)])\n \n+    def test_input_file_name_reset_for_rdd(self):\n+        from pyspark.sql.functions import udf, input_file_name\n+        rdd = self.sc.textFile('python/test_support/hello/hello.txt').map(lambda x: {'data': x})\n+        df = self.spark.createDataFrame(rdd, StructType([StructField('data', StringType(), True)]))\n+        df.select(input_file_name().alias('file')).collect()\n+\n+        non_file_df = self.spark.range(0, 100, 1, 100).select(input_file_name().alias('file'))\n+\n+        results = non_file_df.collect()\n+        self.assertTrue(len(results) == 100)\n+\n+        # [SC-12160]: if everything was properly reset after the last job, this should return"
  }, {
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "+1 for @HyukjinKwon 's comment. Is this the internal issue tracker ID?\r\nCould you update the PR, @jose-torres ?",
    "commit": "c8c8d7bdc81bc741d163c54e285828fba84371bf",
    "createdAt": "2019-05-17T03:50:54Z",
    "diffHunk": "@@ -278,6 +279,22 @@ def test_sort_with_nulls_order(self):\n             df.select(df.name).orderBy(functions.desc_nulls_last('name')).collect(),\n             [Row(name=u'Tom'), Row(name=u'Alice'), Row(name=None)])\n \n+    def test_input_file_name_reset_for_rdd(self):\n+        from pyspark.sql.functions import udf, input_file_name\n+        rdd = self.sc.textFile('python/test_support/hello/hello.txt').map(lambda x: {'data': x})\n+        df = self.spark.createDataFrame(rdd, StructType([StructField('data', StringType(), True)]))\n+        df.select(input_file_name().alias('file')).collect()\n+\n+        non_file_df = self.spark.range(0, 100, 1, 100).select(input_file_name().alias('file'))\n+\n+        results = non_file_df.collect()\n+        self.assertTrue(len(results) == 100)\n+\n+        # [SC-12160]: if everything was properly reset after the last job, this should return"
  }],
  "prId": 24605
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I think we can just import on the top",
    "commit": "c8c8d7bdc81bc741d163c54e285828fba84371bf",
    "createdAt": "2019-05-15T02:58:06Z",
    "diffHunk": "@@ -278,6 +279,22 @@ def test_sort_with_nulls_order(self):\n             df.select(df.name).orderBy(functions.desc_nulls_last('name')).collect(),\n             [Row(name=u'Tom'), Row(name=u'Alice'), Row(name=None)])\n \n+    def test_input_file_name_reset_for_rdd(self):\n+        from pyspark.sql.functions import udf, input_file_name"
  }],
  "prId": 24605
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "actually, you don't have to import types:\r\n\r\n```\r\nspark.createDataFrame(rdd, \"data STRING\")\r\n```",
    "commit": "c8c8d7bdc81bc741d163c54e285828fba84371bf",
    "createdAt": "2019-05-15T02:59:47Z",
    "diffHunk": "@@ -278,6 +279,22 @@ def test_sort_with_nulls_order(self):\n             df.select(df.name).orderBy(functions.desc_nulls_last('name')).collect(),\n             [Row(name=u'Tom'), Row(name=u'Alice'), Row(name=None)])\n \n+    def test_input_file_name_reset_for_rdd(self):\n+        from pyspark.sql.functions import udf, input_file_name\n+        rdd = self.sc.textFile('python/test_support/hello/hello.txt').map(lambda x: {'data': x})\n+        df = self.spark.createDataFrame(rdd, StructType([StructField('data', StringType(), True)]))"
  }],
  "prId": 24605
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "seems like we don;t need an alias `file`.\r\n\r\nand why don't we just use `spark.range(100)`?",
    "commit": "c8c8d7bdc81bc741d163c54e285828fba84371bf",
    "createdAt": "2019-05-15T03:01:20Z",
    "diffHunk": "@@ -278,6 +279,22 @@ def test_sort_with_nulls_order(self):\n             df.select(df.name).orderBy(functions.desc_nulls_last('name')).collect(),\n             [Row(name=u'Tom'), Row(name=u'Alice'), Row(name=None)])\n \n+    def test_input_file_name_reset_for_rdd(self):\n+        from pyspark.sql.functions import udf, input_file_name\n+        rdd = self.sc.textFile('python/test_support/hello/hello.txt').map(lambda x: {'data': x})\n+        df = self.spark.createDataFrame(rdd, StructType([StructField('data', StringType(), True)]))\n+        df.select(input_file_name().alias('file')).collect()\n+\n+        non_file_df = self.spark.range(0, 100, 1, 100).select(input_file_name().alias('file'))"
  }],
  "prId": 24605
}]