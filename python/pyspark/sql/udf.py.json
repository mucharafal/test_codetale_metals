[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "This seems introduced from 1.3.1 - https://issues.apache.org/jira/browse/SPARK-6603",
    "commit": "e121273972d0ec0d94cc01e4426358b4e5fb7e2c",
    "createdAt": "2018-01-17T12:08:33Z",
    "diffHunk": "@@ -181,3 +183,180 @@ def asNondeterministic(self):\n         \"\"\"\n         self.deterministic = False\n         return self\n+\n+\n+class UDFRegistration(object):",
    "line": 22
  }],
  "prId": 20288
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "`registerJavaFunction` and `registerJavaUDAF` look introduced from 2.3.0 - https://issues.apache.org/jira/browse/SPARK-19439",
    "commit": "e121273972d0ec0d94cc01e4426358b4e5fb7e2c",
    "createdAt": "2018-01-17T12:09:45Z",
    "diffHunk": "@@ -181,3 +183,180 @@ def asNondeterministic(self):\n         \"\"\"\n         self.deterministic = False\n         return self\n+\n+\n+class UDFRegistration(object):\n+    \"\"\"\n+    Wrapper for user-defined function registration.\n+\n+    .. versionadded:: 1.3.1\n+    \"\"\"\n+\n+    def __init__(self, sparkSession):\n+        self.sparkSession = sparkSession\n+\n+    @ignore_unicode_prefix\n+    @since(1.3)\n+    def register(self, name, f, returnType=None):\n+        \"\"\"Registers a Python function (including lambda function) or a user-defined function\n+        in SQL statements.\n+\n+        :param name: name of the user-defined function in SQL statements.\n+        :param f: a Python function, or a user-defined function. The user-defined function can\n+            be either row-at-a-time or vectorized. See :meth:`pyspark.sql.functions.udf` and\n+            :meth:`pyspark.sql.functions.pandas_udf`.\n+        :param returnType: the return type of the registered user-defined function.\n+        :return: a user-defined function.\n+\n+        `returnType` can be optionally specified when `f` is a Python function but not\n+        when `f` is a user-defined function. See below:\n+\n+        1. When `f` is a Python function:\n+\n+            `returnType` defaults to string type and can be optionally specified. The produced\n+            object must match the specified type. In this case, this API works as if\n+            `register(name, f, returnType=StringType())`.\n+\n+            >>> strlen = spark.udf.register(\"stringLengthString\", lambda x: len(x))\n+            >>> spark.sql(\"SELECT stringLengthString('test')\").collect()\n+            [Row(stringLengthString(test)=u'4')]\n+\n+            >>> spark.sql(\"SELECT 'foo' AS text\").select(strlen(\"text\")).collect()\n+            [Row(stringLengthString(text)=u'3')]\n+\n+            >>> from pyspark.sql.types import IntegerType\n+            >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n+            >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n+            [Row(stringLengthInt(test)=4)]\n+\n+            >>> from pyspark.sql.types import IntegerType\n+            >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n+            >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n+            [Row(stringLengthInt(test)=4)]\n+\n+\n+        2. When `f` is a user-defined function:\n+\n+            Spark uses the return type of the given user-defined function as the return type of\n+            the registered user-defined function. `returnType` should not be specified.\n+            In this case, this API works as if `register(name, f)`.\n+\n+            >>> from pyspark.sql.types import IntegerType\n+            >>> from pyspark.sql.functions import udf\n+            >>> slen = udf(lambda s: len(s), IntegerType())\n+            >>> _ = spark.udf.register(\"slen\", slen)\n+            >>> spark.sql(\"SELECT slen('test')\").collect()\n+            [Row(slen(test)=4)]\n+\n+            >>> import random\n+            >>> from pyspark.sql.functions import udf\n+            >>> from pyspark.sql.types import IntegerType\n+            >>> random_udf = udf(lambda: random.randint(0, 100), IntegerType()).asNondeterministic()\n+            >>> new_random_udf = spark.udf.register(\"random_udf\", random_udf)\n+            >>> spark.sql(\"SELECT random_udf()\").collect()  # doctest: +SKIP\n+            [Row(random_udf()=82)]\n+\n+            >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n+            >>> @pandas_udf(\"integer\", PandasUDFType.SCALAR)  # doctest: +SKIP\n+            ... def add_one(x):\n+            ...     return x + 1\n+            ...\n+            >>> _ = spark.udf.register(\"add_one\", add_one)  # doctest: +SKIP\n+            >>> spark.sql(\"SELECT add_one(id) FROM range(3)\").collect()  # doctest: +SKIP\n+            [Row(add_one(id)=1), Row(add_one(id)=2), Row(add_one(id)=3)]\n+\n+            .. note:: Registration for a user-defined function (case 2.) was added from\n+                Spark 2.3.0.\n+        \"\"\"\n+\n+        # This is to check whether the input function is from a user-defined function or\n+        # Python function.\n+        if hasattr(f, 'asNondeterministic'):\n+            if returnType is not None:\n+                raise TypeError(\n+                    \"Invalid returnType: data type can not be specified when f is\"\n+                    \"a user-defined function, but got %s.\" % returnType)\n+            if f.evalType not in [PythonEvalType.SQL_BATCHED_UDF,\n+                                  PythonEvalType.SQL_PANDAS_SCALAR_UDF]:\n+                raise ValueError(\n+                    \"Invalid f: f must be either SQL_BATCHED_UDF or SQL_PANDAS_SCALAR_UDF\")\n+            register_udf = UserDefinedFunction(f.func, returnType=f.returnType, name=name,\n+                                               evalType=f.evalType,\n+                                               deterministic=f.deterministic)\n+            return_udf = f\n+        else:\n+            if returnType is None:\n+                returnType = StringType()\n+            register_udf = UserDefinedFunction(f, returnType=returnType, name=name,\n+                                               evalType=PythonEvalType.SQL_BATCHED_UDF)\n+            return_udf = register_udf._wrapped()\n+        self.sparkSession._jsparkSession.udf().registerPython(name, register_udf._judf)\n+        return return_udf\n+\n+    @ignore_unicode_prefix\n+    @since(2.3)\n+    def registerJavaFunction(self, name, javaClassName, returnType=None):",
    "line": 132
  }],
  "prId": 20288
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "<img width=\"715\" alt=\"2018-01-17 9 23 21\" src=\"https://user-images.githubusercontent.com/6477701/35042729-1acaa234-fbcd-11e7-9d3f-4e94dc200e2c.png\">\r\n",
    "commit": "e121273972d0ec0d94cc01e4426358b4e5fb7e2c",
    "createdAt": "2018-01-17T12:26:48Z",
    "diffHunk": "@@ -181,3 +183,179 @@ def asNondeterministic(self):\n         \"\"\"\n         self.deterministic = False\n         return self\n+\n+\n+class UDFRegistration(object):\n+    \"\"\"\n+    Wrapper for user-defined function registration.\n+\n+    .. versionadded:: 1.3.1\n+    \"\"\"\n+\n+    def __init__(self, sparkSession):\n+        self.sparkSession = sparkSession\n+\n+    @ignore_unicode_prefix\n+    @since(1.3)\n+    def register(self, name, f, returnType=None):\n+        \"\"\"Registers a Python function (including lambda function) or a user-defined function\n+        in SQL statements.\n+\n+        :param name: name of the user-defined function in SQL statements.\n+        :param f: a Python function, or a user-defined function. The user-defined function can\n+            be either row-at-a-time or vectorized. See :meth:`pyspark.sql.functions.udf` and\n+            :meth:`pyspark.sql.functions.pandas_udf`.\n+        :param returnType: the return type of the registered user-defined function.\n+        :return: a user-defined function.\n+\n+        `returnType` can be optionally specified when `f` is a Python function but not\n+        when `f` is a user-defined function. Please see below.\n+\n+        1. When `f` is a Python function:\n+\n+            `returnType` defaults to string type and can be optionally specified. The produced\n+            object must match the specified type. In this case, this API works as if\n+            `register(name, f, returnType=StringType())`.\n+\n+            >>> strlen = spark.udf.register(\"stringLengthString\", lambda x: len(x))\n+            >>> spark.sql(\"SELECT stringLengthString('test')\").collect()\n+            [Row(stringLengthString(test)=u'4')]\n+\n+            >>> spark.sql(\"SELECT 'foo' AS text\").select(strlen(\"text\")).collect()\n+            [Row(stringLengthString(text)=u'3')]\n+\n+            >>> from pyspark.sql.types import IntegerType\n+            >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n+            >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n+            [Row(stringLengthInt(test)=4)]\n+\n+            >>> from pyspark.sql.types import IntegerType\n+            >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n+            >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n+            [Row(stringLengthInt(test)=4)]\n+\n+        2. When `f` is a user-defined function:\n+\n+            Spark uses the return type of the given user-defined function as the return type of\n+            the registered user-defined function. `returnType` should not be specified.\n+            In this case, this API works as if `register(name, f)`.\n+\n+            >>> from pyspark.sql.types import IntegerType\n+            >>> from pyspark.sql.functions import udf\n+            >>> slen = udf(lambda s: len(s), IntegerType())\n+            >>> _ = spark.udf.register(\"slen\", slen)\n+            >>> spark.sql(\"SELECT slen('test')\").collect()\n+            [Row(slen(test)=4)]\n+\n+            >>> import random\n+            >>> from pyspark.sql.functions import udf\n+            >>> from pyspark.sql.types import IntegerType\n+            >>> random_udf = udf(lambda: random.randint(0, 100), IntegerType()).asNondeterministic()\n+            >>> new_random_udf = spark.udf.register(\"random_udf\", random_udf)\n+            >>> spark.sql(\"SELECT random_udf()\").collect()  # doctest: +SKIP\n+            [Row(random_udf()=82)]\n+\n+            >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n+            >>> @pandas_udf(\"integer\", PandasUDFType.SCALAR)  # doctest: +SKIP\n+            ... def add_one(x):\n+            ...     return x + 1\n+            ...\n+            >>> _ = spark.udf.register(\"add_one\", add_one)  # doctest: +SKIP\n+            >>> spark.sql(\"SELECT add_one(id) FROM range(3)\").collect()  # doctest: +SKIP\n+            [Row(add_one(id)=1), Row(add_one(id)=2), Row(add_one(id)=3)]\n+\n+            .. note:: Registration for a user-defined function (case 2.) was added from\n+                Spark 2.3.0.\n+        \"\"\"",
    "line": 104
  }],
  "prId": 20288
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "<img width=\"560\" alt=\"2018-01-17 9 23 28\" src=\"https://user-images.githubusercontent.com/6477701/35042749-26db2116-fbcd-11e7-840b-635d019c6ccf.png\">\r\n",
    "commit": "e121273972d0ec0d94cc01e4426358b4e5fb7e2c",
    "createdAt": "2018-01-17T12:27:13Z",
    "diffHunk": "@@ -181,3 +183,179 @@ def asNondeterministic(self):\n         \"\"\"\n         self.deterministic = False\n         return self\n+\n+\n+class UDFRegistration(object):\n+    \"\"\"\n+    Wrapper for user-defined function registration.\n+\n+    .. versionadded:: 1.3.1\n+    \"\"\"\n+\n+    def __init__(self, sparkSession):\n+        self.sparkSession = sparkSession\n+\n+    @ignore_unicode_prefix\n+    @since(1.3)\n+    def register(self, name, f, returnType=None):\n+        \"\"\"Registers a Python function (including lambda function) or a user-defined function\n+        in SQL statements.\n+\n+        :param name: name of the user-defined function in SQL statements.\n+        :param f: a Python function, or a user-defined function. The user-defined function can\n+            be either row-at-a-time or vectorized. See :meth:`pyspark.sql.functions.udf` and\n+            :meth:`pyspark.sql.functions.pandas_udf`.\n+        :param returnType: the return type of the registered user-defined function.\n+        :return: a user-defined function.\n+\n+        `returnType` can be optionally specified when `f` is a Python function but not\n+        when `f` is a user-defined function. Please see below.\n+\n+        1. When `f` is a Python function:\n+\n+            `returnType` defaults to string type and can be optionally specified. The produced\n+            object must match the specified type. In this case, this API works as if\n+            `register(name, f, returnType=StringType())`.\n+\n+            >>> strlen = spark.udf.register(\"stringLengthString\", lambda x: len(x))\n+            >>> spark.sql(\"SELECT stringLengthString('test')\").collect()\n+            [Row(stringLengthString(test)=u'4')]\n+\n+            >>> spark.sql(\"SELECT 'foo' AS text\").select(strlen(\"text\")).collect()\n+            [Row(stringLengthString(text)=u'3')]\n+\n+            >>> from pyspark.sql.types import IntegerType\n+            >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n+            >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n+            [Row(stringLengthInt(test)=4)]\n+\n+            >>> from pyspark.sql.types import IntegerType\n+            >>> _ = spark.udf.register(\"stringLengthInt\", lambda x: len(x), IntegerType())\n+            >>> spark.sql(\"SELECT stringLengthInt('test')\").collect()\n+            [Row(stringLengthInt(test)=4)]\n+\n+        2. When `f` is a user-defined function:\n+\n+            Spark uses the return type of the given user-defined function as the return type of\n+            the registered user-defined function. `returnType` should not be specified.\n+            In this case, this API works as if `register(name, f)`.\n+\n+            >>> from pyspark.sql.types import IntegerType\n+            >>> from pyspark.sql.functions import udf\n+            >>> slen = udf(lambda s: len(s), IntegerType())\n+            >>> _ = spark.udf.register(\"slen\", slen)\n+            >>> spark.sql(\"SELECT slen('test')\").collect()\n+            [Row(slen(test)=4)]\n+\n+            >>> import random\n+            >>> from pyspark.sql.functions import udf\n+            >>> from pyspark.sql.types import IntegerType\n+            >>> random_udf = udf(lambda: random.randint(0, 100), IntegerType()).asNondeterministic()\n+            >>> new_random_udf = spark.udf.register(\"random_udf\", random_udf)\n+            >>> spark.sql(\"SELECT random_udf()\").collect()  # doctest: +SKIP\n+            [Row(random_udf()=82)]\n+\n+            >>> from pyspark.sql.functions import pandas_udf, PandasUDFType\n+            >>> @pandas_udf(\"integer\", PandasUDFType.SCALAR)  # doctest: +SKIP\n+            ... def add_one(x):\n+            ...     return x + 1\n+            ...\n+            >>> _ = spark.udf.register(\"add_one\", add_one)  # doctest: +SKIP\n+            >>> spark.sql(\"SELECT add_one(id) FROM range(3)\").collect()  # doctest: +SKIP\n+            [Row(add_one(id)=1), Row(add_one(id)=2), Row(add_one(id)=3)]\n+\n+            .. note:: Registration for a user-defined function (case 2.) was added from\n+                Spark 2.3.0.\n+        \"\"\"\n+\n+        # This is to check whether the input function is from a user-defined function or\n+        # Python function.\n+        if hasattr(f, 'asNondeterministic'):\n+            if returnType is not None:\n+                raise TypeError(\n+                    \"Invalid returnType: data type can not be specified when f is\"\n+                    \"a user-defined function, but got %s.\" % returnType)\n+            if f.evalType not in [PythonEvalType.SQL_BATCHED_UDF,\n+                                  PythonEvalType.SQL_PANDAS_SCALAR_UDF]:\n+                raise ValueError(\n+                    \"Invalid f: f must be either SQL_BATCHED_UDF or SQL_PANDAS_SCALAR_UDF\")\n+            register_udf = UserDefinedFunction(f.func, returnType=f.returnType, name=name,\n+                                               evalType=f.evalType,\n+                                               deterministic=f.deterministic)\n+            return_udf = f\n+        else:\n+            if returnType is None:\n+                returnType = StringType()\n+            register_udf = UserDefinedFunction(f, returnType=returnType, name=name,\n+                                               evalType=PythonEvalType.SQL_BATCHED_UDF)\n+            return_udf = register_udf._wrapped()\n+        self.sparkSession._jsparkSession.udf().registerPython(name, register_udf._judf)\n+        return return_udf\n+\n+    @ignore_unicode_prefix\n+    @since(2.3)\n+    def registerJavaFunction(self, name, javaClassName, returnType=None):\n+        \"\"\"Register a Java user-defined function so it can be used in SQL statements.\n+\n+        In addition to a name and the function itself, the return type can be optionally specified.\n+        When the return type is not specified we would infer it via reflection.\n+\n+        :param name:  name of the user-defined function\n+        :param javaClassName: fully qualified name of java class\n+        :param returnType: a :class:`pyspark.sql.types.DataType` object\n+\n+        >>> from pyspark.sql.types import IntegerType\n+        >>> spark.udf.registerJavaFunction(\"javaStringLength\",\n+        ...   \"test.org.apache.spark.sql.JavaStringLength\", IntegerType())\n+        >>> spark.sql(\"SELECT javaStringLength('test')\").collect()\n+        [Row(UDF:javaStringLength(test)=4)]\n+        >>> spark.udf.registerJavaFunction(\"javaStringLength2\",\n+        ...   \"test.org.apache.spark.sql.JavaStringLength\")\n+        >>> spark.sql(\"SELECT javaStringLength2('test')\").collect()\n+        [Row(UDF:javaStringLength2(test)=4)]\n+        \"\"\""
  }],
  "prId": 20288
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Could you add another paragraph for explaining how to register a non-deterministic Python function? This sounds a common question from end users.",
    "commit": "e121273972d0ec0d94cc01e4426358b4e5fb7e2c",
    "createdAt": "2018-01-18T19:42:37Z",
    "diffHunk": "@@ -181,3 +183,179 @@ def asNondeterministic(self):\n         \"\"\"\n         self.deterministic = False\n         return self\n+\n+\n+class UDFRegistration(object):\n+    \"\"\"\n+    Wrapper for user-defined function registration. This instance can be accessed by\n+    :attr:`spark.udf` or :attr:`sqlContext.udf`.\n+\n+    .. versionadded:: 1.3.1\n+    \"\"\"\n+\n+    def __init__(self, sparkSession):\n+        self.sparkSession = sparkSession\n+\n+    @ignore_unicode_prefix\n+    @since(\"1.3.1\")\n+    def register(self, name, f, returnType=None):\n+        \"\"\"Registers a Python function (including lambda function) or a user-defined function\n+        in SQL statements.\n+\n+        :param name: name of the user-defined function in SQL statements.\n+        :param f: a Python function, or a user-defined function. The user-defined function can\n+            be either row-at-a-time or vectorized. See :meth:`pyspark.sql.functions.udf` and\n+            :meth:`pyspark.sql.functions.pandas_udf`.\n+        :param returnType: the return type of the registered user-defined function.\n+        :return: a user-defined function.\n+\n+        `returnType` can be optionally specified when `f` is a Python function but not\n+        when `f` is a user-defined function. Please see below.",
    "line": 47
  }],
  "prId": 20288
}]