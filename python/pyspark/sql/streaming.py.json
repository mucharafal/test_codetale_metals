[{
  "comments": [{
    "author": {
      "login": "zsxwing"
    },
    "body": "nit: remove `is`",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-01T18:43:49Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) be done open after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) is will be called if `open()` method exists and"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "done",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-07T20:58:36Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) be done open after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) is will be called if `open()` method exists and"
  }],
  "prId": 21477
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "@tdas, Seems not used.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-05T00:53:50Z",
    "diffHunk": "@@ -30,6 +30,7 @@\n from pyspark.sql.readwriter import OptionUtils, to_str\n from pyspark.sql.types import *\n from pyspark.sql.utils import StreamingQueryException\n+from abc import ABCMeta, abstractmethod"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "done.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-06T00:18:48Z",
    "diffHunk": "@@ -30,6 +30,7 @@\n from pyspark.sql.readwriter import OptionUtils, to_str\n from pyspark.sql.types import *\n from pyspark.sql.utils import StreamingQueryException\n+from abc import ABCMeta, abstractmethod"
  }],
  "prId": 21477
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I believe we should better turn this to comment form..\r\n\r\n```\r\n# The ..\r\n# Construct ...\r\n```",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-05T00:55:18Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) be done open after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) is will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> writer = sdf.writeStream.foreach(lambda x: print(x))\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            \"\"\"\n+            The provided object is a callable function that is supposed to be called on each row.\n+            Construct a function that takes an iterator and calls the provided function on each row.\n+            \"\"\""
  }],
  "prId": 21477
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "ditto",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-05T00:55:39Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) be done open after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) is will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> writer = sdf.writeStream.foreach(lambda x: print(x))\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            \"\"\"\n+            The provided object is a callable function that is supposed to be called on each row.\n+            Construct a function that takes an iterator and calls the provided function on each row.\n+            \"\"\"\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            \"\"\"\n+            The provided object is not a callable function. Then it is expected to have a\n+            'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            'close(error)' methods.\n+            \"\"\""
  }],
  "prId": 21477
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "maybe \r\n\r\n```python\r\nclose_exists  = hasattr(f, \"close\")\r\nif close_exists:\r\n    ...\r\n```",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-05T00:57:06Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) be done open after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) is will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> writer = sdf.writeStream.foreach(lambda x: print(x))\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            \"\"\"\n+            The provided object is a callable function that is supposed to be called on each row.\n+            Construct a function that takes an iterator and calls the provided function on each row.\n+            \"\"\"\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            \"\"\"\n+            The provided object is not a callable function. Then it is expected to have a\n+            'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            'close(error)' methods.\n+            \"\"\"\n+\n+            if not hasattr(f, 'process'):\n+                raise Exception(\n+                    \"Provided object is neither callable nor does it have a 'process' method\")\n+\n+            if not callable(getattr(f, 'process')):\n+                raise Exception(\"Attribute 'process' in provided object is not callable\")\n+\n+            open_exists = False\n+            if hasattr(f, 'open'):\n+                if not callable(getattr(f, 'open')):\n+                    raise Exception(\"Attribute 'open' in provided object is not callable\")\n+                else:\n+                    open_exists = True\n+\n+            close_exists = False"
  }],
  "prId": 21477
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "ditto for \r\n\r\n```python\r\nif open_exists\r\n```",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-05T00:58:52Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) be done open after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) is will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> writer = sdf.writeStream.foreach(lambda x: print(x))\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            \"\"\"\n+            The provided object is a callable function that is supposed to be called on each row.\n+            Construct a function that takes an iterator and calls the provided function on each row.\n+            \"\"\"\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            \"\"\"\n+            The provided object is not a callable function. Then it is expected to have a\n+            'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            'close(error)' methods.\n+            \"\"\"\n+\n+            if not hasattr(f, 'process'):\n+                raise Exception(\n+                    \"Provided object is neither callable nor does it have a 'process' method\")\n+\n+            if not callable(getattr(f, 'process')):\n+                raise Exception(\"Attribute 'process' in provided object is not callable\")\n+\n+            open_exists = False"
  }],
  "prId": 21477
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Should it be put in finally?",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-05T01:00:53Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) be done open after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) is will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> writer = sdf.writeStream.foreach(lambda x: print(x))\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            \"\"\"\n+            The provided object is a callable function that is supposed to be called on each row.\n+            Construct a function that takes an iterator and calls the provided function on each row.\n+            \"\"\"\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            \"\"\"\n+            The provided object is not a callable function. Then it is expected to have a\n+            'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            'close(error)' methods.\n+            \"\"\"\n+\n+            if not hasattr(f, 'process'):\n+                raise Exception(\n+                    \"Provided object is neither callable nor does it have a 'process' method\")\n+\n+            if not callable(getattr(f, 'process')):\n+                raise Exception(\"Attribute 'process' in provided object is not callable\")\n+\n+            open_exists = False\n+            if hasattr(f, 'open'):\n+                if not callable(getattr(f, 'open')):\n+                    raise Exception(\"Attribute 'open' in provided object is not callable\")\n+                else:\n+                    open_exists = True\n+\n+            close_exists = False\n+            if hasattr(f, \"close\"):\n+                if not callable(getattr(f, 'close')):\n+                    raise Exception(\"Attribute 'close' in provided object is not callable\")\n+                else:\n+                    close_exists = True\n+\n+            def func_with_open_process_close(partition_id, iterator):\n+                epoch_id = TaskContext.get().getLocalProperty('streaming.sql.batchId')\n+                if epoch_id:\n+                    epoch_id = int(epoch_id)\n+                else:\n+                    raise Exception(\"Could not get batch id from TaskContext\")\n+\n+                should_process = True\n+                if open_exists:\n+                    should_process = f.open(partition_id, epoch_id)\n+\n+                def call_close_if_needed(error):\n+                    if open_exists and close_exists:\n+                        f.close(error)\n+                try:\n+                    if should_process:\n+                        for x in iterator:\n+                            f.process(x)\n+                except Exception as ex:\n+                    call_close_if_needed(ex)"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "`putting this to finally` sounds like better pattern for me too, and then we may be able to inline `call_close_if_needed`.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-06T04:04:41Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) be done open after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) is will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> writer = sdf.writeStream.foreach(lambda x: print(x))\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            \"\"\"\n+            The provided object is a callable function that is supposed to be called on each row.\n+            Construct a function that takes an iterator and calls the provided function on each row.\n+            \"\"\"\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            \"\"\"\n+            The provided object is not a callable function. Then it is expected to have a\n+            'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            'close(error)' methods.\n+            \"\"\"\n+\n+            if not hasattr(f, 'process'):\n+                raise Exception(\n+                    \"Provided object is neither callable nor does it have a 'process' method\")\n+\n+            if not callable(getattr(f, 'process')):\n+                raise Exception(\"Attribute 'process' in provided object is not callable\")\n+\n+            open_exists = False\n+            if hasattr(f, 'open'):\n+                if not callable(getattr(f, 'open')):\n+                    raise Exception(\"Attribute 'open' in provided object is not callable\")\n+                else:\n+                    open_exists = True\n+\n+            close_exists = False\n+            if hasattr(f, \"close\"):\n+                if not callable(getattr(f, 'close')):\n+                    raise Exception(\"Attribute 'close' in provided object is not callable\")\n+                else:\n+                    close_exists = True\n+\n+            def func_with_open_process_close(partition_id, iterator):\n+                epoch_id = TaskContext.get().getLocalProperty('streaming.sql.batchId')\n+                if epoch_id:\n+                    epoch_id = int(epoch_id)\n+                else:\n+                    raise Exception(\"Could not get batch id from TaskContext\")\n+\n+                should_process = True\n+                if open_exists:\n+                    should_process = f.open(partition_id, epoch_id)\n+\n+                def call_close_if_needed(error):\n+                    if open_exists and close_exists:\n+                        f.close(error)\n+                try:\n+                    if should_process:\n+                        for x in iterator:\n+                            f.process(x)\n+                except Exception as ex:\n+                    call_close_if_needed(ex)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "That is tricky because I have to pass the exception and rethrow the error if any. alternative code is\r\n```\r\nerr = None\r\n...\r\nexcept Exception as ex:\r\n  err = ex\r\nfinally:\r\n  call_close(ex)\r\n  if ex is not None: \r\n     raise ex\r\n```",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-07T05:59:11Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) be done open after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) is will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> writer = sdf.writeStream.foreach(lambda x: print(x))\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            \"\"\"\n+            The provided object is a callable function that is supposed to be called on each row.\n+            Construct a function that takes an iterator and calls the provided function on each row.\n+            \"\"\"\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            \"\"\"\n+            The provided object is not a callable function. Then it is expected to have a\n+            'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            'close(error)' methods.\n+            \"\"\"\n+\n+            if not hasattr(f, 'process'):\n+                raise Exception(\n+                    \"Provided object is neither callable nor does it have a 'process' method\")\n+\n+            if not callable(getattr(f, 'process')):\n+                raise Exception(\"Attribute 'process' in provided object is not callable\")\n+\n+            open_exists = False\n+            if hasattr(f, 'open'):\n+                if not callable(getattr(f, 'open')):\n+                    raise Exception(\"Attribute 'open' in provided object is not callable\")\n+                else:\n+                    open_exists = True\n+\n+            close_exists = False\n+            if hasattr(f, \"close\"):\n+                if not callable(getattr(f, 'close')):\n+                    raise Exception(\"Attribute 'close' in provided object is not callable\")\n+                else:\n+                    close_exists = True\n+\n+            def func_with_open_process_close(partition_id, iterator):\n+                epoch_id = TaskContext.get().getLocalProperty('streaming.sql.batchId')\n+                if epoch_id:\n+                    epoch_id = int(epoch_id)\n+                else:\n+                    raise Exception(\"Could not get batch id from TaskContext\")\n+\n+                should_process = True\n+                if open_exists:\n+                    should_process = f.open(partition_id, epoch_id)\n+\n+                def call_close_if_needed(error):\n+                    if open_exists and close_exists:\n+                        f.close(error)\n+                try:\n+                    if should_process:\n+                        for x in iterator:\n+                            f.process(x)\n+                except Exception as ex:\n+                    call_close_if_needed(ex)"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Ah, sure. I rushed to read. Seems fine for now.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-07T09:14:28Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) be done open after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) is will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> writer = sdf.writeStream.foreach(lambda x: print(x))\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            \"\"\"\n+            The provided object is a callable function that is supposed to be called on each row.\n+            Construct a function that takes an iterator and calls the provided function on each row.\n+            \"\"\"\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            \"\"\"\n+            The provided object is not a callable function. Then it is expected to have a\n+            'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            'close(error)' methods.\n+            \"\"\"\n+\n+            if not hasattr(f, 'process'):\n+                raise Exception(\n+                    \"Provided object is neither callable nor does it have a 'process' method\")\n+\n+            if not callable(getattr(f, 'process')):\n+                raise Exception(\"Attribute 'process' in provided object is not callable\")\n+\n+            open_exists = False\n+            if hasattr(f, 'open'):\n+                if not callable(getattr(f, 'open')):\n+                    raise Exception(\"Attribute 'open' in provided object is not callable\")\n+                else:\n+                    open_exists = True\n+\n+            close_exists = False\n+            if hasattr(f, \"close\"):\n+                if not callable(getattr(f, 'close')):\n+                    raise Exception(\"Attribute 'close' in provided object is not callable\")\n+                else:\n+                    close_exists = True\n+\n+            def func_with_open_process_close(partition_id, iterator):\n+                epoch_id = TaskContext.get().getLocalProperty('streaming.sql.batchId')\n+                if epoch_id:\n+                    epoch_id = int(epoch_id)\n+                else:\n+                    raise Exception(\"Could not get batch id from TaskContext\")\n+\n+                should_process = True\n+                if open_exists:\n+                    should_process = f.open(partition_id, epoch_id)\n+\n+                def call_close_if_needed(error):\n+                    if open_exists and close_exists:\n+                        f.close(error)\n+                try:\n+                    if should_process:\n+                        for x in iterator:\n+                            f.process(x)\n+                except Exception as ex:\n+                    call_close_if_needed(ex)"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Either is fine for me.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-07T13:12:54Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) be done open after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) is will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> writer = sdf.writeStream.foreach(lambda x: print(x))\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            \"\"\"\n+            The provided object is a callable function that is supposed to be called on each row.\n+            Construct a function that takes an iterator and calls the provided function on each row.\n+            \"\"\"\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            \"\"\"\n+            The provided object is not a callable function. Then it is expected to have a\n+            'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            'close(error)' methods.\n+            \"\"\"\n+\n+            if not hasattr(f, 'process'):\n+                raise Exception(\n+                    \"Provided object is neither callable nor does it have a 'process' method\")\n+\n+            if not callable(getattr(f, 'process')):\n+                raise Exception(\"Attribute 'process' in provided object is not callable\")\n+\n+            open_exists = False\n+            if hasattr(f, 'open'):\n+                if not callable(getattr(f, 'open')):\n+                    raise Exception(\"Attribute 'open' in provided object is not callable\")\n+                else:\n+                    open_exists = True\n+\n+            close_exists = False\n+            if hasattr(f, \"close\"):\n+                if not callable(getattr(f, 'close')):\n+                    raise Exception(\"Attribute 'close' in provided object is not callable\")\n+                else:\n+                    close_exists = True\n+\n+            def func_with_open_process_close(partition_id, iterator):\n+                epoch_id = TaskContext.get().getLocalProperty('streaming.sql.batchId')\n+                if epoch_id:\n+                    epoch_id = int(epoch_id)\n+                else:\n+                    raise Exception(\"Could not get batch id from TaskContext\")\n+\n+                should_process = True\n+                if open_exists:\n+                    should_process = f.open(partition_id, epoch_id)\n+\n+                def call_close_if_needed(error):\n+                    if open_exists and close_exists:\n+                        f.close(error)\n+                try:\n+                    if should_process:\n+                        for x in iterator:\n+                            f.process(x)\n+                except Exception as ex:\n+                    call_close_if_needed(ex)"
  }],
  "prId": 21477
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "nit: deserialized` `copy (space)",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-06T03:44:56Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "done",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-07T21:20:45Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly"
  }],
  "prId": 21477
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "> any initialization for writing data (e.g. opening a connection or starting a transaction) be done open after the `open(...)` method has been called\r\n\r\n`be done open` seems a bit odd. If we can polish the sentence it would be better.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-06T03:51:17Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "done",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-07T21:20:43Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a"
  }],
  "prId": 21477
}, {
  "comments": [{
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "We might be able to extract the code block to the function, since the logic for checking open and close are exactly same.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-06T04:00:42Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) be done open after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) is will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> writer = sdf.writeStream.foreach(lambda x: print(x))\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            \"\"\"\n+            The provided object is a callable function that is supposed to be called on each row.\n+            Construct a function that takes an iterator and calls the provided function on each row.\n+            \"\"\"\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            \"\"\"\n+            The provided object is not a callable function. Then it is expected to have a\n+            'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            'close(error)' methods.\n+            \"\"\"\n+\n+            if not hasattr(f, 'process'):\n+                raise Exception(\n+                    \"Provided object is neither callable nor does it have a 'process' method\")\n+\n+            if not callable(getattr(f, 'process')):\n+                raise Exception(\"Attribute 'process' in provided object is not callable\")\n+\n+            open_exists = False"
  }],
  "prId": 21477
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "@tdas, wouldn't we better just have `ForeachWriter` class?",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-06T05:25:48Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.",
    "line": 17
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I discussed this with @marmbrus . If there is a ForeachWriter class in python, then uses will have to additionally import it. That's just another overhead that can be avoided by just allowing any class with the appropriate methods. One less step for python users.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-07T05:45:02Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.",
    "line": 17
  }],
  "prId": 21477
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Seems this variant is specific to Python. I thought we should better match how we support with Scala side.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-06T05:28:38Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.",
    "line": 11
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "This is superset of what we support in scala. Python users are more likely to use simple lambdas instead of defining classes. But they may also want to write transactional stuff in python with open and close methods. Hence providing both alternatives seems to be a good idea.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-07T05:48:14Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.",
    "line": 11
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "(including the response to https://github.com/apache/spark/pull/21477#discussion_r193631209) I kind of agree that it's a-okay idea but I think we usually provide a consistent API support so far unless it's language specific, for example, ContextManager, decorator in Python and etc.\r\n\r\nJust for clarification, does Scala side support function only support too?\r\n\r\nAlso, I know attribute-checking way is kind of more like \"Pythonic\" way but I am seeing the documentation is already diverted between Scala vs Python. It costs maintaining overhead on the other hand.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-07T05:58:36Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.",
    "line": 11
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I mean, we could maybe consider the other ways but wouldn't it better to have the consistent support as the primary, and then see if the other ways are really requested by users? I think we could still incrementally add attribute-checking way or the lambda (or function to be more correct) way later (but we can't in the opposite way).",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-07T06:03:41Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.",
    "line": 11
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Python APIs anyways have slightly divergences from Scala/Java APIs in order to provide better experiences for Python users. For example, `StreamingQuery.lastProgress` returns an object of type `StreamingQueryProgress` in Java/Scala but returns a dict in python. Because python users are more used to dealing with dicts, and java/scala users (typed language) are more comfortable with structures). Similarly, in DataFrame.select, you can refer to columns in scala using `$\"columnName\"` but in python, you can refer to it as `df.columnName`. If we strictly adhere to pure consistency, then we cannot make it convenient for users in different languages. And ultimately convenience is what matters for the user experience. So its okay to have a superset of supported types in python compared to java/scala. \r\n\r\nPersonally, I think we should also add the lambda variant to Scala as well. But that decision for Scala is independent of this PR as there is enough justification for add the lambda variant for ",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-07T20:54:37Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.",
    "line": 11
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I believe `$\"columnName\"` is more like a language specific feature in Scala and I think `df.columnName` is language specific to Python.\r\n\r\n>  And ultimately convenience is what matters for the user experience.\r\n\r\nThing is, it sounded to me like we are kind of prejudging it.. We can't revert it back easily once we go in this way ..\r\n\r\n>  I think we should also add the lambda variant to Scala as well.\r\n\r\n+1\r\n\r\nI am okay but I hope this shouldn't be usually done next time ..",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-08T01:58:59Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.",
    "line": 11
  }],
  "prId": 21477
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Also, for perfectness, we should check argument specification too ... although it's a bit too much. I felt like I had to mention it at least.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-07T06:10:44Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) be done open after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) is will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> writer = sdf.writeStream.foreach(lambda x: print(x))\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            \"\"\"\n+            The provided object is a callable function that is supposed to be called on each row.\n+            Construct a function that takes an iterator and calls the provided function on each row.\n+            \"\"\"\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            \"\"\"\n+            The provided object is not a callable function. Then it is expected to have a\n+            'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            'close(error)' methods.\n+            \"\"\"\n+\n+            if not hasattr(f, 'process'):\n+                raise Exception(\n+                    \"Provided object is neither callable nor does it have a 'process' method\")\n+\n+            if not callable(getattr(f, 'process')):\n+                raise Exception(\"Attribute 'process' in provided object is not callable\")\n+\n+            open_exists = False\n+            if hasattr(f, 'open'):\n+                if not callable(getattr(f, 'open')):\n+                    raise Exception(\"Attribute 'open' in provided object is not callable\")\n+                else:\n+                    open_exists = True\n+\n+            close_exists = False\n+            if hasattr(f, \"close\"):"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "It's all dynamically typed, right? I don't think there is a good way to check where the function accepts the right argument types without actually calling the function (please correct me if I am wrong). And this is a standard problem in python that is everybody using python is used to (i.e. runtime exceptions when using incorrectly typed parameters). Also, no other operations that take lambdas check the types and counts.  So I think we should just be consistent and let that be. ",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-07T20:25:48Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) be done open after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) is will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> writer = sdf.writeStream.foreach(lambda x: print(x))\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            \"\"\"\n+            The provided object is a callable function that is supposed to be called on each row.\n+            Construct a function that takes an iterator and calls the provided function on each row.\n+            \"\"\"\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            \"\"\"\n+            The provided object is not a callable function. Then it is expected to have a\n+            'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            'close(error)' methods.\n+            \"\"\"\n+\n+            if not hasattr(f, 'process'):\n+                raise Exception(\n+                    \"Provided object is neither callable nor does it have a 'process' method\")\n+\n+            if not callable(getattr(f, 'process')):\n+                raise Exception(\"Attribute 'process' in provided object is not callable\")\n+\n+            open_exists = False\n+            if hasattr(f, 'open'):\n+                if not callable(getattr(f, 'open')):\n+                    raise Exception(\"Attribute 'open' in provided object is not callable\")\n+                else:\n+                    open_exists = True\n+\n+            close_exists = False\n+            if hasattr(f, \"close\"):"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "We can check argspec at least I believe. but, yup, I am fine with it as is.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-08T01:42:54Z",
    "diffHunk": "@@ -843,6 +844,169 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserializedcopy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) be done open after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) is will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> writer = sdf.writeStream.foreach(lambda x: print(x))\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            \"\"\"\n+            The provided object is a callable function that is supposed to be called on each row.\n+            Construct a function that takes an iterator and calls the provided function on each row.\n+            \"\"\"\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            \"\"\"\n+            The provided object is not a callable function. Then it is expected to have a\n+            'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            'close(error)' methods.\n+            \"\"\"\n+\n+            if not hasattr(f, 'process'):\n+                raise Exception(\n+                    \"Provided object is neither callable nor does it have a 'process' method\")\n+\n+            if not callable(getattr(f, 'process')):\n+                raise Exception(\"Attribute 'process' in provided object is not callable\")\n+\n+            open_exists = False\n+            if hasattr(f, 'open'):\n+                if not callable(getattr(f, 'open')):\n+                    raise Exception(\"Attribute 'open' in provided object is not callable\")\n+                else:\n+                    open_exists = True\n+\n+            close_exists = False\n+            if hasattr(f, \"close\"):"
  }],
  "prId": 21477
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I usually write try-except-finally without a newline tho .. ",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-15T02:55:27Z",
    "diffHunk": "@@ -843,6 +843,170 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    @since(2.4)\n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserialized copy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) is done after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> def print_row(row):\n+        ...     print(row)\n+        ...\n+        >>> writer = sdf.writeStream.foreach(print_row)\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            # The provided object is a callable function that is supposed to be called on each row.\n+            # Construct a function that takes an iterator and calls the provided function on each\n+            # row.\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            # The provided object is not a callable function. Then it is expected to have a\n+            # 'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            # 'close(error)' methods.\n+\n+            if not hasattr(f, 'process'):\n+                raise Exception(\"Provided object does not have a 'process' method\")\n+\n+            if not callable(getattr(f, 'process')):\n+                raise Exception(\"Attribute 'process' in provided object is not callable\")\n+\n+            def doesMethodExist(method_name):\n+                exists = hasattr(f, method_name)\n+                if exists and not callable(getattr(f, method_name)):\n+                    raise Exception(\n+                        \"Attribute '%s' in provided object is not callable\" % method_name)\n+                return exists\n+\n+            open_exists = doesMethodExist('open')\n+            close_exists = doesMethodExist('close')\n+\n+            def func_with_open_process_close(partition_id, iterator):\n+                epoch_id = TaskContext.get().getLocalProperty('streaming.sql.batchId')\n+                if epoch_id:\n+                    epoch_id = int(epoch_id)\n+                else:\n+                    raise Exception(\"Could not get batch id from TaskContext\")\n+\n+                # Check if the data should be processed\n+                should_process = True\n+                if open_exists:\n+                    should_process = f.open(partition_id, epoch_id)\n+\n+                error = None\n+\n+                try:\n+                    if should_process:\n+                        for x in iterator:\n+                            f.process(x)\n+"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "This looks like strictly related to preference, and I prefer newline. Are you referring to the Spark style guide or PEP 8 or so?",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-15T03:29:28Z",
    "diffHunk": "@@ -843,6 +843,170 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    @since(2.4)\n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserialized copy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) is done after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> def print_row(row):\n+        ...     print(row)\n+        ...\n+        >>> writer = sdf.writeStream.foreach(print_row)\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            # The provided object is a callable function that is supposed to be called on each row.\n+            # Construct a function that takes an iterator and calls the provided function on each\n+            # row.\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            # The provided object is not a callable function. Then it is expected to have a\n+            # 'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            # 'close(error)' methods.\n+\n+            if not hasattr(f, 'process'):\n+                raise Exception(\"Provided object does not have a 'process' method\")\n+\n+            if not callable(getattr(f, 'process')):\n+                raise Exception(\"Attribute 'process' in provided object is not callable\")\n+\n+            def doesMethodExist(method_name):\n+                exists = hasattr(f, method_name)\n+                if exists and not callable(getattr(f, method_name)):\n+                    raise Exception(\n+                        \"Attribute '%s' in provided object is not callable\" % method_name)\n+                return exists\n+\n+            open_exists = doesMethodExist('open')\n+            close_exists = doesMethodExist('close')\n+\n+            def func_with_open_process_close(partition_id, iterator):\n+                epoch_id = TaskContext.get().getLocalProperty('streaming.sql.batchId')\n+                if epoch_id:\n+                    epoch_id = int(epoch_id)\n+                else:\n+                    raise Exception(\"Could not get batch id from TaskContext\")\n+\n+                # Check if the data should be processed\n+                should_process = True\n+                if open_exists:\n+                    should_process = f.open(partition_id, epoch_id)\n+\n+                error = None\n+\n+                try:\n+                    if should_process:\n+                        for x in iterator:\n+                            f.process(x)\n+"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I see no newline between them is more consistent in the current codebase. It should be better to keep consistent.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-15T04:38:36Z",
    "diffHunk": "@@ -843,6 +843,170 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    @since(2.4)\n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserialized copy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) is done after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> def print_row(row):\n+        ...     print(row)\n+        ...\n+        >>> writer = sdf.writeStream.foreach(print_row)\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            # The provided object is a callable function that is supposed to be called on each row.\n+            # Construct a function that takes an iterator and calls the provided function on each\n+            # row.\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            # The provided object is not a callable function. Then it is expected to have a\n+            # 'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            # 'close(error)' methods.\n+\n+            if not hasattr(f, 'process'):\n+                raise Exception(\"Provided object does not have a 'process' method\")\n+\n+            if not callable(getattr(f, 'process')):\n+                raise Exception(\"Attribute 'process' in provided object is not callable\")\n+\n+            def doesMethodExist(method_name):\n+                exists = hasattr(f, method_name)\n+                if exists and not callable(getattr(f, method_name)):\n+                    raise Exception(\n+                        \"Attribute '%s' in provided object is not callable\" % method_name)\n+                return exists\n+\n+            open_exists = doesMethodExist('open')\n+            close_exists = doesMethodExist('close')\n+\n+            def func_with_open_process_close(partition_id, iterator):\n+                epoch_id = TaskContext.get().getLocalProperty('streaming.sql.batchId')\n+                if epoch_id:\n+                    epoch_id = int(epoch_id)\n+                else:\n+                    raise Exception(\"Could not get batch id from TaskContext\")\n+\n+                # Check if the data should be processed\n+                should_process = True\n+                if open_exists:\n+                    should_process = f.open(partition_id, epoch_id)\n+\n+                error = None\n+\n+                try:\n+                    if should_process:\n+                        for x in iterator:\n+                            f.process(x)\n+"
  }, {
    "author": {
      "login": "HeartSaVioR"
    },
    "body": "Oh OK. I agree it might be better to try best to keep consistent though style guide doesn't define it explicitly.",
    "commit": "1ab612f3d965cc6cad2b24c309bb7f11f931ea1e",
    "createdAt": "2018-06-15T04:45:20Z",
    "diffHunk": "@@ -843,6 +843,170 @@ def trigger(self, processingTime=None, once=None, continuous=None):\n         self._jwrite = self._jwrite.trigger(jTrigger)\n         return self\n \n+    @since(2.4)\n+    def foreach(self, f):\n+        \"\"\"\n+        Sets the output of the streaming query to be processed using the provided writer ``f``.\n+        This is often used to write the output of a streaming query to arbitrary storage systems.\n+        The processing logic can be specified in two ways.\n+\n+        #. A **function** that takes a row as input.\n+            This is a simple way to express your processing logic. Note that this does\n+            not allow you to deduplicate generated data when failures cause reprocessing of\n+            some input data. That would require you to specify the processing logic in the next\n+            way.\n+\n+        #. An **object** with a ``process`` method and optional ``open`` and ``close`` methods.\n+            The object can have the following methods.\n+\n+            * ``open(partition_id, epoch_id)``: *Optional* method that initializes the processing\n+                (for example, open a connection, start a transaction, etc). Additionally, you can\n+                use the `partition_id` and `epoch_id` to deduplicate regenerated data\n+                (discussed later).\n+\n+            * ``process(row)``: *Non-optional* method that processes each :class:`Row`.\n+\n+            * ``close(error)``: *Optional* method that finalizes and cleans up (for example,\n+                close connection, commit transaction, etc.) after all rows have been processed.\n+\n+            The object will be used by Spark in the following way.\n+\n+            * A single copy of this object is responsible of all the data generated by a\n+                single task in a query. In other words, one instance is responsible for\n+                processing one partition of the data generated in a distributed manner.\n+\n+            * This object must be serializable because each task will get a fresh\n+                serialized-deserialized copy of the provided object. Hence, it is strongly\n+                recommended that any initialization for writing data (e.g. opening a\n+                connection or starting a transaction) is done after the `open(...)`\n+                method has been called, which signifies that the task is ready to generate data.\n+\n+            * The lifecycle of the methods are as follows.\n+\n+                For each partition with ``partition_id``:\n+\n+                ... For each batch/epoch of streaming data with ``epoch_id``:\n+\n+                ....... Method ``open(partitionId, epochId)`` is called.\n+\n+                ....... If ``open(...)`` returns true, for each row in the partition and\n+                        batch/epoch, method ``process(row)`` is called.\n+\n+                ....... Method ``close(errorOrNull)`` is called with error (if any) seen while\n+                        processing rows.\n+\n+            Important points to note:\n+\n+            * The `partitionId` and `epochId` can be used to deduplicate generated data when\n+                failures cause reprocessing of some input data. This depends on the execution\n+                mode of the query. If the streaming query is being executed in the micro-batch\n+                mode, then every partition represented by a unique tuple (partition_id, epoch_id)\n+                is guaranteed to have the same data. Hence, (partition_id, epoch_id) can be used\n+                to deduplicate and/or transactionally commit data and achieve exactly-once\n+                guarantees. However, if the streaming query is being executed in the continuous\n+                mode, then this guarantee does not hold and therefore should not be used for\n+                deduplication.\n+\n+            * The ``close()`` method (if exists) will be called if `open()` method exists and\n+                returns successfully (irrespective of the return value), except if the Python\n+                crashes in the middle.\n+\n+        .. note:: Evolving.\n+\n+        >>> # Print every row using a function\n+        >>> def print_row(row):\n+        ...     print(row)\n+        ...\n+        >>> writer = sdf.writeStream.foreach(print_row)\n+        >>> # Print every row using a object with process() method\n+        >>> class RowPrinter:\n+        ...     def open(self, partition_id, epoch_id):\n+        ...         print(\"Opened %d, %d\" % (partition_id, epoch_id))\n+        ...         return True\n+        ...     def process(self, row):\n+        ...         print(row)\n+        ...     def close(self, error):\n+        ...         print(\"Closed with error: %s\" % str(error))\n+        ...\n+        >>> writer = sdf.writeStream.foreach(RowPrinter())\n+        \"\"\"\n+\n+        from pyspark.rdd import _wrap_function\n+        from pyspark.serializers import PickleSerializer, AutoBatchedSerializer\n+        from pyspark.taskcontext import TaskContext\n+\n+        if callable(f):\n+            # The provided object is a callable function that is supposed to be called on each row.\n+            # Construct a function that takes an iterator and calls the provided function on each\n+            # row.\n+            def func_without_process(_, iterator):\n+                for x in iterator:\n+                    f(x)\n+                return iter([])\n+\n+            func = func_without_process\n+\n+        else:\n+            # The provided object is not a callable function. Then it is expected to have a\n+            # 'process(row)' method, and optional 'open(partition_id, epoch_id)' and\n+            # 'close(error)' methods.\n+\n+            if not hasattr(f, 'process'):\n+                raise Exception(\"Provided object does not have a 'process' method\")\n+\n+            if not callable(getattr(f, 'process')):\n+                raise Exception(\"Attribute 'process' in provided object is not callable\")\n+\n+            def doesMethodExist(method_name):\n+                exists = hasattr(f, method_name)\n+                if exists and not callable(getattr(f, method_name)):\n+                    raise Exception(\n+                        \"Attribute '%s' in provided object is not callable\" % method_name)\n+                return exists\n+\n+            open_exists = doesMethodExist('open')\n+            close_exists = doesMethodExist('close')\n+\n+            def func_with_open_process_close(partition_id, iterator):\n+                epoch_id = TaskContext.get().getLocalProperty('streaming.sql.batchId')\n+                if epoch_id:\n+                    epoch_id = int(epoch_id)\n+                else:\n+                    raise Exception(\"Could not get batch id from TaskContext\")\n+\n+                # Check if the data should be processed\n+                should_process = True\n+                if open_exists:\n+                    should_process = f.open(partition_id, epoch_id)\n+\n+                error = None\n+\n+                try:\n+                    if should_process:\n+                        for x in iterator:\n+                            f.process(x)\n+"
  }],
  "prId": 21477
}]