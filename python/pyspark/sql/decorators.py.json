[{
  "comments": [{
    "author": {
      "login": "rdblue"
    },
    "body": "I don't think this should conflict with pyspark.sql.functions.udf. If this were defined, then it would break existing code that uses functions.udf. It may be possible to have a decorator definition that maintains that API and acts as a decorator, but otherwise this should be renamed to something like `register_udf`.",
    "commit": "8f8d39e11e18eeff6096b196353d9a0427fd92e1",
    "createdAt": "2017-01-11T00:39:50Z",
    "diffHunk": "@@ -0,0 +1,25 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark.sql.types import StringType\n+from pyspark.sql.functions import udf as _udf\n+\n+\n+def udf(returnType=StringType()):"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "Can you also add docstrings for this?",
    "commit": "8f8d39e11e18eeff6096b196353d9a0427fd92e1",
    "createdAt": "2017-01-11T00:42:10Z",
    "diffHunk": "@@ -0,0 +1,25 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark.sql.types import StringType\n+from pyspark.sql.functions import udf as _udf\n+\n+\n+def udf(returnType=StringType()):"
  }, {
    "author": {
      "login": "zero323"
    },
    "body": "Regarding name clashes I would like to keep it as a separated module and let user decide if prefer to import from `sql.functions` or `sql.decorators`  though technically speaking, with \"parenthless\" version, we could just merge it with `pyspark.sql.functions.udf`. The problem is it becomes rather ugly to document because the argument can be either `DataType` or `Callable`. \r\n\r\nThis is why I wait with adding documentation but I pushed some doctests.",
    "commit": "8f8d39e11e18eeff6096b196353d9a0427fd92e1",
    "createdAt": "2017-01-11T13:12:42Z",
    "diffHunk": "@@ -0,0 +1,25 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark.sql.types import StringType\n+from pyspark.sql.functions import udf as _udf\n+\n+\n+def udf(returnType=StringType()):"
  }, {
    "author": {
      "login": "zero323"
    },
    "body": "Of course it could be just moved to `sql.functions` and renamed explicitly as `udf_decorator`. Then user can use an alias to get shorter call:\r\n\r\n    from sql.functions import udf_decorator as udf",
    "commit": "8f8d39e11e18eeff6096b196353d9a0427fd92e1",
    "createdAt": "2017-01-11T14:10:11Z",
    "diffHunk": "@@ -0,0 +1,25 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark.sql.types import StringType\n+from pyspark.sql.functions import udf as _udf\n+\n+\n+def udf(returnType=StringType()):"
  }, {
    "author": {
      "login": "rdblue"
    },
    "body": "I think that the two should be merged. If they have the same name, then what works in one place breaks in others and the two are mutually exclusive. Our users share code quite a bit and it is common to copy & paste UDFs. That should be fine, or should result in \"NameError: name 'udf' is not defined\", rather than having different behavior.\r\n\r\nWith the changes to the decorator to support using it without parens, it's already close to being compatible with the current `functions.udf`.",
    "commit": "8f8d39e11e18eeff6096b196353d9a0427fd92e1",
    "createdAt": "2017-01-11T19:41:35Z",
    "diffHunk": "@@ -0,0 +1,25 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark.sql.types import StringType\n+from pyspark.sql.functions import udf as _udf\n+\n+\n+def udf(returnType=StringType()):"
  }, {
    "author": {
      "login": "zero323"
    },
    "body": "OK, I moved decorator to `sql.functions` as `udf_decorator`. _Explicit is better than implicit_, right? Nevertheless I am reluctant to further attempts to merge it with the main `udf` function.\r\n\r\nTechnically It is possible, but it will require all kinds of special cases. It would be manageable for now, when we only support `returnType`, but if we ever decide to add further arguments (like `name`) it would be a mess. Especially when project is committed to supporting legacy Python versions. Otherwise we could just use `keyword` only arguments:\r\n\r\n    def udf(func=None, *, returnType=StringType()):\r\n        ... \r\n\r\nWe could mimic that to some extent, without using backwards compatibility, by using keyword arguments:\r\n\r\n    def udf(func=None, **kwargs):\r\n        returnType = kwargs.get(\"returnType\", StringType())\r\n        ...\r\n\r\nbut I don't think it is worth loosing explicit argument list.\r\n",
    "commit": "8f8d39e11e18eeff6096b196353d9a0427fd92e1",
    "createdAt": "2017-01-11T20:41:22Z",
    "diffHunk": "@@ -0,0 +1,25 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark.sql.types import StringType\n+from pyspark.sql.functions import udf as _udf\n+\n+\n+def udf(returnType=StringType()):"
  }],
  "prId": 16533
}]