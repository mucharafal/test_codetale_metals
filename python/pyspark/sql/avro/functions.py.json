[{
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "You mean `col`?",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-02-20T15:08:13Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@since(3.0)\n+def from_avro(col, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Avro is built-in but external data source module since Spark 2.4. Please deploy the application\n+    as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.",
    "line": 40
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Yeah, that's a discrepancy though I would rename the parameter to make it compliant with the old API.",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-02-21T12:02:54Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@since(3.0)\n+def from_avro(col, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Avro is built-in but external data source module since Spark 2.4. Please deploy the application\n+    as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.",
    "line": 40
  }],
  "prId": 23797
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "ditto",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-02-20T15:10:03Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@since(3.0)\n+def from_avro(col, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Avro is built-in but external data source module since Spark 2.4. Please deploy the application\n+    as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.\n+    :param jsonFormatSchema: the avro schema in JSON string format.\n+    :param options: options to control how the Avro record is parsed.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import from_avro, to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))\n+    >>> avroDf.collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    >>> jsonFormatSchema = '''{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\n+    ...     [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\n+    ...     \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\n+    ...     {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}'''\n+    >>> avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()\n+    [Row(value=Row(avro=Row(age=2, name=u'Alice')))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.from_avro(\n+            _to_java_column(col), jsonFormatSchema, options)\n+    except TypeError as e:\n+        if str(e) == \"'JavaPackage' object is not callable\":\n+            _print_missing_jar(\"Avro\", \"avro\", \"avro\", sc.version)\n+        raise\n+    return Column(jc)\n+\n+\n+@since(3.0)\n+def to_avro(col):\n+    \"\"\"\n+    Converts a column into binary of avro format.\n+\n+    Avro is built-in but external data source module since Spark 2.4. Please deploy the application\n+    as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the data column.",
    "line": 79
  }],
  "prId": 23797
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "you can wrap it with `@ignore_unicode_prefix` to trim `u` prefix in doctests for Python 3.",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-02-22T06:19:29Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):",
    "line": 31
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Yeah, that caused the test failure. Fixed.",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-02-22T11:16:48Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):",
    "line": 31
  }],
  "prId": 23797
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "@gaborgsomogyi, I think you can check by, for instance, `Utils.classForName(\"any exposed API of avro class\")`, catch Py4j exception, and shows a proper error message.",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-02-22T06:26:28Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Avro is built-in but external data source module since Spark 2.4. Please deploy the application\n+    as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.\n+    :param jsonFormatSchema: the avro schema in JSON string format.\n+    :param options: options to control how the Avro record is parsed.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import from_avro, to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))\n+    >>> avroDf.collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    >>> jsonFormatSchema = '''{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\n+    ...     [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\n+    ...     \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\n+    ...     {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}'''\n+    >>> avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()\n+    [Row(value=Row(avro=Row(age=2, name=u'Alice')))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.from_avro(\n+            _to_java_column(data), jsonFormatSchema, options)\n+    except TypeError as e:",
    "line": 63
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "You mean write a scala wrapper to call `from_avro` function? Does it mean you think the existing way in kinesis is wrong somehow?",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-02-22T11:15:42Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Avro is built-in but external data source module since Spark 2.4. Please deploy the application\n+    as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.\n+    :param jsonFormatSchema: the avro schema in JSON string format.\n+    :param options: options to control how the Avro record is parsed.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import from_avro, to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))\n+    >>> avroDf.collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    >>> jsonFormatSchema = '''{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\n+    ...     [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\n+    ...     \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\n+    ...     {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}'''\n+    >>> avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()\n+    [Row(value=Row(avro=Row(age=2, name=u'Alice')))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.from_avro(\n+            _to_java_column(data), jsonFormatSchema, options)\n+    except TypeError as e:",
    "line": 63
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Exiting way is okay - it works. But I was thinking catching `TypeError` with checking string `'JavaPackage' object is not callable` is a bit flaky. So, maybe we could check .. \r\n\r\n```python\r\nfrom py4j.protocol import Py4JJavaError\r\n...\r\nis_avro_found = spark._jvm.org.apache.spark.util.Utils.classIsLoadable(\r\n    \"org.apache.spark.sql.avro.functions\")\r\nif not is_avro_found:\r\n    raise RuntimeError(formatted_missing_jar_msg(\"Avro\", \"avro\", \"avro\", sc.version))\r\n```",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-02-22T11:51:32Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Avro is built-in but external data source module since Spark 2.4. Please deploy the application\n+    as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.\n+    :param jsonFormatSchema: the avro schema in JSON string format.\n+    :param options: options to control how the Avro record is parsed.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import from_avro, to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))\n+    >>> avroDf.collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    >>> jsonFormatSchema = '''{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\n+    ...     [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\n+    ...     \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\n+    ...     {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}'''\n+    >>> avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()\n+    [Row(value=Row(avro=Row(age=2, name=u'Alice')))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.from_avro(\n+            _to_java_column(data), jsonFormatSchema, options)\n+    except TypeError as e:",
    "line": 63
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "You can leave Kinesis's codes as are since this PR doesn't target to fix Kinesis's.  `_print_missing_jar` can be renamed to `_formatted_missing_jar_msg` that returns a string.",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-02-22T11:52:38Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Avro is built-in but external data source module since Spark 2.4. Please deploy the application\n+    as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.\n+    :param jsonFormatSchema: the avro schema in JSON string format.\n+    :param options: options to control how the Avro record is parsed.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import from_avro, to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))\n+    >>> avroDf.collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    >>> jsonFormatSchema = '''{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\n+    ...     [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\n+    ...     \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\n+    ...     {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}'''\n+    >>> avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()\n+    [Row(value=Row(avro=Row(age=2, name=u'Alice')))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.from_avro(\n+            _to_java_column(data), jsonFormatSchema, options)\n+    except TypeError as e:",
    "line": 63
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Hm, but okay. Let's leave it as is and fix it later together since I found other places that use similar approaches.",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-02-22T11:54:29Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Avro is built-in but external data source module since Spark 2.4. Please deploy the application\n+    as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.\n+    :param jsonFormatSchema: the avro schema in JSON string format.\n+    :param options: options to control how the Avro record is parsed.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import from_avro, to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))\n+    >>> avroDf.collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    >>> jsonFormatSchema = '''{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\n+    ...     [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\n+    ...     \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\n+    ...     {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}'''\n+    >>> avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()\n+    [Row(value=Row(avro=Row(age=2, name=u'Alice')))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.from_avro(\n+            _to_java_column(data), jsonFormatSchema, options)\n+    except TypeError as e:",
    "line": 63
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I was looking for this suggestion but seems like we can solve this later. Happy to catch this up in a later jira... Thanks.",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-02-27T10:03:24Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Avro is built-in but external data source module since Spark 2.4. Please deploy the application\n+    as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.\n+    :param jsonFormatSchema: the avro schema in JSON string format.\n+    :param options: options to control how the Avro record is parsed.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import from_avro, to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))\n+    >>> avroDf.collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    >>> jsonFormatSchema = '''{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\n+    ...     [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\n+    ...     \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\n+    ...     {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}'''\n+    >>> avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()\n+    [Row(value=Row(avro=Row(age=2, name=u'Alice')))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.from_avro(\n+            _to_java_column(data), jsonFormatSchema, options)\n+    except TypeError as e:",
    "line": 63
  }],
  "prId": 23797
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "BTW, `python/docs/pyspark.sql.rst` would have to be fixed so that it can be shown in PySpark API doc.",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-02-22T06:29:58Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+#",
    "line": 1
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Yeah, that's missing, addressing it...",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-02-22T11:16:21Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+#",
    "line": 1
  }],
  "prId": 23797
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "not a big deal but let's add `.. note:`",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-02-22T06:31:25Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Avro is built-in but external data source module since Spark 2.4. Please deploy the application\n+    as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.\n+    :param jsonFormatSchema: the avro schema in JSON string format.\n+    :param options: options to control how the Avro record is parsed.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import from_avro, to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))\n+    >>> avroDf.collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    >>> jsonFormatSchema = '''{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\n+    ...     [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\n+    ...     \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\n+    ...     {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}'''\n+    >>> avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()\n+    [Row(value=Row(avro=Row(age=2, name=u'Alice')))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.from_avro(\n+            _to_java_column(data), jsonFormatSchema, options)\n+    except TypeError as e:\n+        if str(e) == \"'JavaPackage' object is not callable\":\n+            _print_missing_jar(\"Avro\", \"avro\", \"avro\", sc.version)\n+        raise\n+    return Column(jc)\n+\n+\n+@since(3.0)\n+def to_avro(data):\n+    \"\"\"\n+    Converts a column into binary of avro format.\n+\n+    Avro is built-in but external data source module since Spark 2.4. Please deploy the application"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Added.",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-02-22T11:29:20Z",
    "diffHunk": "@@ -0,0 +1,134 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Avro is built-in but external data source module since Spark 2.4. Please deploy the application\n+    as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.\n+    :param jsonFormatSchema: the avro schema in JSON string format.\n+    :param options: options to control how the Avro record is parsed.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import from_avro, to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))\n+    >>> avroDf.collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    >>> jsonFormatSchema = '''{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\n+    ...     [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\n+    ...     \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\n+    ...     {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}'''\n+    >>> avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()\n+    [Row(value=Row(avro=Row(age=2, name=u'Alice')))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.from_avro(\n+            _to_java_column(data), jsonFormatSchema, options)\n+    except TypeError as e:\n+        if str(e) == \"'JavaPackage' object is not callable\":\n+            _print_missing_jar(\"Avro\", \"avro\", \"avro\", sc.version)\n+        raise\n+    return Column(jc)\n+\n+\n+@since(3.0)\n+def to_avro(data):\n+    \"\"\"\n+    Converts a column into binary of avro format.\n+\n+    Avro is built-in but external data source module since Spark 2.4. Please deploy the application"
  }],
  "prId": 23797
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I think we don't need to define `df` for doctests as globals.",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-03-08T03:47:57Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.rdd import ignore_unicode_prefix\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@ignore_unicode_prefix\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Note: Avro is built-in but external data source module since Spark 2.4. Please deploy the\n+    application as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.\n+    :param jsonFormatSchema: the avro schema in JSON string format.\n+    :param options: options to control how the Avro record is parsed.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import from_avro, to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))\n+    >>> avroDf.collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    >>> jsonFormatSchema = '''{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\n+    ...     [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\n+    ...     \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\n+    ...     {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}'''\n+    >>> avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()\n+    [Row(value=Row(avro=Row(age=2, name=u'Alice')))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.from_avro(\n+            _to_java_column(data), jsonFormatSchema, options)\n+    except TypeError as e:\n+        if str(e) == \"'JavaPackage' object is not callable\":\n+            _print_missing_jar(\"Avro\", \"avro\", \"avro\", sc.version)\n+        raise\n+    return Column(jc)\n+\n+\n+@ignore_unicode_prefix\n+@since(3.0)\n+def to_avro(data):\n+    \"\"\"\n+    Converts a column into binary of avro format.\n+\n+    Note: Avro is built-in but external data source module since Spark 2.4. Please deploy the\n+    application as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the data column.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> df.select(to_avro(df.value).alias(\"avro\")).collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.to_avro(_to_java_column(data))\n+    except TypeError as e:\n+        if str(e) == \"'JavaPackage' object is not callable\":\n+            _print_missing_jar(\"Avro\", \"avro\", \"avro\", sc.version)\n+        raise\n+    return Column(jc)\n+\n+\n+def _test():\n+    import os\n+    import sys\n+    from pyspark.testing.utils import search_jar\n+    avro_jar = search_jar(\"external/avro\", \"spark-avro\")\n+    if avro_jar is None:\n+        print(\n+            \"Skipping all Avro Python tests as the optional Avro project was \"\n+            \"not compiled into a JAR. To run these tests, \"\n+            \"you need to build Spark with 'build/sbt -Pavro package' or \"\n+            \"'build/mvn -Pavro package' before running this test.\")\n+        sys.exit(0)\n+    else:\n+        existing_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"pyspark-shell\")\n+        jars_args = \"--jars %s\" % avro_jar\n+        os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \" \".join([jars_args, existing_args])\n+\n+    import doctest\n+    from pyspark.sql import Row, SparkSession\n+    import pyspark.sql.avro.functions\n+    globs = pyspark.sql.avro.functions.__dict__.copy()\n+    spark = SparkSession.builder\\\n+        .master(\"local[4]\")\\\n+        .appName(\"sql.avro.functions tests\")\\\n+        .getOrCreate()\n+    sc = spark.sparkContext\n+    globs['sc'] = sc\n+    globs['spark'] = spark\n+    globs['df'] = spark.createDataFrame([Row(name='Alice', age=2), Row(name='Bob', age=5)])"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Removed.",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-03-08T09:02:46Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.rdd import ignore_unicode_prefix\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@ignore_unicode_prefix\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Note: Avro is built-in but external data source module since Spark 2.4. Please deploy the\n+    application as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.\n+    :param jsonFormatSchema: the avro schema in JSON string format.\n+    :param options: options to control how the Avro record is parsed.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import from_avro, to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))\n+    >>> avroDf.collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    >>> jsonFormatSchema = '''{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\n+    ...     [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\n+    ...     \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\n+    ...     {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}'''\n+    >>> avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()\n+    [Row(value=Row(avro=Row(age=2, name=u'Alice')))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.from_avro(\n+            _to_java_column(data), jsonFormatSchema, options)\n+    except TypeError as e:\n+        if str(e) == \"'JavaPackage' object is not callable\":\n+            _print_missing_jar(\"Avro\", \"avro\", \"avro\", sc.version)\n+        raise\n+    return Column(jc)\n+\n+\n+@ignore_unicode_prefix\n+@since(3.0)\n+def to_avro(data):\n+    \"\"\"\n+    Converts a column into binary of avro format.\n+\n+    Note: Avro is built-in but external data source module since Spark 2.4. Please deploy the\n+    application as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the data column.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> df.select(to_avro(df.value).alias(\"avro\")).collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.to_avro(_to_java_column(data))\n+    except TypeError as e:\n+        if str(e) == \"'JavaPackage' object is not callable\":\n+            _print_missing_jar(\"Avro\", \"avro\", \"avro\", sc.version)\n+        raise\n+    return Column(jc)\n+\n+\n+def _test():\n+    import os\n+    import sys\n+    from pyspark.testing.utils import search_jar\n+    avro_jar = search_jar(\"external/avro\", \"spark-avro\")\n+    if avro_jar is None:\n+        print(\n+            \"Skipping all Avro Python tests as the optional Avro project was \"\n+            \"not compiled into a JAR. To run these tests, \"\n+            \"you need to build Spark with 'build/sbt -Pavro package' or \"\n+            \"'build/mvn -Pavro package' before running this test.\")\n+        sys.exit(0)\n+    else:\n+        existing_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"pyspark-shell\")\n+        jars_args = \"--jars %s\" % avro_jar\n+        os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \" \".join([jars_args, existing_args])\n+\n+    import doctest\n+    from pyspark.sql import Row, SparkSession\n+    import pyspark.sql.avro.functions\n+    globs = pyspark.sql.avro.functions.__dict__.copy()\n+    spark = SparkSession.builder\\\n+        .master(\"local[4]\")\\\n+        .appName(\"sql.avro.functions tests\")\\\n+        .getOrCreate()\n+    sc = spark.sparkContext\n+    globs['sc'] = sc\n+    globs['spark'] = spark\n+    globs['df'] = spark.createDataFrame([Row(name='Alice', age=2), Row(name='Bob', age=5)])"
  }],
  "prId": 23797
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "this too. I don't think we need it.",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-03-08T03:48:06Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.rdd import ignore_unicode_prefix\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@ignore_unicode_prefix\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Note: Avro is built-in but external data source module since Spark 2.4. Please deploy the\n+    application as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.\n+    :param jsonFormatSchema: the avro schema in JSON string format.\n+    :param options: options to control how the Avro record is parsed.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import from_avro, to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))\n+    >>> avroDf.collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    >>> jsonFormatSchema = '''{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\n+    ...     [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\n+    ...     \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\n+    ...     {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}'''\n+    >>> avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()\n+    [Row(value=Row(avro=Row(age=2, name=u'Alice')))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.from_avro(\n+            _to_java_column(data), jsonFormatSchema, options)\n+    except TypeError as e:\n+        if str(e) == \"'JavaPackage' object is not callable\":\n+            _print_missing_jar(\"Avro\", \"avro\", \"avro\", sc.version)\n+        raise\n+    return Column(jc)\n+\n+\n+@ignore_unicode_prefix\n+@since(3.0)\n+def to_avro(data):\n+    \"\"\"\n+    Converts a column into binary of avro format.\n+\n+    Note: Avro is built-in but external data source module since Spark 2.4. Please deploy the\n+    application as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the data column.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> df.select(to_avro(df.value).alias(\"avro\")).collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.to_avro(_to_java_column(data))\n+    except TypeError as e:\n+        if str(e) == \"'JavaPackage' object is not callable\":\n+            _print_missing_jar(\"Avro\", \"avro\", \"avro\", sc.version)\n+        raise\n+    return Column(jc)\n+\n+\n+def _test():\n+    import os\n+    import sys\n+    from pyspark.testing.utils import search_jar\n+    avro_jar = search_jar(\"external/avro\", \"spark-avro\")\n+    if avro_jar is None:\n+        print(\n+            \"Skipping all Avro Python tests as the optional Avro project was \"\n+            \"not compiled into a JAR. To run these tests, \"\n+            \"you need to build Spark with 'build/sbt -Pavro package' or \"\n+            \"'build/mvn -Pavro package' before running this test.\")\n+        sys.exit(0)\n+    else:\n+        existing_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"pyspark-shell\")\n+        jars_args = \"--jars %s\" % avro_jar\n+        os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \" \".join([jars_args, existing_args])\n+\n+    import doctest\n+    from pyspark.sql import Row, SparkSession\n+    import pyspark.sql.avro.functions\n+    globs = pyspark.sql.avro.functions.__dict__.copy()\n+    spark = SparkSession.builder\\\n+        .master(\"local[4]\")\\\n+        .appName(\"sql.avro.functions tests\")\\\n+        .getOrCreate()\n+    sc = spark.sparkContext\n+    globs['sc'] = sc"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Removed.",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-03-08T09:02:54Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.rdd import ignore_unicode_prefix\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@ignore_unicode_prefix\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Note: Avro is built-in but external data source module since Spark 2.4. Please deploy the\n+    application as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.\n+    :param jsonFormatSchema: the avro schema in JSON string format.\n+    :param options: options to control how the Avro record is parsed.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import from_avro, to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))\n+    >>> avroDf.collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    >>> jsonFormatSchema = '''{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\n+    ...     [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\n+    ...     \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\n+    ...     {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}'''\n+    >>> avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()\n+    [Row(value=Row(avro=Row(age=2, name=u'Alice')))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.from_avro(\n+            _to_java_column(data), jsonFormatSchema, options)\n+    except TypeError as e:\n+        if str(e) == \"'JavaPackage' object is not callable\":\n+            _print_missing_jar(\"Avro\", \"avro\", \"avro\", sc.version)\n+        raise\n+    return Column(jc)\n+\n+\n+@ignore_unicode_prefix\n+@since(3.0)\n+def to_avro(data):\n+    \"\"\"\n+    Converts a column into binary of avro format.\n+\n+    Note: Avro is built-in but external data source module since Spark 2.4. Please deploy the\n+    application as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the data column.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> df.select(to_avro(df.value).alias(\"avro\")).collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.to_avro(_to_java_column(data))\n+    except TypeError as e:\n+        if str(e) == \"'JavaPackage' object is not callable\":\n+            _print_missing_jar(\"Avro\", \"avro\", \"avro\", sc.version)\n+        raise\n+    return Column(jc)\n+\n+\n+def _test():\n+    import os\n+    import sys\n+    from pyspark.testing.utils import search_jar\n+    avro_jar = search_jar(\"external/avro\", \"spark-avro\")\n+    if avro_jar is None:\n+        print(\n+            \"Skipping all Avro Python tests as the optional Avro project was \"\n+            \"not compiled into a JAR. To run these tests, \"\n+            \"you need to build Spark with 'build/sbt -Pavro package' or \"\n+            \"'build/mvn -Pavro package' before running this test.\")\n+        sys.exit(0)\n+    else:\n+        existing_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"pyspark-shell\")\n+        jars_args = \"--jars %s\" % avro_jar\n+        os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \" \".join([jars_args, existing_args])\n+\n+    import doctest\n+    from pyspark.sql import Row, SparkSession\n+    import pyspark.sql.avro.functions\n+    globs = pyspark.sql.avro.functions.__dict__.copy()\n+    spark = SparkSession.builder\\\n+        .master(\"local[4]\")\\\n+        .appName(\"sql.avro.functions tests\")\\\n+        .getOrCreate()\n+    sc = spark.sparkContext\n+    globs['sc'] = sc"
  }],
  "prId": 23797
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Sorry, last nit. Looks we don't need this too.",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-03-09T00:25:24Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.rdd import ignore_unicode_prefix\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@ignore_unicode_prefix\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Note: Avro is built-in but external data source module since Spark 2.4. Please deploy the\n+    application as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the binary column.\n+    :param jsonFormatSchema: the avro schema in JSON string format.\n+    :param options: options to control how the Avro record is parsed.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import from_avro, to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> avroDf = df.select(to_avro(df.value).alias(\"avro\"))\n+    >>> avroDf.collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    >>> jsonFormatSchema = '''{\"type\":\"record\",\"name\":\"topLevelRecord\",\"fields\":\n+    ...     [{\"name\":\"avro\",\"type\":[{\"type\":\"record\",\"name\":\"value\",\"namespace\":\"topLevelRecord\",\n+    ...     \"fields\":[{\"name\":\"age\",\"type\":[\"long\",\"null\"]},\n+    ...     {\"name\":\"name\",\"type\":[\"string\",\"null\"]}]},\"null\"]}]}'''\n+    >>> avroDf.select(from_avro(avroDf.avro, jsonFormatSchema).alias(\"value\")).collect()\n+    [Row(value=Row(avro=Row(age=2, name=u'Alice')))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.from_avro(\n+            _to_java_column(data), jsonFormatSchema, options)\n+    except TypeError as e:\n+        if str(e) == \"'JavaPackage' object is not callable\":\n+            _print_missing_jar(\"Avro\", \"avro\", \"avro\", sc.version)\n+        raise\n+    return Column(jc)\n+\n+\n+@ignore_unicode_prefix\n+@since(3.0)\n+def to_avro(data):\n+    \"\"\"\n+    Converts a column into binary of avro format.\n+\n+    Note: Avro is built-in but external data source module since Spark 2.4. Please deploy the\n+    application as per the deployment section of \"Apache Avro Data Source Guide\".\n+\n+    :param data: the data column.\n+\n+    >>> from pyspark.sql import Row\n+    >>> from pyspark.sql.avro.functions import to_avro\n+    >>> data = [(1, Row(name='Alice', age=2))]\n+    >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n+    >>> df.select(to_avro(df.value).alias(\"avro\")).collect()\n+    [Row(avro=bytearray(b'\\\\x00\\\\x00\\\\x04\\\\x00\\\\nAlice'))]\n+    \"\"\"\n+\n+    sc = SparkContext._active_spark_context\n+    try:\n+        jc = sc._jvm.org.apache.spark.sql.avro.functions.to_avro(_to_java_column(data))\n+    except TypeError as e:\n+        if str(e) == \"'JavaPackage' object is not callable\":\n+            _print_missing_jar(\"Avro\", \"avro\", \"avro\", sc.version)\n+        raise\n+    return Column(jc)\n+\n+\n+def _test():\n+    import os\n+    import sys\n+    from pyspark.testing.utils import search_jar\n+    avro_jar = search_jar(\"external/avro\", \"spark-avro\")\n+    if avro_jar is None:\n+        print(\n+            \"Skipping all Avro Python tests as the optional Avro project was \"\n+            \"not compiled into a JAR. To run these tests, \"\n+            \"you need to build Spark with 'build/sbt -Pavro package' or \"\n+            \"'build/mvn -Pavro package' before running this test.\")\n+        sys.exit(0)\n+    else:\n+        existing_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"pyspark-shell\")\n+        jars_args = \"--jars %s\" % avro_jar\n+        os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \" \".join([jars_args, existing_args])\n+\n+    import doctest\n+    from pyspark.sql import Row, SparkSession\n+    import pyspark.sql.avro.functions\n+    globs = pyspark.sql.avro.functions.__dict__.copy()\n+    spark = SparkSession.builder\\\n+        .master(\"local[4]\")\\\n+        .appName(\"sql.avro.functions tests\")\\\n+        .getOrCreate()\n+    sc = spark.sparkContext"
  }],
  "prId": 23797
}, {
  "comments": [{
    "author": {
      "login": "holdenk"
    },
    "body": "Could we improve the wording here maybe? \"built-in but external\" might be a bit confusing. What do you think of something like it's a supported but optional data source that requires special deployment?",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-03-09T01:50:34Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.rdd import ignore_unicode_prefix\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@ignore_unicode_prefix\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Note: Avro is built-in but external data source module since Spark 2.4. Please deploy the",
    "line": 37
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "This part is taken over from the original feature which introduced in 2.4. I think the users already got used to it. If you still think it worth I suggest to modify the original feature as well.",
    "commit": "f3f0348ef4cce81fee62c7a390c465dd66a54ed2",
    "createdAt": "2019-03-10T20:14:42Z",
    "diffHunk": "@@ -0,0 +1,135 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+A collections of builtin avro functions\n+\"\"\"\n+\n+\n+from pyspark import since, SparkContext\n+from pyspark.rdd import ignore_unicode_prefix\n+from pyspark.sql.column import Column, _to_java_column\n+from pyspark.util import _print_missing_jar\n+\n+\n+@ignore_unicode_prefix\n+@since(3.0)\n+def from_avro(data, jsonFormatSchema, options={}):\n+    \"\"\"\n+    Converts a binary column of avro format into its corresponding catalyst value. The specified\n+    schema must match the read data, otherwise the behavior is undefined: it may fail or return\n+    arbitrary result.\n+\n+    Note: Avro is built-in but external data source module since Spark 2.4. Please deploy the",
    "line": 37
  }],
  "prId": 23797
}]