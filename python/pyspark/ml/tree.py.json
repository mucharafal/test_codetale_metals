[{
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "ditto",
    "commit": "302e98eb8fc9cd8e776b7b6d8507f256ac6b31a2",
    "createdAt": "2019-10-08T02:56:46Z",
    "diffHunk": "@@ -0,0 +1,348 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark import since, keyword_only\n+from pyspark.ml.param.shared import *\n+from pyspark.ml.util import *\n+from pyspark.ml.wrapper import JavaEstimator, JavaModel, JavaParams, \\\n+    JavaPredictor, JavaPredictionModel\n+from pyspark.ml.common import inherit_doc, _java2py, _py2java\n+\n+\n+@inherit_doc\n+class DecisionTreeModel(JavaPredictionModel):\n+    \"\"\"\n+    Abstraction for Decision Tree models.\n+    .. versionadded:: 1.5.0\n+    \"\"\"\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def numNodes(self):\n+        \"\"\"Return number of nodes of the decision tree.\"\"\"\n+        return self._call_java(\"numNodes\")\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def depth(self):\n+        \"\"\"Return depth of the decision tree.\"\"\"\n+        return self._call_java(\"depth\")\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def toDebugString(self):\n+        \"\"\"Full description of model.\"\"\"\n+        return self._call_java(\"toDebugString\")\n+\n+    @since(\"3.0.0\")\n+    def predictLeaf(self, value):\n+        \"\"\"\n+        Predict the indices of the leaves corresponding to the feature vector.\n+        \"\"\"\n+        return self._call_java(\"predictLeaf\", value)\n+\n+    def __repr__(self):\n+        return self._call_java(\"toString\")\n+\n+\n+class DecisionTreeParams(HasCheckpointInterval, HasSeed, HasWeightCol):"
  }],
  "prId": 25929
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "ditto",
    "commit": "302e98eb8fc9cd8e776b7b6d8507f256ac6b31a2",
    "createdAt": "2019-10-08T02:57:19Z",
    "diffHunk": "@@ -0,0 +1,348 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark import since, keyword_only\n+from pyspark.ml.param.shared import *\n+from pyspark.ml.util import *\n+from pyspark.ml.wrapper import JavaEstimator, JavaModel, JavaParams, \\\n+    JavaPredictor, JavaPredictionModel\n+from pyspark.ml.common import inherit_doc, _java2py, _py2java\n+\n+\n+@inherit_doc\n+class DecisionTreeModel(JavaPredictionModel):"
  }],
  "prId": 25929
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "ditto",
    "commit": "302e98eb8fc9cd8e776b7b6d8507f256ac6b31a2",
    "createdAt": "2019-10-08T02:57:32Z",
    "diffHunk": "@@ -0,0 +1,348 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark import since, keyword_only\n+from pyspark.ml.param.shared import *\n+from pyspark.ml.util import *\n+from pyspark.ml.wrapper import JavaEstimator, JavaModel, JavaParams, \\\n+    JavaPredictor, JavaPredictionModel\n+from pyspark.ml.common import inherit_doc, _java2py, _py2java\n+\n+\n+@inherit_doc\n+class DecisionTreeModel(JavaPredictionModel):\n+    \"\"\"\n+    Abstraction for Decision Tree models.\n+    .. versionadded:: 1.5.0\n+    \"\"\"\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def numNodes(self):\n+        \"\"\"Return number of nodes of the decision tree.\"\"\"\n+        return self._call_java(\"numNodes\")\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def depth(self):\n+        \"\"\"Return depth of the decision tree.\"\"\"\n+        return self._call_java(\"depth\")\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def toDebugString(self):\n+        \"\"\"Full description of model.\"\"\"\n+        return self._call_java(\"toDebugString\")\n+\n+    @since(\"3.0.0\")\n+    def predictLeaf(self, value):\n+        \"\"\"\n+        Predict the indices of the leaves corresponding to the feature vector.\n+        \"\"\"\n+        return self._call_java(\"predictLeaf\", value)\n+\n+    def __repr__(self):\n+        return self._call_java(\"toString\")\n+\n+\n+class DecisionTreeParams(HasCheckpointInterval, HasSeed, HasWeightCol):\n+    \"\"\"\n+    Mixin for Decision Tree parameters.\n+    \"\"\"\n+\n+    leafCol = Param(Params._dummy(), \"leafCol\", \"Leaf indices column name. Predicted leaf \" +\n+                    \"index of each instance in each tree by preorder.\",\n+                    typeConverter=TypeConverters.toString)\n+\n+    maxDepth = Param(Params._dummy(), \"maxDepth\", \"Maximum depth of the tree. (>= 0) E.g., \" +\n+                     \"depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\",\n+                     typeConverter=TypeConverters.toInt)\n+\n+    maxBins = Param(Params._dummy(), \"maxBins\", \"Max number of bins for discretizing continuous \" +\n+                    \"features.  Must be >=2 and >= number of categories for any categorical \" +\n+                    \"feature.\", typeConverter=TypeConverters.toInt)\n+\n+    minInstancesPerNode = Param(Params._dummy(), \"minInstancesPerNode\", \"Minimum number of \" +\n+                                \"instances each child must have after split. If a split causes \" +\n+                                \"the left or right child to have fewer than \" +\n+                                \"minInstancesPerNode, the split will be discarded as invalid. \" +\n+                                \"Should be >= 1.\", typeConverter=TypeConverters.toInt)\n+\n+    minWeightFractionPerNode = Param(Params._dummy(), \"minWeightFractionPerNode\", \"Minimum \"\n+                                     \"fraction of the weighted sample count that each child \"\n+                                     \"must have after split. If a split causes the fraction \"\n+                                     \"of the total weight in the left or right child to be \"\n+                                     \"less than minWeightFractionPerNode, the split will be \"\n+                                     \"discarded as invalid. Should be in interval [0.0, 0.5).\",\n+                                     typeConverter=TypeConverters.toFloat)\n+\n+    minInfoGain = Param(Params._dummy(), \"minInfoGain\", \"Minimum information gain for a split \" +\n+                        \"to be considered at a tree node.\", typeConverter=TypeConverters.toFloat)\n+\n+    maxMemoryInMB = Param(Params._dummy(), \"maxMemoryInMB\", \"Maximum memory in MB allocated to \" +\n+                          \"histogram aggregation. If too small, then 1 node will be split per \" +\n+                          \"iteration, and its aggregates may exceed this size.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    cacheNodeIds = Param(Params._dummy(), \"cacheNodeIds\", \"If false, the algorithm will pass \" +\n+                         \"trees to executors to match instances with nodes. If true, the \" +\n+                         \"algorithm will cache node IDs for each instance. Caching can speed \" +\n+                         \"up training of deeper trees. Users can set how often should the cache \" +\n+                         \"be checkpointed or disable it by setting checkpointInterval.\",\n+                         typeConverter=TypeConverters.toBoolean)\n+\n+    def __init__(self):\n+        super(DecisionTreeParams, self).__init__()\n+\n+    def setLeafCol(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`leafCol`.\n+        \"\"\"\n+        return self._set(leafCol=value)\n+\n+    def getLeafCol(self):\n+        \"\"\"\n+        Gets the value of leafCol or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.leafCol)\n+\n+    def getMaxDepth(self):\n+        \"\"\"\n+        Gets the value of maxDepth or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.maxDepth)\n+\n+    def getMaxBins(self):\n+        \"\"\"\n+        Gets the value of maxBins or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.maxBins)\n+\n+    def getMinInstancesPerNode(self):\n+        \"\"\"\n+        Gets the value of minInstancesPerNode or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.minInstancesPerNode)\n+\n+    def getMinWeightFractionPerNode(self):\n+        \"\"\"\n+        Gets the value of minWeightFractionPerNode or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.minWeightFractionPerNode)\n+\n+    def getMinInfoGain(self):\n+        \"\"\"\n+        Gets the value of minInfoGain or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.minInfoGain)\n+\n+    def getMaxMemoryInMB(self):\n+        \"\"\"\n+        Gets the value of maxMemoryInMB or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.maxMemoryInMB)\n+\n+    def getCacheNodeIds(self):\n+        \"\"\"\n+        Gets the value of cacheNodeIds or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.cacheNodeIds)\n+\n+\n+@inherit_doc\n+class TreeEnsembleModel(JavaPredictionModel):"
  }],
  "prId": 25929
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "ditto",
    "commit": "302e98eb8fc9cd8e776b7b6d8507f256ac6b31a2",
    "createdAt": "2019-10-08T02:57:58Z",
    "diffHunk": "@@ -0,0 +1,348 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark import since, keyword_only\n+from pyspark.ml.param.shared import *\n+from pyspark.ml.util import *\n+from pyspark.ml.wrapper import JavaEstimator, JavaModel, JavaParams, \\\n+    JavaPredictor, JavaPredictionModel\n+from pyspark.ml.common import inherit_doc, _java2py, _py2java\n+\n+\n+@inherit_doc\n+class DecisionTreeModel(JavaPredictionModel):\n+    \"\"\"\n+    Abstraction for Decision Tree models.\n+    .. versionadded:: 1.5.0\n+    \"\"\"\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def numNodes(self):\n+        \"\"\"Return number of nodes of the decision tree.\"\"\"\n+        return self._call_java(\"numNodes\")\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def depth(self):\n+        \"\"\"Return depth of the decision tree.\"\"\"\n+        return self._call_java(\"depth\")\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def toDebugString(self):\n+        \"\"\"Full description of model.\"\"\"\n+        return self._call_java(\"toDebugString\")\n+\n+    @since(\"3.0.0\")\n+    def predictLeaf(self, value):\n+        \"\"\"\n+        Predict the indices of the leaves corresponding to the feature vector.\n+        \"\"\"\n+        return self._call_java(\"predictLeaf\", value)\n+\n+    def __repr__(self):\n+        return self._call_java(\"toString\")\n+\n+\n+class DecisionTreeParams(HasCheckpointInterval, HasSeed, HasWeightCol):\n+    \"\"\"\n+    Mixin for Decision Tree parameters.\n+    \"\"\"\n+\n+    leafCol = Param(Params._dummy(), \"leafCol\", \"Leaf indices column name. Predicted leaf \" +\n+                    \"index of each instance in each tree by preorder.\",\n+                    typeConverter=TypeConverters.toString)\n+\n+    maxDepth = Param(Params._dummy(), \"maxDepth\", \"Maximum depth of the tree. (>= 0) E.g., \" +\n+                     \"depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\",\n+                     typeConverter=TypeConverters.toInt)\n+\n+    maxBins = Param(Params._dummy(), \"maxBins\", \"Max number of bins for discretizing continuous \" +\n+                    \"features.  Must be >=2 and >= number of categories for any categorical \" +\n+                    \"feature.\", typeConverter=TypeConverters.toInt)\n+\n+    minInstancesPerNode = Param(Params._dummy(), \"minInstancesPerNode\", \"Minimum number of \" +\n+                                \"instances each child must have after split. If a split causes \" +\n+                                \"the left or right child to have fewer than \" +\n+                                \"minInstancesPerNode, the split will be discarded as invalid. \" +\n+                                \"Should be >= 1.\", typeConverter=TypeConverters.toInt)\n+\n+    minWeightFractionPerNode = Param(Params._dummy(), \"minWeightFractionPerNode\", \"Minimum \"\n+                                     \"fraction of the weighted sample count that each child \"\n+                                     \"must have after split. If a split causes the fraction \"\n+                                     \"of the total weight in the left or right child to be \"\n+                                     \"less than minWeightFractionPerNode, the split will be \"\n+                                     \"discarded as invalid. Should be in interval [0.0, 0.5).\",\n+                                     typeConverter=TypeConverters.toFloat)\n+\n+    minInfoGain = Param(Params._dummy(), \"minInfoGain\", \"Minimum information gain for a split \" +\n+                        \"to be considered at a tree node.\", typeConverter=TypeConverters.toFloat)\n+\n+    maxMemoryInMB = Param(Params._dummy(), \"maxMemoryInMB\", \"Maximum memory in MB allocated to \" +\n+                          \"histogram aggregation. If too small, then 1 node will be split per \" +\n+                          \"iteration, and its aggregates may exceed this size.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    cacheNodeIds = Param(Params._dummy(), \"cacheNodeIds\", \"If false, the algorithm will pass \" +\n+                         \"trees to executors to match instances with nodes. If true, the \" +\n+                         \"algorithm will cache node IDs for each instance. Caching can speed \" +\n+                         \"up training of deeper trees. Users can set how often should the cache \" +\n+                         \"be checkpointed or disable it by setting checkpointInterval.\",\n+                         typeConverter=TypeConverters.toBoolean)\n+\n+    def __init__(self):\n+        super(DecisionTreeParams, self).__init__()\n+\n+    def setLeafCol(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`leafCol`.\n+        \"\"\"\n+        return self._set(leafCol=value)\n+\n+    def getLeafCol(self):\n+        \"\"\"\n+        Gets the value of leafCol or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.leafCol)\n+\n+    def getMaxDepth(self):\n+        \"\"\"\n+        Gets the value of maxDepth or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.maxDepth)\n+\n+    def getMaxBins(self):\n+        \"\"\"\n+        Gets the value of maxBins or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.maxBins)\n+\n+    def getMinInstancesPerNode(self):\n+        \"\"\"\n+        Gets the value of minInstancesPerNode or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.minInstancesPerNode)\n+\n+    def getMinWeightFractionPerNode(self):\n+        \"\"\"\n+        Gets the value of minWeightFractionPerNode or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.minWeightFractionPerNode)\n+\n+    def getMinInfoGain(self):\n+        \"\"\"\n+        Gets the value of minInfoGain or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.minInfoGain)\n+\n+    def getMaxMemoryInMB(self):\n+        \"\"\"\n+        Gets the value of maxMemoryInMB or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.maxMemoryInMB)\n+\n+    def getCacheNodeIds(self):\n+        \"\"\"\n+        Gets the value of cacheNodeIds or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.cacheNodeIds)\n+\n+\n+@inherit_doc\n+class TreeEnsembleModel(JavaPredictionModel):\n+    \"\"\"\n+    (private abstraction)\n+    Represents a tree ensemble model.\n+    \"\"\"\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def trees(self):\n+        \"\"\"Trees in this ensemble. Warning: These have null parent Estimators.\"\"\"\n+        return [DecisionTreeModel(m) for m in list(self._call_java(\"trees\"))]\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def getNumTrees(self):\n+        \"\"\"Number of trees in ensemble.\"\"\"\n+        return self._call_java(\"getNumTrees\")\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def treeWeights(self):\n+        \"\"\"Return the weights for each tree\"\"\"\n+        return list(self._call_java(\"javaTreeWeights\"))\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def totalNumNodes(self):\n+        \"\"\"Total number of nodes, summed over all trees in the ensemble.\"\"\"\n+        return self._call_java(\"totalNumNodes\")\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def toDebugString(self):\n+        \"\"\"Full description of model.\"\"\"\n+        return self._call_java(\"toDebugString\")\n+\n+    @since(\"3.0.0\")\n+    def predictLeaf(self, value):\n+        \"\"\"\n+        Predict the indices of the leaves corresponding to the feature vector.\n+        \"\"\"\n+        return self._call_java(\"predictLeaf\", value)\n+\n+    def __repr__(self):\n+        return self._call_java(\"toString\")\n+\n+\n+class TreeEnsembleParams(DecisionTreeParams):"
  }],
  "prId": 25929
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "ditto",
    "commit": "302e98eb8fc9cd8e776b7b6d8507f256ac6b31a2",
    "createdAt": "2019-10-08T02:58:09Z",
    "diffHunk": "@@ -0,0 +1,348 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark import since, keyword_only\n+from pyspark.ml.param.shared import *\n+from pyspark.ml.util import *\n+from pyspark.ml.wrapper import JavaEstimator, JavaModel, JavaParams, \\\n+    JavaPredictor, JavaPredictionModel\n+from pyspark.ml.common import inherit_doc, _java2py, _py2java\n+\n+\n+@inherit_doc\n+class DecisionTreeModel(JavaPredictionModel):\n+    \"\"\"\n+    Abstraction for Decision Tree models.\n+    .. versionadded:: 1.5.0\n+    \"\"\"\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def numNodes(self):\n+        \"\"\"Return number of nodes of the decision tree.\"\"\"\n+        return self._call_java(\"numNodes\")\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def depth(self):\n+        \"\"\"Return depth of the decision tree.\"\"\"\n+        return self._call_java(\"depth\")\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def toDebugString(self):\n+        \"\"\"Full description of model.\"\"\"\n+        return self._call_java(\"toDebugString\")\n+\n+    @since(\"3.0.0\")\n+    def predictLeaf(self, value):\n+        \"\"\"\n+        Predict the indices of the leaves corresponding to the feature vector.\n+        \"\"\"\n+        return self._call_java(\"predictLeaf\", value)\n+\n+    def __repr__(self):\n+        return self._call_java(\"toString\")\n+\n+\n+class DecisionTreeParams(HasCheckpointInterval, HasSeed, HasWeightCol):\n+    \"\"\"\n+    Mixin for Decision Tree parameters.\n+    \"\"\"\n+\n+    leafCol = Param(Params._dummy(), \"leafCol\", \"Leaf indices column name. Predicted leaf \" +\n+                    \"index of each instance in each tree by preorder.\",\n+                    typeConverter=TypeConverters.toString)\n+\n+    maxDepth = Param(Params._dummy(), \"maxDepth\", \"Maximum depth of the tree. (>= 0) E.g., \" +\n+                     \"depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\",\n+                     typeConverter=TypeConverters.toInt)\n+\n+    maxBins = Param(Params._dummy(), \"maxBins\", \"Max number of bins for discretizing continuous \" +\n+                    \"features.  Must be >=2 and >= number of categories for any categorical \" +\n+                    \"feature.\", typeConverter=TypeConverters.toInt)\n+\n+    minInstancesPerNode = Param(Params._dummy(), \"minInstancesPerNode\", \"Minimum number of \" +\n+                                \"instances each child must have after split. If a split causes \" +\n+                                \"the left or right child to have fewer than \" +\n+                                \"minInstancesPerNode, the split will be discarded as invalid. \" +\n+                                \"Should be >= 1.\", typeConverter=TypeConverters.toInt)\n+\n+    minWeightFractionPerNode = Param(Params._dummy(), \"minWeightFractionPerNode\", \"Minimum \"\n+                                     \"fraction of the weighted sample count that each child \"\n+                                     \"must have after split. If a split causes the fraction \"\n+                                     \"of the total weight in the left or right child to be \"\n+                                     \"less than minWeightFractionPerNode, the split will be \"\n+                                     \"discarded as invalid. Should be in interval [0.0, 0.5).\",\n+                                     typeConverter=TypeConverters.toFloat)\n+\n+    minInfoGain = Param(Params._dummy(), \"minInfoGain\", \"Minimum information gain for a split \" +\n+                        \"to be considered at a tree node.\", typeConverter=TypeConverters.toFloat)\n+\n+    maxMemoryInMB = Param(Params._dummy(), \"maxMemoryInMB\", \"Maximum memory in MB allocated to \" +\n+                          \"histogram aggregation. If too small, then 1 node will be split per \" +\n+                          \"iteration, and its aggregates may exceed this size.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    cacheNodeIds = Param(Params._dummy(), \"cacheNodeIds\", \"If false, the algorithm will pass \" +\n+                         \"trees to executors to match instances with nodes. If true, the \" +\n+                         \"algorithm will cache node IDs for each instance. Caching can speed \" +\n+                         \"up training of deeper trees. Users can set how often should the cache \" +\n+                         \"be checkpointed or disable it by setting checkpointInterval.\",\n+                         typeConverter=TypeConverters.toBoolean)\n+\n+    def __init__(self):\n+        super(DecisionTreeParams, self).__init__()\n+\n+    def setLeafCol(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`leafCol`.\n+        \"\"\"\n+        return self._set(leafCol=value)\n+\n+    def getLeafCol(self):\n+        \"\"\"\n+        Gets the value of leafCol or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.leafCol)\n+\n+    def getMaxDepth(self):\n+        \"\"\"\n+        Gets the value of maxDepth or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.maxDepth)\n+\n+    def getMaxBins(self):\n+        \"\"\"\n+        Gets the value of maxBins or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.maxBins)\n+\n+    def getMinInstancesPerNode(self):\n+        \"\"\"\n+        Gets the value of minInstancesPerNode or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.minInstancesPerNode)\n+\n+    def getMinWeightFractionPerNode(self):\n+        \"\"\"\n+        Gets the value of minWeightFractionPerNode or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.minWeightFractionPerNode)\n+\n+    def getMinInfoGain(self):\n+        \"\"\"\n+        Gets the value of minInfoGain or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.minInfoGain)\n+\n+    def getMaxMemoryInMB(self):\n+        \"\"\"\n+        Gets the value of maxMemoryInMB or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.maxMemoryInMB)\n+\n+    def getCacheNodeIds(self):\n+        \"\"\"\n+        Gets the value of cacheNodeIds or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.cacheNodeIds)\n+\n+\n+@inherit_doc\n+class TreeEnsembleModel(JavaPredictionModel):\n+    \"\"\"\n+    (private abstraction)\n+    Represents a tree ensemble model.\n+    \"\"\"\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def trees(self):\n+        \"\"\"Trees in this ensemble. Warning: These have null parent Estimators.\"\"\"\n+        return [DecisionTreeModel(m) for m in list(self._call_java(\"trees\"))]\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def getNumTrees(self):\n+        \"\"\"Number of trees in ensemble.\"\"\"\n+        return self._call_java(\"getNumTrees\")\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def treeWeights(self):\n+        \"\"\"Return the weights for each tree\"\"\"\n+        return list(self._call_java(\"javaTreeWeights\"))\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def totalNumNodes(self):\n+        \"\"\"Total number of nodes, summed over all trees in the ensemble.\"\"\"\n+        return self._call_java(\"totalNumNodes\")\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def toDebugString(self):\n+        \"\"\"Full description of model.\"\"\"\n+        return self._call_java(\"toDebugString\")\n+\n+    @since(\"3.0.0\")\n+    def predictLeaf(self, value):\n+        \"\"\"\n+        Predict the indices of the leaves corresponding to the feature vector.\n+        \"\"\"\n+        return self._call_java(\"predictLeaf\", value)\n+\n+    def __repr__(self):\n+        return self._call_java(\"toString\")\n+\n+\n+class TreeEnsembleParams(DecisionTreeParams):\n+    \"\"\"\n+    Mixin for Decision Tree-based ensemble algorithms parameters.\n+    \"\"\"\n+\n+    subsamplingRate = Param(Params._dummy(), \"subsamplingRate\", \"Fraction of the training data \" +\n+                            \"used for learning each decision tree, in range (0, 1].\",\n+                            typeConverter=TypeConverters.toFloat)\n+\n+    supportedFeatureSubsetStrategies = [\"auto\", \"all\", \"onethird\", \"sqrt\", \"log2\"]\n+\n+    featureSubsetStrategy = \\\n+        Param(Params._dummy(), \"featureSubsetStrategy\",\n+              \"The number of features to consider for splits at each tree node. Supported \" +\n+              \"options: 'auto' (choose automatically for task: If numTrees == 1, set to \" +\n+              \"'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to \" +\n+              \"'onethird' for regression), 'all' (use all features), 'onethird' (use \" +\n+              \"1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use \" +\n+              \"log2(number of features)), 'n' (when n is in the range (0, 1.0], use \" +\n+              \"n * number of features. When n is in the range (1, number of features), use\" +\n+              \" n features). default = 'auto'\", typeConverter=TypeConverters.toString)\n+\n+    def __init__(self):\n+        super(TreeEnsembleParams, self).__init__()\n+\n+    @since(\"1.4.0\")\n+    def getSubsamplingRate(self):\n+        \"\"\"\n+        Gets the value of subsamplingRate or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.subsamplingRate)\n+\n+    @since(\"1.4.0\")\n+    def getFeatureSubsetStrategy(self):\n+        \"\"\"\n+        Gets the value of featureSubsetStrategy or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.featureSubsetStrategy)\n+\n+\n+class RandomForestParams(TreeEnsembleParams):"
  }],
  "prId": 25929
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "ditto",
    "commit": "302e98eb8fc9cd8e776b7b6d8507f256ac6b31a2",
    "createdAt": "2019-10-08T02:58:21Z",
    "diffHunk": "@@ -0,0 +1,348 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark import since, keyword_only\n+from pyspark.ml.param.shared import *\n+from pyspark.ml.util import *\n+from pyspark.ml.wrapper import JavaEstimator, JavaModel, JavaParams, \\\n+    JavaPredictor, JavaPredictionModel\n+from pyspark.ml.common import inherit_doc, _java2py, _py2java\n+\n+\n+@inherit_doc\n+class DecisionTreeModel(JavaPredictionModel):\n+    \"\"\"\n+    Abstraction for Decision Tree models.\n+    .. versionadded:: 1.5.0\n+    \"\"\"\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def numNodes(self):\n+        \"\"\"Return number of nodes of the decision tree.\"\"\"\n+        return self._call_java(\"numNodes\")\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def depth(self):\n+        \"\"\"Return depth of the decision tree.\"\"\"\n+        return self._call_java(\"depth\")\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def toDebugString(self):\n+        \"\"\"Full description of model.\"\"\"\n+        return self._call_java(\"toDebugString\")\n+\n+    @since(\"3.0.0\")\n+    def predictLeaf(self, value):\n+        \"\"\"\n+        Predict the indices of the leaves corresponding to the feature vector.\n+        \"\"\"\n+        return self._call_java(\"predictLeaf\", value)\n+\n+    def __repr__(self):\n+        return self._call_java(\"toString\")\n+\n+\n+class DecisionTreeParams(HasCheckpointInterval, HasSeed, HasWeightCol):\n+    \"\"\"\n+    Mixin for Decision Tree parameters.\n+    \"\"\"\n+\n+    leafCol = Param(Params._dummy(), \"leafCol\", \"Leaf indices column name. Predicted leaf \" +\n+                    \"index of each instance in each tree by preorder.\",\n+                    typeConverter=TypeConverters.toString)\n+\n+    maxDepth = Param(Params._dummy(), \"maxDepth\", \"Maximum depth of the tree. (>= 0) E.g., \" +\n+                     \"depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\",\n+                     typeConverter=TypeConverters.toInt)\n+\n+    maxBins = Param(Params._dummy(), \"maxBins\", \"Max number of bins for discretizing continuous \" +\n+                    \"features.  Must be >=2 and >= number of categories for any categorical \" +\n+                    \"feature.\", typeConverter=TypeConverters.toInt)\n+\n+    minInstancesPerNode = Param(Params._dummy(), \"minInstancesPerNode\", \"Minimum number of \" +\n+                                \"instances each child must have after split. If a split causes \" +\n+                                \"the left or right child to have fewer than \" +\n+                                \"minInstancesPerNode, the split will be discarded as invalid. \" +\n+                                \"Should be >= 1.\", typeConverter=TypeConverters.toInt)\n+\n+    minWeightFractionPerNode = Param(Params._dummy(), \"minWeightFractionPerNode\", \"Minimum \"\n+                                     \"fraction of the weighted sample count that each child \"\n+                                     \"must have after split. If a split causes the fraction \"\n+                                     \"of the total weight in the left or right child to be \"\n+                                     \"less than minWeightFractionPerNode, the split will be \"\n+                                     \"discarded as invalid. Should be in interval [0.0, 0.5).\",\n+                                     typeConverter=TypeConverters.toFloat)\n+\n+    minInfoGain = Param(Params._dummy(), \"minInfoGain\", \"Minimum information gain for a split \" +\n+                        \"to be considered at a tree node.\", typeConverter=TypeConverters.toFloat)\n+\n+    maxMemoryInMB = Param(Params._dummy(), \"maxMemoryInMB\", \"Maximum memory in MB allocated to \" +\n+                          \"histogram aggregation. If too small, then 1 node will be split per \" +\n+                          \"iteration, and its aggregates may exceed this size.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    cacheNodeIds = Param(Params._dummy(), \"cacheNodeIds\", \"If false, the algorithm will pass \" +\n+                         \"trees to executors to match instances with nodes. If true, the \" +\n+                         \"algorithm will cache node IDs for each instance. Caching can speed \" +\n+                         \"up training of deeper trees. Users can set how often should the cache \" +\n+                         \"be checkpointed or disable it by setting checkpointInterval.\",\n+                         typeConverter=TypeConverters.toBoolean)\n+\n+    def __init__(self):\n+        super(DecisionTreeParams, self).__init__()\n+\n+    def setLeafCol(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`leafCol`.\n+        \"\"\"\n+        return self._set(leafCol=value)\n+\n+    def getLeafCol(self):\n+        \"\"\"\n+        Gets the value of leafCol or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.leafCol)\n+\n+    def getMaxDepth(self):\n+        \"\"\"\n+        Gets the value of maxDepth or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.maxDepth)\n+\n+    def getMaxBins(self):\n+        \"\"\"\n+        Gets the value of maxBins or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.maxBins)\n+\n+    def getMinInstancesPerNode(self):\n+        \"\"\"\n+        Gets the value of minInstancesPerNode or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.minInstancesPerNode)\n+\n+    def getMinWeightFractionPerNode(self):\n+        \"\"\"\n+        Gets the value of minWeightFractionPerNode or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.minWeightFractionPerNode)\n+\n+    def getMinInfoGain(self):\n+        \"\"\"\n+        Gets the value of minInfoGain or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.minInfoGain)\n+\n+    def getMaxMemoryInMB(self):\n+        \"\"\"\n+        Gets the value of maxMemoryInMB or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.maxMemoryInMB)\n+\n+    def getCacheNodeIds(self):\n+        \"\"\"\n+        Gets the value of cacheNodeIds or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.cacheNodeIds)\n+\n+\n+@inherit_doc\n+class TreeEnsembleModel(JavaPredictionModel):\n+    \"\"\"\n+    (private abstraction)\n+    Represents a tree ensemble model.\n+    \"\"\"\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def trees(self):\n+        \"\"\"Trees in this ensemble. Warning: These have null parent Estimators.\"\"\"\n+        return [DecisionTreeModel(m) for m in list(self._call_java(\"trees\"))]\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def getNumTrees(self):\n+        \"\"\"Number of trees in ensemble.\"\"\"\n+        return self._call_java(\"getNumTrees\")\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def treeWeights(self):\n+        \"\"\"Return the weights for each tree\"\"\"\n+        return list(self._call_java(\"javaTreeWeights\"))\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def totalNumNodes(self):\n+        \"\"\"Total number of nodes, summed over all trees in the ensemble.\"\"\"\n+        return self._call_java(\"totalNumNodes\")\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def toDebugString(self):\n+        \"\"\"Full description of model.\"\"\"\n+        return self._call_java(\"toDebugString\")\n+\n+    @since(\"3.0.0\")\n+    def predictLeaf(self, value):\n+        \"\"\"\n+        Predict the indices of the leaves corresponding to the feature vector.\n+        \"\"\"\n+        return self._call_java(\"predictLeaf\", value)\n+\n+    def __repr__(self):\n+        return self._call_java(\"toString\")\n+\n+\n+class TreeEnsembleParams(DecisionTreeParams):\n+    \"\"\"\n+    Mixin for Decision Tree-based ensemble algorithms parameters.\n+    \"\"\"\n+\n+    subsamplingRate = Param(Params._dummy(), \"subsamplingRate\", \"Fraction of the training data \" +\n+                            \"used for learning each decision tree, in range (0, 1].\",\n+                            typeConverter=TypeConverters.toFloat)\n+\n+    supportedFeatureSubsetStrategies = [\"auto\", \"all\", \"onethird\", \"sqrt\", \"log2\"]\n+\n+    featureSubsetStrategy = \\\n+        Param(Params._dummy(), \"featureSubsetStrategy\",\n+              \"The number of features to consider for splits at each tree node. Supported \" +\n+              \"options: 'auto' (choose automatically for task: If numTrees == 1, set to \" +\n+              \"'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to \" +\n+              \"'onethird' for regression), 'all' (use all features), 'onethird' (use \" +\n+              \"1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use \" +\n+              \"log2(number of features)), 'n' (when n is in the range (0, 1.0], use \" +\n+              \"n * number of features. When n is in the range (1, number of features), use\" +\n+              \" n features). default = 'auto'\", typeConverter=TypeConverters.toString)\n+\n+    def __init__(self):\n+        super(TreeEnsembleParams, self).__init__()\n+\n+    @since(\"1.4.0\")\n+    def getSubsamplingRate(self):\n+        \"\"\"\n+        Gets the value of subsamplingRate or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.subsamplingRate)\n+\n+    @since(\"1.4.0\")\n+    def getFeatureSubsetStrategy(self):\n+        \"\"\"\n+        Gets the value of featureSubsetStrategy or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.featureSubsetStrategy)\n+\n+\n+class RandomForestParams(TreeEnsembleParams):\n+    \"\"\"\n+    Private class to track supported random forest parameters.\n+    \"\"\"\n+\n+    numTrees = Param(Params._dummy(), \"numTrees\", \"Number of trees to train (>= 1).\",\n+                     typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(RandomForestParams, self).__init__()\n+\n+    @since(\"1.4.0\")\n+    def getNumTrees(self):\n+        \"\"\"\n+        Gets the value of numTrees or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numTrees)\n+\n+\n+class GBTParams(TreeEnsembleParams, HasMaxIter, HasStepSize, HasValidationIndicatorCol):\n+    \"\"\"\n+    Private class to track supported GBT params.\n+    \"\"\"\n+\n+    stepSize = Param(Params._dummy(), \"stepSize\",\n+                     \"Step size (a.k.a. learning rate) in interval (0, 1] for shrinking \" +\n+                     \"the contribution of each estimator.\",\n+                     typeConverter=TypeConverters.toFloat)\n+\n+    validationTol = Param(Params._dummy(), \"validationTol\",\n+                          \"Threshold for stopping early when fit with validation is used. \" +\n+                          \"If the error rate on the validation input changes by less than the \" +\n+                          \"validationTol, then learning will stop early (before `maxIter`). \" +\n+                          \"This parameter is ignored when fit without validation is used.\",\n+                          typeConverter=TypeConverters.toFloat)\n+\n+    @since(\"3.0.0\")\n+    def getValidationTol(self):\n+        \"\"\"\n+        Gets the value of validationTol or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.validationTol)\n+\n+\n+class HasVarianceImpurity(Params):\n+    \"\"\"\n+    Private class to track supported impurity measures.\n+    \"\"\"\n+\n+    supportedImpurities = [\"variance\"]\n+\n+    impurity = Param(Params._dummy(), \"impurity\",\n+                     \"Criterion used for information gain calculation (case-insensitive). \" +\n+                     \"Supported options: \" +\n+                     \", \".join(supportedImpurities), typeConverter=TypeConverters.toString)\n+\n+    def __init__(self):\n+        super(HasVarianceImpurity, self).__init__()\n+\n+    @since(\"1.4.0\")\n+    def getImpurity(self):\n+        \"\"\"\n+        Gets the value of impurity or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.impurity)\n+\n+\n+class TreeClassifierParams(object):"
  }],
  "prId": 25929
}, {
  "comments": [{
    "author": {
      "login": "zhengruifeng"
    },
    "body": "ditto",
    "commit": "302e98eb8fc9cd8e776b7b6d8507f256ac6b31a2",
    "createdAt": "2019-10-08T02:58:30Z",
    "diffHunk": "@@ -0,0 +1,348 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark import since, keyword_only\n+from pyspark.ml.param.shared import *\n+from pyspark.ml.util import *\n+from pyspark.ml.wrapper import JavaEstimator, JavaModel, JavaParams, \\\n+    JavaPredictor, JavaPredictionModel\n+from pyspark.ml.common import inherit_doc, _java2py, _py2java\n+\n+\n+@inherit_doc\n+class DecisionTreeModel(JavaPredictionModel):\n+    \"\"\"\n+    Abstraction for Decision Tree models.\n+    .. versionadded:: 1.5.0\n+    \"\"\"\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def numNodes(self):\n+        \"\"\"Return number of nodes of the decision tree.\"\"\"\n+        return self._call_java(\"numNodes\")\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def depth(self):\n+        \"\"\"Return depth of the decision tree.\"\"\"\n+        return self._call_java(\"depth\")\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def toDebugString(self):\n+        \"\"\"Full description of model.\"\"\"\n+        return self._call_java(\"toDebugString\")\n+\n+    @since(\"3.0.0\")\n+    def predictLeaf(self, value):\n+        \"\"\"\n+        Predict the indices of the leaves corresponding to the feature vector.\n+        \"\"\"\n+        return self._call_java(\"predictLeaf\", value)\n+\n+    def __repr__(self):\n+        return self._call_java(\"toString\")\n+\n+\n+class DecisionTreeParams(HasCheckpointInterval, HasSeed, HasWeightCol):\n+    \"\"\"\n+    Mixin for Decision Tree parameters.\n+    \"\"\"\n+\n+    leafCol = Param(Params._dummy(), \"leafCol\", \"Leaf indices column name. Predicted leaf \" +\n+                    \"index of each instance in each tree by preorder.\",\n+                    typeConverter=TypeConverters.toString)\n+\n+    maxDepth = Param(Params._dummy(), \"maxDepth\", \"Maximum depth of the tree. (>= 0) E.g., \" +\n+                     \"depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes.\",\n+                     typeConverter=TypeConverters.toInt)\n+\n+    maxBins = Param(Params._dummy(), \"maxBins\", \"Max number of bins for discretizing continuous \" +\n+                    \"features.  Must be >=2 and >= number of categories for any categorical \" +\n+                    \"feature.\", typeConverter=TypeConverters.toInt)\n+\n+    minInstancesPerNode = Param(Params._dummy(), \"minInstancesPerNode\", \"Minimum number of \" +\n+                                \"instances each child must have after split. If a split causes \" +\n+                                \"the left or right child to have fewer than \" +\n+                                \"minInstancesPerNode, the split will be discarded as invalid. \" +\n+                                \"Should be >= 1.\", typeConverter=TypeConverters.toInt)\n+\n+    minWeightFractionPerNode = Param(Params._dummy(), \"minWeightFractionPerNode\", \"Minimum \"\n+                                     \"fraction of the weighted sample count that each child \"\n+                                     \"must have after split. If a split causes the fraction \"\n+                                     \"of the total weight in the left or right child to be \"\n+                                     \"less than minWeightFractionPerNode, the split will be \"\n+                                     \"discarded as invalid. Should be in interval [0.0, 0.5).\",\n+                                     typeConverter=TypeConverters.toFloat)\n+\n+    minInfoGain = Param(Params._dummy(), \"minInfoGain\", \"Minimum information gain for a split \" +\n+                        \"to be considered at a tree node.\", typeConverter=TypeConverters.toFloat)\n+\n+    maxMemoryInMB = Param(Params._dummy(), \"maxMemoryInMB\", \"Maximum memory in MB allocated to \" +\n+                          \"histogram aggregation. If too small, then 1 node will be split per \" +\n+                          \"iteration, and its aggregates may exceed this size.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    cacheNodeIds = Param(Params._dummy(), \"cacheNodeIds\", \"If false, the algorithm will pass \" +\n+                         \"trees to executors to match instances with nodes. If true, the \" +\n+                         \"algorithm will cache node IDs for each instance. Caching can speed \" +\n+                         \"up training of deeper trees. Users can set how often should the cache \" +\n+                         \"be checkpointed or disable it by setting checkpointInterval.\",\n+                         typeConverter=TypeConverters.toBoolean)\n+\n+    def __init__(self):\n+        super(DecisionTreeParams, self).__init__()\n+\n+    def setLeafCol(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`leafCol`.\n+        \"\"\"\n+        return self._set(leafCol=value)\n+\n+    def getLeafCol(self):\n+        \"\"\"\n+        Gets the value of leafCol or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.leafCol)\n+\n+    def getMaxDepth(self):\n+        \"\"\"\n+        Gets the value of maxDepth or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.maxDepth)\n+\n+    def getMaxBins(self):\n+        \"\"\"\n+        Gets the value of maxBins or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.maxBins)\n+\n+    def getMinInstancesPerNode(self):\n+        \"\"\"\n+        Gets the value of minInstancesPerNode or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.minInstancesPerNode)\n+\n+    def getMinWeightFractionPerNode(self):\n+        \"\"\"\n+        Gets the value of minWeightFractionPerNode or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.minWeightFractionPerNode)\n+\n+    def getMinInfoGain(self):\n+        \"\"\"\n+        Gets the value of minInfoGain or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.minInfoGain)\n+\n+    def getMaxMemoryInMB(self):\n+        \"\"\"\n+        Gets the value of maxMemoryInMB or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.maxMemoryInMB)\n+\n+    def getCacheNodeIds(self):\n+        \"\"\"\n+        Gets the value of cacheNodeIds or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.cacheNodeIds)\n+\n+\n+@inherit_doc\n+class TreeEnsembleModel(JavaPredictionModel):\n+    \"\"\"\n+    (private abstraction)\n+    Represents a tree ensemble model.\n+    \"\"\"\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def trees(self):\n+        \"\"\"Trees in this ensemble. Warning: These have null parent Estimators.\"\"\"\n+        return [DecisionTreeModel(m) for m in list(self._call_java(\"trees\"))]\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def getNumTrees(self):\n+        \"\"\"Number of trees in ensemble.\"\"\"\n+        return self._call_java(\"getNumTrees\")\n+\n+    @property\n+    @since(\"1.5.0\")\n+    def treeWeights(self):\n+        \"\"\"Return the weights for each tree\"\"\"\n+        return list(self._call_java(\"javaTreeWeights\"))\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def totalNumNodes(self):\n+        \"\"\"Total number of nodes, summed over all trees in the ensemble.\"\"\"\n+        return self._call_java(\"totalNumNodes\")\n+\n+    @property\n+    @since(\"2.0.0\")\n+    def toDebugString(self):\n+        \"\"\"Full description of model.\"\"\"\n+        return self._call_java(\"toDebugString\")\n+\n+    @since(\"3.0.0\")\n+    def predictLeaf(self, value):\n+        \"\"\"\n+        Predict the indices of the leaves corresponding to the feature vector.\n+        \"\"\"\n+        return self._call_java(\"predictLeaf\", value)\n+\n+    def __repr__(self):\n+        return self._call_java(\"toString\")\n+\n+\n+class TreeEnsembleParams(DecisionTreeParams):\n+    \"\"\"\n+    Mixin for Decision Tree-based ensemble algorithms parameters.\n+    \"\"\"\n+\n+    subsamplingRate = Param(Params._dummy(), \"subsamplingRate\", \"Fraction of the training data \" +\n+                            \"used for learning each decision tree, in range (0, 1].\",\n+                            typeConverter=TypeConverters.toFloat)\n+\n+    supportedFeatureSubsetStrategies = [\"auto\", \"all\", \"onethird\", \"sqrt\", \"log2\"]\n+\n+    featureSubsetStrategy = \\\n+        Param(Params._dummy(), \"featureSubsetStrategy\",\n+              \"The number of features to consider for splits at each tree node. Supported \" +\n+              \"options: 'auto' (choose automatically for task: If numTrees == 1, set to \" +\n+              \"'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to \" +\n+              \"'onethird' for regression), 'all' (use all features), 'onethird' (use \" +\n+              \"1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use \" +\n+              \"log2(number of features)), 'n' (when n is in the range (0, 1.0], use \" +\n+              \"n * number of features. When n is in the range (1, number of features), use\" +\n+              \" n features). default = 'auto'\", typeConverter=TypeConverters.toString)\n+\n+    def __init__(self):\n+        super(TreeEnsembleParams, self).__init__()\n+\n+    @since(\"1.4.0\")\n+    def getSubsamplingRate(self):\n+        \"\"\"\n+        Gets the value of subsamplingRate or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.subsamplingRate)\n+\n+    @since(\"1.4.0\")\n+    def getFeatureSubsetStrategy(self):\n+        \"\"\"\n+        Gets the value of featureSubsetStrategy or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.featureSubsetStrategy)\n+\n+\n+class RandomForestParams(TreeEnsembleParams):\n+    \"\"\"\n+    Private class to track supported random forest parameters.\n+    \"\"\"\n+\n+    numTrees = Param(Params._dummy(), \"numTrees\", \"Number of trees to train (>= 1).\",\n+                     typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(RandomForestParams, self).__init__()\n+\n+    @since(\"1.4.0\")\n+    def getNumTrees(self):\n+        \"\"\"\n+        Gets the value of numTrees or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numTrees)\n+\n+\n+class GBTParams(TreeEnsembleParams, HasMaxIter, HasStepSize, HasValidationIndicatorCol):\n+    \"\"\"\n+    Private class to track supported GBT params.\n+    \"\"\"\n+\n+    stepSize = Param(Params._dummy(), \"stepSize\",\n+                     \"Step size (a.k.a. learning rate) in interval (0, 1] for shrinking \" +\n+                     \"the contribution of each estimator.\",\n+                     typeConverter=TypeConverters.toFloat)\n+\n+    validationTol = Param(Params._dummy(), \"validationTol\",\n+                          \"Threshold for stopping early when fit with validation is used. \" +\n+                          \"If the error rate on the validation input changes by less than the \" +\n+                          \"validationTol, then learning will stop early (before `maxIter`). \" +\n+                          \"This parameter is ignored when fit without validation is used.\",\n+                          typeConverter=TypeConverters.toFloat)\n+\n+    @since(\"3.0.0\")\n+    def getValidationTol(self):\n+        \"\"\"\n+        Gets the value of validationTol or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.validationTol)\n+\n+\n+class HasVarianceImpurity(Params):\n+    \"\"\"\n+    Private class to track supported impurity measures.\n+    \"\"\"\n+\n+    supportedImpurities = [\"variance\"]\n+\n+    impurity = Param(Params._dummy(), \"impurity\",\n+                     \"Criterion used for information gain calculation (case-insensitive). \" +\n+                     \"Supported options: \" +\n+                     \", \".join(supportedImpurities), typeConverter=TypeConverters.toString)\n+\n+    def __init__(self):\n+        super(HasVarianceImpurity, self).__init__()\n+\n+    @since(\"1.4.0\")\n+    def getImpurity(self):\n+        \"\"\"\n+        Gets the value of impurity or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.impurity)\n+\n+\n+class TreeClassifierParams(object):\n+    \"\"\"\n+    Private class to track supported impurity measures.\n+    .. versionadded:: 1.4.0\n+    \"\"\"\n+    supportedImpurities = [\"entropy\", \"gini\"]\n+\n+    impurity = Param(Params._dummy(), \"impurity\",\n+                     \"Criterion used for information gain calculation (case-insensitive). \" +\n+                     \"Supported options: \" +\n+                     \", \".join(supportedImpurities), typeConverter=TypeConverters.toString)\n+\n+    def __init__(self):\n+        super(TreeClassifierParams, self).__init__()\n+\n+    @since(\"1.6.0\")\n+    def getImpurity(self):\n+        \"\"\"\n+        Gets the value of impurity or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.impurity)\n+\n+\n+class TreeRegressorParams(HasVarianceImpurity):"
  }],
  "prId": 25929
}]