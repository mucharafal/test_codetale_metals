[{
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "@inherit_doc\n",
    "commit": "f37c1c1d4e0c15ccc7182b8ee074d77bab5b89bd",
    "createdAt": "2016-04-27T13:14:50Z",
    "diffHunk": "@@ -453,6 +454,447 @@ def _create_model(self, java_model):\n         return BisectingKMeansModel(java_model)\n \n \n+class LDAModel(JavaModel):\n+    \"\"\"\n+    Latent Dirichlet Allocation (LDA) model.\n+    This abstraction permits for different underlying representations,\n+    including local and distributed data structures.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    @since(\"2.0.0\")\n+    def isDistributed(self):\n+        \"\"\"\n+        Indicates whether this instance is of type DistributedLDAModel\n+        \"\"\"\n+        return self._call_java(\"isDistributed\")\n+\n+    @since(\"2.0.0\")\n+    def vocabSize(self):\n+        \"\"\"Vocabulary size (number of terms or words in the vocabulary)\"\"\"\n+        return self._call_java(\"vocabSize\")\n+\n+    @since(\"2.0.0\")\n+    def topicsMatrix(self):\n+        \"\"\"\n+        Inferred topics, where each topic is represented by a distribution over terms.\n+        This is a matrix of size vocabSize x k, where each column is a topic.\n+        No guarantees are given about the ordering of the topics.\n+\n+        WARNING: If this model is actually a :py:class:`DistributedLDAModel` instance produced by\n+        the Expectation-Maximization (\"em\") `optimizer`, then this method could involve\n+        collecting a large amount of data to the driver (on the order of vocabSize x k).\n+        \"\"\"\n+        return self._call_java(\"topicsMatrix\")\n+\n+    @since(\"2.0.0\")\n+    def logLikelihood(self, dataset):\n+        \"\"\"\n+        Calculates a lower bound on the log likelihood of the entire corpus.\n+        See Equation (16) in the Online LDA paper (Hoffman et al., 2010).\n+\n+        WARNING: If this model is an instance of :py:class:`DistributedLDAModel` (produced when\n+        :py:attr:`optimizer` is set to \"em\"), this involves collecting a large\n+        :py:func:`topicsMatrix` to the driver. This implementation may be changed in the future.\n+        \"\"\"\n+        return self._call_java(\"logLikelihood\", dataset)\n+\n+    @since(\"2.0.0\")\n+    def logPerplexity(self, dataset):\n+        \"\"\"\n+        Calculate an upper bound bound on perplexity.  (Lower is better.)\n+        See Equation (16) in the Online LDA paper (Hoffman et al., 2010).\n+\n+        WARNING: If this model is an instance of :py:class:`DistributedLDAModel` (produced when\n+        :py:attr:`optimizer` is set to \"em\"), this involves collecting a large\n+        :py:func:`topicsMatrix` to the driver. This implementation may be changed in the future.\n+        \"\"\"\n+        return self._call_java(\"logPerplexity\", dataset)\n+\n+    @since(\"2.0.0\")\n+    def describeTopics(self, maxTermsPerTopic=10):\n+        \"\"\"\n+        Return the topics described by their top-weighted terms.\n+        \"\"\"\n+        return self._call_java(\"describeTopics\", maxTermsPerTopic)\n+\n+    @since(\"2.0.0\")\n+    def estimatedDocConcentration(self):\n+        \"\"\"\n+        Value for :py:attr:`LDA.docConcentration` estimated from data.\n+        If Online LDA was used and :py:attr::`LDA.optimizeDocConcentration` was set to false,\n+        then this returns the fixed (given) value for the :py:attr:`LDA.docConcentration` parameter.\n+        \"\"\"\n+        return self._call_java(\"estimatedDocConcentration\")\n+\n+\n+class DistributedLDAModel(LDAModel, JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Distributed model fitted by :py:class:`LDA`.\n+    This type of model is currently only produced by Expectation-Maximization (EM).\n+\n+    This model stores the inferred topics, the full training dataset, and the topic distribution\n+    for each training document.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    @since(\"2.0.0\")\n+    def toLocal(self):\n+        \"\"\"\n+        Convert this distributed model to a local representation.  This discards info about the\n+        training dataset.\n+\n+        WARNING: This involves collecting a large :py:func:`topicsMatrix` to the driver.\n+        \"\"\"\n+        return LocalLDAModel(self._call_java(\"toLocal\"))\n+\n+    @since(\"2.0.0\")\n+    def trainingLogLikelihood(self):\n+        \"\"\"\n+        Log likelihood of the observed tokens in the training set,\n+        given the current parameter estimates:\n+        log P(docs | topics, topic distributions for docs, Dirichlet hyperparameters)\n+\n+        Notes:\n+          - This excludes the prior; for that, use :py:func:`logPrior`.\n+          - Even with :py:func:`logPrior`, this is NOT the same as the data log likelihood given\n+            the hyperparameters.\n+          - This is computed from the topic distributions computed during training. If you call\n+            :py:func:`logLikelihood` on the same training dataset, the topic distributions\n+            will be computed again, possibly giving different results.\n+        \"\"\"\n+        return self._call_java(\"trainingLogLikelihood\")\n+\n+    @since(\"2.0.0\")\n+    def logPrior(self):\n+        \"\"\"\n+        Log probability of the current parameter estimate:\n+        log P(topics, topic distributions for docs | alpha, eta)\n+        \"\"\"\n+        return self._call_java(\"logPrior\")\n+\n+    @since(\"2.0.0\")\n+    def getCheckpointFiles(self):\n+        \"\"\"\n+        If using checkpointing and :py:attr:`LDA.keepLastCheckpoint` is set to true, then there may\n+        be saved checkpoint files.  This method is provided so that users can manage those files.\n+\n+        Note that removing the checkpoints can cause failures if a partition is lost and is needed\n+        by certain :py:class:`DistributedLDAModel` methods.  Reference counting will clean up the\n+        checkpoints when this model and derivative data go out of scope.\n+\n+        :return  List of checkpoint files from training\n+        \"\"\"\n+        return self._call_java(\"getCheckpointFiles\")\n+\n+\n+class LocalLDAModel(LDAModel, JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Local (non-distributed) model fitted by :py:class:`LDA`.\n+    This model stores the inferred topics only; it does not store info about the training dataset.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+    pass\n+\n+\n+class LDA(JavaEstimator, HasFeaturesCol, HasMaxIter, HasSeed, HasCheckpointInterval,"
  }],
  "prId": 12723
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "`.. note:: Experimental`, should be consistent with Scala.\n",
    "commit": "f37c1c1d4e0c15ccc7182b8ee074d77bab5b89bd",
    "createdAt": "2016-04-27T13:16:22Z",
    "diffHunk": "@@ -453,6 +454,447 @@ def _create_model(self, java_model):\n         return BisectingKMeansModel(java_model)\n \n \n+class LDAModel(JavaModel):\n+    \"\"\"\n+    Latent Dirichlet Allocation (LDA) model.\n+    This abstraction permits for different underlying representations,\n+    including local and distributed data structures.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    @since(\"2.0.0\")\n+    def isDistributed(self):\n+        \"\"\"\n+        Indicates whether this instance is of type DistributedLDAModel\n+        \"\"\"\n+        return self._call_java(\"isDistributed\")\n+\n+    @since(\"2.0.0\")\n+    def vocabSize(self):\n+        \"\"\"Vocabulary size (number of terms or words in the vocabulary)\"\"\"\n+        return self._call_java(\"vocabSize\")\n+\n+    @since(\"2.0.0\")\n+    def topicsMatrix(self):\n+        \"\"\"\n+        Inferred topics, where each topic is represented by a distribution over terms.\n+        This is a matrix of size vocabSize x k, where each column is a topic.\n+        No guarantees are given about the ordering of the topics.\n+\n+        WARNING: If this model is actually a :py:class:`DistributedLDAModel` instance produced by\n+        the Expectation-Maximization (\"em\") `optimizer`, then this method could involve\n+        collecting a large amount of data to the driver (on the order of vocabSize x k).\n+        \"\"\"\n+        return self._call_java(\"topicsMatrix\")\n+\n+    @since(\"2.0.0\")\n+    def logLikelihood(self, dataset):\n+        \"\"\"\n+        Calculates a lower bound on the log likelihood of the entire corpus.\n+        See Equation (16) in the Online LDA paper (Hoffman et al., 2010).\n+\n+        WARNING: If this model is an instance of :py:class:`DistributedLDAModel` (produced when\n+        :py:attr:`optimizer` is set to \"em\"), this involves collecting a large\n+        :py:func:`topicsMatrix` to the driver. This implementation may be changed in the future.\n+        \"\"\"\n+        return self._call_java(\"logLikelihood\", dataset)\n+\n+    @since(\"2.0.0\")\n+    def logPerplexity(self, dataset):\n+        \"\"\"\n+        Calculate an upper bound bound on perplexity.  (Lower is better.)\n+        See Equation (16) in the Online LDA paper (Hoffman et al., 2010).\n+\n+        WARNING: If this model is an instance of :py:class:`DistributedLDAModel` (produced when\n+        :py:attr:`optimizer` is set to \"em\"), this involves collecting a large\n+        :py:func:`topicsMatrix` to the driver. This implementation may be changed in the future.\n+        \"\"\"\n+        return self._call_java(\"logPerplexity\", dataset)\n+\n+    @since(\"2.0.0\")\n+    def describeTopics(self, maxTermsPerTopic=10):\n+        \"\"\"\n+        Return the topics described by their top-weighted terms.\n+        \"\"\"\n+        return self._call_java(\"describeTopics\", maxTermsPerTopic)\n+\n+    @since(\"2.0.0\")\n+    def estimatedDocConcentration(self):\n+        \"\"\"\n+        Value for :py:attr:`LDA.docConcentration` estimated from data.\n+        If Online LDA was used and :py:attr::`LDA.optimizeDocConcentration` was set to false,\n+        then this returns the fixed (given) value for the :py:attr:`LDA.docConcentration` parameter.\n+        \"\"\"\n+        return self._call_java(\"estimatedDocConcentration\")\n+\n+\n+class DistributedLDAModel(LDAModel, JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Distributed model fitted by :py:class:`LDA`.\n+    This type of model is currently only produced by Expectation-Maximization (EM).\n+\n+    This model stores the inferred topics, the full training dataset, and the topic distribution\n+    for each training document.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    @since(\"2.0.0\")\n+    def toLocal(self):\n+        \"\"\"\n+        Convert this distributed model to a local representation.  This discards info about the\n+        training dataset.\n+\n+        WARNING: This involves collecting a large :py:func:`topicsMatrix` to the driver.\n+        \"\"\"\n+        return LocalLDAModel(self._call_java(\"toLocal\"))\n+\n+    @since(\"2.0.0\")\n+    def trainingLogLikelihood(self):\n+        \"\"\"\n+        Log likelihood of the observed tokens in the training set,\n+        given the current parameter estimates:\n+        log P(docs | topics, topic distributions for docs, Dirichlet hyperparameters)\n+\n+        Notes:\n+          - This excludes the prior; for that, use :py:func:`logPrior`.\n+          - Even with :py:func:`logPrior`, this is NOT the same as the data log likelihood given\n+            the hyperparameters.\n+          - This is computed from the topic distributions computed during training. If you call\n+            :py:func:`logLikelihood` on the same training dataset, the topic distributions\n+            will be computed again, possibly giving different results.\n+        \"\"\"\n+        return self._call_java(\"trainingLogLikelihood\")\n+\n+    @since(\"2.0.0\")\n+    def logPrior(self):\n+        \"\"\"\n+        Log probability of the current parameter estimate:\n+        log P(topics, topic distributions for docs | alpha, eta)\n+        \"\"\"\n+        return self._call_java(\"logPrior\")\n+\n+    @since(\"2.0.0\")\n+    def getCheckpointFiles(self):\n+        \"\"\"\n+        If using checkpointing and :py:attr:`LDA.keepLastCheckpoint` is set to true, then there may\n+        be saved checkpoint files.  This method is provided so that users can manage those files.\n+\n+        Note that removing the checkpoints can cause failures if a partition is lost and is needed\n+        by certain :py:class:`DistributedLDAModel` methods.  Reference counting will clean up the\n+        checkpoints when this model and derivative data go out of scope.\n+\n+        :return  List of checkpoint files from training\n+        \"\"\"\n+        return self._call_java(\"getCheckpointFiles\")\n+\n+\n+class LocalLDAModel(LDAModel, JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Local (non-distributed) model fitted by :py:class:`LDA`.\n+    This model stores the inferred topics only; it does not store info about the training dataset.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+    pass\n+\n+\n+class LDA(JavaEstimator, HasFeaturesCol, HasMaxIter, HasSeed, HasCheckpointInterval,\n+          JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Latent Dirichlet Allocation (LDA), a topic model designed for text documents.\n+"
  }],
  "prId": 12723
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "`.. note:: Experimental`\n",
    "commit": "f37c1c1d4e0c15ccc7182b8ee074d77bab5b89bd",
    "createdAt": "2016-04-27T13:17:00Z",
    "diffHunk": "@@ -453,6 +454,447 @@ def _create_model(self, java_model):\n         return BisectingKMeansModel(java_model)\n \n \n+class LDAModel(JavaModel):\n+    \"\"\"\n+    Latent Dirichlet Allocation (LDA) model.\n+    This abstraction permits for different underlying representations,\n+    including local and distributed data structures.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    @since(\"2.0.0\")\n+    def isDistributed(self):\n+        \"\"\"\n+        Indicates whether this instance is of type DistributedLDAModel\n+        \"\"\"\n+        return self._call_java(\"isDistributed\")\n+\n+    @since(\"2.0.0\")\n+    def vocabSize(self):\n+        \"\"\"Vocabulary size (number of terms or words in the vocabulary)\"\"\"\n+        return self._call_java(\"vocabSize\")\n+\n+    @since(\"2.0.0\")\n+    def topicsMatrix(self):\n+        \"\"\"\n+        Inferred topics, where each topic is represented by a distribution over terms.\n+        This is a matrix of size vocabSize x k, where each column is a topic.\n+        No guarantees are given about the ordering of the topics.\n+\n+        WARNING: If this model is actually a :py:class:`DistributedLDAModel` instance produced by\n+        the Expectation-Maximization (\"em\") `optimizer`, then this method could involve\n+        collecting a large amount of data to the driver (on the order of vocabSize x k).\n+        \"\"\"\n+        return self._call_java(\"topicsMatrix\")\n+\n+    @since(\"2.0.0\")\n+    def logLikelihood(self, dataset):\n+        \"\"\"\n+        Calculates a lower bound on the log likelihood of the entire corpus.\n+        See Equation (16) in the Online LDA paper (Hoffman et al., 2010).\n+\n+        WARNING: If this model is an instance of :py:class:`DistributedLDAModel` (produced when\n+        :py:attr:`optimizer` is set to \"em\"), this involves collecting a large\n+        :py:func:`topicsMatrix` to the driver. This implementation may be changed in the future.\n+        \"\"\"\n+        return self._call_java(\"logLikelihood\", dataset)\n+\n+    @since(\"2.0.0\")\n+    def logPerplexity(self, dataset):\n+        \"\"\"\n+        Calculate an upper bound bound on perplexity.  (Lower is better.)\n+        See Equation (16) in the Online LDA paper (Hoffman et al., 2010).\n+\n+        WARNING: If this model is an instance of :py:class:`DistributedLDAModel` (produced when\n+        :py:attr:`optimizer` is set to \"em\"), this involves collecting a large\n+        :py:func:`topicsMatrix` to the driver. This implementation may be changed in the future.\n+        \"\"\"\n+        return self._call_java(\"logPerplexity\", dataset)\n+\n+    @since(\"2.0.0\")\n+    def describeTopics(self, maxTermsPerTopic=10):\n+        \"\"\"\n+        Return the topics described by their top-weighted terms.\n+        \"\"\"\n+        return self._call_java(\"describeTopics\", maxTermsPerTopic)\n+\n+    @since(\"2.0.0\")\n+    def estimatedDocConcentration(self):\n+        \"\"\"\n+        Value for :py:attr:`LDA.docConcentration` estimated from data.\n+        If Online LDA was used and :py:attr::`LDA.optimizeDocConcentration` was set to false,\n+        then this returns the fixed (given) value for the :py:attr:`LDA.docConcentration` parameter.\n+        \"\"\"\n+        return self._call_java(\"estimatedDocConcentration\")\n+\n+\n+class DistributedLDAModel(LDAModel, JavaMLReadable, JavaMLWritable):\n+    \"\"\""
  }],
  "prId": 12723
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "`.. note:: Experimental`\n",
    "commit": "f37c1c1d4e0c15ccc7182b8ee074d77bab5b89bd",
    "createdAt": "2016-04-27T13:17:10Z",
    "diffHunk": "@@ -453,6 +454,447 @@ def _create_model(self, java_model):\n         return BisectingKMeansModel(java_model)\n \n \n+class LDAModel(JavaModel):\n+    \"\"\"\n+    Latent Dirichlet Allocation (LDA) model."
  }],
  "prId": 12723
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "Usually we have `+` at the end of all Param doc line. Can you check whether it produced correct Python doc w/o `+`? \n",
    "commit": "f37c1c1d4e0c15ccc7182b8ee074d77bab5b89bd",
    "createdAt": "2016-04-27T13:20:28Z",
    "diffHunk": "@@ -453,6 +454,447 @@ def _create_model(self, java_model):\n         return BisectingKMeansModel(java_model)\n \n \n+class LDAModel(JavaModel):\n+    \"\"\"\n+    Latent Dirichlet Allocation (LDA) model.\n+    This abstraction permits for different underlying representations,\n+    including local and distributed data structures.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    @since(\"2.0.0\")\n+    def isDistributed(self):\n+        \"\"\"\n+        Indicates whether this instance is of type DistributedLDAModel\n+        \"\"\"\n+        return self._call_java(\"isDistributed\")\n+\n+    @since(\"2.0.0\")\n+    def vocabSize(self):\n+        \"\"\"Vocabulary size (number of terms or words in the vocabulary)\"\"\"\n+        return self._call_java(\"vocabSize\")\n+\n+    @since(\"2.0.0\")\n+    def topicsMatrix(self):\n+        \"\"\"\n+        Inferred topics, where each topic is represented by a distribution over terms.\n+        This is a matrix of size vocabSize x k, where each column is a topic.\n+        No guarantees are given about the ordering of the topics.\n+\n+        WARNING: If this model is actually a :py:class:`DistributedLDAModel` instance produced by\n+        the Expectation-Maximization (\"em\") `optimizer`, then this method could involve\n+        collecting a large amount of data to the driver (on the order of vocabSize x k).\n+        \"\"\"\n+        return self._call_java(\"topicsMatrix\")\n+\n+    @since(\"2.0.0\")\n+    def logLikelihood(self, dataset):\n+        \"\"\"\n+        Calculates a lower bound on the log likelihood of the entire corpus.\n+        See Equation (16) in the Online LDA paper (Hoffman et al., 2010).\n+\n+        WARNING: If this model is an instance of :py:class:`DistributedLDAModel` (produced when\n+        :py:attr:`optimizer` is set to \"em\"), this involves collecting a large\n+        :py:func:`topicsMatrix` to the driver. This implementation may be changed in the future.\n+        \"\"\"\n+        return self._call_java(\"logLikelihood\", dataset)\n+\n+    @since(\"2.0.0\")\n+    def logPerplexity(self, dataset):\n+        \"\"\"\n+        Calculate an upper bound bound on perplexity.  (Lower is better.)\n+        See Equation (16) in the Online LDA paper (Hoffman et al., 2010).\n+\n+        WARNING: If this model is an instance of :py:class:`DistributedLDAModel` (produced when\n+        :py:attr:`optimizer` is set to \"em\"), this involves collecting a large\n+        :py:func:`topicsMatrix` to the driver. This implementation may be changed in the future.\n+        \"\"\"\n+        return self._call_java(\"logPerplexity\", dataset)\n+\n+    @since(\"2.0.0\")\n+    def describeTopics(self, maxTermsPerTopic=10):\n+        \"\"\"\n+        Return the topics described by their top-weighted terms.\n+        \"\"\"\n+        return self._call_java(\"describeTopics\", maxTermsPerTopic)\n+\n+    @since(\"2.0.0\")\n+    def estimatedDocConcentration(self):\n+        \"\"\"\n+        Value for :py:attr:`LDA.docConcentration` estimated from data.\n+        If Online LDA was used and :py:attr::`LDA.optimizeDocConcentration` was set to false,\n+        then this returns the fixed (given) value for the :py:attr:`LDA.docConcentration` parameter.\n+        \"\"\"\n+        return self._call_java(\"estimatedDocConcentration\")\n+\n+\n+class DistributedLDAModel(LDAModel, JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Distributed model fitted by :py:class:`LDA`.\n+    This type of model is currently only produced by Expectation-Maximization (EM).\n+\n+    This model stores the inferred topics, the full training dataset, and the topic distribution\n+    for each training document.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    @since(\"2.0.0\")\n+    def toLocal(self):\n+        \"\"\"\n+        Convert this distributed model to a local representation.  This discards info about the\n+        training dataset.\n+\n+        WARNING: This involves collecting a large :py:func:`topicsMatrix` to the driver.\n+        \"\"\"\n+        return LocalLDAModel(self._call_java(\"toLocal\"))\n+\n+    @since(\"2.0.0\")\n+    def trainingLogLikelihood(self):\n+        \"\"\"\n+        Log likelihood of the observed tokens in the training set,\n+        given the current parameter estimates:\n+        log P(docs | topics, topic distributions for docs, Dirichlet hyperparameters)\n+\n+        Notes:\n+          - This excludes the prior; for that, use :py:func:`logPrior`.\n+          - Even with :py:func:`logPrior`, this is NOT the same as the data log likelihood given\n+            the hyperparameters.\n+          - This is computed from the topic distributions computed during training. If you call\n+            :py:func:`logLikelihood` on the same training dataset, the topic distributions\n+            will be computed again, possibly giving different results.\n+        \"\"\"\n+        return self._call_java(\"trainingLogLikelihood\")\n+\n+    @since(\"2.0.0\")\n+    def logPrior(self):\n+        \"\"\"\n+        Log probability of the current parameter estimate:\n+        log P(topics, topic distributions for docs | alpha, eta)\n+        \"\"\"\n+        return self._call_java(\"logPrior\")\n+\n+    @since(\"2.0.0\")\n+    def getCheckpointFiles(self):\n+        \"\"\"\n+        If using checkpointing and :py:attr:`LDA.keepLastCheckpoint` is set to true, then there may\n+        be saved checkpoint files.  This method is provided so that users can manage those files.\n+\n+        Note that removing the checkpoints can cause failures if a partition is lost and is needed\n+        by certain :py:class:`DistributedLDAModel` methods.  Reference counting will clean up the\n+        checkpoints when this model and derivative data go out of scope.\n+\n+        :return  List of checkpoint files from training\n+        \"\"\"\n+        return self._call_java(\"getCheckpointFiles\")\n+\n+\n+class LocalLDAModel(LDAModel, JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Local (non-distributed) model fitted by :py:class:`LDA`.\n+    This model stores the inferred topics only; it does not store info about the training dataset.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+    pass\n+\n+\n+class LDA(JavaEstimator, HasFeaturesCol, HasMaxIter, HasSeed, HasCheckpointInterval,\n+          JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Latent Dirichlet Allocation (LDA), a topic model designed for text documents.\n+\n+    Terminology:\n+\n+     - \"term\" = \"word\": an el\n+     - \"token\": instance of a term appearing in a document\n+     - \"topic\": multinomial distribution over terms representing some concept\n+     - \"document\": one piece of text, corresponding to one row in the input data\n+\n+    Original LDA paper (journal version):\n+      Blei, Ng, and Jordan.  \"Latent Dirichlet Allocation.\"  JMLR, 2003.\n+\n+    Input data (featuresCol):\n+    LDA is given a collection of documents as input data, via the featuresCol parameter.\n+    Each document is specified as a :py:class:`Vector` of length vocabSize, where each entry is the\n+    count for the corresponding term (word) in the document.  Feature transformers such as\n+    :py:class:`pyspark.ml.feature.Tokenizer` and :py:class:`pyspark.ml.feature.CountVectorizer`\n+    can be useful for converting text to word count vectors.\n+\n+    >>> from pyspark.mllib.linalg import Vectors, SparseVector\n+    >>> from pyspark.ml.clustering import LDA\n+    >>> df = sqlContext.createDataFrame([[1, Vectors.dense([0.0, 1.0])],\n+    ...      [2, SparseVector(2, {0: 1.0})],], [\"id\", \"features\"])\n+    >>> lda = LDA(k=2, seed=1, optimizer=\"em\")\n+    >>> model = lda.fit(df)\n+    >>> model.isDistributed()\n+    True\n+    >>> localModel = model.toLocal()\n+    >>> localModel.isDistributed()\n+    False\n+    >>> model.vocabSize()\n+    2\n+    >>> model.describeTopics().show()\n+    +-----+-----------+--------------------+\n+    |topic|termIndices|         termWeights|\n+    +-----+-----------+--------------------+\n+    |    0|     [1, 0]|[0.50401530077160...|\n+    |    1|     [0, 1]|[0.50401530077160...|\n+    +-----+-----------+--------------------+\n+    ...\n+    >>> model.topicsMatrix()\n+    DenseMatrix(2, 2, [0.496, 0.504, 0.504, 0.496], 0)\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    k = Param(Params._dummy(), \"k\", \"number of topics (clusters) to infer\",\n+              typeConverter=TypeConverters.toInt)\n+    optimizer = Param(Params._dummy(), \"optimizer\",\n+                      \"Optimizer or inference algorithm used to estimate the LDA model.  \"",
    "line": 233
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "It does generate correctly\n",
    "commit": "f37c1c1d4e0c15ccc7182b8ee074d77bab5b89bd",
    "createdAt": "2016-04-27T19:07:28Z",
    "diffHunk": "@@ -453,6 +454,447 @@ def _create_model(self, java_model):\n         return BisectingKMeansModel(java_model)\n \n \n+class LDAModel(JavaModel):\n+    \"\"\"\n+    Latent Dirichlet Allocation (LDA) model.\n+    This abstraction permits for different underlying representations,\n+    including local and distributed data structures.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    @since(\"2.0.0\")\n+    def isDistributed(self):\n+        \"\"\"\n+        Indicates whether this instance is of type DistributedLDAModel\n+        \"\"\"\n+        return self._call_java(\"isDistributed\")\n+\n+    @since(\"2.0.0\")\n+    def vocabSize(self):\n+        \"\"\"Vocabulary size (number of terms or words in the vocabulary)\"\"\"\n+        return self._call_java(\"vocabSize\")\n+\n+    @since(\"2.0.0\")\n+    def topicsMatrix(self):\n+        \"\"\"\n+        Inferred topics, where each topic is represented by a distribution over terms.\n+        This is a matrix of size vocabSize x k, where each column is a topic.\n+        No guarantees are given about the ordering of the topics.\n+\n+        WARNING: If this model is actually a :py:class:`DistributedLDAModel` instance produced by\n+        the Expectation-Maximization (\"em\") `optimizer`, then this method could involve\n+        collecting a large amount of data to the driver (on the order of vocabSize x k).\n+        \"\"\"\n+        return self._call_java(\"topicsMatrix\")\n+\n+    @since(\"2.0.0\")\n+    def logLikelihood(self, dataset):\n+        \"\"\"\n+        Calculates a lower bound on the log likelihood of the entire corpus.\n+        See Equation (16) in the Online LDA paper (Hoffman et al., 2010).\n+\n+        WARNING: If this model is an instance of :py:class:`DistributedLDAModel` (produced when\n+        :py:attr:`optimizer` is set to \"em\"), this involves collecting a large\n+        :py:func:`topicsMatrix` to the driver. This implementation may be changed in the future.\n+        \"\"\"\n+        return self._call_java(\"logLikelihood\", dataset)\n+\n+    @since(\"2.0.0\")\n+    def logPerplexity(self, dataset):\n+        \"\"\"\n+        Calculate an upper bound bound on perplexity.  (Lower is better.)\n+        See Equation (16) in the Online LDA paper (Hoffman et al., 2010).\n+\n+        WARNING: If this model is an instance of :py:class:`DistributedLDAModel` (produced when\n+        :py:attr:`optimizer` is set to \"em\"), this involves collecting a large\n+        :py:func:`topicsMatrix` to the driver. This implementation may be changed in the future.\n+        \"\"\"\n+        return self._call_java(\"logPerplexity\", dataset)\n+\n+    @since(\"2.0.0\")\n+    def describeTopics(self, maxTermsPerTopic=10):\n+        \"\"\"\n+        Return the topics described by their top-weighted terms.\n+        \"\"\"\n+        return self._call_java(\"describeTopics\", maxTermsPerTopic)\n+\n+    @since(\"2.0.0\")\n+    def estimatedDocConcentration(self):\n+        \"\"\"\n+        Value for :py:attr:`LDA.docConcentration` estimated from data.\n+        If Online LDA was used and :py:attr::`LDA.optimizeDocConcentration` was set to false,\n+        then this returns the fixed (given) value for the :py:attr:`LDA.docConcentration` parameter.\n+        \"\"\"\n+        return self._call_java(\"estimatedDocConcentration\")\n+\n+\n+class DistributedLDAModel(LDAModel, JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Distributed model fitted by :py:class:`LDA`.\n+    This type of model is currently only produced by Expectation-Maximization (EM).\n+\n+    This model stores the inferred topics, the full training dataset, and the topic distribution\n+    for each training document.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    @since(\"2.0.0\")\n+    def toLocal(self):\n+        \"\"\"\n+        Convert this distributed model to a local representation.  This discards info about the\n+        training dataset.\n+\n+        WARNING: This involves collecting a large :py:func:`topicsMatrix` to the driver.\n+        \"\"\"\n+        return LocalLDAModel(self._call_java(\"toLocal\"))\n+\n+    @since(\"2.0.0\")\n+    def trainingLogLikelihood(self):\n+        \"\"\"\n+        Log likelihood of the observed tokens in the training set,\n+        given the current parameter estimates:\n+        log P(docs | topics, topic distributions for docs, Dirichlet hyperparameters)\n+\n+        Notes:\n+          - This excludes the prior; for that, use :py:func:`logPrior`.\n+          - Even with :py:func:`logPrior`, this is NOT the same as the data log likelihood given\n+            the hyperparameters.\n+          - This is computed from the topic distributions computed during training. If you call\n+            :py:func:`logLikelihood` on the same training dataset, the topic distributions\n+            will be computed again, possibly giving different results.\n+        \"\"\"\n+        return self._call_java(\"trainingLogLikelihood\")\n+\n+    @since(\"2.0.0\")\n+    def logPrior(self):\n+        \"\"\"\n+        Log probability of the current parameter estimate:\n+        log P(topics, topic distributions for docs | alpha, eta)\n+        \"\"\"\n+        return self._call_java(\"logPrior\")\n+\n+    @since(\"2.0.0\")\n+    def getCheckpointFiles(self):\n+        \"\"\"\n+        If using checkpointing and :py:attr:`LDA.keepLastCheckpoint` is set to true, then there may\n+        be saved checkpoint files.  This method is provided so that users can manage those files.\n+\n+        Note that removing the checkpoints can cause failures if a partition is lost and is needed\n+        by certain :py:class:`DistributedLDAModel` methods.  Reference counting will clean up the\n+        checkpoints when this model and derivative data go out of scope.\n+\n+        :return  List of checkpoint files from training\n+        \"\"\"\n+        return self._call_java(\"getCheckpointFiles\")\n+\n+\n+class LocalLDAModel(LDAModel, JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Local (non-distributed) model fitted by :py:class:`LDA`.\n+    This model stores the inferred topics only; it does not store info about the training dataset.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+    pass\n+\n+\n+class LDA(JavaEstimator, HasFeaturesCol, HasMaxIter, HasSeed, HasCheckpointInterval,\n+          JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Latent Dirichlet Allocation (LDA), a topic model designed for text documents.\n+\n+    Terminology:\n+\n+     - \"term\" = \"word\": an el\n+     - \"token\": instance of a term appearing in a document\n+     - \"topic\": multinomial distribution over terms representing some concept\n+     - \"document\": one piece of text, corresponding to one row in the input data\n+\n+    Original LDA paper (journal version):\n+      Blei, Ng, and Jordan.  \"Latent Dirichlet Allocation.\"  JMLR, 2003.\n+\n+    Input data (featuresCol):\n+    LDA is given a collection of documents as input data, via the featuresCol parameter.\n+    Each document is specified as a :py:class:`Vector` of length vocabSize, where each entry is the\n+    count for the corresponding term (word) in the document.  Feature transformers such as\n+    :py:class:`pyspark.ml.feature.Tokenizer` and :py:class:`pyspark.ml.feature.CountVectorizer`\n+    can be useful for converting text to word count vectors.\n+\n+    >>> from pyspark.mllib.linalg import Vectors, SparseVector\n+    >>> from pyspark.ml.clustering import LDA\n+    >>> df = sqlContext.createDataFrame([[1, Vectors.dense([0.0, 1.0])],\n+    ...      [2, SparseVector(2, {0: 1.0})],], [\"id\", \"features\"])\n+    >>> lda = LDA(k=2, seed=1, optimizer=\"em\")\n+    >>> model = lda.fit(df)\n+    >>> model.isDistributed()\n+    True\n+    >>> localModel = model.toLocal()\n+    >>> localModel.isDistributed()\n+    False\n+    >>> model.vocabSize()\n+    2\n+    >>> model.describeTopics().show()\n+    +-----+-----------+--------------------+\n+    |topic|termIndices|         termWeights|\n+    +-----+-----------+--------------------+\n+    |    0|     [1, 0]|[0.50401530077160...|\n+    |    1|     [0, 1]|[0.50401530077160...|\n+    +-----+-----------+--------------------+\n+    ...\n+    >>> model.topicsMatrix()\n+    DenseMatrix(2, 2, [0.496, 0.504, 0.504, 0.496], 0)\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    k = Param(Params._dummy(), \"k\", \"number of topics (clusters) to infer\",\n+              typeConverter=TypeConverters.toInt)\n+    optimizer = Param(Params._dummy(), \"optimizer\",\n+                      \"Optimizer or inference algorithm used to estimate the LDA model.  \"",
    "line": 233
  }],
  "prId": 12723
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "Set default value: `topicDistributionCol -> \"topicDistribution\"`.\n",
    "commit": "f37c1c1d4e0c15ccc7182b8ee074d77bab5b89bd",
    "createdAt": "2016-04-27T13:26:32Z",
    "diffHunk": "@@ -453,6 +454,447 @@ def _create_model(self, java_model):\n         return BisectingKMeansModel(java_model)\n \n \n+class LDAModel(JavaModel):\n+    \"\"\"\n+    Latent Dirichlet Allocation (LDA) model.\n+    This abstraction permits for different underlying representations,\n+    including local and distributed data structures.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    @since(\"2.0.0\")\n+    def isDistributed(self):\n+        \"\"\"\n+        Indicates whether this instance is of type DistributedLDAModel\n+        \"\"\"\n+        return self._call_java(\"isDistributed\")\n+\n+    @since(\"2.0.0\")\n+    def vocabSize(self):\n+        \"\"\"Vocabulary size (number of terms or words in the vocabulary)\"\"\"\n+        return self._call_java(\"vocabSize\")\n+\n+    @since(\"2.0.0\")\n+    def topicsMatrix(self):\n+        \"\"\"\n+        Inferred topics, where each topic is represented by a distribution over terms.\n+        This is a matrix of size vocabSize x k, where each column is a topic.\n+        No guarantees are given about the ordering of the topics.\n+\n+        WARNING: If this model is actually a :py:class:`DistributedLDAModel` instance produced by\n+        the Expectation-Maximization (\"em\") `optimizer`, then this method could involve\n+        collecting a large amount of data to the driver (on the order of vocabSize x k).\n+        \"\"\"\n+        return self._call_java(\"topicsMatrix\")\n+\n+    @since(\"2.0.0\")\n+    def logLikelihood(self, dataset):\n+        \"\"\"\n+        Calculates a lower bound on the log likelihood of the entire corpus.\n+        See Equation (16) in the Online LDA paper (Hoffman et al., 2010).\n+\n+        WARNING: If this model is an instance of :py:class:`DistributedLDAModel` (produced when\n+        :py:attr:`optimizer` is set to \"em\"), this involves collecting a large\n+        :py:func:`topicsMatrix` to the driver. This implementation may be changed in the future.\n+        \"\"\"\n+        return self._call_java(\"logLikelihood\", dataset)\n+\n+    @since(\"2.0.0\")\n+    def logPerplexity(self, dataset):\n+        \"\"\"\n+        Calculate an upper bound bound on perplexity.  (Lower is better.)\n+        See Equation (16) in the Online LDA paper (Hoffman et al., 2010).\n+\n+        WARNING: If this model is an instance of :py:class:`DistributedLDAModel` (produced when\n+        :py:attr:`optimizer` is set to \"em\"), this involves collecting a large\n+        :py:func:`topicsMatrix` to the driver. This implementation may be changed in the future.\n+        \"\"\"\n+        return self._call_java(\"logPerplexity\", dataset)\n+\n+    @since(\"2.0.0\")\n+    def describeTopics(self, maxTermsPerTopic=10):\n+        \"\"\"\n+        Return the topics described by their top-weighted terms.\n+        \"\"\"\n+        return self._call_java(\"describeTopics\", maxTermsPerTopic)\n+\n+    @since(\"2.0.0\")\n+    def estimatedDocConcentration(self):\n+        \"\"\"\n+        Value for :py:attr:`LDA.docConcentration` estimated from data.\n+        If Online LDA was used and :py:attr::`LDA.optimizeDocConcentration` was set to false,\n+        then this returns the fixed (given) value for the :py:attr:`LDA.docConcentration` parameter.\n+        \"\"\"\n+        return self._call_java(\"estimatedDocConcentration\")\n+\n+\n+class DistributedLDAModel(LDAModel, JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Distributed model fitted by :py:class:`LDA`.\n+    This type of model is currently only produced by Expectation-Maximization (EM).\n+\n+    This model stores the inferred topics, the full training dataset, and the topic distribution\n+    for each training document.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    @since(\"2.0.0\")\n+    def toLocal(self):\n+        \"\"\"\n+        Convert this distributed model to a local representation.  This discards info about the\n+        training dataset.\n+\n+        WARNING: This involves collecting a large :py:func:`topicsMatrix` to the driver.\n+        \"\"\"\n+        return LocalLDAModel(self._call_java(\"toLocal\"))\n+\n+    @since(\"2.0.0\")\n+    def trainingLogLikelihood(self):\n+        \"\"\"\n+        Log likelihood of the observed tokens in the training set,\n+        given the current parameter estimates:\n+        log P(docs | topics, topic distributions for docs, Dirichlet hyperparameters)\n+\n+        Notes:\n+          - This excludes the prior; for that, use :py:func:`logPrior`.\n+          - Even with :py:func:`logPrior`, this is NOT the same as the data log likelihood given\n+            the hyperparameters.\n+          - This is computed from the topic distributions computed during training. If you call\n+            :py:func:`logLikelihood` on the same training dataset, the topic distributions\n+            will be computed again, possibly giving different results.\n+        \"\"\"\n+        return self._call_java(\"trainingLogLikelihood\")\n+\n+    @since(\"2.0.0\")\n+    def logPrior(self):\n+        \"\"\"\n+        Log probability of the current parameter estimate:\n+        log P(topics, topic distributions for docs | alpha, eta)\n+        \"\"\"\n+        return self._call_java(\"logPrior\")\n+\n+    @since(\"2.0.0\")\n+    def getCheckpointFiles(self):\n+        \"\"\"\n+        If using checkpointing and :py:attr:`LDA.keepLastCheckpoint` is set to true, then there may\n+        be saved checkpoint files.  This method is provided so that users can manage those files.\n+\n+        Note that removing the checkpoints can cause failures if a partition is lost and is needed\n+        by certain :py:class:`DistributedLDAModel` methods.  Reference counting will clean up the\n+        checkpoints when this model and derivative data go out of scope.\n+\n+        :return  List of checkpoint files from training\n+        \"\"\"\n+        return self._call_java(\"getCheckpointFiles\")\n+\n+\n+class LocalLDAModel(LDAModel, JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Local (non-distributed) model fitted by :py:class:`LDA`.\n+    This model stores the inferred topics only; it does not store info about the training dataset.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+    pass\n+\n+\n+class LDA(JavaEstimator, HasFeaturesCol, HasMaxIter, HasSeed, HasCheckpointInterval,\n+          JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Latent Dirichlet Allocation (LDA), a topic model designed for text documents.\n+\n+    Terminology:\n+\n+     - \"term\" = \"word\": an el\n+     - \"token\": instance of a term appearing in a document\n+     - \"topic\": multinomial distribution over terms representing some concept\n+     - \"document\": one piece of text, corresponding to one row in the input data\n+\n+    Original LDA paper (journal version):\n+      Blei, Ng, and Jordan.  \"Latent Dirichlet Allocation.\"  JMLR, 2003.\n+\n+    Input data (featuresCol):\n+    LDA is given a collection of documents as input data, via the featuresCol parameter.\n+    Each document is specified as a :py:class:`Vector` of length vocabSize, where each entry is the\n+    count for the corresponding term (word) in the document.  Feature transformers such as\n+    :py:class:`pyspark.ml.feature.Tokenizer` and :py:class:`pyspark.ml.feature.CountVectorizer`\n+    can be useful for converting text to word count vectors.\n+\n+    >>> from pyspark.mllib.linalg import Vectors, SparseVector\n+    >>> from pyspark.ml.clustering import LDA\n+    >>> df = sqlContext.createDataFrame([[1, Vectors.dense([0.0, 1.0])],\n+    ...      [2, SparseVector(2, {0: 1.0})],], [\"id\", \"features\"])\n+    >>> lda = LDA(k=2, seed=1, optimizer=\"em\")\n+    >>> model = lda.fit(df)\n+    >>> model.isDistributed()\n+    True\n+    >>> localModel = model.toLocal()\n+    >>> localModel.isDistributed()\n+    False\n+    >>> model.vocabSize()\n+    2\n+    >>> model.describeTopics().show()\n+    +-----+-----------+--------------------+\n+    |topic|termIndices|         termWeights|\n+    +-----+-----------+--------------------+\n+    |    0|     [1, 0]|[0.50401530077160...|\n+    |    1|     [0, 1]|[0.50401530077160...|\n+    +-----+-----------+--------------------+\n+    ...\n+    >>> model.topicsMatrix()\n+    DenseMatrix(2, 2, [0.496, 0.504, 0.504, 0.496], 0)\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    k = Param(Params._dummy(), \"k\", \"number of topics (clusters) to infer\",\n+              typeConverter=TypeConverters.toInt)\n+    optimizer = Param(Params._dummy(), \"optimizer\",\n+                      \"Optimizer or inference algorithm used to estimate the LDA model.  \"\n+                      \"Supported: online, em\", typeConverter=TypeConverters.toString)\n+    learningOffset = Param(Params._dummy(), \"learningOffset\",\n+                           \"A (positive) learning parameter that downweights early iterations.\"\n+                           \" Larger values make early iterations count less\",\n+                           typeConverter=TypeConverters.toFloat)\n+    learningDecay = Param(Params._dummy(), \"learningDecay\", \"Learning rate, set as an\"\n+                          \"exponential decay rate. This should be between (0.5, 1.0] to \"\n+                          \"guarantee asymptotic convergence.\", typeConverter=TypeConverters.toFloat)\n+    subsamplingRate = Param(Params._dummy(), \"subsamplingRate\",\n+                            \"Fraction of the corpus to be sampled and used in each iteration \"\n+                            \"of mini-batch gradient descent, in range (0, 1].\",\n+                            typeConverter=TypeConverters.toFloat)\n+    optimizeDocConcentration = Param(Params._dummy(), \"optimizeDocConcentration\",\n+                                     \"Indicates whether the docConcentration (Dirichlet parameter \"\n+                                     \"for document-topic distribution) will be optimized during \"\n+                                     \"training.\", typeConverter=TypeConverters.toBoolean)\n+    docConcentration = Param(Params._dummy(), \"docConcentration\",\n+                             \"Concentration parameter (commonly named \\\"alpha\\\") for the \"\n+                             \"prior placed on documents' distributions over topics (\\\"theta\\\").\",\n+                             typeConverter=TypeConverters.toListFloat)\n+    topicConcentration = Param(Params._dummy(), \"topicConcentration\",\n+                               \"Concentration parameter (commonly named \\\"beta\\\" or \\\"eta\\\") for \"\n+                               \"the prior placed on topic' distributions over terms.\",\n+                               typeConverter=TypeConverters.toFloat)\n+    topicDistributionCol = Param(Params._dummy(), \"topicDistributionCol\",\n+                                 \"Output column with estimates of the topic mixture distribution \"\n+                                 \"for each document (often called \\\"theta\\\" in the literature). \"\n+                                 \"Returns a vector of zeros for an empty document.\",\n+                                 typeConverter=TypeConverters.toString)\n+\n+    @keyword_only\n+    def __init__(self, featuresCol=\"features\", k=10,\n+                 optimizer=\"online\", learningOffset=1024.0, learningDecay=0.51,\n+                 subsamplingRate=0.05, optimizeDocConcentration=True,\n+                 checkpointInterval=10, maxIter=20, docConcentration=None,\n+                 topicConcentration=None, topicDistributionCol=\"topicDistribution\", seed=None):\n+        \"\"\"\n+        __init__(self, featuresCol=\"features\", k=10, \\\n+                 optimizer=\"online\", learningOffset=1024.0, learningDecay=0.51, \\\n+                 subsamplingRate=0.05, optimizeDocConcentration=True, \\\n+                 checkpointInterval=10, maxIter=20, docConcentration=None, \\\n+                 topicConcentration=None, topicDistributionCol=\"topicDistribution\", seed=None):\n+        \"\"\"\n+        super(LDA, self).__init__()\n+        self._java_obj = self._new_java_obj(\"org.apache.spark.ml.clustering.LDA\", self.uid)\n+        self._setDefault(k=10, optimizer=\"online\", learningOffset=1024.0, learningDecay=0.51,\n+                         subsamplingRate=0.05, optimizeDocConcentration=True,\n+                         checkpointInterval=10, maxIter=20)"
  }],
  "prId": 12723
}, {
  "comments": [{
    "author": {
      "login": "yanboliang"
    },
    "body": "#12166 has been merged, it would better to add `keepLastCheckpoint` and corresponding setter/getter.\n",
    "commit": "f37c1c1d4e0c15ccc7182b8ee074d77bab5b89bd",
    "createdAt": "2016-04-27T13:34:32Z",
    "diffHunk": "@@ -453,6 +454,447 @@ def _create_model(self, java_model):\n         return BisectingKMeansModel(java_model)\n \n \n+class LDAModel(JavaModel):\n+    \"\"\"\n+    Latent Dirichlet Allocation (LDA) model.\n+    This abstraction permits for different underlying representations,\n+    including local and distributed data structures.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    @since(\"2.0.0\")\n+    def isDistributed(self):\n+        \"\"\"\n+        Indicates whether this instance is of type DistributedLDAModel\n+        \"\"\"\n+        return self._call_java(\"isDistributed\")\n+\n+    @since(\"2.0.0\")\n+    def vocabSize(self):\n+        \"\"\"Vocabulary size (number of terms or words in the vocabulary)\"\"\"\n+        return self._call_java(\"vocabSize\")\n+\n+    @since(\"2.0.0\")\n+    def topicsMatrix(self):\n+        \"\"\"\n+        Inferred topics, where each topic is represented by a distribution over terms.\n+        This is a matrix of size vocabSize x k, where each column is a topic.\n+        No guarantees are given about the ordering of the topics.\n+\n+        WARNING: If this model is actually a :py:class:`DistributedLDAModel` instance produced by\n+        the Expectation-Maximization (\"em\") `optimizer`, then this method could involve\n+        collecting a large amount of data to the driver (on the order of vocabSize x k).\n+        \"\"\"\n+        return self._call_java(\"topicsMatrix\")\n+\n+    @since(\"2.0.0\")\n+    def logLikelihood(self, dataset):\n+        \"\"\"\n+        Calculates a lower bound on the log likelihood of the entire corpus.\n+        See Equation (16) in the Online LDA paper (Hoffman et al., 2010).\n+\n+        WARNING: If this model is an instance of :py:class:`DistributedLDAModel` (produced when\n+        :py:attr:`optimizer` is set to \"em\"), this involves collecting a large\n+        :py:func:`topicsMatrix` to the driver. This implementation may be changed in the future.\n+        \"\"\"\n+        return self._call_java(\"logLikelihood\", dataset)\n+\n+    @since(\"2.0.0\")\n+    def logPerplexity(self, dataset):\n+        \"\"\"\n+        Calculate an upper bound bound on perplexity.  (Lower is better.)\n+        See Equation (16) in the Online LDA paper (Hoffman et al., 2010).\n+\n+        WARNING: If this model is an instance of :py:class:`DistributedLDAModel` (produced when\n+        :py:attr:`optimizer` is set to \"em\"), this involves collecting a large\n+        :py:func:`topicsMatrix` to the driver. This implementation may be changed in the future.\n+        \"\"\"\n+        return self._call_java(\"logPerplexity\", dataset)\n+\n+    @since(\"2.0.0\")\n+    def describeTopics(self, maxTermsPerTopic=10):\n+        \"\"\"\n+        Return the topics described by their top-weighted terms.\n+        \"\"\"\n+        return self._call_java(\"describeTopics\", maxTermsPerTopic)\n+\n+    @since(\"2.0.0\")\n+    def estimatedDocConcentration(self):\n+        \"\"\"\n+        Value for :py:attr:`LDA.docConcentration` estimated from data.\n+        If Online LDA was used and :py:attr::`LDA.optimizeDocConcentration` was set to false,\n+        then this returns the fixed (given) value for the :py:attr:`LDA.docConcentration` parameter.\n+        \"\"\"\n+        return self._call_java(\"estimatedDocConcentration\")\n+\n+\n+class DistributedLDAModel(LDAModel, JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Distributed model fitted by :py:class:`LDA`.\n+    This type of model is currently only produced by Expectation-Maximization (EM).\n+\n+    This model stores the inferred topics, the full training dataset, and the topic distribution\n+    for each training document.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    @since(\"2.0.0\")\n+    def toLocal(self):\n+        \"\"\"\n+        Convert this distributed model to a local representation.  This discards info about the\n+        training dataset.\n+\n+        WARNING: This involves collecting a large :py:func:`topicsMatrix` to the driver.\n+        \"\"\"\n+        return LocalLDAModel(self._call_java(\"toLocal\"))\n+\n+    @since(\"2.0.0\")\n+    def trainingLogLikelihood(self):\n+        \"\"\"\n+        Log likelihood of the observed tokens in the training set,\n+        given the current parameter estimates:\n+        log P(docs | topics, topic distributions for docs, Dirichlet hyperparameters)\n+\n+        Notes:\n+          - This excludes the prior; for that, use :py:func:`logPrior`.\n+          - Even with :py:func:`logPrior`, this is NOT the same as the data log likelihood given\n+            the hyperparameters.\n+          - This is computed from the topic distributions computed during training. If you call\n+            :py:func:`logLikelihood` on the same training dataset, the topic distributions\n+            will be computed again, possibly giving different results.\n+        \"\"\"\n+        return self._call_java(\"trainingLogLikelihood\")\n+\n+    @since(\"2.0.0\")\n+    def logPrior(self):\n+        \"\"\"\n+        Log probability of the current parameter estimate:\n+        log P(topics, topic distributions for docs | alpha, eta)\n+        \"\"\"\n+        return self._call_java(\"logPrior\")\n+\n+    @since(\"2.0.0\")\n+    def getCheckpointFiles(self):\n+        \"\"\"\n+        If using checkpointing and :py:attr:`LDA.keepLastCheckpoint` is set to true, then there may\n+        be saved checkpoint files.  This method is provided so that users can manage those files.\n+\n+        Note that removing the checkpoints can cause failures if a partition is lost and is needed\n+        by certain :py:class:`DistributedLDAModel` methods.  Reference counting will clean up the\n+        checkpoints when this model and derivative data go out of scope.\n+\n+        :return  List of checkpoint files from training\n+        \"\"\"\n+        return self._call_java(\"getCheckpointFiles\")\n+\n+\n+class LocalLDAModel(LDAModel, JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Local (non-distributed) model fitted by :py:class:`LDA`.\n+    This model stores the inferred topics only; it does not store info about the training dataset.\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+    pass\n+\n+\n+class LDA(JavaEstimator, HasFeaturesCol, HasMaxIter, HasSeed, HasCheckpointInterval,\n+          JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    Latent Dirichlet Allocation (LDA), a topic model designed for text documents.\n+\n+    Terminology:\n+\n+     - \"term\" = \"word\": an el\n+     - \"token\": instance of a term appearing in a document\n+     - \"topic\": multinomial distribution over terms representing some concept\n+     - \"document\": one piece of text, corresponding to one row in the input data\n+\n+    Original LDA paper (journal version):\n+      Blei, Ng, and Jordan.  \"Latent Dirichlet Allocation.\"  JMLR, 2003.\n+\n+    Input data (featuresCol):\n+    LDA is given a collection of documents as input data, via the featuresCol parameter.\n+    Each document is specified as a :py:class:`Vector` of length vocabSize, where each entry is the\n+    count for the corresponding term (word) in the document.  Feature transformers such as\n+    :py:class:`pyspark.ml.feature.Tokenizer` and :py:class:`pyspark.ml.feature.CountVectorizer`\n+    can be useful for converting text to word count vectors.\n+\n+    >>> from pyspark.mllib.linalg import Vectors, SparseVector\n+    >>> from pyspark.ml.clustering import LDA\n+    >>> df = sqlContext.createDataFrame([[1, Vectors.dense([0.0, 1.0])],\n+    ...      [2, SparseVector(2, {0: 1.0})],], [\"id\", \"features\"])\n+    >>> lda = LDA(k=2, seed=1, optimizer=\"em\")\n+    >>> model = lda.fit(df)\n+    >>> model.isDistributed()\n+    True\n+    >>> localModel = model.toLocal()\n+    >>> localModel.isDistributed()\n+    False\n+    >>> model.vocabSize()\n+    2\n+    >>> model.describeTopics().show()\n+    +-----+-----------+--------------------+\n+    |topic|termIndices|         termWeights|\n+    +-----+-----------+--------------------+\n+    |    0|     [1, 0]|[0.50401530077160...|\n+    |    1|     [0, 1]|[0.50401530077160...|\n+    +-----+-----------+--------------------+\n+    ...\n+    >>> model.topicsMatrix()\n+    DenseMatrix(2, 2, [0.496, 0.504, 0.504, 0.496], 0)\n+\n+    .. versionadded:: 2.0.0\n+    \"\"\"\n+\n+    k = Param(Params._dummy(), \"k\", \"number of topics (clusters) to infer\",\n+              typeConverter=TypeConverters.toInt)\n+    optimizer = Param(Params._dummy(), \"optimizer\",\n+                      \"Optimizer or inference algorithm used to estimate the LDA model.  \"\n+                      \"Supported: online, em\", typeConverter=TypeConverters.toString)\n+    learningOffset = Param(Params._dummy(), \"learningOffset\",\n+                           \"A (positive) learning parameter that downweights early iterations.\"\n+                           \" Larger values make early iterations count less\",\n+                           typeConverter=TypeConverters.toFloat)\n+    learningDecay = Param(Params._dummy(), \"learningDecay\", \"Learning rate, set as an\"\n+                          \"exponential decay rate. This should be between (0.5, 1.0] to \"\n+                          \"guarantee asymptotic convergence.\", typeConverter=TypeConverters.toFloat)\n+    subsamplingRate = Param(Params._dummy(), \"subsamplingRate\",\n+                            \"Fraction of the corpus to be sampled and used in each iteration \"\n+                            \"of mini-batch gradient descent, in range (0, 1].\",\n+                            typeConverter=TypeConverters.toFloat)\n+    optimizeDocConcentration = Param(Params._dummy(), \"optimizeDocConcentration\",\n+                                     \"Indicates whether the docConcentration (Dirichlet parameter \"\n+                                     \"for document-topic distribution) will be optimized during \"\n+                                     \"training.\", typeConverter=TypeConverters.toBoolean)\n+    docConcentration = Param(Params._dummy(), \"docConcentration\",\n+                             \"Concentration parameter (commonly named \\\"alpha\\\") for the \"\n+                             \"prior placed on documents' distributions over topics (\\\"theta\\\").\",\n+                             typeConverter=TypeConverters.toListFloat)\n+    topicConcentration = Param(Params._dummy(), \"topicConcentration\",\n+                               \"Concentration parameter (commonly named \\\"beta\\\" or \\\"eta\\\") for \"\n+                               \"the prior placed on topic' distributions over terms.\",\n+                               typeConverter=TypeConverters.toFloat)\n+    topicDistributionCol = Param(Params._dummy(), \"topicDistributionCol\",\n+                                 \"Output column with estimates of the topic mixture distribution \"\n+                                 \"for each document (often called \\\"theta\\\" in the literature). \"\n+                                 \"Returns a vector of zeros for an empty document.\",\n+                                 typeConverter=TypeConverters.toString)",
    "line": 262
  }],
  "prId": 12723
}]