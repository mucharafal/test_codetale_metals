[{
  "comments": [{
    "author": {
      "login": "thunterdb"
    },
    "body": "can you explain why you need this change (and add a comment about it)?\n",
    "commit": "cc85a929afbf992abf1aa47bb0c938a5c804f908",
    "createdAt": "2016-05-20T18:54:15Z",
    "diffHunk": "@@ -316,17 +335,27 @@ def setParams(self, predictionCol=\"prediction\", labelCol=\"label\",\n \n if __name__ == \"__main__\":\n     import doctest\n+    import tempfile\n+    import pyspark.ml.evaluation\n     from pyspark.context import SparkContext\n     from pyspark.sql import SQLContext\n-    globs = globals().copy()\n+    globs = pyspark.ml.evaluation.__dict__.copy()"
  }, {
    "author": {
      "login": "yanboliang"
    },
    "body": "@thunterdb If we did not make this change, we can work well when using `./bin/pyspark` to run all tests, but it failed when run with `./python/run-tests --modules=pyspark-ml`.This issue is weird, I still not figure out the root cause, and we did the same changes when we add other save/load.\n",
    "commit": "cc85a929afbf992abf1aa47bb0c938a5c804f908",
    "createdAt": "2016-05-21T05:19:07Z",
    "diffHunk": "@@ -316,17 +335,27 @@ def setParams(self, predictionCol=\"prediction\", labelCol=\"label\",\n \n if __name__ == \"__main__\":\n     import doctest\n+    import tempfile\n+    import pyspark.ml.evaluation\n     from pyspark.context import SparkContext\n     from pyspark.sql import SQLContext\n-    globs = globals().copy()\n+    globs = pyspark.ml.evaluation.__dict__.copy()"
  }],
  "prId": 13194
}, {
  "comments": [{
    "author": {
      "login": "BryanCutler"
    },
    "body": "Just wondering why change to import from `mllib.common`?\n",
    "commit": "cc85a929afbf992abf1aa47bb0c938a5c804f908",
    "createdAt": "2016-10-11T17:46:43Z",
    "diffHunk": "@@ -21,7 +21,8 @@\n from pyspark.ml.wrapper import JavaParams\n from pyspark.ml.param import Param, Params, TypeConverters\n from pyspark.ml.param.shared import HasLabelCol, HasPredictionCol, HasRawPredictionCol\n-from pyspark.ml.common import inherit_doc\n+from pyspark.ml.util import JavaMLReadable, JavaMLWritable\n+from pyspark.mllib.common import inherit_doc"
  }, {
    "author": {
      "login": "yanboliang"
    },
    "body": "Oops, typo.\n",
    "commit": "cc85a929afbf992abf1aa47bb0c938a5c804f908",
    "createdAt": "2016-10-12T15:31:56Z",
    "diffHunk": "@@ -21,7 +21,8 @@\n from pyspark.ml.wrapper import JavaParams\n from pyspark.ml.param import Param, Params, TypeConverters\n from pyspark.ml.param.shared import HasLabelCol, HasPredictionCol, HasRawPredictionCol\n-from pyspark.ml.common import inherit_doc\n+from pyspark.ml.util import JavaMLReadable, JavaMLWritable\n+from pyspark.mllib.common import inherit_doc"
  }],
  "prId": 13194
}, {
  "comments": [{
    "author": {
      "login": "BryanCutler"
    },
    "body": "should there still be an `exit(-1)` when failures?  If not then you can remove the returned variables from `doctest.testmod`\n",
    "commit": "cc85a929afbf992abf1aa47bb0c938a5c804f908",
    "createdAt": "2016-10-11T20:08:24Z",
    "diffHunk": "@@ -311,19 +330,25 @@ def setParams(self, predictionCol=\"prediction\", labelCol=\"label\",\n \n if __name__ == \"__main__\":\n     import doctest\n+    import tempfile\n+    import pyspark.ml.evaluation\n     from pyspark.sql import SparkSession\n-    globs = globals().copy()\n+    globs = pyspark.ml.evaluation.__dict__.copy()\n     # The small batch size here ensures that we see multiple batches,\n     # even in these small test examples:\n     spark = SparkSession.builder\\\n         .master(\"local[2]\")\\\n         .appName(\"ml.evaluation tests\")\\\n         .getOrCreate()\n-    sc = spark.sparkContext\n-    globs['sc'] = sc\n     globs['spark'] = spark\n-    (failure_count, test_count) = doctest.testmod(\n-        globs=globs, optionflags=doctest.ELLIPSIS)\n-    spark.stop()\n-    if failure_count:\n-        exit(-1)"
  }, {
    "author": {
      "login": "yanboliang"
    },
    "body": "Thanks for remind, added check `failure_count`.\n",
    "commit": "cc85a929afbf992abf1aa47bb0c938a5c804f908",
    "createdAt": "2016-10-12T15:33:01Z",
    "diffHunk": "@@ -311,19 +330,25 @@ def setParams(self, predictionCol=\"prediction\", labelCol=\"label\",\n \n if __name__ == \"__main__\":\n     import doctest\n+    import tempfile\n+    import pyspark.ml.evaluation\n     from pyspark.sql import SparkSession\n-    globs = globals().copy()\n+    globs = pyspark.ml.evaluation.__dict__.copy()\n     # The small batch size here ensures that we see multiple batches,\n     # even in these small test examples:\n     spark = SparkSession.builder\\\n         .master(\"local[2]\")\\\n         .appName(\"ml.evaluation tests\")\\\n         .getOrCreate()\n-    sc = spark.sparkContext\n-    globs['sc'] = sc\n     globs['spark'] = spark\n-    (failure_count, test_count) = doctest.testmod(\n-        globs=globs, optionflags=doctest.ELLIPSIS)\n-    spark.stop()\n-    if failure_count:\n-        exit(-1)"
  }],
  "prId": 13194
}]