[{
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "space after \"Hashing\"",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-07T16:52:06Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters."
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Done.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-08T22:52:28Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters."
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "space here too",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-07T16:52:31Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) models."
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Done.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-08T22:53:42Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) models."
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "Can we just leave singe probing out since it has no effect and we aren't including it in the doc?",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-07T16:54:51Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, singleProbing=True,"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Done.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-08T22:53:47Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, singleProbing=True,"
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "\"two datasets\"",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-07T16:56:51Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, singleProbing=True,\n+                               distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two dataset to approximately find all pairs of rows whose distance are smaller than"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Done.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-08T22:53:57Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, singleProbing=True,\n+                               distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two dataset to approximately find all pairs of rows whose distance are smaller than"
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "This notation is not correct. For Python it should be `Vectors.sparse(10, [(2, 1.0), (3, 1.0), (5, 1.0)])`.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-07T16:58:03Z",
    "diffHunk": "@@ -755,6 +951,102 @@ def maxAbs(self):\n \n \n @inherit_doc\n+class MinHashLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                 JavaMLReadable, JavaMLWritable):\n+\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Jaccard distance.\n+    The input can be dense or sparse vectors, but it is more efficient if it is sparse.\n+    For example, `Vectors.sparse(10, Array[(2, 1.0), (3, 1.0), (5, 1.0)])`"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Fixed",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-08T22:58:03Z",
    "diffHunk": "@@ -755,6 +951,102 @@ def maxAbs(self):\n \n \n @inherit_doc\n+class MinHashLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                 JavaMLReadable, JavaMLWritable):\n+\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Jaccard distance.\n+    The input can be dense or sparse vectors, but it is more efficient if it is sparse.\n+    For example, `Vectors.sparse(10, Array[(2, 1.0), (3, 1.0), (5, 1.0)])`"
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "Can you match the Scala doc here and below?",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-07T16:58:59Z",
    "diffHunk": "@@ -755,6 +951,102 @@ def maxAbs(self):\n \n \n @inherit_doc\n+class MinHashLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                 JavaMLReadable, JavaMLWritable):\n+\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Jaccard distance.\n+    The input can be dense or sparse vectors, but it is more efficient if it is sparse.\n+    For example, `Vectors.sparse(10, Array[(2, 1.0), (3, 1.0), (5, 1.0)])`\n+    means there are 10 elements in the space. This set contains elem 2, elem 3 and elem 5."
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Done.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-08T22:57:58Z",
    "diffHunk": "@@ -755,6 +951,102 @@ def maxAbs(self):\n \n \n @inherit_doc\n+class MinHashLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                 JavaMLReadable, JavaMLWritable):\n+\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Jaccard distance.\n+    The input can be dense or sparse vectors, but it is more efficient if it is sparse.\n+    For example, `Vectors.sparse(10, Array[(2, 1.0), (3, 1.0), (5, 1.0)])`\n+    means there are 10 elements in the space. This set contains elem 2, elem 3 and elem 5."
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "\"Hash values in the same dimension are calculated...\"",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-07T19:04:15Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, singleProbing=True,\n+                               distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two dataset to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A joined dataset containing pairs of rows. The original rows are in columns\n+                \"datasetA\" and \"datasetB\", and a distCol is added to show the distance of\n+                each pair.\n+        \"\"\"\n+        return self._call_java(\"approxSimilarityJoin\", datasetA, datasetB, threshold, distCol)\n+\n+\n+@inherit_doc\n+class BucketedRandomProjectionLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                                  JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Euclidean distance metrics.\n+    The input is dense or sparse vectors, each of which represents a point in the Euclidean\n+    distance space. The output will be vectors of configurable dimension. Hash value in the"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Done in Scala/Java doc as well.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-08T22:54:18Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, singleProbing=True,\n+                               distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two dataset to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A joined dataset containing pairs of rows. The original rows are in columns\n+                \"datasetA\" and \"datasetB\", and a distCol is added to show the distance of\n+                each pair.\n+        \"\"\"\n+        return self._call_java(\"approxSimilarityJoin\", datasetA, datasetB, threshold, distCol)\n+\n+\n+@inherit_doc\n+class BucketedRandomProjectionLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                                  JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Euclidean distance metrics.\n+    The input is dense or sparse vectors, each of which represents a point in the Euclidean\n+    distance space. The output will be vectors of configurable dimension. Hash value in the"
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "Make this inherit `JavaModel` and then the subclasses just inherit this.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-07T19:40:43Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Done.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-08T22:52:33Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():"
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "can we call them \"features\" and \"hashes\" ? I'm open to other names but \"keys\" and \"values\" is unclear to me\r\n\r\nEDIT: I see this is the Scala example convention. I still think \"features\" and \"hashes\" is better, but either way is acceptable.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-07T19:52:14Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, singleProbing=True,\n+                               distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two dataset to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A joined dataset containing pairs of rows. The original rows are in columns\n+                \"datasetA\" and \"datasetB\", and a distCol is added to show the distance of\n+                each pair.\n+        \"\"\"\n+        return self._call_java(\"approxSimilarityJoin\", datasetA, datasetB, threshold, distCol)\n+\n+\n+@inherit_doc\n+class BucketedRandomProjectionLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                                  JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Euclidean distance metrics.\n+    The input is dense or sparse vectors, each of which represents a point in the Euclidean\n+    distance space. The output will be vectors of configurable dimension. Hash value in the\n+    same dimension is calculated by the same hash function.\n+\n+    .. seealso:: `Stable Distributions \\\n+    <https://en.wikipedia.org/wiki/Locality-sensitive_hashing#Stable_distributions>`_\n+    .. seealso:: `Hashing for Similarity Search: A Survey <https://arxiv.org/abs/1408.2927>`_\n+\n+    >>> from pyspark.ml.linalg import Vectors\n+    >>> data = [(Vectors.dense([-1.0, -1.0 ]),),\n+    ...         (Vectors.dense([-1.0, 1.0 ]),),\n+    ...         (Vectors.dense([1.0, -1.0 ]),),\n+    ...         (Vectors.dense([1.0, 1.0]),)]\n+    >>> df = spark.createDataFrame(data, [\"keys\"])\n+    >>> rp = BucketedRandomProjectionLSH(inputCol=\"keys\", outputCol=\"values\","
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "I agree with you. I have changed the terms to \"features\" and \"hashes\"",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-08T23:33:35Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, singleProbing=True,\n+                               distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two dataset to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A joined dataset containing pairs of rows. The original rows are in columns\n+                \"datasetA\" and \"datasetB\", and a distCol is added to show the distance of\n+                each pair.\n+        \"\"\"\n+        return self._call_java(\"approxSimilarityJoin\", datasetA, datasetB, threshold, distCol)\n+\n+\n+@inherit_doc\n+class BucketedRandomProjectionLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                                  JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Euclidean distance metrics.\n+    The input is dense or sparse vectors, each of which represents a point in the Euclidean\n+    distance space. The output will be vectors of configurable dimension. Hash value in the\n+    same dimension is calculated by the same hash function.\n+\n+    .. seealso:: `Stable Distributions \\\n+    <https://en.wikipedia.org/wiki/Locality-sensitive_hashing#Stable_distributions>`_\n+    .. seealso:: `Hashing for Similarity Search: A Survey <https://arxiv.org/abs/1408.2927>`_\n+\n+    >>> from pyspark.ml.linalg import Vectors\n+    >>> data = [(Vectors.dense([-1.0, -1.0 ]),),\n+    ...         (Vectors.dense([-1.0, 1.0 ]),),\n+    ...         (Vectors.dense([1.0, -1.0 ]),),\n+    ...         (Vectors.dense([1.0, 1.0]),)]\n+    >>> df = spark.createDataFrame(data, [\"keys\"])\n+    >>> rp = BucketedRandomProjectionLSH(inputCol=\"keys\", outputCol=\"values\","
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "Since doctests are also partly used to demonstrate the usage of the algorithm, I don't think this line is particularly useful. It is quite hard to interpret. I think it might be nicer to add an \"id\" column to the dataframes and then do a \"show\" here to see the joined dataframes, as in the Scala example. Then again, you end up with:\r\n\r\n````\r\n+--------------------+--------------------+----------------+\r\n|            datasetA|            datasetB|         distCol|\r\n+--------------------+--------------------+----------------+\r\n|[[1.0,1.0],Wrappe...|[[3.0,2.0],Wrappe...|2.23606797749979|\r\n+--------------------+--------------------+----------------+\r\n````\r\nWhich is also confusing! Thoughts on which option is better?",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-07T20:41:32Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, singleProbing=True,\n+                               distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two dataset to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A joined dataset containing pairs of rows. The original rows are in columns\n+                \"datasetA\" and \"datasetB\", and a distCol is added to show the distance of\n+                each pair.\n+        \"\"\"\n+        return self._call_java(\"approxSimilarityJoin\", datasetA, datasetB, threshold, distCol)\n+\n+\n+@inherit_doc\n+class BucketedRandomProjectionLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                                  JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Euclidean distance metrics.\n+    The input is dense or sparse vectors, each of which represents a point in the Euclidean\n+    distance space. The output will be vectors of configurable dimension. Hash value in the\n+    same dimension is calculated by the same hash function.\n+\n+    .. seealso:: `Stable Distributions \\\n+    <https://en.wikipedia.org/wiki/Locality-sensitive_hashing#Stable_distributions>`_\n+    .. seealso:: `Hashing for Similarity Search: A Survey <https://arxiv.org/abs/1408.2927>`_\n+\n+    >>> from pyspark.ml.linalg import Vectors\n+    >>> data = [(Vectors.dense([-1.0, -1.0 ]),),\n+    ...         (Vectors.dense([-1.0, 1.0 ]),),\n+    ...         (Vectors.dense([1.0, -1.0 ]),),\n+    ...         (Vectors.dense([1.0, 1.0]),)]\n+    >>> df = spark.createDataFrame(data, [\"keys\"])\n+    >>> rp = BucketedRandomProjectionLSH(inputCol=\"keys\", outputCol=\"values\",\n+    ...                                  seed=12345, bucketLength=1.0)\n+    >>> model = rp.fit(df)\n+    >>> model.randUnitVectors\n+    [DenseVector([-0.3041, 0.9527])]\n+    >>> model.transform(df).head()\n+    Row(keys=DenseVector([-1.0, -1.0]), values=[DenseVector([-1.0])])\n+    >>> data2 = [(Vectors.dense([2.0, 2.0 ]),),\n+    ...          (Vectors.dense([2.0, 3.0 ]),),\n+    ...          (Vectors.dense([3.0, 2.0 ]),),\n+    ...          (Vectors.dense([3.0, 3.0]),)]\n+    >>> df2 = spark.createDataFrame(data2, [\"keys\"])\n+    >>> model.approxNearestNeighbors(df2, Vectors.dense([1.0, 2.0]), 1).collect()\n+    [Row(keys=DenseVector([2.0, 2.0]), values=[DenseVector([1.0])], distCol=1.0)]\n+    >>> model.approxSimilarityJoin(df, df2, 3.0).select(\"distCol\").head()[0]"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "I think showing the ids would be more interpretable, as users are able to see the feature vectors of the ids from the examples.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-08T23:36:51Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, singleProbing=True,\n+                               distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two dataset to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A joined dataset containing pairs of rows. The original rows are in columns\n+                \"datasetA\" and \"datasetB\", and a distCol is added to show the distance of\n+                each pair.\n+        \"\"\"\n+        return self._call_java(\"approxSimilarityJoin\", datasetA, datasetB, threshold, distCol)\n+\n+\n+@inherit_doc\n+class BucketedRandomProjectionLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                                  JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Euclidean distance metrics.\n+    The input is dense or sparse vectors, each of which represents a point in the Euclidean\n+    distance space. The output will be vectors of configurable dimension. Hash value in the\n+    same dimension is calculated by the same hash function.\n+\n+    .. seealso:: `Stable Distributions \\\n+    <https://en.wikipedia.org/wiki/Locality-sensitive_hashing#Stable_distributions>`_\n+    .. seealso:: `Hashing for Similarity Search: A Survey <https://arxiv.org/abs/1408.2927>`_\n+\n+    >>> from pyspark.ml.linalg import Vectors\n+    >>> data = [(Vectors.dense([-1.0, -1.0 ]),),\n+    ...         (Vectors.dense([-1.0, 1.0 ]),),\n+    ...         (Vectors.dense([1.0, -1.0 ]),),\n+    ...         (Vectors.dense([1.0, 1.0]),)]\n+    >>> df = spark.createDataFrame(data, [\"keys\"])\n+    >>> rp = BucketedRandomProjectionLSH(inputCol=\"keys\", outputCol=\"values\",\n+    ...                                  seed=12345, bucketLength=1.0)\n+    >>> model = rp.fit(df)\n+    >>> model.randUnitVectors\n+    [DenseVector([-0.3041, 0.9527])]\n+    >>> model.transform(df).head()\n+    Row(keys=DenseVector([-1.0, -1.0]), values=[DenseVector([-1.0])])\n+    >>> data2 = [(Vectors.dense([2.0, 2.0 ]),),\n+    ...          (Vectors.dense([2.0, 3.0 ]),),\n+    ...          (Vectors.dense([3.0, 2.0 ]),),\n+    ...          (Vectors.dense([3.0, 3.0]),)]\n+    >>> df2 = spark.createDataFrame(data2, [\"keys\"])\n+    >>> model.approxNearestNeighbors(df2, Vectors.dense([1.0, 2.0]), 1).collect()\n+    [Row(keys=DenseVector([2.0, 2.0]), values=[DenseVector([1.0])], distCol=1.0)]\n+    >>> model.approxSimilarityJoin(df, df2, 3.0).select(\"distCol\").head()[0]"
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "This should not be exposed, since it's private in Scala. Also, `Array[(Int, Int)]` does not serialize to Python.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-07T21:05:20Z",
    "diffHunk": "@@ -755,6 +951,102 @@ def maxAbs(self):\n \n \n @inherit_doc\n+class MinHashLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                 JavaMLReadable, JavaMLWritable):\n+\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Jaccard distance.\n+    The input can be dense or sparse vectors, but it is more efficient if it is sparse.\n+    For example, `Vectors.sparse(10, Array[(2, 1.0), (3, 1.0), (5, 1.0)])`\n+    means there are 10 elements in the space. This set contains elem 2, elem 3 and elem 5.\n+    Also, any input vector must have at least 1 non-zero indices, and all non-zero values\n+    are treated as binary \"1\" values.\n+\n+    .. seealso:: `MinHash <https://en.wikipedia.org/wiki/MinHash>`_\n+\n+    >>> from pyspark.ml.linalg import Vectors\n+    >>> data = [(Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),),\n+    ...         (Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),),\n+    ...         (Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),)]\n+    >>> df = spark.createDataFrame(data, [\"keys\"])\n+    >>> mh = MinHashLSH(inputCol=\"keys\", outputCol=\"values\", seed=12345)\n+    >>> model = mh.fit(df)\n+    >>> model.transform(df).head()\n+    Row(keys=SparseVector(6, {0: 1.0, 1: 1.0, 2: 1.0}), values=[DenseVector([-1638925712.0])])\n+    >>> data2 = [(Vectors.sparse(6, [1, 3, 5], [1.0, 1.0, 1.0]),),\n+    ...          (Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),),\n+    ...          (Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),)]\n+    >>> df2 = spark.createDataFrame(data2, [\"keys\"])\n+    >>> key = Vectors.sparse(6, [1], [1.0])\n+    >>> model.approxNearestNeighbors(df2, key, 1).select(\"distCol\").head()[0]\n+    0.66666...\n+    >>> model.approxSimilarityJoin(df, df2, 1.0).select(\"distCol\").head()[0]\n+    0.5\n+    >>> mhPath = temp_path + \"/mh\"\n+    >>> mh.save(mhPath)\n+    >>> mh2 = MinHashLSH.load(mhPath)\n+    >>> mh2.getOutputCol() == mh.getOutputCol()\n+    True\n+    >>> modelPath = temp_path + \"/mh-model\"\n+    >>> model.save(modelPath)\n+    >>> model2 = MinHashLSHModel.load(modelPath)\n+\n+    .. versionadded:: 2.2.0\n+    \"\"\"\n+\n+    @keyword_only\n+    def __init__(self, inputCol=None, outputCol=None, seed=None, numHashTables=1):\n+        \"\"\"\n+        __init__(self, inputCol=None, outputCol=None, seed=None, numHashTables=1)\n+        \"\"\"\n+        super(MinHashLSH, self).__init__()\n+        self._java_obj = self._new_java_obj(\"org.apache.spark.ml.feature.MinHashLSH\", self.uid)\n+        self._setDefault(numHashTables=1)\n+        kwargs = self.__init__._input_kwargs\n+        self.setParams(**kwargs)\n+\n+    @keyword_only\n+    @since(\"2.2.0\")\n+    def setParams(self, inputCol=None, outputCol=None, seed=None, numHashTables=1):\n+        \"\"\"\n+        setParams(self, inputCol=None, outputCol=None, seed=None, numHashTables=1)\n+        Sets params for this MinHashLSH.\n+        \"\"\"\n+        kwargs = self.setParams._input_kwargs\n+        return self._set(**kwargs)\n+\n+    def _create_model(self, java_model):\n+        return MinHashLSHModel(java_model)\n+\n+\n+class MinHashLSHModel(JavaModel, LSHModel, JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Model produced by :py:class:`MinHashLSH`, where where multiple hash functions are stored. Each\n+    hash function is picked from the following family of hash functions, where :math:`a_i` and\n+    :math:`b_i` are randomly chosen integers less than prime:\n+    :math:`h_i(x) = ((x \\cdot a_i + b_i) \\mod prime)` This hash family is approximately min-wise\n+    independent according to the reference.\n+\n+    .. seealso:: Tom Bohman, Colin Cooper, and Alan Frieze. \"Min-wise independent linear \\\n+    permutations.\" Electronic Journal of Combinatorics 7 (2000): R26.\n+\n+    .. versionadded:: 2.2.0\n+    \"\"\"\n+\n+    @property\n+    @since(\"2.2.0\")\n+    def randCoefficients(self):"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Removed",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-08T22:57:50Z",
    "diffHunk": "@@ -755,6 +951,102 @@ def maxAbs(self):\n \n \n @inherit_doc\n+class MinHashLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                 JavaMLReadable, JavaMLWritable):\n+\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Jaccard distance.\n+    The input can be dense or sparse vectors, but it is more efficient if it is sparse.\n+    For example, `Vectors.sparse(10, Array[(2, 1.0), (3, 1.0), (5, 1.0)])`\n+    means there are 10 elements in the space. This set contains elem 2, elem 3 and elem 5.\n+    Also, any input vector must have at least 1 non-zero indices, and all non-zero values\n+    are treated as binary \"1\" values.\n+\n+    .. seealso:: `MinHash <https://en.wikipedia.org/wiki/MinHash>`_\n+\n+    >>> from pyspark.ml.linalg import Vectors\n+    >>> data = [(Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),),\n+    ...         (Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),),\n+    ...         (Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),)]\n+    >>> df = spark.createDataFrame(data, [\"keys\"])\n+    >>> mh = MinHashLSH(inputCol=\"keys\", outputCol=\"values\", seed=12345)\n+    >>> model = mh.fit(df)\n+    >>> model.transform(df).head()\n+    Row(keys=SparseVector(6, {0: 1.0, 1: 1.0, 2: 1.0}), values=[DenseVector([-1638925712.0])])\n+    >>> data2 = [(Vectors.sparse(6, [1, 3, 5], [1.0, 1.0, 1.0]),),\n+    ...          (Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),),\n+    ...          (Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),)]\n+    >>> df2 = spark.createDataFrame(data2, [\"keys\"])\n+    >>> key = Vectors.sparse(6, [1], [1.0])\n+    >>> model.approxNearestNeighbors(df2, key, 1).select(\"distCol\").head()[0]\n+    0.66666...\n+    >>> model.approxSimilarityJoin(df, df2, 1.0).select(\"distCol\").head()[0]\n+    0.5\n+    >>> mhPath = temp_path + \"/mh\"\n+    >>> mh.save(mhPath)\n+    >>> mh2 = MinHashLSH.load(mhPath)\n+    >>> mh2.getOutputCol() == mh.getOutputCol()\n+    True\n+    >>> modelPath = temp_path + \"/mh-model\"\n+    >>> model.save(modelPath)\n+    >>> model2 = MinHashLSHModel.load(modelPath)\n+\n+    .. versionadded:: 2.2.0\n+    \"\"\"\n+\n+    @keyword_only\n+    def __init__(self, inputCol=None, outputCol=None, seed=None, numHashTables=1):\n+        \"\"\"\n+        __init__(self, inputCol=None, outputCol=None, seed=None, numHashTables=1)\n+        \"\"\"\n+        super(MinHashLSH, self).__init__()\n+        self._java_obj = self._new_java_obj(\"org.apache.spark.ml.feature.MinHashLSH\", self.uid)\n+        self._setDefault(numHashTables=1)\n+        kwargs = self.__init__._input_kwargs\n+        self.setParams(**kwargs)\n+\n+    @keyword_only\n+    @since(\"2.2.0\")\n+    def setParams(self, inputCol=None, outputCol=None, seed=None, numHashTables=1):\n+        \"\"\"\n+        setParams(self, inputCol=None, outputCol=None, seed=None, numHashTables=1)\n+        Sets params for this MinHashLSH.\n+        \"\"\"\n+        kwargs = self.setParams._input_kwargs\n+        return self._set(**kwargs)\n+\n+    def _create_model(self, java_model):\n+        return MinHashLSHModel(java_model)\n+\n+\n+class MinHashLSHModel(JavaModel, LSHModel, JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Model produced by :py:class:`MinHashLSH`, where where multiple hash functions are stored. Each\n+    hash function is picked from the following family of hash functions, where :math:`a_i` and\n+    :math:`b_i` are randomly chosen integers less than prime:\n+    :math:`h_i(x) = ((x \\cdot a_i + b_i) \\mod prime)` This hash family is approximately min-wise\n+    independent according to the reference.\n+\n+    .. seealso:: Tom Bohman, Colin Cooper, and Alan Frieze. \"Min-wise independent linear \\\n+    permutations.\" Electronic Journal of Combinatorics 7 (2000): R26.\n+\n+    .. versionadded:: 2.2.0\n+    \"\"\"\n+\n+    @property\n+    @since(\"2.2.0\")\n+    def randCoefficients(self):"
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "add the note that's in the scala doc?",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-07T21:09:47Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, singleProbing=True,\n+                               distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key."
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Done.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-08T22:53:53Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel():\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing(LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, singleProbing=True,\n+                               distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key."
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "The classes in this file are alphabetized for the most part. Let's keep the convention here.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-07T21:11:11Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):",
    "line": 20
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "It's not alphabetized here because the declaration order matters for PySpark shell.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-08T22:52:15Z",
    "diffHunk": "@@ -120,6 +122,200 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):",
    "line": 20
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "can we make it so the columns are \"id_a\" and \"id_b\" or something similar. And the distance col is called \"exactDistance\"",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-09T22:23:03Z",
    "diffHunk": "@@ -120,6 +122,196 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel(JavaModel):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        .. note:: This method is experimental and will likely change behavior in the next release.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two datasets to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A joined dataset containing pairs of rows. The original rows are in columns\n+                \"datasetA\" and \"datasetB\", and a distCol is added to show the distance of\n+                each pair.\n+        \"\"\"\n+        return self._call_java(\"approxSimilarityJoin\", datasetA, datasetB, threshold, distCol)\n+\n+\n+@inherit_doc\n+class BucketedRandomProjectionLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                                  JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Euclidean distance metrics.\n+    The input is dense or sparse vectors, each of which represents a point in the Euclidean\n+    distance space. The output will be vectors of configurable dimension. Hash values in the same\n+    dimension are calculated by the same hash function.\n+\n+    .. seealso:: `Stable Distributions \\\n+    <https://en.wikipedia.org/wiki/Locality-sensitive_hashing#Stable_distributions>`_\n+    .. seealso:: `Hashing for Similarity Search: A Survey <https://arxiv.org/abs/1408.2927>`_\n+\n+    >>> from pyspark.ml.linalg import Vectors\n+    >>> data = [(0, Vectors.dense([-1.0, -1.0 ]),),\n+    ...         (1, Vectors.dense([-1.0, 1.0 ]),),\n+    ...         (2, Vectors.dense([1.0, -1.0 ]),),\n+    ...         (3, Vectors.dense([1.0, 1.0]),)]\n+    >>> df = spark.createDataFrame(data, [\"id\", \"features\"])\n+    >>> brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\",\n+    ...                                   seed=12345, bucketLength=1.0)\n+    >>> model = brp.fit(df)\n+    >>> model.transform(df).head()\n+    Row(id=0, features=DenseVector([-1.0, -1.0]), hashes=[DenseVector([-1.0])])\n+    >>> data2 = [(4, Vectors.dense([2.0, 2.0 ]),),\n+    ...          (5, Vectors.dense([2.0, 3.0 ]),),\n+    ...          (6, Vectors.dense([3.0, 2.0 ]),),\n+    ...          (7, Vectors.dense([3.0, 3.0]),)]\n+    >>> df2 = spark.createDataFrame(data2, [\"id\", \"features\"])\n+    >>> model.approxNearestNeighbors(df2, Vectors.dense([1.0, 2.0]), 1).collect()\n+    [Row(id=4, features=DenseVector([2.0, 2.0]), hashes=[DenseVector([1.0])], distCol=1.0)]\n+    >>> model.approxSimilarityJoin(df, df2, 3.0).select(\"datasetA.id\","
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "same as above, except distcol could be \"jaccardSimilarity\" or something.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-09T22:23:37Z",
    "diffHunk": "@@ -755,6 +947,101 @@ def maxAbs(self):\n \n \n @inherit_doc\n+class MinHashLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                 JavaMLReadable, JavaMLWritable):\n+\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Jaccard distance.\n+    The input can be dense or sparse vectors, but it is more efficient if it is sparse.\n+    For example, `Vectors.sparse(10, [(2, 1.0), (3, 1.0), (5, 1.0)])` means there are 10 elements\n+    in the space. This set contains elements 2, 3, and 5. Also, any input vector must have at\n+    least 1 non-zero index, and all non-zero values are treated as binary \"1\" values.\n+\n+    .. seealso:: `Wikipedia on MinHash <https://en.wikipedia.org/wiki/MinHash>`_\n+\n+    >>> from pyspark.ml.linalg import Vectors\n+    >>> data = [(0, Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),),\n+    ...         (1, Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),),\n+    ...         (2, Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),)]\n+    >>> df = spark.createDataFrame(data, [\"id\", \"features\"])\n+    >>> mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", seed=12345)\n+    >>> model = mh.fit(df)\n+    >>> model.transform(df).head()\n+    Row(id=0, features=SparseVector(6, {0: 1.0, 1: 1.0, 2: 1.0}), hashes=[DenseVector([-1638925...\n+    >>> data2 = [(3, Vectors.sparse(6, [1, 3, 5], [1.0, 1.0, 1.0]),),\n+    ...          (4, Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),),\n+    ...          (5, Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),)]\n+    >>> df2 = spark.createDataFrame(data2, [\"id\", \"features\"])\n+    >>> key = Vectors.sparse(6, [1, 2], [1.0, 1.0])\n+    >>> model.approxNearestNeighbors(df2, key, 1).collect()\n+    [Row(id=5, features=SparseVector(6, {1: 1.0, 2: 1.0, 4: 1.0}), hashes=[DenseVector([-163892...\n+    >>> model.approxSimilarityJoin(df, df2, 0.6).select(\"datasetA.id\","
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "\"A distCol\" -> \"A column 'distCol'\"",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-13T18:53:32Z",
    "diffHunk": "@@ -120,6 +122,198 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel(JavaModel):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        .. note:: This method is experimental and will likely change behavior in the next release.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Done.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-14T06:19:32Z",
    "diffHunk": "@@ -120,6 +122,198 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel(JavaModel):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        .. note:: This method is experimental and will likely change behavior in the next release.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added"
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "indentation\r\n\r\n\"a distCol\" -> \"a column `distCol`\"",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-13T18:54:30Z",
    "diffHunk": "@@ -120,6 +122,198 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel(JavaModel):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        .. note:: This method is experimental and will likely change behavior in the next release.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two datasets to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A joined dataset containing pairs of rows. The original rows are in columns\n+                \"datasetA\" and \"datasetB\", and a distCol is added to show the distance of"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Done.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-14T06:19:36Z",
    "diffHunk": "@@ -120,6 +122,198 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel(JavaModel):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        .. note:: This method is experimental and will likely change behavior in the next release.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two datasets to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A joined dataset containing pairs of rows. The original rows are in columns\n+                \"datasetA\" and \"datasetB\", and a distCol is added to show the distance of"
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "I think we've decided not to put since tags in parent classes, since they'll be wrong for future derived classes.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-13T18:57:39Z",
    "diffHunk": "@@ -120,6 +122,198 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel(JavaModel):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        .. note:: This method is experimental and will likely change behavior in the next release.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Removed in 4 places.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-14T06:19:34Z",
    "diffHunk": "@@ -120,6 +122,198 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel(JavaModel):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        .. note:: This method is experimental and will likely change behavior in the next release.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")"
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "shouldn't this be \"distance between each pair of rows\", rather than \"between each result row and the key\"?",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-13T23:39:09Z",
    "diffHunk": "@@ -120,6 +122,198 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel(JavaModel):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        .. note:: This method is experimental and will likely change behavior in the next release.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two datasets to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each result row and the key."
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Fixed.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-14T06:19:49Z",
    "diffHunk": "@@ -120,6 +122,198 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel(JavaModel):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        .. note:: This method is experimental and will likely change behavior in the next release.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two datasets to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each result row and the key."
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "nit - \"distance between each pair\" rather than \"distance of\"",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-13T23:39:48Z",
    "diffHunk": "@@ -120,6 +122,198 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel(JavaModel):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        .. note:: This method is experimental and will likely change behavior in the next release.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two datasets to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A joined dataset containing pairs of rows. The original rows are in columns\n+                \"datasetA\" and \"datasetB\", and a distCol is added to show the distance of"
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Done.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-14T06:19:50Z",
    "diffHunk": "@@ -120,6 +122,198 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    @since(\"2.2.0\")\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    @since(\"2.2.0\")\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel(JavaModel):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) models.\n+    \"\"\"\n+\n+    @since(\"2.2.0\")\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        .. note:: This method is experimental and will likely change behavior in the next release.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A distCol is added\n+                 to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    @since(\"2.2.0\")\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two datasets to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A joined dataset containing pairs of rows. The original rows are in columns\n+                \"datasetA\" and \"datasetB\", and a distCol is added to show the distance of"
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "let's add an equality check here and for BRP. For example for IDFModel we have:\r\n\r\n`loadedModel.transform(df).head().idf == model.transform(df).head().idf`\r\n",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-14T07:16:43Z",
    "diffHunk": "@@ -755,6 +945,103 @@ def maxAbs(self):\n \n \n @inherit_doc\n+class MinHashLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                 JavaMLReadable, JavaMLWritable):\n+\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Jaccard distance.\n+    The input can be dense or sparse vectors, but it is more efficient if it is sparse.\n+    For example, `Vectors.sparse(10, [(2, 1.0), (3, 1.0), (5, 1.0)])` means there are 10 elements\n+    in the space. This set contains elements 2, 3, and 5. Also, any input vector must have at\n+    least 1 non-zero index, and all non-zero values are treated as binary \"1\" values.\n+\n+    .. seealso:: `Wikipedia on MinHash <https://en.wikipedia.org/wiki/MinHash>`_\n+\n+    >>> from pyspark.ml.linalg import Vectors\n+    >>> from pyspark.sql.functions import col\n+    >>> data = [(0, Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),),\n+    ...         (1, Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),),\n+    ...         (2, Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),)]\n+    >>> df = spark.createDataFrame(data, [\"id\", \"features\"])\n+    >>> mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", seed=12345)\n+    >>> model = mh.fit(df)\n+    >>> model.transform(df).head()\n+    Row(id=0, features=SparseVector(6, {0: 1.0, 1: 1.0, 2: 1.0}), hashes=[DenseVector([-1638925...\n+    >>> data2 = [(3, Vectors.sparse(6, [1, 3, 5], [1.0, 1.0, 1.0]),),\n+    ...          (4, Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),),\n+    ...          (5, Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),)]\n+    >>> df2 = spark.createDataFrame(data2, [\"id\", \"features\"])\n+    >>> key = Vectors.sparse(6, [1, 2], [1.0, 1.0])\n+    >>> model.approxNearestNeighbors(df2, key, 1).collect()\n+    [Row(id=5, features=SparseVector(6, {1: 1.0, 2: 1.0, 4: 1.0}), hashes=[DenseVector([-163892...\n+    >>> model.approxSimilarityJoin(df, df2, 0.6, distCol=\"JaccardDistance\").select(\n+    ...     col(\"datasetA.id\").alias(\"idA\"),\n+    ...     col(\"datasetB.id\").alias(\"idB\"),\n+    ...     col(\"JaccardDistance\")).show()\n+    +---+---+---------------+\n+    |idA|idB|JaccardDistance|\n+    +---+---+---------------+\n+    |  1|  4|            0.5|\n+    |  0|  5|            0.5|\n+    +---+---+---------------+\n+    ...\n+    >>> mhPath = temp_path + \"/mh\"\n+    >>> mh.save(mhPath)\n+    >>> mh2 = MinHashLSH.load(mhPath)\n+    >>> mh2.getOutputCol() == mh.getOutputCol()\n+    True\n+    >>> modelPath = temp_path + \"/mh-model\"\n+    >>> model.save(modelPath)\n+    >>> model2 = MinHashLSHModel.load(modelPath)",
    "line": 266
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Added.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-14T17:14:07Z",
    "diffHunk": "@@ -755,6 +945,103 @@ def maxAbs(self):\n \n \n @inherit_doc\n+class MinHashLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                 JavaMLReadable, JavaMLWritable):\n+\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Jaccard distance.\n+    The input can be dense or sparse vectors, but it is more efficient if it is sparse.\n+    For example, `Vectors.sparse(10, [(2, 1.0), (3, 1.0), (5, 1.0)])` means there are 10 elements\n+    in the space. This set contains elements 2, 3, and 5. Also, any input vector must have at\n+    least 1 non-zero index, and all non-zero values are treated as binary \"1\" values.\n+\n+    .. seealso:: `Wikipedia on MinHash <https://en.wikipedia.org/wiki/MinHash>`_\n+\n+    >>> from pyspark.ml.linalg import Vectors\n+    >>> from pyspark.sql.functions import col\n+    >>> data = [(0, Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),),\n+    ...         (1, Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),),\n+    ...         (2, Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),)]\n+    >>> df = spark.createDataFrame(data, [\"id\", \"features\"])\n+    >>> mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", seed=12345)\n+    >>> model = mh.fit(df)\n+    >>> model.transform(df).head()\n+    Row(id=0, features=SparseVector(6, {0: 1.0, 1: 1.0, 2: 1.0}), hashes=[DenseVector([-1638925...\n+    >>> data2 = [(3, Vectors.sparse(6, [1, 3, 5], [1.0, 1.0, 1.0]),),\n+    ...          (4, Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),),\n+    ...          (5, Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),)]\n+    >>> df2 = spark.createDataFrame(data2, [\"id\", \"features\"])\n+    >>> key = Vectors.sparse(6, [1, 2], [1.0, 1.0])\n+    >>> model.approxNearestNeighbors(df2, key, 1).collect()\n+    [Row(id=5, features=SparseVector(6, {1: 1.0, 2: 1.0, 4: 1.0}), hashes=[DenseVector([-163892...\n+    >>> model.approxSimilarityJoin(df, df2, 0.6, distCol=\"JaccardDistance\").select(\n+    ...     col(\"datasetA.id\").alias(\"idA\"),\n+    ...     col(\"datasetB.id\").alias(\"idB\"),\n+    ...     col(\"JaccardDistance\")).show()\n+    +---+---+---------------+\n+    |idA|idB|JaccardDistance|\n+    +---+---+---------------+\n+    |  1|  4|            0.5|\n+    |  0|  5|            0.5|\n+    +---+---+---------------+\n+    ...\n+    >>> mhPath = temp_path + \"/mh\"\n+    >>> mh.save(mhPath)\n+    >>> mh2 = MinHashLSH.load(mhPath)\n+    >>> mh2.getOutputCol() == mh.getOutputCol()\n+    True\n+    >>> modelPath = temp_path + \"/mh-model\"\n+    >>> model.save(modelPath)\n+    >>> model2 = MinHashLSHModel.load(modelPath)",
    "line": 266
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Missing \"\\\\\" at end of line",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-28T01:32:37Z",
    "diffHunk": "@@ -120,6 +122,196 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel(JavaModel):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) models.\n+    \"\"\"\n+\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        .. note:: This method is experimental and will likely change behavior in the next release.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A column \"distCol\" is\n+                 added to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two datasets to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each pair of rows. Use\n+                        \"distCol\" as default value if it's not specified.\n+        :return: A joined dataset containing pairs of rows. The original rows are in columns\n+                 \"datasetA\" and \"datasetB\", and a column \"distCol\" is added to show the distance\n+                 between each pair.\n+        \"\"\"\n+        return self._call_java(\"approxSimilarityJoin\", datasetA, datasetB, threshold, distCol)\n+\n+\n+@inherit_doc\n+class BucketedRandomProjectionLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                                  JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Euclidean distance metrics.\n+    The input is dense or sparse vectors, each of which represents a point in the Euclidean\n+    distance space. The output will be vectors of configurable dimension. Hash values in the same\n+    dimension are calculated by the same hash function.\n+\n+    .. seealso:: `Stable Distributions \\\n+    <https://en.wikipedia.org/wiki/Locality-sensitive_hashing#Stable_distributions>`_\n+    .. seealso:: `Hashing for Similarity Search: A Survey <https://arxiv.org/abs/1408.2927>`_\n+\n+    >>> from pyspark.ml.linalg import Vectors\n+    >>> from pyspark.sql.functions import col\n+    >>> data = [(0, Vectors.dense([-1.0, -1.0 ]),),\n+    ...         (1, Vectors.dense([-1.0, 1.0 ]),),\n+    ...         (2, Vectors.dense([1.0, -1.0 ]),),\n+    ...         (3, Vectors.dense([1.0, 1.0]),)]\n+    >>> df = spark.createDataFrame(data, [\"id\", \"features\"])\n+    >>> brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\",\n+    ...                                   seed=12345, bucketLength=1.0)\n+    >>> model = brp.fit(df)\n+    >>> model.transform(df).head()\n+    Row(id=0, features=DenseVector([-1.0, -1.0]), hashes=[DenseVector([-1.0])])\n+    >>> data2 = [(4, Vectors.dense([2.0, 2.0 ]),),\n+    ...          (5, Vectors.dense([2.0, 3.0 ]),),\n+    ...          (6, Vectors.dense([3.0, 2.0 ]),),\n+    ...          (7, Vectors.dense([3.0, 3.0]),)]\n+    >>> df2 = spark.createDataFrame(data2, [\"id\", \"features\"])\n+    >>> model.approxNearestNeighbors(df2, Vectors.dense([1.0, 2.0]), 1).collect()\n+    [Row(id=4, features=DenseVector([2.0, 2.0]), hashes=[DenseVector([1.0])], distCol=1.0)]\n+    >>> model.approxSimilarityJoin(df, df2, 3.0, distCol=\"EuclideanDistance\").select(\n+    ...     col(\"datasetA.id\").alias(\"idA\"),\n+    ...     col(\"datasetB.id\").alias(\"idB\"),\n+    ...     col(\"EuclideanDistance\")).show()\n+    +---+---+-----------------+\n+    |idA|idB|EuclideanDistance|\n+    +---+---+-----------------+\n+    |  3|  6| 2.23606797749979|\n+    +---+---+-----------------+\n+    ...\n+    >>> brpPath = temp_path + \"/brp\"\n+    >>> brp.save(brpPath)\n+    >>> brp2 = BucketedRandomProjectionLSH.load(brpPath)\n+    >>> brp2.getBucketLength() == brp.getBucketLength()\n+    True\n+    >>> modelPath = temp_path + \"/brp-model\"\n+    >>> model.save(modelPath)\n+    >>> model2 = BucketedRandomProjectionLSHModel.load(modelPath)\n+    >>> model.transform(df).head().hashes == model2.transform(df).head().hashes\n+    True\n+\n+    .. versionadded:: 2.2.0\n+    \"\"\"\n+\n+    bucketLength = Param(Params._dummy(), \"bucketLength\", \"the length of each hash bucket, \" +\n+                         \"a larger bucket lowers the false negative rate.\",\n+                         typeConverter=TypeConverters.toFloat)\n+\n+    @keyword_only\n+    def __init__(self, inputCol=None, outputCol=None, seed=None, numHashTables=1,\n+                 bucketLength=None):\n+        \"\"\"\n+        __init__(self, inputCol=None, outputCol=None, seed=None, numHashTables=1,",
    "line": 156
  }, {
    "author": {
      "login": "Yunni"
    },
    "body": "Sure. Will do.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-28T02:07:47Z",
    "diffHunk": "@@ -120,6 +122,196 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel(JavaModel):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) models.\n+    \"\"\"\n+\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        .. note:: This method is experimental and will likely change behavior in the next release.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A column \"distCol\" is\n+                 added to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two datasets to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each pair of rows. Use\n+                        \"distCol\" as default value if it's not specified.\n+        :return: A joined dataset containing pairs of rows. The original rows are in columns\n+                 \"datasetA\" and \"datasetB\", and a column \"distCol\" is added to show the distance\n+                 between each pair.\n+        \"\"\"\n+        return self._call_java(\"approxSimilarityJoin\", datasetA, datasetB, threshold, distCol)\n+\n+\n+@inherit_doc\n+class BucketedRandomProjectionLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                                  JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Euclidean distance metrics.\n+    The input is dense or sparse vectors, each of which represents a point in the Euclidean\n+    distance space. The output will be vectors of configurable dimension. Hash values in the same\n+    dimension are calculated by the same hash function.\n+\n+    .. seealso:: `Stable Distributions \\\n+    <https://en.wikipedia.org/wiki/Locality-sensitive_hashing#Stable_distributions>`_\n+    .. seealso:: `Hashing for Similarity Search: A Survey <https://arxiv.org/abs/1408.2927>`_\n+\n+    >>> from pyspark.ml.linalg import Vectors\n+    >>> from pyspark.sql.functions import col\n+    >>> data = [(0, Vectors.dense([-1.0, -1.0 ]),),\n+    ...         (1, Vectors.dense([-1.0, 1.0 ]),),\n+    ...         (2, Vectors.dense([1.0, -1.0 ]),),\n+    ...         (3, Vectors.dense([1.0, 1.0]),)]\n+    >>> df = spark.createDataFrame(data, [\"id\", \"features\"])\n+    >>> brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\",\n+    ...                                   seed=12345, bucketLength=1.0)\n+    >>> model = brp.fit(df)\n+    >>> model.transform(df).head()\n+    Row(id=0, features=DenseVector([-1.0, -1.0]), hashes=[DenseVector([-1.0])])\n+    >>> data2 = [(4, Vectors.dense([2.0, 2.0 ]),),\n+    ...          (5, Vectors.dense([2.0, 3.0 ]),),\n+    ...          (6, Vectors.dense([3.0, 2.0 ]),),\n+    ...          (7, Vectors.dense([3.0, 3.0]),)]\n+    >>> df2 = spark.createDataFrame(data2, [\"id\", \"features\"])\n+    >>> model.approxNearestNeighbors(df2, Vectors.dense([1.0, 2.0]), 1).collect()\n+    [Row(id=4, features=DenseVector([2.0, 2.0]), hashes=[DenseVector([1.0])], distCol=1.0)]\n+    >>> model.approxSimilarityJoin(df, df2, 3.0, distCol=\"EuclideanDistance\").select(\n+    ...     col(\"datasetA.id\").alias(\"idA\"),\n+    ...     col(\"datasetB.id\").alias(\"idB\"),\n+    ...     col(\"EuclideanDistance\")).show()\n+    +---+---+-----------------+\n+    |idA|idB|EuclideanDistance|\n+    +---+---+-----------------+\n+    |  3|  6| 2.23606797749979|\n+    +---+---+-----------------+\n+    ...\n+    >>> brpPath = temp_path + \"/brp\"\n+    >>> brp.save(brpPath)\n+    >>> brp2 = BucketedRandomProjectionLSH.load(brpPath)\n+    >>> brp2.getBucketLength() == brp.getBucketLength()\n+    True\n+    >>> modelPath = temp_path + \"/brp-model\"\n+    >>> model.save(modelPath)\n+    >>> model2 = BucketedRandomProjectionLSHModel.load(modelPath)\n+    >>> model.transform(df).head().hashes == model2.transform(df).head().hashes\n+    True\n+\n+    .. versionadded:: 2.2.0\n+    \"\"\"\n+\n+    bucketLength = Param(Params._dummy(), \"bucketLength\", \"the length of each hash bucket, \" +\n+                         \"a larger bucket lowers the false negative rate.\",\n+                         typeConverter=TypeConverters.toFloat)\n+\n+    @keyword_only\n+    def __init__(self, inputCol=None, outputCol=None, seed=None, numHashTables=1,\n+                 bucketLength=None):\n+        \"\"\"\n+        __init__(self, inputCol=None, outputCol=None, seed=None, numHashTables=1,",
    "line": 156
  }, {
    "author": {
      "login": "yanboliang"
    },
    "body": "Actually I found this issue when reviewing this PR, but I found the generated Python API doc is correct, so I ignored it. @jkbradley Could you let me know the effect of ```\\``` at the end of line? Thanks.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-28T14:49:58Z",
    "diffHunk": "@@ -120,6 +122,196 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel(JavaModel):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) models.\n+    \"\"\"\n+\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        .. note:: This method is experimental and will likely change behavior in the next release.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A column \"distCol\" is\n+                 added to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two datasets to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each pair of rows. Use\n+                        \"distCol\" as default value if it's not specified.\n+        :return: A joined dataset containing pairs of rows. The original rows are in columns\n+                 \"datasetA\" and \"datasetB\", and a column \"distCol\" is added to show the distance\n+                 between each pair.\n+        \"\"\"\n+        return self._call_java(\"approxSimilarityJoin\", datasetA, datasetB, threshold, distCol)\n+\n+\n+@inherit_doc\n+class BucketedRandomProjectionLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                                  JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Euclidean distance metrics.\n+    The input is dense or sparse vectors, each of which represents a point in the Euclidean\n+    distance space. The output will be vectors of configurable dimension. Hash values in the same\n+    dimension are calculated by the same hash function.\n+\n+    .. seealso:: `Stable Distributions \\\n+    <https://en.wikipedia.org/wiki/Locality-sensitive_hashing#Stable_distributions>`_\n+    .. seealso:: `Hashing for Similarity Search: A Survey <https://arxiv.org/abs/1408.2927>`_\n+\n+    >>> from pyspark.ml.linalg import Vectors\n+    >>> from pyspark.sql.functions import col\n+    >>> data = [(0, Vectors.dense([-1.0, -1.0 ]),),\n+    ...         (1, Vectors.dense([-1.0, 1.0 ]),),\n+    ...         (2, Vectors.dense([1.0, -1.0 ]),),\n+    ...         (3, Vectors.dense([1.0, 1.0]),)]\n+    >>> df = spark.createDataFrame(data, [\"id\", \"features\"])\n+    >>> brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\",\n+    ...                                   seed=12345, bucketLength=1.0)\n+    >>> model = brp.fit(df)\n+    >>> model.transform(df).head()\n+    Row(id=0, features=DenseVector([-1.0, -1.0]), hashes=[DenseVector([-1.0])])\n+    >>> data2 = [(4, Vectors.dense([2.0, 2.0 ]),),\n+    ...          (5, Vectors.dense([2.0, 3.0 ]),),\n+    ...          (6, Vectors.dense([3.0, 2.0 ]),),\n+    ...          (7, Vectors.dense([3.0, 3.0]),)]\n+    >>> df2 = spark.createDataFrame(data2, [\"id\", \"features\"])\n+    >>> model.approxNearestNeighbors(df2, Vectors.dense([1.0, 2.0]), 1).collect()\n+    [Row(id=4, features=DenseVector([2.0, 2.0]), hashes=[DenseVector([1.0])], distCol=1.0)]\n+    >>> model.approxSimilarityJoin(df, df2, 3.0, distCol=\"EuclideanDistance\").select(\n+    ...     col(\"datasetA.id\").alias(\"idA\"),\n+    ...     col(\"datasetB.id\").alias(\"idB\"),\n+    ...     col(\"EuclideanDistance\")).show()\n+    +---+---+-----------------+\n+    |idA|idB|EuclideanDistance|\n+    +---+---+-----------------+\n+    |  3|  6| 2.23606797749979|\n+    +---+---+-----------------+\n+    ...\n+    >>> brpPath = temp_path + \"/brp\"\n+    >>> brp.save(brpPath)\n+    >>> brp2 = BucketedRandomProjectionLSH.load(brpPath)\n+    >>> brp2.getBucketLength() == brp.getBucketLength()\n+    True\n+    >>> modelPath = temp_path + \"/brp-model\"\n+    >>> model.save(modelPath)\n+    >>> model2 = BucketedRandomProjectionLSHModel.load(modelPath)\n+    >>> model.transform(df).head().hashes == model2.transform(df).head().hashes\n+    True\n+\n+    .. versionadded:: 2.2.0\n+    \"\"\"\n+\n+    bucketLength = Param(Params._dummy(), \"bucketLength\", \"the length of each hash bucket, \" +\n+                         \"a larger bucket lowers the false negative rate.\",\n+                         typeConverter=TypeConverters.toFloat)\n+\n+    @keyword_only\n+    def __init__(self, inputCol=None, outputCol=None, seed=None, numHashTables=1,\n+                 bucketLength=None):\n+        \"\"\"\n+        __init__(self, inputCol=None, outputCol=None, seed=None, numHashTables=1,",
    "line": 156
  }, {
    "author": {
      "login": "jkbradley"
    },
    "body": "Oh, I thought it was necessary for proper doc generation, but maybe it's not.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-03-06T01:02:22Z",
    "diffHunk": "@@ -120,6 +122,196 @@ def getThreshold(self):\n         return self.getOrDefault(self.threshold)\n \n \n+class LSHParams(Params):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) algorithm parameters.\n+    \"\"\"\n+\n+    numHashTables = Param(Params._dummy(), \"numHashTables\", \"number of hash tables, where \" +\n+                          \"increasing number of hash tables lowers the false negative rate, \" +\n+                          \"and decreasing it improves the running performance.\",\n+                          typeConverter=TypeConverters.toInt)\n+\n+    def __init__(self):\n+        super(LSHParams, self).__init__()\n+\n+    def setNumHashTables(self, value):\n+        \"\"\"\n+        Sets the value of :py:attr:`numHashTables`.\n+        \"\"\"\n+        return self._set(numHashTables=value)\n+\n+    def getNumHashTables(self):\n+        \"\"\"\n+        Gets the value of numHashTables or its default value.\n+        \"\"\"\n+        return self.getOrDefault(self.numHashTables)\n+\n+\n+class LSHModel(JavaModel):\n+    \"\"\"\n+    Mixin for Locality Sensitive Hashing (LSH) models.\n+    \"\"\"\n+\n+    def approxNearestNeighbors(self, dataset, key, numNearestNeighbors, distCol=\"distCol\"):\n+        \"\"\"\n+        Given a large dataset and an item, approximately find at most k items which have the\n+        closest distance to the item. If the :py:attr:`outputCol` is missing, the method will\n+        transform the data; if the :py:attr:`outputCol` exists, it will use that. This allows\n+        caching of the transformed data when necessary.\n+\n+        .. note:: This method is experimental and will likely change behavior in the next release.\n+\n+        :param dataset: The dataset to search for nearest neighbors of the key.\n+        :param key: Feature vector representing the item to search for.\n+        :param numNearestNeighbors: The maximum number of nearest neighbors.\n+        :param distCol: Output column for storing the distance between each result row and the key.\n+                        Use \"distCol\" as default value if it's not specified.\n+        :return: A dataset containing at most k items closest to the key. A column \"distCol\" is\n+                 added to show the distance between each row and the key.\n+        \"\"\"\n+        return self._call_java(\"approxNearestNeighbors\", dataset, key, numNearestNeighbors,\n+                               distCol)\n+\n+    def approxSimilarityJoin(self, datasetA, datasetB, threshold, distCol=\"distCol\"):\n+        \"\"\"\n+        Join two datasets to approximately find all pairs of rows whose distance are smaller than\n+        the threshold. If the :py:attr:`outputCol` is missing, the method will transform the data;\n+        if the :py:attr:`outputCol` exists, it will use that. This allows caching of the\n+        transformed data when necessary.\n+\n+        :param datasetA: One of the datasets to join.\n+        :param datasetB: Another dataset to join.\n+        :param threshold: The threshold for the distance of row pairs.\n+        :param distCol: Output column for storing the distance between each pair of rows. Use\n+                        \"distCol\" as default value if it's not specified.\n+        :return: A joined dataset containing pairs of rows. The original rows are in columns\n+                 \"datasetA\" and \"datasetB\", and a column \"distCol\" is added to show the distance\n+                 between each pair.\n+        \"\"\"\n+        return self._call_java(\"approxSimilarityJoin\", datasetA, datasetB, threshold, distCol)\n+\n+\n+@inherit_doc\n+class BucketedRandomProjectionLSH(JavaEstimator, LSHParams, HasInputCol, HasOutputCol, HasSeed,\n+                                  JavaMLReadable, JavaMLWritable):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    LSH class for Euclidean distance metrics.\n+    The input is dense or sparse vectors, each of which represents a point in the Euclidean\n+    distance space. The output will be vectors of configurable dimension. Hash values in the same\n+    dimension are calculated by the same hash function.\n+\n+    .. seealso:: `Stable Distributions \\\n+    <https://en.wikipedia.org/wiki/Locality-sensitive_hashing#Stable_distributions>`_\n+    .. seealso:: `Hashing for Similarity Search: A Survey <https://arxiv.org/abs/1408.2927>`_\n+\n+    >>> from pyspark.ml.linalg import Vectors\n+    >>> from pyspark.sql.functions import col\n+    >>> data = [(0, Vectors.dense([-1.0, -1.0 ]),),\n+    ...         (1, Vectors.dense([-1.0, 1.0 ]),),\n+    ...         (2, Vectors.dense([1.0, -1.0 ]),),\n+    ...         (3, Vectors.dense([1.0, 1.0]),)]\n+    >>> df = spark.createDataFrame(data, [\"id\", \"features\"])\n+    >>> brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\",\n+    ...                                   seed=12345, bucketLength=1.0)\n+    >>> model = brp.fit(df)\n+    >>> model.transform(df).head()\n+    Row(id=0, features=DenseVector([-1.0, -1.0]), hashes=[DenseVector([-1.0])])\n+    >>> data2 = [(4, Vectors.dense([2.0, 2.0 ]),),\n+    ...          (5, Vectors.dense([2.0, 3.0 ]),),\n+    ...          (6, Vectors.dense([3.0, 2.0 ]),),\n+    ...          (7, Vectors.dense([3.0, 3.0]),)]\n+    >>> df2 = spark.createDataFrame(data2, [\"id\", \"features\"])\n+    >>> model.approxNearestNeighbors(df2, Vectors.dense([1.0, 2.0]), 1).collect()\n+    [Row(id=4, features=DenseVector([2.0, 2.0]), hashes=[DenseVector([1.0])], distCol=1.0)]\n+    >>> model.approxSimilarityJoin(df, df2, 3.0, distCol=\"EuclideanDistance\").select(\n+    ...     col(\"datasetA.id\").alias(\"idA\"),\n+    ...     col(\"datasetB.id\").alias(\"idB\"),\n+    ...     col(\"EuclideanDistance\")).show()\n+    +---+---+-----------------+\n+    |idA|idB|EuclideanDistance|\n+    +---+---+-----------------+\n+    |  3|  6| 2.23606797749979|\n+    +---+---+-----------------+\n+    ...\n+    >>> brpPath = temp_path + \"/brp\"\n+    >>> brp.save(brpPath)\n+    >>> brp2 = BucketedRandomProjectionLSH.load(brpPath)\n+    >>> brp2.getBucketLength() == brp.getBucketLength()\n+    True\n+    >>> modelPath = temp_path + \"/brp-model\"\n+    >>> model.save(modelPath)\n+    >>> model2 = BucketedRandomProjectionLSHModel.load(modelPath)\n+    >>> model.transform(df).head().hashes == model2.transform(df).head().hashes\n+    True\n+\n+    .. versionadded:: 2.2.0\n+    \"\"\"\n+\n+    bucketLength = Param(Params._dummy(), \"bucketLength\", \"the length of each hash bucket, \" +\n+                         \"a larger bucket lowers the false negative rate.\",\n+                         typeConverter=TypeConverters.toFloat)\n+\n+    @keyword_only\n+    def __init__(self, inputCol=None, outputCol=None, seed=None, numHashTables=1,\n+                 bucketLength=None):\n+        \"\"\"\n+        __init__(self, inputCol=None, outputCol=None, seed=None, numHashTables=1,",
    "line": 156
  }],
  "prId": 16715
}]