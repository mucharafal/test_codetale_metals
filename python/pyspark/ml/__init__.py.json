[{
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "Would you like to add some comments about why do this? (avoid the unnecessary transforms)\n",
    "commit": "415268e19ebf9d48cb4a50773f446ebbe8a902cd",
    "createdAt": "2015-01-28T00:13:32Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from abc import ABCMeta, abstractmethod, abstractproperty\n+\n+from pyspark import SparkContext\n+from pyspark.sql import SchemaRDD, inherit_doc  # TODO: move inherit_doc to Spark Core\n+from pyspark.ml.param import Param, Params\n+from pyspark.ml.util import Identifiable\n+\n+__all__ = [\"Pipeline\", \"Transformer\", \"Estimator\", \"param\", \"feature\", \"classification\"]\n+\n+\n+def _jvm():\n+    \"\"\"\n+    Returns the JVM view associated with SparkContext. Must be called\n+    after SparkContext is initialized.\n+    \"\"\"\n+    jvm = SparkContext._jvm\n+    if jvm:\n+        return jvm\n+    else:\n+        raise AttributeError(\"Cannot load _jvm from SparkContext. Is SparkContext initialized?\")\n+\n+\n+@inherit_doc\n+class PipelineStage(Params):\n+    \"\"\"\n+    A stage in a pipeline, either an :py:class:`Estimator` or a\n+    :py:class:`Transformer`.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self):\n+        super(PipelineStage, self).__init__()\n+\n+\n+@inherit_doc\n+class Estimator(PipelineStage):\n+    \"\"\"\n+    Abstract class for estimators that fit models to data.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self):\n+        super(Estimator, self).__init__()\n+\n+    @abstractmethod\n+    def fit(self, dataset, params={}):\n+        \"\"\"\n+        Fits a model to the input dataset with optional parameters.\n+\n+        :param dataset: input dataset, which is an instance of\n+                        :py:class:`pyspark.sql.SchemaRDD`\n+        :param params: an optional param map that overwrites embedded\n+                       params\n+        :returns: fitted model\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+\n+@inherit_doc\n+class Transformer(PipelineStage):\n+    \"\"\"\n+    Abstract class for transformers that transform one dataset into\n+    another.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self):\n+        super(Transformer, self).__init__()\n+\n+    @abstractmethod\n+    def transform(self, dataset, params={}):\n+        \"\"\"\n+        Transforms the input dataset with optional parameters.\n+\n+        :param dataset: input dataset, which is an instance of\n+                        :py:class:`pyspark.sql.SchemaRDD`\n+        :param params: an optional param map that overwrites embedded\n+                       params\n+        :returns: transformed dataset\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+\n+@inherit_doc\n+class Model(Transformer):\n+    \"\"\"\n+    Abstract class for models fitted by :py:class:`Estimator`s.\n+    \"\"\"\n+\n+    ___metaclass__ = ABCMeta\n+\n+    def __init__(self):\n+        super(Model, self).__init__()\n+\n+\n+@inherit_doc\n+class Pipeline(Estimator):\n+    \"\"\"\n+    A simple pipeline, which acts as an estimator. A Pipeline consists\n+    of a sequence of stages, each of which is either an\n+    :py:class:`Estimator` or a :py:class:`Transformer`. When\n+    :py:meth:`Pipeline.fit` is called, the stages are executed in\n+    order. If a stage is an :py:class:`Estimator`, its\n+    :py:meth:`Estimator.fit` method will be called on the input\n+    dataset to fit a model. Then the model, which is a transformer,\n+    will be used to transform the dataset as the input to the next\n+    stage. If a stage is a :py:class:`Transformer`, its\n+    :py:meth:`Transformer.transform` method will be called to produce\n+    the dataset for the next stage. The fitted model from a\n+    :py:class:`Pipeline` is an :py:class:`PipelineModel`, which\n+    consists of fitted models and transformers, corresponding to the\n+    pipeline stages. If there are no stages, the pipeline acts as an\n+    identity transformer.\n+    \"\"\"\n+\n+    def __init__(self):\n+        super(Pipeline, self).__init__()\n+        #: Param for pipeline stages.\n+        self.stages = Param(self, \"stages\", \"pipeline stages\")\n+\n+    def setStages(self, value):\n+        \"\"\"\n+        Set pipeline stages.\n+        :param value: a list of transformers or estimators\n+        :return: the pipeline instance\n+        \"\"\"\n+        self.paramMap[self.stages] = value\n+        return self\n+\n+    def getStages(self):\n+        \"\"\"\n+        Get pipeline stages.\n+        \"\"\"\n+        if self.stages in self.paramMap:\n+            return self.paramMap[self.stages]\n+\n+    def fit(self, dataset, params={}):\n+        paramMap = self._merge_params(params)\n+        stages = paramMap[self.stages]\n+        for stage in stages:\n+            if not (isinstance(stage, Estimator) or isinstance(stage, Transformer)):\n+                raise ValueError(\n+                    \"Cannot recognize a pipeline stage of type %s.\" % type(stage).__name__)\n+        indexOfLastEstimator = -1"
  }],
  "prId": 4151
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "PipeplineModel should be included in docs.\n",
    "commit": "415268e19ebf9d48cb4a50773f446ebbe8a902cd",
    "createdAt": "2015-01-28T00:14:11Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from abc import ABCMeta, abstractmethod, abstractproperty\n+\n+from pyspark import SparkContext\n+from pyspark.sql import SchemaRDD, inherit_doc  # TODO: move inherit_doc to Spark Core\n+from pyspark.ml.param import Param, Params\n+from pyspark.ml.util import Identifiable\n+\n+__all__ = [\"Pipeline\", \"Transformer\", \"Estimator\", \"param\", \"feature\", \"classification\"]"
  }],
  "prId": 4151
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "Could we put these classes in an separated file? (such as pipeline.py) \n",
    "commit": "415268e19ebf9d48cb4a50773f446ebbe8a902cd",
    "createdAt": "2015-01-28T00:18:17Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from abc import ABCMeta, abstractmethod, abstractproperty\n+\n+from pyspark import SparkContext\n+from pyspark.sql import SchemaRDD, inherit_doc  # TODO: move inherit_doc to Spark Core\n+from pyspark.ml.param import Param, Params\n+from pyspark.ml.util import Identifiable\n+\n+__all__ = [\"Pipeline\", \"Transformer\", \"Estimator\", \"param\", \"feature\", \"classification\"]\n+\n+\n+def _jvm():\n+    \"\"\"\n+    Returns the JVM view associated with SparkContext. Must be called\n+    after SparkContext is initialized.\n+    \"\"\"\n+    jvm = SparkContext._jvm\n+    if jvm:\n+        return jvm\n+    else:\n+        raise AttributeError(\"Cannot load _jvm from SparkContext. Is SparkContext initialized?\")\n+\n+\n+@inherit_doc\n+class PipelineStage(Params):"
  }],
  "prId": 4151
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "This part is hard to understand, may need some refactor\n",
    "commit": "415268e19ebf9d48cb4a50773f446ebbe8a902cd",
    "createdAt": "2015-01-28T00:43:53Z",
    "diffHunk": "@@ -0,0 +1,324 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from abc import ABCMeta, abstractmethod, abstractproperty\n+\n+from pyspark import SparkContext\n+from pyspark.sql import SchemaRDD, inherit_doc  # TODO: move inherit_doc to Spark Core\n+from pyspark.ml.param import Param, Params\n+from pyspark.ml.util import Identifiable\n+\n+__all__ = [\"Pipeline\", \"Transformer\", \"Estimator\", \"param\", \"feature\", \"classification\"]\n+\n+\n+def _jvm():\n+    \"\"\"\n+    Returns the JVM view associated with SparkContext. Must be called\n+    after SparkContext is initialized.\n+    \"\"\"\n+    jvm = SparkContext._jvm\n+    if jvm:\n+        return jvm\n+    else:\n+        raise AttributeError(\"Cannot load _jvm from SparkContext. Is SparkContext initialized?\")\n+\n+\n+@inherit_doc\n+class PipelineStage(Params):\n+    \"\"\"\n+    A stage in a pipeline, either an :py:class:`Estimator` or a\n+    :py:class:`Transformer`.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self):\n+        super(PipelineStage, self).__init__()\n+\n+\n+@inherit_doc\n+class Estimator(PipelineStage):\n+    \"\"\"\n+    Abstract class for estimators that fit models to data.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self):\n+        super(Estimator, self).__init__()\n+\n+    @abstractmethod\n+    def fit(self, dataset, params={}):\n+        \"\"\"\n+        Fits a model to the input dataset with optional parameters.\n+\n+        :param dataset: input dataset, which is an instance of\n+                        :py:class:`pyspark.sql.SchemaRDD`\n+        :param params: an optional param map that overwrites embedded\n+                       params\n+        :returns: fitted model\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+\n+@inherit_doc\n+class Transformer(PipelineStage):\n+    \"\"\"\n+    Abstract class for transformers that transform one dataset into\n+    another.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self):\n+        super(Transformer, self).__init__()\n+\n+    @abstractmethod\n+    def transform(self, dataset, params={}):\n+        \"\"\"\n+        Transforms the input dataset with optional parameters.\n+\n+        :param dataset: input dataset, which is an instance of\n+                        :py:class:`pyspark.sql.SchemaRDD`\n+        :param params: an optional param map that overwrites embedded\n+                       params\n+        :returns: transformed dataset\n+        \"\"\"\n+        raise NotImplementedError()\n+\n+\n+@inherit_doc\n+class Model(Transformer):\n+    \"\"\"\n+    Abstract class for models fitted by :py:class:`Estimator`s.\n+    \"\"\"\n+\n+    ___metaclass__ = ABCMeta\n+\n+    def __init__(self):\n+        super(Model, self).__init__()\n+\n+\n+@inherit_doc\n+class Pipeline(Estimator):\n+    \"\"\"\n+    A simple pipeline, which acts as an estimator. A Pipeline consists\n+    of a sequence of stages, each of which is either an\n+    :py:class:`Estimator` or a :py:class:`Transformer`. When\n+    :py:meth:`Pipeline.fit` is called, the stages are executed in\n+    order. If a stage is an :py:class:`Estimator`, its\n+    :py:meth:`Estimator.fit` method will be called on the input\n+    dataset to fit a model. Then the model, which is a transformer,\n+    will be used to transform the dataset as the input to the next\n+    stage. If a stage is a :py:class:`Transformer`, its\n+    :py:meth:`Transformer.transform` method will be called to produce\n+    the dataset for the next stage. The fitted model from a\n+    :py:class:`Pipeline` is an :py:class:`PipelineModel`, which\n+    consists of fitted models and transformers, corresponding to the\n+    pipeline stages. If there are no stages, the pipeline acts as an\n+    identity transformer.\n+    \"\"\"\n+\n+    def __init__(self):\n+        super(Pipeline, self).__init__()\n+        #: Param for pipeline stages.\n+        self.stages = Param(self, \"stages\", \"pipeline stages\")\n+\n+    def setStages(self, value):\n+        \"\"\"\n+        Set pipeline stages.\n+        :param value: a list of transformers or estimators\n+        :return: the pipeline instance\n+        \"\"\"\n+        self.paramMap[self.stages] = value\n+        return self\n+\n+    def getStages(self):\n+        \"\"\"\n+        Get pipeline stages.\n+        \"\"\"\n+        if self.stages in self.paramMap:\n+            return self.paramMap[self.stages]\n+\n+    def fit(self, dataset, params={}):\n+        paramMap = self._merge_params(params)\n+        stages = paramMap[self.stages]\n+        for stage in stages:\n+            if not (isinstance(stage, Estimator) or isinstance(stage, Transformer)):\n+                raise ValueError(\n+                    \"Cannot recognize a pipeline stage of type %s.\" % type(stage).__name__)\n+        indexOfLastEstimator = -1\n+        for i, stage in enumerate(stages):\n+            if isinstance(stage, Estimator):\n+                indexOfLastEstimator = i\n+        transformers = []\n+        for i, stage in enumerate(stages):\n+            if i <= indexOfLastEstimator:\n+                if isinstance(stage, Transformer):\n+                    transformers.append(stage)\n+                    dataset = stage.transform(dataset, paramMap)\n+                else:  # must be an Estimator\n+                    model = stage.fit(dataset, paramMap)\n+                    transformers.append(model)\n+                    if i < indexOfLastEstimator:\n+                        dataset = model.transform(dataset, paramMap)\n+            else:\n+                transformers.append(stage)\n+        return PipelineModel(transformers)\n+\n+\n+@inherit_doc\n+class PipelineModel(Model):\n+    \"\"\"\n+    Represents a compiled pipeline with transformers and fitted models.\n+    \"\"\"\n+\n+    def __init__(self, transformers):\n+        super(PipelineModel, self).__init__()\n+        self.transformers = transformers\n+\n+    def transform(self, dataset, params={}):\n+        paramMap = self._merge_params(params)\n+        for t in self.transformers:\n+            dataset = t.transform(dataset, paramMap)\n+        return dataset\n+\n+\n+@inherit_doc\n+class JavaWrapper(Params):\n+    \"\"\"\n+    Utility class to help create wrapper classes from Java/Scala\n+    implementations of pipeline components.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self):\n+        super(JavaWrapper, self).__init__()\n+\n+    @abstractproperty\n+    def _java_class(self):\n+        \"\"\"\n+        Fully-qualified class name of the wrapped Java component.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def _java_obj(self):\n+        \"\"\"\n+        Returns or creates a Java object.\n+        \"\"\"\n+        java_obj = _jvm()\n+        for name in self._java_class.split(\".\"):\n+            java_obj = getattr(java_obj, name)\n+        return java_obj()\n+\n+    def _transfer_params_to_java(self, params, java_obj):\n+        \"\"\"\n+        Transforms the embedded params and additional params to the\n+        input Java object.\n+        :param params: additional params (overwriting embedded values)\n+        :param java_obj: Java object to receive the params\n+        \"\"\"\n+        paramMap = self._merge_params(params)\n+        for param in self.params:\n+            if param in paramMap:\n+                java_obj.set(param.name, paramMap[param])\n+\n+    def _empty_java_param_map(self):\n+        \"\"\"\n+        Returns an empty Java ParamMap reference.\n+        \"\"\"\n+        return _jvm().org.apache.spark.ml.param.ParamMap()\n+\n+    def _create_java_param_map(self, params, java_obj):\n+        paramMap = self._empty_java_param_map()\n+        for param, value in params.items():\n+            if param.parent is self:\n+                paramMap.put(java_obj.getParam(param.name), value)\n+        return paramMap\n+\n+\n+@inherit_doc\n+class JavaEstimator(Estimator, JavaWrapper):\n+    \"\"\"\n+    Base class for :py:class:`Estimator`s that wrap Java/Scala\n+    implementations.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self):\n+        super(JavaEstimator, self).__init__()\n+\n+    @abstractmethod\n+    def _create_model(self, java_model):\n+        \"\"\"\n+        Creates a model from the input Java model reference.\n+        \"\"\"\n+        raise NotImplementedError\n+\n+    def _fit_java(self, dataset, params={}):\n+        \"\"\"\n+        Fits a Java model to the input dataset.\n+        :param dataset: input dataset, which is an instance of\n+                        :py:class:`pyspark.sql.SchemaRDD`\n+        :param params: additional params (overwriting embedded values)\n+        :return: fitted Java model\n+        \"\"\"\n+        java_obj = self._java_obj()\n+        self._transfer_params_to_java(params, java_obj)\n+        return java_obj.fit(dataset._jschema_rdd, self._empty_java_param_map())\n+\n+    def fit(self, dataset, params={}):\n+        java_model = self._fit_java(dataset, params)\n+        return self._create_model(java_model)\n+\n+\n+@inherit_doc\n+class JavaTransformer(Transformer, JavaWrapper):\n+    \"\"\"\n+    Base class for :py:class:`Transformer`s that wrap Java/Scala\n+    implementations.\n+    \"\"\"\n+\n+    __metaclass__ = ABCMeta\n+\n+    def __init__(self):\n+        super(JavaTransformer, self).__init__()\n+\n+    def transform(self, dataset, params={}):\n+        java_obj = self._java_obj()\n+        self._transfer_params_to_java({}, java_obj)\n+        java_param_map = self._create_java_param_map(params, java_obj)"
  }],
  "prId": 4151
}]