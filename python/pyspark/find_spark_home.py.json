[{
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "Instead of building paths with `+`, we should be using `os.path.join()`.\n\nIt takes care of things like repeated slashes, and it will also do the right thing on Windows (which `+` won't, in this case).\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-27T14:57:25Z",
    "diffHunk": "@@ -0,0 +1,65 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(path + \"/bin/spark-submit\") and os.path.isdir(path + \"/jars\"))"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Is this `ImportError` used to deal with `import imp` above? If so, I think it should be put in `try` block?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-03T05:18:46Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]\n+\n+    # Add the path of the PySpark module if it exists\n+    if sys.version < \"3\":\n+        import imp\n+        try:\n+            paths.append(imp.find_module(\"pyspark\")[1])\n+        except ImportError:"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "It's to deal with the case where PySpark isn't pip installed (so where imp.find_module fails).\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-03T15:09:20Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]\n+\n+    # Add the path of the PySpark module if it exists\n+    if sys.version < \"3\":\n+        import imp\n+        try:\n+            paths.append(imp.find_module(\"pyspark\")[1])\n+        except ImportError:"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Same as above.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-03T05:19:00Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]\n+\n+    # Add the path of the PySpark module if it exists\n+    if sys.version < \"3\":\n+        import imp\n+        try:\n+            paths.append(imp.find_module(\"pyspark\")[1])\n+        except ImportError:\n+            # Not pip installed no worries\n+            True\n+    else:\n+        from importlib.util import find_spec\n+        try:\n+            paths.append(os.path.dirname(find_spec(\"pyspark\").origin))\n+        except ImportError:"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "Same as above, this is to handle the case where it can't find PySpark not importlib.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-03T15:10:37Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]\n+\n+    # Add the path of the PySpark module if it exists\n+    if sys.version < \"3\":\n+        import imp\n+        try:\n+            paths.append(imp.find_module(\"pyspark\")[1])\n+        except ImportError:\n+            # Not pip installed no worries\n+            True\n+    else:\n+        from importlib.util import find_spec\n+        try:\n+            paths.append(os.path.dirname(find_spec(\"pyspark\").origin))\n+        except ImportError:"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "``` python\nprint(\"Could not find valid SPARK_HOME while searching {}\".format(paths), file=sys.stderr)\n```\n\nMinor point, but `%` is discouraged these days in favor of `format()`.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-06T17:02:04Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]\n+\n+    # Add the path of the PySpark module if it exists\n+    if sys.version < \"3\":\n+        import imp\n+        try:\n+            paths.append(imp.find_module(\"pyspark\")[1])\n+        except ImportError:\n+            # Not pip installed no worries\n+            True\n+    else:\n+        from importlib.util import find_spec\n+        try:\n+            paths.append(os.path.dirname(find_spec(\"pyspark\").origin))\n+        except ImportError:\n+            # Not pip installed no worries\n+            True\n+\n+    # Normalize the paths\n+    paths = map(lambda path: os.path.abspath(path), paths)\n+\n+    try:\n+        return next(path for path in paths if is_spark_home(path))\n+    except StopIteration:\n+        print(\"Could not find valid SPARK_HOME while searching %s\" % paths, file=sys.stderr)"
  }, {
    "author": {
      "login": "nchammas"
    },
    "body": "We should raise an exception here or `exit(1)` since this is a fatal error.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-06T17:08:15Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]\n+\n+    # Add the path of the PySpark module if it exists\n+    if sys.version < \"3\":\n+        import imp\n+        try:\n+            paths.append(imp.find_module(\"pyspark\")[1])\n+        except ImportError:\n+            # Not pip installed no worries\n+            True\n+    else:\n+        from importlib.util import find_spec\n+        try:\n+            paths.append(os.path.dirname(find_spec(\"pyspark\").origin))\n+        except ImportError:\n+            # Not pip installed no worries\n+            True\n+\n+    # Normalize the paths\n+    paths = map(lambda path: os.path.abspath(path), paths)\n+\n+    try:\n+        return next(path for path in paths if is_spark_home(path))\n+    except StopIteration:\n+        print(\"Could not find valid SPARK_HOME while searching %s\" % paths, file=sys.stderr)"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "Exit sounds reasonable, I'll follow up with the change to format once we fix the find_spark_home issues inside of Python3 venv.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-06T18:04:38Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]\n+\n+    # Add the path of the PySpark module if it exists\n+    if sys.version < \"3\":\n+        import imp\n+        try:\n+            paths.append(imp.find_module(\"pyspark\")[1])\n+        except ImportError:\n+            # Not pip installed no worries\n+            True\n+    else:\n+        from importlib.util import find_spec\n+        try:\n+            paths.append(os.path.dirname(find_spec(\"pyspark\").origin))\n+        except ImportError:\n+            # Not pip installed no worries\n+            True\n+\n+    # Normalize the paths\n+    paths = map(lambda path: os.path.abspath(path), paths)\n+\n+    try:\n+        return next(path for path in paths if is_spark_home(path))\n+    except StopIteration:\n+        print(\"Could not find valid SPARK_HOME while searching %s\" % paths, file=sys.stderr)"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "``` python\npaths = [os.path.abspath(p) for p in paths]\n```\n\nThis is more Pythonic and eliminates the need to call `list()` on the output of `map()` since `map()` returns an iterator.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-06T17:05:02Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]\n+\n+    # Add the path of the PySpark module if it exists\n+    if sys.version < \"3\":\n+        import imp\n+        try:\n+            paths.append(imp.find_module(\"pyspark\")[1])\n+        except ImportError:\n+            # Not pip installed no worries\n+            True\n+    else:\n+        from importlib.util import find_spec\n+        try:\n+            paths.append(os.path.dirname(find_spec(\"pyspark\").origin))\n+        except ImportError:\n+            # Not pip installed no worries\n+            True\n+\n+    # Normalize the paths\n+    paths = map(lambda path: os.path.abspath(path), paths)"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "Couple of comments here:\n1. A better way to get a directory relative to the current file is to have something like this at the top of the file and refer to it as necessary:\n   \n   ```\n   THIS_DIR = os.path.dirname(os.path.realpath(__file__))\n   ```\n2. The correct way to go up one directory is to just call `dirname()` again. `os.path.join(..., '../')` will just append `'../'` to the end of the path, which may not work as expected later on.\n   \n   So I think you're looking for `THIS_DIR` and `os.path.dirname(THIS_DIR)`.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-06T17:22:18Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "So in python 2.7 os.path.dirname only expects one argument, but I can do part 1.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-06T17:55:48Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]"
  }, {
    "author": {
      "login": "nchammas"
    },
    "body": "I meant you are probably looking for\n\n``` python\npaths = [THIS_DIR, os.path.dirname(THIS_DIR)]\n```\n\nThe signature of `os.path.dirname()` is the same in Python 3. When I was talking about going up one directory in my previous comment, I meant to write `os.path.join(..., '../')` (fixed now).\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-06T18:02:06Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "Ah that makes more sense - but it's not quite the desired search set - one is relative the pwd and the other is relative the script location (they most likely are not the same).\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-07T15:18:15Z",
    "diffHunk": "@@ -0,0 +1,66 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "Nit: The idiom in Python for \"do nothing\" is usually `pass`.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-06T22:52:48Z",
    "diffHunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\")) or\n+                 os.path.isdir(os.path.join(path, \"assembly\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]\n+\n+    # Add the path of the PySpark module if it exists\n+    if sys.version < \"3\":\n+        import imp\n+        try:\n+            module_home = imp.find_module(\"pyspark\")[1]\n+            paths.append(module_home)\n+            # If we are installed in edit mode also look two dirs up\n+            paths.append(os.path.join(module_home, \"../../\"))\n+        except ImportError:\n+            # Not pip installed no worries\n+            True"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "I guess if this works we don't have to change it, but to clarify my earlier comment about why `dirname()` is better than joining to `'../'`:\n\n```\n>>> os.path.join('/example/path', '../')\n'/example/path/../'\n>>> os.path.dirname('/example/path')\n'/example'\n```\n\nThere are a few places where this could be changed, but it's not a big deal.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-06T23:00:52Z",
    "diffHunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\")) or\n+                 os.path.isdir(os.path.join(path, \"assembly\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "To be clear it's going to the parent of the pwd and dirname respectively.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-07T04:40:47Z",
    "diffHunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\")) or\n+                 os.path.isdir(os.path.join(path, \"assembly\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "I guess we could normalize the path of the pwd first and then use os.path.dirname on it. Is this something you think would make a difference?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-07T05:15:47Z",
    "diffHunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\")) or\n+                 os.path.isdir(os.path.join(path, \"assembly\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "Same nit about `pass`.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-06T23:01:36Z",
    "diffHunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\")) or\n+                 os.path.isdir(os.path.join(path, \"assembly\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]\n+\n+    # Add the path of the PySpark module if it exists\n+    if sys.version < \"3\":\n+        import imp\n+        try:\n+            module_home = imp.find_module(\"pyspark\")[1]\n+            paths.append(module_home)\n+            # If we are installed in edit mode also look two dirs up\n+            paths.append(os.path.join(module_home, \"../../\"))\n+        except ImportError:\n+            # Not pip installed no worries\n+            True\n+    else:\n+        from importlib.util import find_spec\n+        try:\n+            module_home = os.path.dirname(find_spec(\"pyspark\").origin)\n+            paths.append(module_home)\n+            # If we are installed in edit mode also look two dirs up\n+            paths.append(os.path.join(module_home, \"../../\"))\n+        except ImportError:\n+            # Not pip installed no worries\n+            True"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "Hmm, did a commit get gobbled up accidentally? This line still uses `%` and is missing an `exit(1)`. I see you changed it for another file, so I assume you meant to do it here too.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-06T23:09:09Z",
    "diffHunk": "@@ -0,0 +1,73 @@\n+#!/usr/bin/python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# This script attempt to determine the correct setting for SPARK_HOME given\n+# that Spark may have been installed on the system with pip.\n+\n+from __future__ import print_function\n+import os\n+import sys\n+\n+\n+def _find_spark_home():\n+    \"\"\"Find the SPARK_HOME.\"\"\"\n+    # If the enviroment has SPARK_HOME set trust it.\n+    if \"SPARK_HOME\" in os.environ:\n+        return os.environ[\"SPARK_HOME\"]\n+\n+    def is_spark_home(path):\n+        \"\"\"Takes a path and returns true if the provided path could be a reasonable SPARK_HOME\"\"\"\n+        return (os.path.isfile(os.path.join(path, \"bin/spark-submit\")) and\n+                (os.path.isdir(os.path.join(path, \"jars\")) or\n+                 os.path.isdir(os.path.join(path, \"assembly\"))))\n+\n+    paths = [\"../\", os.path.join(os.path.dirname(sys.argv[0]), \"../\")]\n+\n+    # Add the path of the PySpark module if it exists\n+    if sys.version < \"3\":\n+        import imp\n+        try:\n+            module_home = imp.find_module(\"pyspark\")[1]\n+            paths.append(module_home)\n+            # If we are installed in edit mode also look two dirs up\n+            paths.append(os.path.join(module_home, \"../../\"))\n+        except ImportError:\n+            # Not pip installed no worries\n+            True\n+    else:\n+        from importlib.util import find_spec\n+        try:\n+            module_home = os.path.dirname(find_spec(\"pyspark\").origin)\n+            paths.append(module_home)\n+            # If we are installed in edit mode also look two dirs up\n+            paths.append(os.path.join(module_home, \"../../\"))\n+        except ImportError:\n+            # Not pip installed no worries\n+            True\n+\n+    # Normalize the paths\n+    paths = [os.path.abspath(p) for p in paths]\n+\n+    try:\n+        return next(path for path in paths if is_spark_home(path))\n+    except StopIteration:\n+        print(\"Could not find valid SPARK_HOME while searching %s\".format(paths), file=sys.stderr)"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "Just double checking: Do we not want an `exit(1)` here?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-07T02:12:45Z",
    "diffHunk": "@@ -59,15 +59,15 @@ def is_spark_home(path):\n             paths.append(os.path.join(module_home, \"../../\"))\n         except ImportError:\n             # Not pip installed no worries\n-            True\n+            pass\n \n     # Normalize the paths\n     paths = [os.path.abspath(p) for p in paths]\n \n     try:\n         return next(path for path in paths if is_spark_home(path))\n     except StopIteration:\n-        print(\"Could not find valid SPARK_HOME while searching %s\".format(paths), file=sys.stderr)\n+        print(\"Could not find valid SPARK_HOME while searching {0}\".format(paths), file=sys.stderr)"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "Added, sorry (got left out when I was working on the change to for edit mode tests and I reset part of the way through).\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-07T02:36:41Z",
    "diffHunk": "@@ -59,15 +59,15 @@ def is_spark_home(path):\n             paths.append(os.path.join(module_home, \"../../\"))\n         except ImportError:\n             # Not pip installed no worries\n-            True\n+            pass\n \n     # Normalize the paths\n     paths = [os.path.abspath(p) for p in paths]\n \n     try:\n         return next(path for path in paths if is_spark_home(path))\n     except StopIteration:\n-        print(\"Could not find valid SPARK_HOME while searching %s\".format(paths), file=sys.stderr)\n+        print(\"Could not find valid SPARK_HOME while searching {0}\".format(paths), file=sys.stderr)"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "/usr/bin/env python\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-07T22:18:10Z",
    "diffHunk": "@@ -0,0 +1,74 @@\n+#!/usr/bin/python"
  }],
  "prId": 15659
}]