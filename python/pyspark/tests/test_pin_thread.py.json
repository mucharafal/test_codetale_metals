[{
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I dunno how reliably this will really interleave the threads ... what about adding a Barrier here?  Maybe that is too much determinism, but at least makes sure there is some interleaving. ",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-10-25T14:29:58Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # Sleep for some arbitrary time.\n+                time.sleep(random.random())"
  }],
  "prId": 24898
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "typo: treading",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-10-25T14:30:22Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # Sleep for some arbitrary time.\n+                time.sleep(random.random())\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-treading mode in Py4J."
  }],
  "prId": 24898
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I think we can change to barriers & events to avoid sleeps to make this more reliable.  A barrier to check that all jobs have been submitted, and an event to check the jobs have been cancelled.\r\n\r\n```python\r\njobs_submitted = threading.Barrier(num_jobs + 1)\r\njob_cancelled = threading.Event()\r\n...\r\n# in threads that submit jobs\r\ndef job_func(x):\r\n  jobs_submitted.wait()\r\n  job_cancelled.wait()\r\n  return x\r\nself.sc.parallelize([3]).map(job_func).collect()\r\n...\r\n\r\n# in main thread, after starting all job-submitting threads\r\njobs_submitted.wait()\r\nself.sc.cancelJobGroup(group_a)\r\njobs_cancelled.set()\r\n```",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-10-25T14:42:30Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # Sleep for some arbitrary time.\n+                time.sleep(random.random())\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-treading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([3]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True\n+\n+        # Test if job succeeded when not cancelled.\n+        run_job(group_a, 0)\n+        self.assertFalse(is_job_cancelled[0])\n+\n+        # Run jobs\n+        for i in thread_ids_to_cancel:\n+            t = threading.Thread(target=run_job, args=(group_a, i))\n+            t.start()\n+            threads.append(t)\n+\n+        for i in thread_ids_to_run:\n+            t = threading.Thread(target=run_job, args=(group_b, i))\n+            t.start()\n+            threads.append(t)\n+\n+        # Wait to make sure all jobs are executed.\n+        time.sleep(1)"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Seems like `threading.Barrier` is new as of Python 3.2 and it won't work actually because `job_cancelled` will be serialized and deserialized at `sc.parallelize([3]).map(job_func)` .. :-(. Anyway I got the point - let me try to fix it ",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-10-29T03:52:52Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # Sleep for some arbitrary time.\n+                time.sleep(random.random())\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-treading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([3]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True\n+\n+        # Test if job succeeded when not cancelled.\n+        run_job(group_a, 0)\n+        self.assertFalse(is_job_cancelled[0])\n+\n+        # Run jobs\n+        for i in thread_ids_to_cancel:\n+            t = threading.Thread(target=run_job, args=(group_a, i))\n+            t.start()\n+            threads.append(t)\n+\n+        for i in thread_ids_to_run:\n+            t = threading.Thread(target=run_job, args=(group_b, i))\n+            t.start()\n+            threads.append(t)\n+\n+        # Wait to make sure all jobs are executed.\n+        time.sleep(1)"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Hmmm .. It's kind of tricky to handle it with some kind of minimsed changes ...",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-10-29T04:32:35Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # Sleep for some arbitrary time.\n+                time.sleep(random.random())\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-treading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([3]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True\n+\n+        # Test if job succeeded when not cancelled.\n+        run_job(group_a, 0)\n+        self.assertFalse(is_job_cancelled[0])\n+\n+        # Run jobs\n+        for i in thread_ids_to_cancel:\n+            t = threading.Thread(target=run_job, args=(group_a, i))\n+            t.start()\n+            threads.append(t)\n+\n+        for i in thread_ids_to_run:\n+            t = threading.Thread(target=run_job, args=(group_b, i))\n+            t.start()\n+            threads.append(t)\n+\n+        # Wait to make sure all jobs are executed.\n+        time.sleep(1)"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "Ugh thats a headache, but you can work around this, I think -- you can define a holder object which explicitly does not serialize the `Event`, and on un-pickling it'll reset to the shared object. https://stackoverflow.com/a/6313474/1442961\r\n\r\nuntested, but I think something like:\r\n\r\n```python\r\n# module-level global variable -- maybe there is a better way to do this?\r\njob_cancelled = threading.Event()\r\n\r\nclass EventWrapper:\r\n  event = None\r\n  def __getstate__(self):\r\n    # can't be empty, because then __setstate__ doesn't get called :(\r\n    return {1:2}\r\n\r\n  def __setstate__(self, state):\r\n    # ignore the pickled state, just set to the shared event as we're in the same PVM\r\n    self.event = job_cancelled\r\n...\r\n\r\njob_cancelled_wrapper = EventWrapper()\r\ndef job_func(x):\r\n  ... # something here to replace the barrier ...\r\n  job_cancelled_wrapper.event.wait()\r\n  return x\r\n...\r\n```\r\n\r\nI'm surprised this hasn't already come up in pyspark tests.\r\n\r\nyou're concerned about `Barrier` for spark 2.4,?  because for 3.0, we'll only support 3.4+, right?",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-10-29T16:20:05Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # Sleep for some arbitrary time.\n+                time.sleep(random.random())\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-treading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([3]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True\n+\n+        # Test if job succeeded when not cancelled.\n+        run_job(group_a, 0)\n+        self.assertFalse(is_job_cancelled[0])\n+\n+        # Run jobs\n+        for i in thread_ids_to_cancel:\n+            t = threading.Thread(target=run_job, args=(group_a, i))\n+            t.start()\n+            threads.append(t)\n+\n+        for i in thread_ids_to_run:\n+            t = threading.Thread(target=run_job, args=(group_b, i))\n+            t.start()\n+            threads.append(t)\n+\n+        # Wait to make sure all jobs are executed.\n+        time.sleep(1)"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Oh, no. We will still support python2 in 3.0. It's deprecated in 3.0. Seems like we might depreciate 3.4 and 3.5 too - it's being discussed in the mailing list.",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-10-29T22:56:59Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # Sleep for some arbitrary time.\n+                time.sleep(random.random())\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-treading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([3]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True\n+\n+        # Test if job succeeded when not cancelled.\n+        run_job(group_a, 0)\n+        self.assertFalse(is_job_cancelled[0])\n+\n+        # Run jobs\n+        for i in thread_ids_to_cancel:\n+            t = threading.Thread(target=run_job, args=(group_a, i))\n+            t.start()\n+            threads.append(t)\n+\n+        for i in thread_ids_to_run:\n+            t = threading.Thread(target=run_job, args=(group_b, i))\n+            t.start()\n+            threads.append(t)\n+\n+        # Wait to make sure all jobs are executed.\n+        time.sleep(1)"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Yeah but I thought it's a bit complicated. Given multiple test runs, seems not flaky in Jenkins at least. So I guess it might be fine as is for now. I'll just change it by using barrier after we release 3.0 since Python 2 (and maybe python 3.4 and 3.5) will be dropped at 3.1.",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-10-29T22:58:53Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # Sleep for some arbitrary time.\n+                time.sleep(random.random())\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-treading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([3]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True\n+\n+        # Test if job succeeded when not cancelled.\n+        run_job(group_a, 0)\n+        self.assertFalse(is_job_cancelled[0])\n+\n+        # Run jobs\n+        for i in thread_ids_to_cancel:\n+            t = threading.Thread(target=run_job, args=(group_a, i))\n+            t.start()\n+            threads.append(t)\n+\n+        for i in thread_ids_to_run:\n+            t = threading.Thread(target=run_job, args=(group_b, i))\n+            t.start()\n+            threads.append(t)\n+\n+        # Wait to make sure all jobs are executed.\n+        time.sleep(1)"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "oops, sorry I was way off on python support, thanks for correcting me :)\r\n\r\nI really think we should make this more reliable, though -- lots of tests have had to be deflaked because of sleeps which usually more than long enough, but fail every so often.  Things which normally take < 1ms have had pauses of 5s (overloaded host machine?  JVM full gc pause?).  A couple of test runs on jenkins here won't find that.",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-10-31T16:35:21Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # Sleep for some arbitrary time.\n+                time.sleep(random.random())\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-treading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([3]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True\n+\n+        # Test if job succeeded when not cancelled.\n+        run_job(group_a, 0)\n+        self.assertFalse(is_job_cancelled[0])\n+\n+        # Run jobs\n+        for i in thread_ids_to_cancel:\n+            t = threading.Thread(target=run_job, args=(group_a, i))\n+            t.start()\n+            threads.append(t)\n+\n+        for i in thread_ids_to_run:\n+            t = threading.Thread(target=run_job, args=(group_b, i))\n+            t.start()\n+            threads.append(t)\n+\n+        # Wait to make sure all jobs are executed.\n+        time.sleep(1)"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Hm, for the suggested codes, I think we won't sill be able to access because `job_func` will be de/serialized in a different PVM.\r\n\r\nSo, driver side won't be able to access what happens inside the worker sides .. we should open, for instance, another socket or check it, for instance, via writing a file...",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-11-01T02:42:29Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # Sleep for some arbitrary time.\n+                time.sleep(random.random())\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-treading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([3]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True\n+\n+        # Test if job succeeded when not cancelled.\n+        run_job(group_a, 0)\n+        self.assertFalse(is_job_cancelled[0])\n+\n+        # Run jobs\n+        for i in thread_ids_to_cancel:\n+            t = threading.Thread(target=run_job, args=(group_a, i))\n+            t.start()\n+            threads.append(t)\n+\n+        for i in thread_ids_to_run:\n+            t = threading.Thread(target=run_job, args=(group_b, i))\n+            t.start()\n+            threads.append(t)\n+\n+        # Wait to make sure all jobs are executed.\n+        time.sleep(1)"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Seems like Scala side test (`JobCancellationSuite`) is also written with sleep too ..",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-11-01T02:48:58Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # Sleep for some arbitrary time.\n+                time.sleep(random.random())\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-treading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([3]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True\n+\n+        # Test if job succeeded when not cancelled.\n+        run_job(group_a, 0)\n+        self.assertFalse(is_job_cancelled[0])\n+\n+        # Run jobs\n+        for i in thread_ids_to_cancel:\n+            t = threading.Thread(target=run_job, args=(group_a, i))\n+            t.start()\n+            threads.append(t)\n+\n+        for i in thread_ids_to_run:\n+            t = threading.Thread(target=run_job, args=(group_b, i))\n+            t.start()\n+            threads.append(t)\n+\n+        # Wait to make sure all jobs are executed.\n+        time.sleep(1)"
  }, {
    "author": {
      "login": "squito"
    },
    "body": "you're right, sorry I was thinking this was like the spark local-mode tests, where everything is in the same JVM.  OK, lets try it this way for now, and then we can change if we it ends up being flaky.",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-11-02T20:15:27Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # Sleep for some arbitrary time.\n+                time.sleep(random.random())\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-treading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([3]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True\n+\n+        # Test if job succeeded when not cancelled.\n+        run_job(group_a, 0)\n+        self.assertFalse(is_job_cancelled[0])\n+\n+        # Run jobs\n+        for i in thread_ids_to_cancel:\n+            t = threading.Thread(target=run_job, args=(group_a, i))\n+            t.start()\n+            threads.append(t)\n+\n+        for i in thread_ids_to_run:\n+            t = threading.Thread(target=run_job, args=(group_b, i))\n+            t.start()\n+            threads.append(t)\n+\n+        # Wait to make sure all jobs are executed.\n+        time.sleep(1)"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Thanks @squito. I increased the timeout to be safer for now.",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-11-04T05:39:27Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # Sleep for some arbitrary time.\n+                time.sleep(random.random())\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-treading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([3]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True\n+\n+        # Test if job succeeded when not cancelled.\n+        run_job(group_a, 0)\n+        self.assertFalse(is_job_cancelled[0])\n+\n+        # Run jobs\n+        for i in thread_ids_to_cancel:\n+            t = threading.Thread(target=run_job, args=(group_a, i))\n+            t.start()\n+            threads.append(t)\n+\n+        for i in thread_ids_to_run:\n+            t = threading.Thread(target=run_job, args=(group_b, i))\n+            t.start()\n+            threads.append(t)\n+\n+        # Wait to make sure all jobs are executed.\n+        time.sleep(1)"
  }],
  "prId": 24898
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "just a general question about pyspark code -- how come we don't use docstrings for these class & method comments?  Seems we do in a few cases, but not consistently, mostly they're just comments.",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-10-25T14:52:24Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.",
    "line": 28
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Yeah, agree. I don't think there's kind of a consistent rule for that. I usually just write comments when the contents are not directly related to the description of the class or method. e.g.) \"this class contains tests blah blah\" -> docstring, \"this test class don't use something because blah blah\" -> comments . But I guess it's just me :-).\r\n\r\n\r\nActually I don't bother docstrings often in test classes because it's unlikely the docstrings in class are read via pydoc (e.g., `help(PinThreadTests)` in Python shall) or generated as a doc.\r\n\r\n",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-10-29T04:24:23Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.",
    "line": 28
  }],
  "prId": 24898
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "I have always been confused about the guarantees of python around mutating a variable like this from multiple threads -- I can't find anything which makes it clear that this mutation is *visible* to other threads.  The section on the GIL says they'll be atomic (https://docs.python.org/3/faq/library.html#what-kinds-of-global-value-mutation-are-thread-safe) but that isn't quite the same.\r\n\r\nI guess this OK?  again something to be aware of it we see flakiness",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-11-06T20:11:51Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # 5 threads, 1 second sleep. 5 threads without a sleep.\n+                time.sleep(i % 2)\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-threading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([15]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True",
    "line": 110
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Ah, yeah, such pattern is considered safe given my experience. I think `D[x] = y` infers this case .. ? I think it's fine anyway.",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-11-07T10:30:49Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # 5 threads, 1 second sleep. 5 threads without a sleep.\n+                time.sleep(i % 2)\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-threading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([15]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True",
    "line": 110
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "BTW, there's `dis` package to check Python's opcodes (e.g., `import dis; func = lambda: 1 + 1;  dis.dis(func)`). seems assignment is a single atomic instruction in Python so looks fine.",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-11-07T10:44:12Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # 5 threads, 1 second sleep. 5 threads without a sleep.\n+                time.sleep(i % 2)\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-threading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([15]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True",
    "line": 110
  }, {
    "author": {
      "login": "squito"
    },
    "body": "that link above says it'll be atomic, but that's not exactly the same as knowing the change is *visible* -- there will be some per-core cache which isn't always flushed.  Or, at least, its not in lower-level languages, but maybe it really is in python?  I guess it must be (or flushed every time the GIL changes threads); otherwise this would have to be discussed *somewhere* in python docs",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-11-07T17:34:44Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # 5 threads, 1 second sleep. 5 threads without a sleep.\n+                time.sleep(i % 2)\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-threading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([15]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True",
    "line": 110
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Ah, I see. Yeah, we were talking about visibility. I think it must be ...",
    "commit": "9e2d83277635b02cc03f35502294892dc7d8a3d2",
    "createdAt": "2019-11-07T20:34:10Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import time\n+import random\n+import threading\n+import unittest\n+\n+from pyspark import SparkContext, SparkConf\n+\n+\n+class PinThreadTests(unittest.TestCase):\n+    # These tests are in a separate class because it uses\n+    # 'PYSPARK_PIN_THREAD' environment variable to test thread pin feature.\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.old_pin_thread = os.environ.get(\"PYSPARK_PIN_THREAD\")\n+        os.environ[\"PYSPARK_PIN_THREAD\"] = \"true\"\n+        cls.sc = SparkContext('local[4]', cls.__name__, conf=SparkConf())\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.sc.stop()\n+        if cls.old_pin_thread is not None:\n+            os.environ[\"PYSPARK_PIN_THREAD\"] = cls.old_pin_thread\n+        else:\n+            del os.environ[\"PYSPARK_PIN_THREAD\"]\n+\n+    def test_pinned_thread(self):\n+        threads = []\n+        exceptions = []\n+        property_name = \"test_property_%s\" % PinThreadTests.__name__\n+        jvm_thread_ids = []\n+\n+        for i in range(10):\n+            def test_local_property():\n+                jvm_thread_id = self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                jvm_thread_ids.append(jvm_thread_id)\n+\n+                # If a property is set in this thread, later it should get the same property\n+                # within this thread.\n+                self.sc.setLocalProperty(property_name, str(i))\n+\n+                # 5 threads, 1 second sleep. 5 threads without a sleep.\n+                time.sleep(i % 2)\n+\n+                try:\n+                    assert self.sc.getLocalProperty(property_name) == str(i)\n+\n+                    # Each command might create a thread in multi-threading mode in Py4J.\n+                    # This assert makes sure that the created thread is being reused.\n+                    assert jvm_thread_id == self.sc._jvm.java.lang.Thread.currentThread().getId()\n+                except Exception as e:\n+                    exceptions.append(e)\n+            threads.append(threading.Thread(target=test_local_property))\n+\n+        for t in threads:\n+            t.start()\n+\n+        for t in threads:\n+            t.join()\n+\n+        for e in exceptions:\n+            raise e\n+\n+        # Created JVM threads should be 10 because Python thread are 10.\n+        assert len(set(jvm_thread_ids)) == 10\n+\n+    def test_multiple_group_jobs(self):\n+        # SPARK-22340 Add a mode to pin Python thread into JVM's\n+\n+        group_a = \"job_ids_to_cancel\"\n+        group_b = \"job_ids_to_run\"\n+\n+        threads = []\n+        thread_ids = range(4)\n+        thread_ids_to_cancel = [i for i in thread_ids if i % 2 == 0]\n+        thread_ids_to_run = [i for i in thread_ids if i % 2 != 0]\n+\n+        # A list which records whether job is cancelled.\n+        # The index of the array is the thread index which job run in.\n+        is_job_cancelled = [False for _ in thread_ids]\n+\n+        def run_job(job_group, index):\n+            \"\"\"\n+            Executes a job with the group ``job_group``. Each job waits for 3 seconds\n+            and then exits.\n+            \"\"\"\n+            try:\n+                self.sc.setJobGroup(job_group, \"test rdd collect with setting job group\")\n+                self.sc.parallelize([15]).map(lambda x: time.sleep(x)).collect()\n+                is_job_cancelled[index] = False\n+            except Exception:\n+                # Assume that exception means job cancellation.\n+                is_job_cancelled[index] = True",
    "line": 110
  }],
  "prId": 24898
}]