[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Mind explaining why it has to be changed?",
    "commit": "b0df927d6ab842fd07e073aaad8b34307bdf26b0",
    "createdAt": "2019-01-15T10:55:03Z",
    "diffHunk": "@@ -605,7 +605,7 @@ def test_distinct(self):\n \n     def test_external_group_by_key(self):\n         self.sc._conf.set(\"spark.python.worker.memory\", \"1m\")\n-        N = 200001\n+        N = 2000001",
    "line": 5
  }, {
    "author": {
      "login": "inpefess"
    },
    "body": "Now I can only say that the type of `result.data` in this test depends on the size (N) of the input object and the pickle protocol version. Let me investigate it further.",
    "commit": "b0df927d6ab842fd07e073aaad8b34307bdf26b0",
    "createdAt": "2019-01-15T11:24:49Z",
    "diffHunk": "@@ -605,7 +605,7 @@ def test_distinct(self):\n \n     def test_external_group_by_key(self):\n         self.sc._conf.set(\"spark.python.worker.memory\", \"1m\")\n-        N = 200001\n+        N = 2000001",
    "line": 5
  }, {
    "author": {
      "login": "inpefess"
    },
    "body": "Well, if the object that you'd serialised is too large to fit in memory (like here https://github.com/apache/spark/blob/05cf81e6de3d61ddb0af81cd179665693f23351f/python/pyspark/shuffle.py#L774), then the `result.data` will have type `shuffle.ExternalListOfList`. If the object is small, then the type will be just `list`.\r\nWhen using protocol version 4 (since the test fails only for Python 3.4) we serialise more efficiently, so we need more data not to fit in memory.\r\nIs that enough to justify the change? @HyukjinKwon ",
    "commit": "b0df927d6ab842fd07e073aaad8b34307bdf26b0",
    "createdAt": "2019-01-15T12:52:31Z",
    "diffHunk": "@@ -605,7 +605,7 @@ def test_distinct(self):\n \n     def test_external_group_by_key(self):\n         self.sc._conf.set(\"spark.python.worker.memory\", \"1m\")\n-        N = 200001\n+        N = 2000001",
    "line": 5
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Yup, sounds good.",
    "commit": "b0df927d6ab842fd07e073aaad8b34307bdf26b0",
    "createdAt": "2019-01-18T15:30:46Z",
    "diffHunk": "@@ -605,7 +605,7 @@ def test_distinct(self):\n \n     def test_external_group_by_key(self):\n         self.sc._conf.set(\"spark.python.worker.memory\", \"1m\")\n-        N = 200001\n+        N = 2000001",
    "line": 5
  }],
  "prId": 20691
}]