[{
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Minor formatting nit, but these lines can be longer (up to 100 characters).\n",
    "commit": "d0a747901af273c5eb8578ec191cc7673f6efce5",
    "createdAt": "2015-01-26T19:26:19Z",
    "diffHunk": "@@ -0,0 +1,330 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+Python bindings for VertexRDD in GraphX\n+\"\"\"\n+\n+import itertools\n+import os\n+from tempfile import NamedTemporaryFile\n+from numpy.numarray.numerictypes import Long\n+\n+from py4j.java_collections import MapConverter, ListConverter\n+import operator\n+from pyspark.accumulators import PStatsParam\n+from pyspark.rdd import PipelinedRDD\n+from pyspark.serializers import CloudPickleSerializer, NoOpSerializer, AutoBatchedSerializer\n+from pyspark import RDD, PickleSerializer, StorageLevel, SparkContext\n+from pyspark.traceback_utils import SCCallSiteSync\n+\n+\n+__all__ = [\"VertexRDD\", \"VertexId\"]\n+\n+\n+\"\"\"\n+The default type of vertex id is Long\n+A separate VertexId type is defined",
    "line": 41
  }],
  "prId": 4205
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "There's a lot of duplication between this method, the one in edge RDD, and the regular PySpark RDD.  I'm worried that this could become a maintenance problem since lots of common code is duplicated and will fall out of sync.  Do you know if there's a good way to add some abstraction here so that we're not copying, say, the profiling code into three places?\n",
    "commit": "d0a747901af273c5eb8578ec191cc7673f6efce5",
    "createdAt": "2015-01-26T19:28:11Z",
    "diffHunk": "@@ -0,0 +1,330 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+Python bindings for VertexRDD in GraphX\n+\"\"\"\n+\n+import itertools\n+import os\n+from tempfile import NamedTemporaryFile\n+from numpy.numarray.numerictypes import Long\n+\n+from py4j.java_collections import MapConverter, ListConverter\n+import operator\n+from pyspark.accumulators import PStatsParam\n+from pyspark.rdd import PipelinedRDD\n+from pyspark.serializers import CloudPickleSerializer, NoOpSerializer, AutoBatchedSerializer\n+from pyspark import RDD, PickleSerializer, StorageLevel, SparkContext\n+from pyspark.traceback_utils import SCCallSiteSync\n+\n+\n+__all__ = [\"VertexRDD\", \"VertexId\"]\n+\n+\n+\"\"\"\n+The default type of vertex id is Long\n+A separate VertexId type is defined\n+here so that other types can be used\n+for vertex ids in future\n+\"\"\"\n+VertexId = Long\n+\n+\n+class VertexRDD(object):\n+    \"\"\"\n+    VertexRDD class defines the vertex actions and transformations. The complete list of\n+    transformations and actions for vertices are available at\n+    `http://spark.apache.org/docs/latest/graphx-programming-guide.html`\n+    These operations are mapped to Scala functions defined\n+    in `org.apache.spark.graphx.impl.VertexRDDImpl`\n+    \"\"\"\n+\n+    def __init__(self, jrdd, jrdd_deserializer=AutoBatchedSerializer(PickleSerializer())):\n+        \"\"\"\n+        Constructor\n+        :param jrdd:               A JavaRDD reference passed from the parent\n+                                   RDD object\n+        :param jrdd_deserializer:  The deserializer used in Python workers\n+                                   created from PythonRDD to execute a\n+                                   serialized Python function and RDD\n+        \"\"\"\n+\n+        self.name = \"VertexRDD\"\n+        self.jrdd = jrdd\n+        self.is_cached = False\n+        self.is_checkpointed = False\n+        self.ctx = SparkContext._active_spark_context\n+        self.jvertex_rdd_deserializer = jrdd_deserializer\n+        self.id = jrdd.id()\n+        self.partitionFunc = None\n+        self.bypass_serializer = False\n+        self.preserve_partitioning = False\n+\n+        self.jvertex_rdd = self.getJavaVertexRDD(jrdd, jrdd_deserializer)\n+\n+    def __repr__(self):\n+        return self.jvertex_rdd.toString()\n+\n+    def cache(self):\n+        \"\"\"\n+        Persist this vertex RDD with the default storage level (C{MEMORY_ONLY_SER}).\n+        \"\"\"\n+        self.is_cached = True\n+        self.persist(StorageLevel.MEMORY_ONLY_SER)\n+        return self\n+\n+    def checkpoint(self):\n+        self.is_checkpointed = True\n+        self.jvertex_rdd.checkpoint()\n+\n+    def count(self):\n+        return self.jvertex_rdd.count()\n+\n+    def diff(self, other, numPartitions=2):\n+        \"\"\"\n+        Hides vertices that are the same between `this` and `other`.\n+        For vertices that are different, keeps the values from `other`.\n+\n+        TODO: give an example\n+        \"\"\"\n+        if (isinstance(other, RDD)):\n+            vs = self.map(lambda (k, v): (k, (1, v)))\n+            ws = other.map(lambda (k, v): (k, (2, v)))\n+        return vs.union(ws).groupByKey(numPartitions).mapValues(lambda x: x.diff(x.__iter__()))\n+\n+    def isCheckpointed(self):\n+        \"\"\"\n+        Return whether this RDD has been checkpointed or not\n+        \"\"\"\n+        return self.is_checkpointed\n+\n+    def mapValues(self, f, preserves_partitioning=False):\n+        \"\"\"\n+        Return a new vertex RDD by applying a function to each vertex attributes,\n+        preserving the index\n+\n+        >>> rdd = sc.parallelize([(1, \"b\"), (2, \"a\"), (3, \"c\")])\n+        >>> vertices = VertexRDD(rdd)\n+        >>> sorted(vertices.mapValues(lambda x: (x + \":\" + x)).collect())\n+        [(1, 'a:a'), (2, 'b:b'), (3, 'c:c')]\n+        \"\"\"\n+        def func(_, iterator):\n+            return itertools.imap(lambda (k, v): (k, f(v)), iterator)\n+        return PipelinedVertexRDD(self, func, preserves_partitioning)\n+\n+    def persist(self, storageLevel=StorageLevel.MEMORY_ONLY_SER):\n+        self.is_cached = True\n+        java_storage_level = self.ctx._getJavaStorageLevel(storageLevel)\n+        self.jvertex_rdd.persist(java_storage_level)\n+        return self\n+\n+    # TODO: This is a hack. take() must call JavaVertexRDD.take()\n+    def take(self, num=10):\n+        return self.jrdd.take(num)\n+\n+    def unpersist(self, blocking = False):\n+        self.is_cached = False\n+        self.jvertex_rdd.unpersist(blocking)\n+        return self\n+\n+    def mapVertexPartitions(self, f, preserve_partitioning=False):\n+        def func(s, iterator):\n+            return f(iterator)\n+        return PipelinedVertexRDD(self, func, preserve_partitioning)\n+\n+    # TODO\n+    def filter(self, f):\n+        \"\"\"\n+        Return a new vertex RDD containing only the elements that satisfy a predicate.\n+\n+        >>> rdd = sc.parallelize([(1, \"b\"), (2, \"a\"), (3, \"c\")])\n+        >>> vertices = VertexRDD(rdd)\n+        >>> vertices.filter(lambda x: x._1 % 2 == 0).collect()\n+        [2]\n+        \"\"\"\n+        return self.jvertex_rdd.filter(f)\n+\n+    # TODO: The best way to do an innerJoin on vertex RDDs is to use the optimized inner\n+    # TODO: technique defined in VertexRDDImpl. This solution does not scale\n+    def innerJoin(self, other):\n+        return self.jrdd.join(other.jrdd)\n+\n+    def leftJoin(self, other, numPartitions=None):\n+        return self.jrdd.leftOuterJoin(other.jrdd, numPartitions)\n+\n+    def collect(self):\n+        \"\"\"\n+        Return a list that contains all of the elements in this RDD.\n+        \"\"\"\n+        with SCCallSiteSync(self.ctx) as css:\n+            bytesInJava = self.jvertex_rdd.collect().iterator()\n+        return list(self._collect_iterator_through_file(bytesInJava))\n+\n+    def _collect_iterator_through_file(self, iterator):\n+        # Transferring lots of data through Py4J can be slow because\n+        # socket.readline() is inefficient.  Instead, we'll dump the data to a\n+        # file and read it back.\n+        tempFile = NamedTemporaryFile(delete=False, dir=self.ctx._temp_dir)\n+        tempFile.close()\n+        self.ctx._writeToFile(iterator, tempFile.name)\n+        # Read the data into Python and deserialize it:\n+        with open(tempFile.name, 'rb') as tempFile:\n+            for item in self.jvertex_rdd_deserializer.load_stream(tempFile):\n+                yield item\n+        os.unlink(tempFile.name)\n+\n+    def getJavaVertexRDD(self, rdd, rdd_deserializer):\n+        if self.bypass_serializer:\n+            self.jvertex_rdd_deserializer = NoOpSerializer()\n+            rdd_deserializer = NoOpSerializer()\n+        enable_profile = self.ctx._conf.get(\"spark.python.profile\", \"false\") == \"true\"\n+        profileStats = self.ctx.accumulator(None, PStatsParam) if enable_profile else None\n+        def f(index, iterator):\n+            return iterator\n+        command = (f, profileStats, rdd_deserializer,\n+                   rdd_deserializer)\n+        # the serialized command will be compressed by broadcast\n+        ser = CloudPickleSerializer()\n+        pickled_command = ser.dumps(command)\n+        if len(pickled_command) > (1 << 20):  # 1M\n+            self.broadcast = self.ctx.broadcast(pickled_command)\n+            pickled_command = ser.dumps(self.broadcast)\n+\n+        # the serialized command will be compressed by broadcast\n+        broadcast_vars = ListConverter().convert(\n+            [x._jbroadcast for x in self.ctx._pickled_broadcast_vars],\n+            self.ctx._gateway._gateway_client)\n+        self.ctx._pickled_broadcast_vars.clear()\n+        env = MapConverter().convert(self.ctx.environment,\n+                                     self.ctx._gateway._gateway_client)\n+        includes = ListConverter().convert(self.ctx._python_includes,\n+                                           self.ctx._gateway._gateway_client)\n+        target_storage_level = StorageLevel.MEMORY_ONLY\n+        java_storage_level = self.ctx._getJavaStorageLevel(target_storage_level)\n+        prdd = self.ctx._jvm.PythonVertexRDD(rdd._jrdd,\n+                                                   bytearray(pickled_command),\n+                                                   env, includes, self.preserve_partitioning,\n+                                                   self.ctx.pythonExec,\n+                                                   broadcast_vars, self.ctx._javaAccumulator,\n+                                                   java_storage_level)\n+        self.jvertex_rdd = prdd.asJavaVertexRDD()\n+        if enable_profile:\n+            self.id = self.jvertex_rdd.id()\n+            self.ctx._add_profile(self.id, profileStats)\n+        return self.jvertex_rdd\n+\n+\n+class PipelinedVertexRDD(VertexRDD):\n+\n+    \"\"\"\n+    Pipelined mapValues in VertexRDD:\n+\n+    >>> rdd = sc.parallelize([(1, (\"Alice\", 29)), (2, (\"Bob\", 30)), \\\n+                              (3, (\"Charlie\", 31)), (4, (\"Dwayne\", 32))])\n+    >>> vertices = VertexRDD(rdd)\n+    >>> vertices.mapValues(lambda x: x[1] * 2).cache().collect()\n+    [(1, (\"Alice\", 58)), (2, (\"Bob\", 60)), \\\n+     (3, (\"Charlie\", 62)), (4, (\"Dwayne\", 64))]\n+\n+    Pipelined reduces in VertexRDD:\n+    >>> from operator import add\n+    >>> rdd.map(lambda x: 2 * x).reduce(add)\n+    20\n+    >>> rdd.flatMap(lambda x: [x, x]).reduce(add)\n+    20\n+    \"\"\"\n+\n+    def __init__(self, prev, func, preservesPartitioning=False):\n+        if not isinstance(prev, PipelinedVertexRDD) or not prev.is_pipelinable():\n+            # This transformation is the first in its stage:\n+            self.func = func\n+            self.preservesPartitioning = preservesPartitioning\n+            self.prev_jvertex_rdd = prev.jvertex_rdd\n+            self.prev_jvertex_rdd_deserializer = prev.jvertex_rdd_deserializer\n+        else:\n+            prev_func = prev.func\n+\n+            def pipeline_func(split, iterator):\n+                return func(split, prev_func(split, iterator))\n+            self.func = pipeline_func\n+            self.preservesPartitioning = \\\n+                prev.preservesPartitioning and preservesPartitioning\n+            self.prev_jvertex_rdd = prev.jvertex_rdd\n+            self.prev_jvertex_rdd_deserializer = prev.prev_jvertex_rdd_deserializer\n+\n+        self.is_cached = False\n+        self.is_checkpointed = False\n+        self.ctx = prev.ctx\n+        self.prev = prev\n+        self.jvrdd_val = None\n+        self.id = None\n+        self.jvertex_rdd_deserializer = self.ctx.serializer\n+        self.bypass_serializer = False\n+        self.partitionFunc = prev._partitionFunc if self.preservesPartitioning else None\n+        self.broadcast = None\n+\n+    def __del__(self):\n+        if self.broadcast:\n+            self.broadcast.unpersist()\n+            self.broadcast = None\n+\n+    @property\n+    def jvertex_rdd(self):",
    "line": 287
  }],
  "prId": 4205
}]