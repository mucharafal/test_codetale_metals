[{
  "comments": [{
    "author": {
      "login": "giwa"
    },
    "body": "Sorry this may be my typo.\n\n```\n\"\"\"Sort the list base onf first value.\"\"\"\n```\n\nbased on the first value\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-26T08:50:58Z",
    "diffHunk": "@@ -0,0 +1,321 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+Unit tests for Python SparkStreaming; additional tests are implemented as doctests in\n+individual modules.\n+\n+Callback server is sometimes unstable sometimes, which cause error in test case.\n+But this is very rare case.\n+\"\"\"\n+from itertools import chain\n+import time\n+import operator\n+import unittest\n+\n+from pyspark.context import SparkContext\n+from pyspark.streaming.context import StreamingContext\n+from pyspark.streaming.duration import Seconds\n+\n+\n+class PySparkStreamingTestCase(unittest.TestCase):\n+    def setUp(self):\n+        class_name = self.__class__.__name__\n+        self.sc = SparkContext(appName=class_name)\n+        self.ssc = StreamingContext(self.sc, duration=Seconds(1))\n+\n+    def tearDown(self):\n+        # Do not call pyspark.streaming.context.StreamingContext.stop directly because\n+        # we do not wait to shutdown py4j client.\n+        self.ssc.stop()\n+        self.sc.stop()\n+        time.sleep(1)\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        # Make sure tp shutdown the callback server\n+        SparkContext._gateway._shutdown_callback_server()\n+\n+\n+class TestBasicOperations(PySparkStreamingTestCase):\n+    \"\"\"\n+    2 tests for each function for batach deserializer and unbatch deserilizer because\n+    the deserializer is not changed dunamically after streaming process starts.\n+    Default numInputPartitions is 2.\n+    If the number of input element is over 3, that DStream use batach deserializer.\n+    If not, that DStream use unbatch deserializer.\n+\n+    All tests input should have list of lists(3 lists are default). This list represents stream.\n+    Every batch interval, the first object of list are chosen to make DStream.\n+    e.g The first list in the list is input of the first batch.\n+    Please see the BasicTestSuits in Scala which is close to this implementation.\n+    \"\"\"\n+    def setUp(self):\n+        PySparkStreamingTestCase.setUp(self)\n+        self.timeout = 10  # seconds\n+        self.numInputPartitions = 2\n+\n+    def test_map(self):\n+        \"\"\"Basic operation test for DStream.map.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+\n+        def func(dstream):\n+            return dstream.map(str)\n+        expected = map(lambda x: map(str, x), input)\n+        self._test_func(input, func, expected)\n+\n+    def test_flatMap(self):\n+        \"\"\"Basic operation test for DStream.faltMap.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+\n+        def func(dstream):\n+            return dstream.flatMap(lambda x: (x, x * 2))\n+        expected = map(lambda x: list(chain.from_iterable((map(lambda y: [y, y * 2], x)))),\n+                       input)\n+        self._test_func(input, func, expected)\n+\n+    def test_filter(self):\n+        \"\"\"Basic operation test for DStream.filter.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+\n+        def func(dstream):\n+            return dstream.filter(lambda x: x % 2 == 0)\n+        expected = map(lambda x: filter(lambda y: y % 2 == 0, x), input)\n+        self._test_func(input, func, expected)\n+\n+    def test_count(self):\n+        \"\"\"Basic operation test for DStream.count.\"\"\"\n+        input = [range(1, 5), range(1, 10), range(1, 20)]\n+\n+        def func(dstream):\n+            return dstream.count()\n+        expected = map(lambda x: [len(x)], input)\n+        self._test_func(input, func, expected)\n+\n+    def test_reduce(self):\n+        \"\"\"Basic operation test for DStream.reduce.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+\n+        def func(dstream):\n+            return dstream.reduce(operator.add)\n+        expected = map(lambda x: [reduce(operator.add, x)], input)\n+        self._test_func(input, func, expected)\n+\n+    def test_reduceByKey(self):\n+        \"\"\"Basic operation test for DStream.reduceByKey.\"\"\"\n+        input = [[(\"a\", 1), (\"a\", 1), (\"b\", 1), (\"b\", 1)],\n+                 [(\"\", 1), (\"\", 1), (\"\", 1), (\"\", 1)],\n+                 [(1, 1), (1, 1), (2, 1), (2, 1), (3, 1)]]\n+\n+        def func(dstream):\n+            return dstream.reduceByKey(operator.add)\n+        expected = [[(\"a\", 2), (\"b\", 2)], [(\"\", 4)], [(1, 2), (2, 2), (3, 1)]]\n+        self._test_func(input, func, expected, sort=True)\n+\n+    def test_mapValues(self):\n+        \"\"\"Basic operation test for DStream.mapValues.\"\"\"\n+        input = [[(\"a\", 2), (\"b\", 2), (\"c\", 1), (\"d\", 1)],\n+                 [(\"\", 4), (1, 1), (2, 2), (3, 3)],\n+                 [(1, 1), (2, 1), (3, 1), (4, 1)]]\n+\n+        def func(dstream):\n+            return dstream.mapValues(lambda x: x + 10)\n+        expected = [[(\"a\", 12), (\"b\", 12), (\"c\", 11), (\"d\", 11)],\n+                    [(\"\", 14), (1, 11), (2, 12), (3, 13)],\n+                    [(1, 11), (2, 11), (3, 11), (4, 11)]]\n+        self._test_func(input, func, expected, sort=True)\n+\n+    def test_flatMapValues(self):\n+        \"\"\"Basic operation test for DStream.flatMapValues.\"\"\"\n+        input = [[(\"a\", 2), (\"b\", 2), (\"c\", 1), (\"d\", 1)],\n+                 [(\"\", 4), (1, 1), (2, 1), (3, 1)],\n+                 [(1, 1), (2, 1), (3, 1), (4, 1)]]\n+\n+        def func(dstream):\n+            return dstream.flatMapValues(lambda x: (x, x + 10))\n+        expected = [[(\"a\", 2), (\"a\", 12), (\"b\", 2), (\"b\", 12),\n+                     (\"c\", 1), (\"c\", 11), (\"d\", 1), (\"d\", 11)],\n+                    [(\"\", 4), (\"\", 14), (1, 1), (1, 11), (2, 1), (2, 11), (3, 1), (3, 11)],\n+                    [(1, 1), (1, 11), (2, 1), (2, 11), (3, 1), (3, 11), (4, 1), (4, 11)]]\n+        self._test_func(input, func, expected)\n+\n+    def test_glom(self):\n+        \"\"\"Basic operation test for DStream.glom.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+        numSlices = 2\n+\n+        def func(dstream):\n+            return dstream.glom()\n+        expected = [[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]]\n+        self._test_func(input, func, expected, numSlices)\n+\n+    def test_mapPartitions(self):\n+        \"\"\"Basic operation test for DStream.mapPartitions.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+        numSlices = 2\n+\n+        def func(dstream):\n+            def f(iterator):\n+                yield sum(iterator)\n+            return dstream.mapPartitions(f)\n+        expected = [[3, 7], [11, 15], [19, 23]]\n+        self._test_func(input, func, expected, numSlices)\n+\n+    def test_countByValue(self):\n+        \"\"\"Basic operation test for DStream.countByValue.\"\"\"\n+        input = [range(1, 5) * 2, range(5, 7) + range(5, 9), [\"a\", \"a\", \"b\", \"\"]]\n+\n+        def func(dstream):\n+            return dstream.countByValue()\n+        expected = [[4], [4], [3]]\n+        self._test_func(input, func, expected)\n+\n+    def test_groupByKey(self):\n+        \"\"\"Basic operation test for DStream.groupByKey.\"\"\"\n+        input = [[(1, 1), (2, 1), (3, 1), (4, 1)],\n+                 [(1, 1), (1, 1), (1, 1), (2, 1), (2, 1), (3, 1)],\n+                 [(\"a\", 1), (\"a\", 1), (\"b\", 1), (\"\", 1), (\"\", 1), (\"\", 1)]]\n+\n+        def func(dstream):\n+            return dstream.groupByKey().mapValues(list)\n+\n+        expected = [[(1, [1]), (2, [1]), (3, [1]), (4, [1])],\n+                    [(1, [1, 1, 1]), (2, [1, 1]), (3, [1])],\n+                    [(\"a\", [1, 1]), (\"b\", [1]), (\"\", [1, 1, 1])]]\n+        self._test_func(input, func, expected, sort=True)\n+\n+    def test_combineByKey(self):\n+        \"\"\"Basic operation test for DStream.combineByKey.\"\"\"\n+        input = [[(1, 1), (2, 1), (3, 1), (4, 1)],\n+                 [(1, 1), (1, 1), (1, 1), (2, 1), (2, 1), (3, 1)],\n+                 [(\"a\", 1), (\"a\", 1), (\"b\", 1), (\"\", 1), (\"\", 1), (\"\", 1)]]\n+\n+        def func(dstream):\n+            def add(a, b):\n+                return a + str(b)\n+            return dstream.combineByKey(str, add, add)\n+        expected = [[(1, \"1\"), (2, \"1\"), (3, \"1\"), (4, \"1\")],\n+                    [(1, \"111\"), (2, \"11\"), (3, \"1\")],\n+                    [(\"a\", \"11\"), (\"b\", \"1\"), (\"\", \"111\")]]\n+        self._test_func(input, func, expected, sort=True)\n+\n+    def test_union(self):\n+        input1 = [range(3), range(5), range(1)]\n+        input2 = [range(3, 6), range(5, 6), range(1, 6)]\n+\n+        d1 = self.ssc._makeStream(input1)\n+        d2 = self.ssc._makeStream(input2)\n+        d = d1.union(d2)\n+        result = d.collect()\n+        expected = [range(6), range(6), range(6)]\n+\n+        self.ssc.start()\n+        start_time = time.time()\n+        # Loop until get the expected the number of the result from the stream.\n+        while True:\n+            current_time = time.time()\n+            # Check time out.\n+            if (current_time - start_time) > self.timeout * 2:\n+                break\n+            # StreamingContext.awaitTermination is not used to wait because\n+            # if py4j server is called every 50 milliseconds, it gets an error.\n+            time.sleep(0.05)\n+            # Check if the output is the same length of expected output.\n+            if len(expected) == len(result):\n+                break\n+        self.assertEqual(expected, result)\n+\n+    def _sort_result_based_on_key(self, outputs):\n+        \"\"\"Sort the list base onf first value.\"\"\""
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "giwa"
    },
    "body": "We do not need this comments because StreaminigContext.stop does not stop call back server.\n\n```\n        # Do not call pyspark.streaming.context.StreamingContext.stop directly because\n        # we do not wait to shutdown py4j client.\n```\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-26T09:29:54Z",
    "diffHunk": "@@ -0,0 +1,321 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+Unit tests for Python SparkStreaming; additional tests are implemented as doctests in\n+individual modules.\n+\n+Callback server is sometimes unstable sometimes, which cause error in test case.\n+But this is very rare case.\n+\"\"\"\n+from itertools import chain\n+import time\n+import operator\n+import unittest\n+\n+from pyspark.context import SparkContext\n+from pyspark.streaming.context import StreamingContext\n+from pyspark.streaming.duration import Seconds\n+\n+\n+class PySparkStreamingTestCase(unittest.TestCase):\n+    def setUp(self):\n+        class_name = self.__class__.__name__\n+        self.sc = SparkContext(appName=class_name)\n+        self.ssc = StreamingContext(self.sc, duration=Seconds(1))\n+\n+    def tearDown(self):\n+        # Do not call pyspark.streaming.context.StreamingContext.stop directly because\n+        # we do not wait to shutdown py4j client."
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "giwa"
    },
    "body": "Sorry, this is my typo.\n\n```\n# Make sure t'o' shutdown the callback server\n```\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-28T21:21:58Z",
    "diffHunk": "@@ -0,0 +1,385 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+Unit tests for Python SparkStreaming; additional tests are implemented as doctests in\n+individual modules.\n+\n+Callback server is sometimes unstable sometimes, which cause error in test case.\n+But this is very rare case.\n+\"\"\"\n+from itertools import chain\n+import time\n+import operator\n+import unittest\n+\n+from pyspark.context import SparkContext\n+from pyspark.streaming.context import StreamingContext\n+from pyspark.streaming.duration import Seconds\n+\n+\n+class PySparkStreamingTestCase(unittest.TestCase):\n+\n+    timeout = 10  # seconds\n+\n+    def setUp(self):\n+        class_name = self.__class__.__name__\n+        self.sc = SparkContext(appName=class_name)\n+        self.sc.setCheckpointDir(\"/tmp\")\n+        # TODO: decrease duration to speed up tests\n+        self.ssc = StreamingContext(self.sc, duration=1)\n+\n+    def tearDown(self):\n+        self.ssc.stop()\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        # Make sure tp shutdown the callback server"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This is not a window operation, and hence should go into basic operations suite. This will be consistent with Scala testsuites.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T00:58:36Z",
    "diffHunk": "@@ -0,0 +1,473 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+import os\n+from itertools import chain\n+import time\n+import operator\n+import unittest\n+import tempfile\n+\n+from pyspark.context import SparkContext\n+from pyspark.streaming.context import StreamingContext\n+\n+\n+class PySparkStreamingTestCase(unittest.TestCase):\n+\n+    timeout = 10  # seconds\n+\n+    def setUp(self):\n+        class_name = self.__class__.__name__\n+        self.sc = SparkContext(appName=class_name)\n+        self.sc.setCheckpointDir(\"/tmp\")\n+        # TODO: decrease duration to speed up tests\n+        self.ssc = StreamingContext(self.sc, duration=1)\n+\n+    def tearDown(self):\n+        self.ssc.stop()\n+\n+    def _test_func(self, input, func, expected, sort=False, input2=None):\n+        \"\"\"\n+        @param input: dataset for the test. This should be list of lists.\n+        @param func: wrapped function. This function should return PythonDStream object.\n+        @param expected: expected output for this testcase.\n+        \"\"\"\n+        input_stream = self.ssc.queueStream(input)\n+        input_stream2 = self.ssc.queueStream(input2) if input2 is not None else None\n+        # Apply test function to stream.\n+        if input2:\n+            stream = func(input_stream, input_stream2)\n+        else:\n+            stream = func(input_stream)\n+\n+        result = stream._collect()\n+        self.ssc.start()\n+\n+        start_time = time.time()\n+        # Loop until get the expected the number of the result from the stream.\n+        while True:\n+            current_time = time.time()\n+            # Check time out.\n+            if (current_time - start_time) > self.timeout:\n+                break\n+            # StreamingContext.awaitTermination is not used to wait because\n+            # if py4j server is called every 50 milliseconds, it gets an error.\n+            time.sleep(0.05)\n+            # Check if the output is the same length of expected output.\n+            if len(expected) == len(result):\n+                break\n+        if sort:\n+            self._sort_result_based_on_key(result)\n+            self._sort_result_based_on_key(expected)\n+        self.assertEqual(expected, result)\n+\n+    def _sort_result_based_on_key(self, outputs):\n+        \"\"\"Sort the list based on first value.\"\"\"\n+        for output in outputs:\n+            output.sort(key=lambda x: x[0])\n+\n+\n+class TestBasicOperations(PySparkStreamingTestCase):\n+\n+    def test_take(self):\n+        input = [range(i) for i in range(3)]\n+        dstream = self.ssc.queueStream(input)\n+        self.assertEqual([0, 0, 1], dstream._take(3))\n+\n+    def test_first(self):\n+        input = [range(10)]\n+        dstream = self.ssc.queueStream(input)\n+        self.assertEqual(0, dstream._first())\n+\n+    def test_map(self):\n+        \"\"\"Basic operation test for DStream.map.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+\n+        def func(dstream):\n+            return dstream.map(str)\n+        expected = map(lambda x: map(str, x), input)\n+        self._test_func(input, func, expected)\n+\n+    def test_flatMap(self):\n+        \"\"\"Basic operation test for DStream.faltMap.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+\n+        def func(dstream):\n+            return dstream.flatMap(lambda x: (x, x * 2))\n+        expected = map(lambda x: list(chain.from_iterable((map(lambda y: [y, y * 2], x)))),\n+                       input)\n+        self._test_func(input, func, expected)\n+\n+    def test_filter(self):\n+        \"\"\"Basic operation test for DStream.filter.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+\n+        def func(dstream):\n+            return dstream.filter(lambda x: x % 2 == 0)\n+        expected = map(lambda x: filter(lambda y: y % 2 == 0, x), input)\n+        self._test_func(input, func, expected)\n+\n+    def test_count(self):\n+        \"\"\"Basic operation test for DStream.count.\"\"\"\n+        input = [range(5), range(10), range(20)]\n+\n+        def func(dstream):\n+            return dstream.count()\n+        expected = map(lambda x: [len(x)], input)\n+        self._test_func(input, func, expected)\n+\n+    def test_reduce(self):\n+        \"\"\"Basic operation test for DStream.reduce.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+\n+        def func(dstream):\n+            return dstream.reduce(operator.add)\n+        expected = map(lambda x: [reduce(operator.add, x)], input)\n+        self._test_func(input, func, expected)\n+\n+    def test_reduceByKey(self):\n+        \"\"\"Basic operation test for DStream.reduceByKey.\"\"\"\n+        input = [[(\"a\", 1), (\"a\", 1), (\"b\", 1), (\"b\", 1)],\n+                 [(\"\", 1), (\"\", 1), (\"\", 1), (\"\", 1)],\n+                 [(1, 1), (1, 1), (2, 1), (2, 1), (3, 1)]]\n+\n+        def func(dstream):\n+            return dstream.reduceByKey(operator.add)\n+        expected = [[(\"a\", 2), (\"b\", 2)], [(\"\", 4)], [(1, 2), (2, 2), (3, 1)]]\n+        self._test_func(input, func, expected, sort=True)\n+\n+    def test_mapValues(self):\n+        \"\"\"Basic operation test for DStream.mapValues.\"\"\"\n+        input = [[(\"a\", 2), (\"b\", 2), (\"c\", 1), (\"d\", 1)],\n+                 [(\"\", 4), (1, 1), (2, 2), (3, 3)],\n+                 [(1, 1), (2, 1), (3, 1), (4, 1)]]\n+\n+        def func(dstream):\n+            return dstream.mapValues(lambda x: x + 10)\n+        expected = [[(\"a\", 12), (\"b\", 12), (\"c\", 11), (\"d\", 11)],\n+                    [(\"\", 14), (1, 11), (2, 12), (3, 13)],\n+                    [(1, 11), (2, 11), (3, 11), (4, 11)]]\n+        self._test_func(input, func, expected, sort=True)\n+\n+    def test_flatMapValues(self):\n+        \"\"\"Basic operation test for DStream.flatMapValues.\"\"\"\n+        input = [[(\"a\", 2), (\"b\", 2), (\"c\", 1), (\"d\", 1)],\n+                 [(\"\", 4), (1, 1), (2, 1), (3, 1)],\n+                 [(1, 1), (2, 1), (3, 1), (4, 1)]]\n+\n+        def func(dstream):\n+            return dstream.flatMapValues(lambda x: (x, x + 10))\n+        expected = [[(\"a\", 2), (\"a\", 12), (\"b\", 2), (\"b\", 12),\n+                     (\"c\", 1), (\"c\", 11), (\"d\", 1), (\"d\", 11)],\n+                    [(\"\", 4), (\"\", 14), (1, 1), (1, 11), (2, 1), (2, 11), (3, 1), (3, 11)],\n+                    [(1, 1), (1, 11), (2, 1), (2, 11), (3, 1), (3, 11), (4, 1), (4, 11)]]\n+        self._test_func(input, func, expected)\n+\n+    def test_glom(self):\n+        \"\"\"Basic operation test for DStream.glom.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+        rdds = [self.sc.parallelize(r, 2) for r in input]\n+\n+        def func(dstream):\n+            return dstream.glom()\n+        expected = [[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]]\n+        self._test_func(rdds, func, expected)\n+\n+    def test_mapPartitions(self):\n+        \"\"\"Basic operation test for DStream.mapPartitions.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+        rdds = [self.sc.parallelize(r, 2) for r in input]\n+\n+        def func(dstream):\n+            def f(iterator):\n+                yield sum(iterator)\n+            return dstream.mapPartitions(f)\n+        expected = [[3, 7], [11, 15], [19, 23]]\n+        self._test_func(rdds, func, expected)\n+\n+    def test_countByValue(self):\n+        \"\"\"Basic operation test for DStream.countByValue.\"\"\"\n+        input = [range(1, 5) * 2, range(5, 7) + range(5, 9), [\"a\", \"a\", \"b\", \"\"]]\n+\n+        def func(dstream):\n+            return dstream.countByValue()\n+        expected = [[4], [4], [3]]\n+        self._test_func(input, func, expected)\n+\n+    def test_groupByKey(self):\n+        \"\"\"Basic operation test for DStream.groupByKey.\"\"\"\n+        input = [[(1, 1), (2, 1), (3, 1), (4, 1)],\n+                 [(1, 1), (1, 1), (1, 1), (2, 1), (2, 1), (3, 1)],\n+                 [(\"a\", 1), (\"a\", 1), (\"b\", 1), (\"\", 1), (\"\", 1), (\"\", 1)]]\n+\n+        def func(dstream):\n+            return dstream.groupByKey().mapValues(list)\n+\n+        expected = [[(1, [1]), (2, [1]), (3, [1]), (4, [1])],\n+                    [(1, [1, 1, 1]), (2, [1, 1]), (3, [1])],\n+                    [(\"a\", [1, 1]), (\"b\", [1]), (\"\", [1, 1, 1])]]\n+        self._test_func(input, func, expected, sort=True)\n+\n+    def test_combineByKey(self):\n+        \"\"\"Basic operation test for DStream.combineByKey.\"\"\"\n+        input = [[(1, 1), (2, 1), (3, 1), (4, 1)],\n+                 [(1, 1), (1, 1), (1, 1), (2, 1), (2, 1), (3, 1)],\n+                 [(\"a\", 1), (\"a\", 1), (\"b\", 1), (\"\", 1), (\"\", 1), (\"\", 1)]]\n+\n+        def func(dstream):\n+            def add(a, b):\n+                return a + str(b)\n+            return dstream.combineByKey(str, add, add)\n+        expected = [[(1, \"1\"), (2, \"1\"), (3, \"1\"), (4, \"1\")],\n+                    [(1, \"111\"), (2, \"11\"), (3, \"1\")],\n+                    [(\"a\", \"11\"), (\"b\", \"1\"), (\"\", \"111\")]]\n+        self._test_func(input, func, expected, sort=True)\n+\n+    def test_repartition(self):\n+        input = [range(1, 5), range(5, 9)]\n+        rdds = [self.sc.parallelize(r, 2) for r in input]\n+\n+        def func(dstream):\n+            return dstream.repartitions(1).glom()\n+        expected = [[[1, 2, 3, 4]], [[5, 6, 7, 8]]]\n+        self._test_func(rdds, func, expected)\n+\n+    def test_union(self):\n+        input1 = [range(3), range(5), range(6)]\n+        input2 = [range(3, 6), range(5, 6)]\n+\n+        def func(d1, d2):\n+            return d1.union(d2)\n+\n+        expected = [range(6), range(6), range(6)]\n+        self._test_func(input1, func, expected, input2=input2)\n+\n+    def test_cogroup(self):\n+        input = [[(1, 1), (2, 1), (3, 1)],\n+                 [(1, 1), (1, 1), (1, 1), (2, 1)],\n+                 [(\"a\", 1), (\"a\", 1), (\"b\", 1), (\"\", 1), (\"\", 1)]]\n+        input2 = [[(1, 2)],\n+                  [(4, 1)],\n+                  [(\"a\", 1), (\"a\", 1), (\"b\", 1), (\"\", 1), (\"\", 2)]]\n+\n+        def func(d1, d2):\n+            return d1.cogroup(d2).mapValues(lambda vs: tuple(map(list, vs)))\n+\n+        expected = [[(1, ([1], [2])), (2, ([1], [])), (3, ([1], []))],\n+                    [(1, ([1, 1, 1], [])), (2, ([1], [])), (4, ([], [1]))],\n+                    [(\"a\", ([1, 1], [1, 1])), (\"b\", ([1], [1])), (\"\", ([1, 1], [1, 2]))]]\n+        self._test_func(input, func, expected, sort=True, input2=input2)\n+\n+    def test_join(self):\n+        input = [[('a', 1), ('b', 2)]]\n+        input2 = [[('b', 3), ('c', 4)]]\n+\n+        def func(a, b):\n+            return a.join(b)\n+\n+        expected = [[('b', (2, 3))]]\n+        self._test_func(input, func, expected, True, input2)\n+\n+    def test_left_outer_join(self):\n+        input = [[('a', 1), ('b', 2)]]\n+        input2 = [[('b', 3), ('c', 4)]]\n+\n+        def func(a, b):\n+            return a.leftOuterJoin(b)\n+\n+        expected = [[('a', (1, None)), ('b', (2, 3))]]\n+        self._test_func(input, func, expected, True, input2)\n+\n+    def test_right_outer_join(self):\n+        input = [[('a', 1), ('b', 2)]]\n+        input2 = [[('b', 3), ('c', 4)]]\n+\n+        def func(a, b):\n+            return a.rightOuterJoin(b)\n+\n+        expected = [[('b', (2, 3)), ('c', (None, 4))]]\n+        self._test_func(input, func, expected, True, input2)\n+\n+    def test_full_outer_join(self):\n+        input = [[('a', 1), ('b', 2)]]\n+        input2 = [[('b', 3), ('c', 4)]]\n+\n+        def func(a, b):\n+            return a.fullOuterJoin(b)\n+\n+        expected = [[('a', (1, None)), ('b', (2, 3)), ('c', (None, 4))]]\n+        self._test_func(input, func, expected, True, input2)\n+\n+\n+class TestWindowFunctions(PySparkStreamingTestCase):\n+\n+    timeout = 20\n+\n+    def test_window(self):\n+        input = [range(1), range(2), range(3), range(4), range(5)]\n+\n+        def func(dstream):\n+            return dstream.window(3, 1).count()\n+\n+        expected = [[1], [3], [6], [9], [12], [9], [5]]\n+        self._test_func(input, func, expected)\n+\n+    def test_count_by_window(self):\n+        input = [range(1), range(2), range(3), range(4), range(5)]\n+\n+        def func(dstream):\n+            return dstream.countByWindow(3, 1)\n+\n+        expected = [[1], [3], [6], [9], [12], [9], [5]]\n+        self._test_func(input, func, expected)\n+\n+    def test_count_by_window_large(self):\n+        input = [range(1), range(2), range(3), range(4), range(5), range(6)]\n+\n+        def func(dstream):\n+            return dstream.countByWindow(5, 1)\n+\n+        expected = [[1], [3], [6], [10], [15], [20], [18], [15], [11], [6]]\n+        self._test_func(input, func, expected)\n+\n+    def test_count_by_value_and_window(self):\n+        input = [range(1), range(2), range(3), range(4), range(5), range(6)]\n+\n+        def func(dstream):\n+            return dstream.countByValueAndWindow(5, 1)\n+\n+        expected = [[1], [2], [3], [4], [5], [6], [6], [6], [6], [6]]\n+        self._test_func(input, func, expected)\n+\n+    def test_group_by_key_and_window(self):\n+        input = [[('a', i)] for i in range(5)]\n+\n+        def func(dstream):\n+            return dstream.groupByKeyAndWindow(3, 1).mapValues(list)\n+\n+        expected = [[('a', [0])], [('a', [0, 1])], [('a', [0, 1, 2])], [('a', [1, 2, 3])],\n+                    [('a', [2, 3, 4])], [('a', [3, 4])], [('a', [4])]]\n+        self._test_func(input, func, expected)\n+\n+    def test_reduce_by_invalid_window(self):\n+        input1 = [range(3), range(5), range(1), range(6)]\n+        d1 = self.ssc.queueStream(input1)\n+        self.assertRaises(ValueError, lambda: d1.reduceByKeyAndWindow(None, None, 0.1, 0.1))\n+        self.assertRaises(ValueError, lambda: d1.reduceByKeyAndWindow(None, None, 1, 0.1))\n+\n+    def update_state_by_key(self):"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "nit: move this function either before or above all the tests.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-02T01:11:08Z",
    "diffHunk": "@@ -0,0 +1,532 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+import os\n+from itertools import chain\n+import time\n+import operator\n+import unittest\n+import tempfile\n+\n+from pyspark.context import SparkConf, SparkContext, RDD\n+from pyspark.streaming.context import StreamingContext\n+\n+\n+class PySparkStreamingTestCase(unittest.TestCase):\n+\n+    timeout = 10  # seconds\n+    duration = 1\n+\n+    def setUp(self):\n+        class_name = self.__class__.__name__\n+        conf = SparkConf().set(\"spark.default.parallelism\", 1)\n+        self.sc = SparkContext(appName=class_name, conf=conf)\n+        self.sc.setCheckpointDir(\"/tmp\")\n+        # TODO: decrease duration to speed up tests\n+        self.ssc = StreamingContext(self.sc, self.duration)\n+\n+    def tearDown(self):\n+        self.ssc.stop()\n+\n+    def _take(self, dstream, n):\n+        \"\"\"\n+        Return the first `n` elements in the stream (will start and stop).\n+        \"\"\"\n+        results = []\n+\n+        def take(_, rdd):\n+            if rdd and len(results) < n:\n+                results.extend(rdd.take(n - len(results)))\n+\n+        dstream.foreachRDD(take)\n+\n+        self.ssc.start()\n+        while len(results) < n:\n+            time.sleep(0.01)\n+        self.ssc.stop(False, True)\n+        return results\n+\n+    def _collect(self, dstream):\n+        \"\"\"\n+        Collect each RDDs into the returned list.\n+\n+        :return: list, which will have the collected items.\n+        \"\"\"\n+        result = []\n+\n+        def get_output(_, rdd):\n+            r = rdd.collect()\n+            if r:\n+                result.append(r)\n+        dstream.foreachRDD(get_output)\n+        return result\n+\n+    def _test_func(self, input, func, expected, sort=False, input2=None):\n+        \"\"\"\n+        @param input: dataset for the test. This should be list of lists.\n+        @param func: wrapped function. This function should return PythonDStream object.\n+        @param expected: expected output for this testcase.\n+        \"\"\"\n+        if not isinstance(input[0], RDD):\n+            input = [self.sc.parallelize(d, 1) for d in input]\n+        input_stream = self.ssc.queueStream(input)\n+        if input2 and not isinstance(input2[0], RDD):\n+            input2 = [self.sc.parallelize(d, 1) for d in input2]\n+        input_stream2 = self.ssc.queueStream(input2) if input2 is not None else None\n+\n+        # Apply test function to stream.\n+        if input2:\n+            stream = func(input_stream, input_stream2)\n+        else:\n+            stream = func(input_stream)\n+\n+        result = self._collect(stream)\n+        self.ssc.start()\n+\n+        start_time = time.time()\n+        # Loop until get the expected the number of the result from the stream.\n+        while True:\n+            current_time = time.time()\n+            # Check time out.\n+            if (current_time - start_time) > self.timeout:\n+                print \"timeout after\", self.timeout\n+                break\n+            # StreamingContext.awaitTermination is not used to wait because\n+            # if py4j server is called every 50 milliseconds, it gets an error.\n+            time.sleep(0.05)\n+            # Check if the output is the same length of expected output.\n+            if len(expected) == len(result):\n+                break\n+        if sort:\n+            self._sort_result_based_on_key(result)\n+            self._sort_result_based_on_key(expected)\n+        self.assertEqual(expected, result)\n+\n+    def _sort_result_based_on_key(self, outputs):\n+        \"\"\"Sort the list based on first value.\"\"\"\n+        for output in outputs:\n+            output.sort(key=lambda x: x[0])\n+\n+\n+class TestBasicOperations(PySparkStreamingTestCase):\n+\n+    def test_map(self):\n+        \"\"\"Basic operation test for DStream.map.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+\n+        def func(dstream):\n+            return dstream.map(str)\n+        expected = map(lambda x: map(str, x), input)\n+        self._test_func(input, func, expected)\n+\n+    def test_flatMap(self):\n+        \"\"\"Basic operation test for DStream.faltMap.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+\n+        def func(dstream):\n+            return dstream.flatMap(lambda x: (x, x * 2))\n+        expected = map(lambda x: list(chain.from_iterable((map(lambda y: [y, y * 2], x)))),\n+                       input)\n+        self._test_func(input, func, expected)\n+\n+    def test_filter(self):\n+        \"\"\"Basic operation test for DStream.filter.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+\n+        def func(dstream):\n+            return dstream.filter(lambda x: x % 2 == 0)\n+        expected = map(lambda x: filter(lambda y: y % 2 == 0, x), input)\n+        self._test_func(input, func, expected)\n+\n+    def test_count(self):\n+        \"\"\"Basic operation test for DStream.count.\"\"\"\n+        input = [range(5), range(10), range(20)]\n+\n+        def func(dstream):\n+            return dstream.count()\n+        expected = map(lambda x: [len(x)], input)\n+        self._test_func(input, func, expected)\n+\n+    def test_reduce(self):\n+        \"\"\"Basic operation test for DStream.reduce.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+\n+        def func(dstream):\n+            return dstream.reduce(operator.add)\n+        expected = map(lambda x: [reduce(operator.add, x)], input)\n+        self._test_func(input, func, expected)\n+\n+    def test_reduceByKey(self):\n+        \"\"\"Basic operation test for DStream.reduceByKey.\"\"\"\n+        input = [[(\"a\", 1), (\"a\", 1), (\"b\", 1), (\"b\", 1)],\n+                 [(\"\", 1), (\"\", 1), (\"\", 1), (\"\", 1)],\n+                 [(1, 1), (1, 1), (2, 1), (2, 1), (3, 1)]]\n+\n+        def func(dstream):\n+            return dstream.reduceByKey(operator.add)\n+        expected = [[(\"a\", 2), (\"b\", 2)], [(\"\", 4)], [(1, 2), (2, 2), (3, 1)]]\n+        self._test_func(input, func, expected, sort=True)\n+\n+    def test_mapValues(self):\n+        \"\"\"Basic operation test for DStream.mapValues.\"\"\"\n+        input = [[(\"a\", 2), (\"b\", 2), (\"c\", 1), (\"d\", 1)],\n+                 [(\"\", 4), (1, 1), (2, 2), (3, 3)],\n+                 [(1, 1), (2, 1), (3, 1), (4, 1)]]\n+\n+        def func(dstream):\n+            return dstream.mapValues(lambda x: x + 10)\n+        expected = [[(\"a\", 12), (\"b\", 12), (\"c\", 11), (\"d\", 11)],\n+                    [(\"\", 14), (1, 11), (2, 12), (3, 13)],\n+                    [(1, 11), (2, 11), (3, 11), (4, 11)]]\n+        self._test_func(input, func, expected, sort=True)\n+\n+    def test_flatMapValues(self):\n+        \"\"\"Basic operation test for DStream.flatMapValues.\"\"\"\n+        input = [[(\"a\", 2), (\"b\", 2), (\"c\", 1), (\"d\", 1)],\n+                 [(\"\", 4), (1, 1), (2, 1), (3, 1)],\n+                 [(1, 1), (2, 1), (3, 1), (4, 1)]]\n+\n+        def func(dstream):\n+            return dstream.flatMapValues(lambda x: (x, x + 10))\n+        expected = [[(\"a\", 2), (\"a\", 12), (\"b\", 2), (\"b\", 12),\n+                     (\"c\", 1), (\"c\", 11), (\"d\", 1), (\"d\", 11)],\n+                    [(\"\", 4), (\"\", 14), (1, 1), (1, 11), (2, 1), (2, 11), (3, 1), (3, 11)],\n+                    [(1, 1), (1, 11), (2, 1), (2, 11), (3, 1), (3, 11), (4, 1), (4, 11)]]\n+        self._test_func(input, func, expected)\n+\n+    def test_glom(self):\n+        \"\"\"Basic operation test for DStream.glom.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+        rdds = [self.sc.parallelize(r, 2) for r in input]\n+\n+        def func(dstream):\n+            return dstream.glom()\n+        expected = [[[1, 2], [3, 4]], [[5, 6], [7, 8]], [[9, 10], [11, 12]]]\n+        self._test_func(rdds, func, expected)\n+\n+    def test_mapPartitions(self):\n+        \"\"\"Basic operation test for DStream.mapPartitions.\"\"\"\n+        input = [range(1, 5), range(5, 9), range(9, 13)]\n+        rdds = [self.sc.parallelize(r, 2) for r in input]\n+\n+        def func(dstream):\n+            def f(iterator):\n+                yield sum(iterator)\n+            return dstream.mapPartitions(f)\n+        expected = [[3, 7], [11, 15], [19, 23]]\n+        self._test_func(rdds, func, expected)\n+\n+    def test_countByValue(self):\n+        \"\"\"Basic operation test for DStream.countByValue.\"\"\"\n+        input = [range(1, 5) * 2, range(5, 7) + range(5, 9), [\"a\", \"a\", \"b\", \"\"]]\n+\n+        def func(dstream):\n+            return dstream.countByValue()\n+        expected = [[4], [4], [3]]\n+        self._test_func(input, func, expected)\n+\n+    def test_groupByKey(self):\n+        \"\"\"Basic operation test for DStream.groupByKey.\"\"\"\n+        input = [[(1, 1), (2, 1), (3, 1), (4, 1)],\n+                 [(1, 1), (1, 1), (1, 1), (2, 1), (2, 1), (3, 1)],\n+                 [(\"a\", 1), (\"a\", 1), (\"b\", 1), (\"\", 1), (\"\", 1), (\"\", 1)]]\n+\n+        def func(dstream):\n+            return dstream.groupByKey().mapValues(list)\n+\n+        expected = [[(1, [1]), (2, [1]), (3, [1]), (4, [1])],\n+                    [(1, [1, 1, 1]), (2, [1, 1]), (3, [1])],\n+                    [(\"a\", [1, 1]), (\"b\", [1]), (\"\", [1, 1, 1])]]\n+        self._test_func(input, func, expected, sort=True)\n+\n+    def test_combineByKey(self):\n+        \"\"\"Basic operation test for DStream.combineByKey.\"\"\"\n+        input = [[(1, 1), (2, 1), (3, 1), (4, 1)],\n+                 [(1, 1), (1, 1), (1, 1), (2, 1), (2, 1), (3, 1)],\n+                 [(\"a\", 1), (\"a\", 1), (\"b\", 1), (\"\", 1), (\"\", 1), (\"\", 1)]]\n+\n+        def func(dstream):\n+            def add(a, b):\n+                return a + str(b)\n+            return dstream.combineByKey(str, add, add)\n+        expected = [[(1, \"1\"), (2, \"1\"), (3, \"1\"), (4, \"1\")],\n+                    [(1, \"111\"), (2, \"11\"), (3, \"1\")],\n+                    [(\"a\", \"11\"), (\"b\", \"1\"), (\"\", \"111\")]]\n+        self._test_func(input, func, expected, sort=True)\n+\n+    def test_repartition(self):\n+        input = [range(1, 5), range(5, 9)]\n+        rdds = [self.sc.parallelize(r, 2) for r in input]\n+\n+        def func(dstream):\n+            return dstream.repartition(1).glom()\n+        expected = [[[1, 2, 3, 4]], [[5, 6, 7, 8]]]\n+        self._test_func(rdds, func, expected)\n+\n+    def test_union(self):\n+        input1 = [range(3), range(5), range(6)]\n+        input2 = [range(3, 6), range(5, 6)]\n+\n+        def func(d1, d2):\n+            return d1.union(d2)\n+\n+        expected = [range(6), range(6), range(6)]\n+        self._test_func(input1, func, expected, input2=input2)\n+\n+    def test_cogroup(self):\n+        input = [[(1, 1), (2, 1), (3, 1)],\n+                 [(1, 1), (1, 1), (1, 1), (2, 1)],\n+                 [(\"a\", 1), (\"a\", 1), (\"b\", 1), (\"\", 1), (\"\", 1)]]\n+        input2 = [[(1, 2)],\n+                  [(4, 1)],\n+                  [(\"a\", 1), (\"a\", 1), (\"b\", 1), (\"\", 1), (\"\", 2)]]\n+\n+        def func(d1, d2):\n+            return d1.cogroup(d2).mapValues(lambda vs: tuple(map(list, vs)))\n+\n+        expected = [[(1, ([1], [2])), (2, ([1], [])), (3, ([1], []))],\n+                    [(1, ([1, 1, 1], [])), (2, ([1], [])), (4, ([], [1]))],\n+                    [(\"a\", ([1, 1], [1, 1])), (\"b\", ([1], [1])), (\"\", ([1, 1], [1, 2]))]]\n+        self._test_func(input, func, expected, sort=True, input2=input2)\n+\n+    def test_join(self):\n+        input = [[('a', 1), ('b', 2)]]\n+        input2 = [[('b', 3), ('c', 4)]]\n+\n+        def func(a, b):\n+            return a.join(b)\n+\n+        expected = [[('b', (2, 3))]]\n+        self._test_func(input, func, expected, True, input2)\n+\n+    def test_left_outer_join(self):\n+        input = [[('a', 1), ('b', 2)]]\n+        input2 = [[('b', 3), ('c', 4)]]\n+\n+        def func(a, b):\n+            return a.leftOuterJoin(b)\n+\n+        expected = [[('a', (1, None)), ('b', (2, 3))]]\n+        self._test_func(input, func, expected, True, input2)\n+\n+    def test_right_outer_join(self):\n+        input = [[('a', 1), ('b', 2)]]\n+        input2 = [[('b', 3), ('c', 4)]]\n+\n+        def func(a, b):\n+            return a.rightOuterJoin(b)\n+\n+        expected = [[('b', (2, 3)), ('c', (None, 4))]]\n+        self._test_func(input, func, expected, True, input2)\n+\n+    def test_full_outer_join(self):\n+        input = [[('a', 1), ('b', 2)]]\n+        input2 = [[('b', 3), ('c', 4)]]\n+\n+        def func(a, b):\n+            return a.fullOuterJoin(b)\n+\n+        expected = [[('a', (1, None)), ('b', (2, 3)), ('c', (None, 4))]]\n+        self._test_func(input, func, expected, True, input2)\n+\n+    def update_state_by_key(self):\n+\n+        def updater(it):\n+            for k, vs, s in it:\n+                if not s:\n+                    s = vs\n+                else:\n+                    s.extend(vs)\n+                yield (k, s)\n+\n+        input = [[('k', i)] for i in range(5)]\n+\n+        def func(dstream):\n+            return dstream.updateStateByKey(updater)\n+\n+        expected = [[0], [0, 1], [0, 1, 2], [0, 1, 2, 3], [0, 1, 2, 3, 4]]\n+        expected = [[('k', v)] for v in expected]\n+        self._test_func(input, func, expected)\n+\n+\n+class TestWindowFunctions(PySparkStreamingTestCase):\n+\n+    timeout = 20\n+\n+    def test_window(self):\n+        input = [range(1), range(2), range(3), range(4), range(5)]\n+\n+        def func(dstream):\n+            return dstream.window(3, 1).count()\n+\n+        expected = [[1], [3], [6], [9], [12], [9], [5]]\n+        self._test_func(input, func, expected)\n+\n+    def test_count_by_window(self):\n+        input = [range(1), range(2), range(3), range(4), range(5)]\n+\n+        def func(dstream):\n+            return dstream.countByWindow(3, 1)\n+\n+        expected = [[1], [3], [6], [9], [12], [9], [5]]\n+        self._test_func(input, func, expected)\n+\n+    def test_count_by_window_large(self):\n+        input = [range(1), range(2), range(3), range(4), range(5), range(6)]\n+\n+        def func(dstream):\n+            return dstream.countByWindow(5, 1)\n+\n+        expected = [[1], [3], [6], [10], [15], [20], [18], [15], [11], [6]]\n+        self._test_func(input, func, expected)\n+\n+    def test_count_by_value_and_window(self):\n+        input = [range(1), range(2), range(3), range(4), range(5), range(6)]\n+\n+        def func(dstream):\n+            return dstream.countByValueAndWindow(5, 1)\n+\n+        expected = [[1], [2], [3], [4], [5], [6], [6], [6], [6], [6]]\n+        self._test_func(input, func, expected)\n+\n+    def test_group_by_key_and_window(self):\n+        input = [[('a', i)] for i in range(5)]\n+\n+        def func(dstream):\n+            return dstream.groupByKeyAndWindow(3, 1).mapValues(list)\n+\n+        expected = [[('a', [0])], [('a', [0, 1])], [('a', [0, 1, 2])], [('a', [1, 2, 3])],\n+                    [('a', [2, 3, 4])], [('a', [3, 4])], [('a', [4])]]\n+        self._test_func(input, func, expected)\n+\n+    def test_reduce_by_invalid_window(self):\n+        input1 = [range(3), range(5), range(1), range(6)]\n+        d1 = self.ssc.queueStream(input1)\n+        self.assertRaises(ValueError, lambda: d1.reduceByKeyAndWindow(None, None, 0.1, 0.1))\n+        self.assertRaises(ValueError, lambda: d1.reduceByKeyAndWindow(None, None, 1, 0.1))\n+\n+\n+class TestStreamingContext(PySparkStreamingTestCase):\n+\n+    duration = 0.1\n+\n+    def test_stop_only_streaming_context(self):\n+        self._addInputStream()\n+        self.ssc.start()\n+        self.ssc.stop(False)\n+        self.assertEqual(len(self.sc.parallelize(range(5), 5).glom().collect()), 5)\n+\n+    def test_stop_multiple_times(self):\n+        self._addInputStream()\n+        self.ssc.start()\n+        self.ssc.stop()\n+        self.ssc.stop()\n+\n+    def _addInputStream(self):"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Why do we need to set this?  I wonder whether this could mask any bugs that might be exposed by having multiple partitions within a stage vs. a single partition.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-07T22:31:47Z",
    "diffHunk": "@@ -0,0 +1,548 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+import os\n+from itertools import chain\n+import time\n+import operator\n+import unittest\n+import tempfile\n+\n+from pyspark.context import SparkConf, SparkContext, RDD\n+from pyspark.streaming.context import StreamingContext\n+\n+\n+class PySparkStreamingTestCase(unittest.TestCase):\n+\n+    timeout = 10  # seconds\n+    duration = 1\n+\n+    def setUp(self):\n+        class_name = self.__class__.__name__\n+        conf = SparkConf().set(\"spark.default.parallelism\", 1)",
    "line": 36
  }, {
    "author": {
      "login": "davies"
    },
    "body": "Setting this to minimize the runtime in each iteration, or the tests will be flaky if one batch can not finish in the batch window, we can increase this after improve the latency.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-07T23:41:37Z",
    "diffHunk": "@@ -0,0 +1,548 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+import os\n+from itertools import chain\n+import time\n+import operator\n+import unittest\n+import tempfile\n+\n+from pyspark.context import SparkConf, SparkContext, RDD\n+from pyspark.streaming.context import StreamingContext\n+\n+\n+class PySparkStreamingTestCase(unittest.TestCase):\n+\n+    timeout = 10  # seconds\n+    duration = 1\n+\n+    def setUp(self):\n+        class_name = self.__class__.__name__\n+        conf = SparkConf().set(\"spark.default.parallelism\", 1)",
    "line": 36
  }],
  "prId": 2538
}]