[{
  "comments": [{
    "author": {
      "login": "koeninger"
    },
    "body": "I don't think this verbiage is as relevant to Kafka 0.10, since the kafka managed api for offsets uses kafka topics, not zookeeper.\n",
    "commit": "d155c8d819a7fc2f26c1a91cafc37c6d70c5e89b",
    "createdAt": "2016-07-25T18:51:34Z",
    "diffHunk": "@@ -0,0 +1,370 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark.rdd import RDD\n+from pyspark.serializers import AutoBatchedSerializer, PickleSerializer\n+from pyspark.streaming import DStream\n+from pyspark.streaming.kafka import KafkaDStream, KafkaRDD, OffsetRange\n+\n+__all__ = ['Assign', 'KafkaConsumerRecord', 'KafkaUtils', 'PreferBrokers', 'PreferConsistent',\n+           'PreferFixed', 'Subscribe', 'SubscribePattern', 'TopicPartition', 'utf8_decoder']\n+\n+\n+def utf8_decoder(s):\n+    \"\"\" Decode the unicode as UTF-8 \"\"\"\n+    if s is None:\n+        return None\n+    return s.decode('utf-8')\n+\n+\n+class KafkaUtils(object):\n+\n+    @staticmethod\n+    def createDirectStream(ssc, locationStrategy, consumerStrategy,\n+                           keyDecoder=utf8_decoder, valueDecoder=utf8_decoder):\n+        \"\"\"\n+        .. note:: Experimental\n+\n+        Create an input stream that directly pulls messages from Kafka 0.10 brokers with different\n+        location strategy and consumer strategy.\n+\n+        This does not use Zookeeper to store offsets. The consumed offsets are tracked"
  }],
  "prId": 14340
}, {
  "comments": [{
    "author": {
      "login": "koeninger"
    },
    "body": "The scala / javadoc has a note on what the most common option to pass in is (preferConsistent and subscribe), probably worth adding here\n",
    "commit": "d155c8d819a7fc2f26c1a91cafc37c6d70c5e89b",
    "createdAt": "2016-07-25T18:53:26Z",
    "diffHunk": "@@ -0,0 +1,370 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark.rdd import RDD\n+from pyspark.serializers import AutoBatchedSerializer, PickleSerializer\n+from pyspark.streaming import DStream\n+from pyspark.streaming.kafka import KafkaDStream, KafkaRDD, OffsetRange\n+\n+__all__ = ['Assign', 'KafkaConsumerRecord', 'KafkaUtils', 'PreferBrokers', 'PreferConsistent',\n+           'PreferFixed', 'Subscribe', 'SubscribePattern', 'TopicPartition', 'utf8_decoder']\n+\n+\n+def utf8_decoder(s):\n+    \"\"\" Decode the unicode as UTF-8 \"\"\"\n+    if s is None:\n+        return None\n+    return s.decode('utf-8')\n+\n+\n+class KafkaUtils(object):\n+\n+    @staticmethod\n+    def createDirectStream(ssc, locationStrategy, consumerStrategy,\n+                           keyDecoder=utf8_decoder, valueDecoder=utf8_decoder):\n+        \"\"\"\n+        .. note:: Experimental\n+\n+        Create an input stream that directly pulls messages from Kafka 0.10 brokers with different\n+        location strategy and consumer strategy.\n+\n+        This does not use Zookeeper to store offsets. The consumed offsets are tracked\n+        by the stream itself. For interoperability with Kafka monitoring tools that depend on\n+        Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.\n+        You can access the offsets used in each batch from the generated RDDs (see\n+\n+        To recover from driver failures, you have to enable checkpointing in the StreamingContext.\n+        The information on consumed offset can be recovered from the checkpoint.\n+        See the programming guide for details (constraints, etc.).\n+\n+        :param ssc: StreamingContext object,\n+        :param locationStrategy: Strategy to schedule consumers for a given TopicPartition on an",
    "line": 50
  }],
  "prId": 14340
}, {
  "comments": [{
    "author": {
      "login": "koeninger"
    },
    "body": "top should be topic\n",
    "commit": "d155c8d819a7fc2f26c1a91cafc37c6d70c5e89b",
    "createdAt": "2016-07-25T18:54:26Z",
    "diffHunk": "@@ -0,0 +1,370 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark.rdd import RDD\n+from pyspark.serializers import AutoBatchedSerializer, PickleSerializer\n+from pyspark.streaming import DStream\n+from pyspark.streaming.kafka import KafkaDStream, KafkaRDD, OffsetRange\n+\n+__all__ = ['Assign', 'KafkaConsumerRecord', 'KafkaUtils', 'PreferBrokers', 'PreferConsistent',\n+           'PreferFixed', 'Subscribe', 'SubscribePattern', 'TopicPartition', 'utf8_decoder']\n+\n+\n+def utf8_decoder(s):\n+    \"\"\" Decode the unicode as UTF-8 \"\"\"\n+    if s is None:\n+        return None\n+    return s.decode('utf-8')\n+\n+\n+class KafkaUtils(object):\n+\n+    @staticmethod\n+    def createDirectStream(ssc, locationStrategy, consumerStrategy,\n+                           keyDecoder=utf8_decoder, valueDecoder=utf8_decoder):\n+        \"\"\"\n+        .. note:: Experimental\n+\n+        Create an input stream that directly pulls messages from Kafka 0.10 brokers with different\n+        location strategy and consumer strategy.\n+\n+        This does not use Zookeeper to store offsets. The consumed offsets are tracked\n+        by the stream itself. For interoperability with Kafka monitoring tools that depend on\n+        Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.\n+        You can access the offsets used in each batch from the generated RDDs (see\n+\n+        To recover from driver failures, you have to enable checkpointing in the StreamingContext.\n+        The information on consumed offset can be recovered from the checkpoint.\n+        See the programming guide for details (constraints, etc.).\n+\n+        :param ssc: StreamingContext object,\n+        :param locationStrategy: Strategy to schedule consumers for a given TopicPartition on an\n+               executor.\n+        :param consumerStrategy: Choices of how to create and configure underlying Kafka\n+               Consumers on driver and executors.\n+        :param keyDecoder: A function to decode key (default is utf8_decoder).\n+        :param valueDecoder: A function to decode value (default is utf8_decoder).\n+        :return: A DStream object.\n+        \"\"\"\n+\n+        helper = KafkaUtils._get_helper(ssc._sc)\n+        ser = AutoBatchedSerializer(PickleSerializer())\n+\n+        jlocationStrategy = locationStrategy._jLocationStrategy(helper)\n+        jconsumerStrategy = consumerStrategy._jConsumerStrategy(helper)\n+\n+        jstream = helper.createDirectStream(ssc._jssc, jlocationStrategy, jconsumerStrategy)\n+\n+        def func(m):\n+            m._set_key_deserializer(keyDecoder)\n+            m._set_value_deserializer(valueDecoder)\n+            return m\n+\n+        stream = DStream(jstream, ssc, ser).map(func)\n+\n+        return KafkaDStream(stream._jdstream, ssc, stream._jrdd_deserializer)\n+\n+    @staticmethod\n+    def createRDD(sc, kafkaParams, offsetRanges, locationStrategy,\n+                  keyDecoder=utf8_decoder, valueDecoder=utf8_decoder):\n+        \"\"\"\n+        .. note:: Experimental\n+\n+        Create a Kafka RDD using offset ranges and location strategy.\n+\n+        :param sc: SparkContext object.\n+        :param kafkaParams: Additional params for Kafka.\n+        :param offsetRanges: list of offsetRange to specify topic:partition:[start, end) to consume.\n+        :param locationStrategy: Strategy to schedule consumers for a given TopicPartition on an\n+               executor.\n+        :param keyDecoder: A function to decode key (default is utf8_decoder).\n+        :param valueDecoder: A function to decode value (default is utf8_decoder).\n+        :return:  A RDD object.\n+        \"\"\"\n+\n+        if not isinstance(kafkaParams, dict):\n+            raise TypeError(\"kafkaParams should be dict\")\n+\n+        helper = KafkaUtils._get_helper(sc)\n+        joffsetRanges = [o._jOffsetRange(helper) for o in offsetRanges]\n+        jlocationStrategy = locationStrategy._jLocationStrategy(helper)\n+\n+        jrdd = helper.createRDD(sc._jsc, kafkaParams, joffsetRanges, jlocationStrategy)\n+\n+        def func(m):\n+            m._set_key_deserializer(keyDecoder)\n+            m._set_value_deserializer(valueDecoder)\n+            return m\n+\n+        rdd = RDD(jrdd, sc).map(func)\n+\n+        return KafkaRDD(rdd._jrdd, sc, rdd._jrdd_deserializer)\n+\n+    @staticmethod\n+    def _get_helper(sc):\n+        try:\n+            helper = sc._jvm.org.apache.spark.streaming.kafka010.KafkaUtilsPythonHelper()\n+            KafkaRDD.set_helper(helper)\n+            return helper\n+        except TypeError as e:\n+            if str(e) == \"'JavaPackage' object is not callable\":\n+                KafkaUtils._printErrorMsg(sc)\n+            raise\n+\n+    @staticmethod\n+    def _printErrorMsg(sc):\n+        print(\"\"\"\n+________________________________________________________________________________________________\n+\n+  Spark Streaming's Kafka libraries not found in class path. Try one of the following.\n+\n+  1. Include the Kafka library and its dependencies with in the\n+     spark-submit command as\n+\n+     $ bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-10:%s ...\n+\n+  2. Download the JAR of the artifact from Maven Central http://search.maven.org/,\n+     Group Id = org.apache.spark, Artifact Id = spark-streaming-kafka-0-10-assembly, Version = %s.\n+     Then, include the jar in the spark-submit command as\n+\n+     $ bin/spark-submit --jars <spark-streaming-kafka-0-10-assembly.jar> ...\n+\n+________________________________________________________________________________________________\n+\n+\"\"\" % (sc.version, sc.version))\n+\n+\n+class LocationStrategy(object):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    A python wrapper of Scala LocationStrategy.\n+    \"\"\"\n+\n+    def _jLocationStrategy(self, helper):\n+        pass\n+\n+\n+class PreferBrokers(LocationStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Use this only if your executors are on the same nodes as your kafka brokers.\n+\n+    \"\"\"\n+    def _jLocationStrategy(self, helper):\n+        return helper.createPreferBrokers()\n+\n+\n+class PreferConsistent(LocationStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Use this in most cases, it will consistently distribute partitions across all executors.\n+    \"\"\"\n+\n+    def _jLocationStrategy(self, helper):\n+        return helper.createPreferConsistent()\n+\n+\n+class PreferFixed(LocationStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Use this to place particular TopicPartitions on particular hosts if your load is uneven. Any\n+    TopicPartition not specified in the map will use a consistent location.\n+    \"\"\"\n+\n+    def __init__(self, hostMap):\n+        \"\"\"\n+        Python wrapper of Scala PreferFixed.\n+\n+        :param hostMap: A dict of TopicPartition to hostname.\n+        \"\"\"\n+        self.hostMap = hostMap\n+\n+    def _jLocationStrategy(self, helper):\n+        jhostMap = dict([(k._jTopicPartition(helper), v) for (k, v) in self.hostMap.items()])\n+        return helper.createPreferFixed(jhostMap)\n+\n+\n+class ConsumerStrategy(object):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    A python wrapper of Scala ConsumerStrategy.\n+    \"\"\"\n+\n+    def _jConsumerStrategy(self, helper):\n+        pass\n+\n+\n+class Subscribe(ConsumerStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Subscribe to a collection of topics.\n+    \"\"\"\n+\n+    def __init__(self, topics, kafkaParams, offsets=None):\n+        \"\"\"\n+        Subscribe to a collection of topics.\n+\n+        :param topics: List of topics to subscribe.\n+        :param kafkaParams: Kafka parameters.\n+        :param offsets: offsets to begin at on initial startup. If no offset is given for a\n+               TopicPartition, the committed offset (if applicable) or kafka param\n+               auto.offset.reset will be used.\n+        \"\"\"\n+        self.topics = set(topics)\n+        self.kafkaParams = kafkaParams\n+        self.offsets = dict() if offsets is None else offsets\n+\n+    def _jConsumerStrategy(self, helper):\n+        jOffsets = dict([k._jTopicPartition(helper), v] for (k, v) in self.offsets.items())\n+        return helper.createSubscribe(self.topics, self.kafkaParams, jOffsets)\n+\n+\n+class SubscribePattern(ConsumerStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Subscribe to all topics matching specified pattern to get dynamically assigned partitions.\n+    \"\"\"\n+\n+    def __init__(self, pattern, kafkaParams, offsets=None):\n+        \"\"\"\n+        Subscribe to all topics matching specified pattern to get dynamically assigned partitions.\n+\n+        :param pattern: pattern to subscribe to.\n+        :param kafkaParams: Kafka parameters.\n+        :param offsets: offsets to begin at on initial startup. If no offset is given for a\n+               TopicPartition, the committed offset (if applicable) or kafka param\n+               auto.offset.reset will be used.\n+        \"\"\"\n+        self.pattern = pattern\n+        self.kafkaParams = kafkaParams\n+        self.offsets = dict() if offsets is None else offsets\n+\n+    def _jConsumerStrategy(self, helper):\n+        jOffsets = dict([k._jTopicPartition(helper), v] for (k, v) in self.offsets.items())\n+        return helper.createSubscribePattern(self.pattern, self.kafkaParams, jOffsets)\n+\n+\n+class Assign(ConsumerStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Assign a fixed collection of TopicPartitions.\n+    \"\"\"\n+\n+    def __init__(self, topicPartitions, kafkaParams, offsets=None):\n+        \"\"\"\n+        Assign a fixed collection of TopicPartitions.\n+\n+        :param topicPartitions: List of TopicPartitions to assign.\n+        :param kafkaParams: kafka parameters.\n+        :param offsets: offsets to begin at on initial startup. If no offset is given for a\n+               TopicPartition, the committed offset (if applicable) or kafka param\n+               auto.offset.reset will be used.\n+        \"\"\"\n+        self.topicPartitions = set(topicPartitions)\n+        self.kafkaParams = kafkaParams\n+        self.offsets = dict() if offsets is None else offsets\n+\n+    def _jConsumerStrategy(self, helper):\n+        jTopicPartitions = [i._jTopicPartition(helper) for i in self.topicPartitions]\n+        jOffsets = dict([k._jTopicPartition(helper), v] for (k, v) in self.offsets.items())\n+        return helper.createAssign(set(jTopicPartitions), self.kafkaParams, jOffsets)\n+\n+\n+class TopicPartition(object):\n+    \"\"\"\n+    Represents a specific top and partition for Kafka."
  }],
  "prId": 14340
}, {
  "comments": [{
    "author": {
      "login": "koeninger"
    },
    "body": "This is a single consumer record, not plural, right?\n",
    "commit": "d155c8d819a7fc2f26c1a91cafc37c6d70c5e89b",
    "createdAt": "2016-07-25T18:59:50Z",
    "diffHunk": "@@ -0,0 +1,370 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark.rdd import RDD\n+from pyspark.serializers import AutoBatchedSerializer, PickleSerializer\n+from pyspark.streaming import DStream\n+from pyspark.streaming.kafka import KafkaDStream, KafkaRDD, OffsetRange\n+\n+__all__ = ['Assign', 'KafkaConsumerRecord', 'KafkaUtils', 'PreferBrokers', 'PreferConsistent',\n+           'PreferFixed', 'Subscribe', 'SubscribePattern', 'TopicPartition', 'utf8_decoder']\n+\n+\n+def utf8_decoder(s):\n+    \"\"\" Decode the unicode as UTF-8 \"\"\"\n+    if s is None:\n+        return None\n+    return s.decode('utf-8')\n+\n+\n+class KafkaUtils(object):\n+\n+    @staticmethod\n+    def createDirectStream(ssc, locationStrategy, consumerStrategy,\n+                           keyDecoder=utf8_decoder, valueDecoder=utf8_decoder):\n+        \"\"\"\n+        .. note:: Experimental\n+\n+        Create an input stream that directly pulls messages from Kafka 0.10 brokers with different\n+        location strategy and consumer strategy.\n+\n+        This does not use Zookeeper to store offsets. The consumed offsets are tracked\n+        by the stream itself. For interoperability with Kafka monitoring tools that depend on\n+        Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.\n+        You can access the offsets used in each batch from the generated RDDs (see\n+\n+        To recover from driver failures, you have to enable checkpointing in the StreamingContext.\n+        The information on consumed offset can be recovered from the checkpoint.\n+        See the programming guide for details (constraints, etc.).\n+\n+        :param ssc: StreamingContext object,\n+        :param locationStrategy: Strategy to schedule consumers for a given TopicPartition on an\n+               executor.\n+        :param consumerStrategy: Choices of how to create and configure underlying Kafka\n+               Consumers on driver and executors.\n+        :param keyDecoder: A function to decode key (default is utf8_decoder).\n+        :param valueDecoder: A function to decode value (default is utf8_decoder).\n+        :return: A DStream object.\n+        \"\"\"\n+\n+        helper = KafkaUtils._get_helper(ssc._sc)\n+        ser = AutoBatchedSerializer(PickleSerializer())\n+\n+        jlocationStrategy = locationStrategy._jLocationStrategy(helper)\n+        jconsumerStrategy = consumerStrategy._jConsumerStrategy(helper)\n+\n+        jstream = helper.createDirectStream(ssc._jssc, jlocationStrategy, jconsumerStrategy)\n+\n+        def func(m):\n+            m._set_key_deserializer(keyDecoder)\n+            m._set_value_deserializer(valueDecoder)\n+            return m\n+\n+        stream = DStream(jstream, ssc, ser).map(func)\n+\n+        return KafkaDStream(stream._jdstream, ssc, stream._jrdd_deserializer)\n+\n+    @staticmethod\n+    def createRDD(sc, kafkaParams, offsetRanges, locationStrategy,\n+                  keyDecoder=utf8_decoder, valueDecoder=utf8_decoder):\n+        \"\"\"\n+        .. note:: Experimental\n+\n+        Create a Kafka RDD using offset ranges and location strategy.\n+\n+        :param sc: SparkContext object.\n+        :param kafkaParams: Additional params for Kafka.\n+        :param offsetRanges: list of offsetRange to specify topic:partition:[start, end) to consume.\n+        :param locationStrategy: Strategy to schedule consumers for a given TopicPartition on an\n+               executor.\n+        :param keyDecoder: A function to decode key (default is utf8_decoder).\n+        :param valueDecoder: A function to decode value (default is utf8_decoder).\n+        :return:  A RDD object.\n+        \"\"\"\n+\n+        if not isinstance(kafkaParams, dict):\n+            raise TypeError(\"kafkaParams should be dict\")\n+\n+        helper = KafkaUtils._get_helper(sc)\n+        joffsetRanges = [o._jOffsetRange(helper) for o in offsetRanges]\n+        jlocationStrategy = locationStrategy._jLocationStrategy(helper)\n+\n+        jrdd = helper.createRDD(sc._jsc, kafkaParams, joffsetRanges, jlocationStrategy)\n+\n+        def func(m):\n+            m._set_key_deserializer(keyDecoder)\n+            m._set_value_deserializer(valueDecoder)\n+            return m\n+\n+        rdd = RDD(jrdd, sc).map(func)\n+\n+        return KafkaRDD(rdd._jrdd, sc, rdd._jrdd_deserializer)\n+\n+    @staticmethod\n+    def _get_helper(sc):\n+        try:\n+            helper = sc._jvm.org.apache.spark.streaming.kafka010.KafkaUtilsPythonHelper()\n+            KafkaRDD.set_helper(helper)\n+            return helper\n+        except TypeError as e:\n+            if str(e) == \"'JavaPackage' object is not callable\":\n+                KafkaUtils._printErrorMsg(sc)\n+            raise\n+\n+    @staticmethod\n+    def _printErrorMsg(sc):\n+        print(\"\"\"\n+________________________________________________________________________________________________\n+\n+  Spark Streaming's Kafka libraries not found in class path. Try one of the following.\n+\n+  1. Include the Kafka library and its dependencies with in the\n+     spark-submit command as\n+\n+     $ bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-10:%s ...\n+\n+  2. Download the JAR of the artifact from Maven Central http://search.maven.org/,\n+     Group Id = org.apache.spark, Artifact Id = spark-streaming-kafka-0-10-assembly, Version = %s.\n+     Then, include the jar in the spark-submit command as\n+\n+     $ bin/spark-submit --jars <spark-streaming-kafka-0-10-assembly.jar> ...\n+\n+________________________________________________________________________________________________\n+\n+\"\"\" % (sc.version, sc.version))\n+\n+\n+class LocationStrategy(object):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    A python wrapper of Scala LocationStrategy.\n+    \"\"\"\n+\n+    def _jLocationStrategy(self, helper):\n+        pass\n+\n+\n+class PreferBrokers(LocationStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Use this only if your executors are on the same nodes as your kafka brokers.\n+\n+    \"\"\"\n+    def _jLocationStrategy(self, helper):\n+        return helper.createPreferBrokers()\n+\n+\n+class PreferConsistent(LocationStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Use this in most cases, it will consistently distribute partitions across all executors.\n+    \"\"\"\n+\n+    def _jLocationStrategy(self, helper):\n+        return helper.createPreferConsistent()\n+\n+\n+class PreferFixed(LocationStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Use this to place particular TopicPartitions on particular hosts if your load is uneven. Any\n+    TopicPartition not specified in the map will use a consistent location.\n+    \"\"\"\n+\n+    def __init__(self, hostMap):\n+        \"\"\"\n+        Python wrapper of Scala PreferFixed.\n+\n+        :param hostMap: A dict of TopicPartition to hostname.\n+        \"\"\"\n+        self.hostMap = hostMap\n+\n+    def _jLocationStrategy(self, helper):\n+        jhostMap = dict([(k._jTopicPartition(helper), v) for (k, v) in self.hostMap.items()])\n+        return helper.createPreferFixed(jhostMap)\n+\n+\n+class ConsumerStrategy(object):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    A python wrapper of Scala ConsumerStrategy.\n+    \"\"\"\n+\n+    def _jConsumerStrategy(self, helper):\n+        pass\n+\n+\n+class Subscribe(ConsumerStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Subscribe to a collection of topics.\n+    \"\"\"\n+\n+    def __init__(self, topics, kafkaParams, offsets=None):\n+        \"\"\"\n+        Subscribe to a collection of topics.\n+\n+        :param topics: List of topics to subscribe.\n+        :param kafkaParams: Kafka parameters.\n+        :param offsets: offsets to begin at on initial startup. If no offset is given for a\n+               TopicPartition, the committed offset (if applicable) or kafka param\n+               auto.offset.reset will be used.\n+        \"\"\"\n+        self.topics = set(topics)\n+        self.kafkaParams = kafkaParams\n+        self.offsets = dict() if offsets is None else offsets\n+\n+    def _jConsumerStrategy(self, helper):\n+        jOffsets = dict([k._jTopicPartition(helper), v] for (k, v) in self.offsets.items())\n+        return helper.createSubscribe(self.topics, self.kafkaParams, jOffsets)\n+\n+\n+class SubscribePattern(ConsumerStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Subscribe to all topics matching specified pattern to get dynamically assigned partitions.\n+    \"\"\"\n+\n+    def __init__(self, pattern, kafkaParams, offsets=None):\n+        \"\"\"\n+        Subscribe to all topics matching specified pattern to get dynamically assigned partitions.\n+\n+        :param pattern: pattern to subscribe to.\n+        :param kafkaParams: Kafka parameters.\n+        :param offsets: offsets to begin at on initial startup. If no offset is given for a\n+               TopicPartition, the committed offset (if applicable) or kafka param\n+               auto.offset.reset will be used.\n+        \"\"\"\n+        self.pattern = pattern\n+        self.kafkaParams = kafkaParams\n+        self.offsets = dict() if offsets is None else offsets\n+\n+    def _jConsumerStrategy(self, helper):\n+        jOffsets = dict([k._jTopicPartition(helper), v] for (k, v) in self.offsets.items())\n+        return helper.createSubscribePattern(self.pattern, self.kafkaParams, jOffsets)\n+\n+\n+class Assign(ConsumerStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Assign a fixed collection of TopicPartitions.\n+    \"\"\"\n+\n+    def __init__(self, topicPartitions, kafkaParams, offsets=None):\n+        \"\"\"\n+        Assign a fixed collection of TopicPartitions.\n+\n+        :param topicPartitions: List of TopicPartitions to assign.\n+        :param kafkaParams: kafka parameters.\n+        :param offsets: offsets to begin at on initial startup. If no offset is given for a\n+               TopicPartition, the committed offset (if applicable) or kafka param\n+               auto.offset.reset will be used.\n+        \"\"\"\n+        self.topicPartitions = set(topicPartitions)\n+        self.kafkaParams = kafkaParams\n+        self.offsets = dict() if offsets is None else offsets\n+\n+    def _jConsumerStrategy(self, helper):\n+        jTopicPartitions = [i._jTopicPartition(helper) for i in self.topicPartitions]\n+        jOffsets = dict([k._jTopicPartition(helper), v] for (k, v) in self.offsets.items())\n+        return helper.createAssign(set(jTopicPartitions), self.kafkaParams, jOffsets)\n+\n+\n+class TopicPartition(object):\n+    \"\"\"\n+    Represents a specific top and partition for Kafka.\n+    \"\"\"\n+\n+    def __init__(self, topic, partition):\n+        \"\"\"\n+        Create a Python TopicPartition to map to the Java related object\n+        :param topic: Kafka topic name.\n+        :param partition: Kafka partition id.\n+        \"\"\"\n+        self._topic = topic\n+        self._partition = partition\n+\n+    def _jTopicPartition(self, helper):\n+        return helper.createTopicPartition(self._topic, self._partition)\n+\n+    def __eq__(self, other):\n+        if isinstance(other, self.__class__):\n+            return (self._topic == other._topic\n+                    and self._partition == other._partition)\n+        else:\n+            return False\n+\n+    def __ne__(self, other):\n+        return not self.__eq__(other)\n+\n+    def __hash__(self):\n+        return hash(self._topic) ^ hash(self._partition)\n+\n+\n+class KafkaConsumerRecord(object):\n+    \"\"\"\n+    Kafka consumer record fetch from Kafka brokers, including metadata information and message.\n+    \"\"\"\n+\n+    def __init__(self, topic, partition, offset, timestamp, timestampType, checksum,\n+                 serializedKeySize, serializedValueSize, key, value):\n+        self.topic = topic\n+        self.partition = partition\n+        self.offset = offset\n+        self.timestamp = timestamp\n+        self.timestampType = timestampType\n+        self.checksum = checksum\n+        self.serializedKeySize = serializedKeySize\n+        self.serializedValueSize = serializedValueSize\n+        self._rawKey = key\n+        self._rawValue = value\n+        self._keyDecoder = utf8_decoder\n+        self._valueDecoder = utf8_decoder\n+\n+    def __str__(self):\n+        return \"Kafka ConsumerRecords(topic: %s, partition: %d, offset: %d, timestamp: %d, \" \\"
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Sorry, I will fix it.\n",
    "commit": "d155c8d819a7fc2f26c1a91cafc37c6d70c5e89b",
    "createdAt": "2016-07-26T06:14:22Z",
    "diffHunk": "@@ -0,0 +1,370 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark.rdd import RDD\n+from pyspark.serializers import AutoBatchedSerializer, PickleSerializer\n+from pyspark.streaming import DStream\n+from pyspark.streaming.kafka import KafkaDStream, KafkaRDD, OffsetRange\n+\n+__all__ = ['Assign', 'KafkaConsumerRecord', 'KafkaUtils', 'PreferBrokers', 'PreferConsistent',\n+           'PreferFixed', 'Subscribe', 'SubscribePattern', 'TopicPartition', 'utf8_decoder']\n+\n+\n+def utf8_decoder(s):\n+    \"\"\" Decode the unicode as UTF-8 \"\"\"\n+    if s is None:\n+        return None\n+    return s.decode('utf-8')\n+\n+\n+class KafkaUtils(object):\n+\n+    @staticmethod\n+    def createDirectStream(ssc, locationStrategy, consumerStrategy,\n+                           keyDecoder=utf8_decoder, valueDecoder=utf8_decoder):\n+        \"\"\"\n+        .. note:: Experimental\n+\n+        Create an input stream that directly pulls messages from Kafka 0.10 brokers with different\n+        location strategy and consumer strategy.\n+\n+        This does not use Zookeeper to store offsets. The consumed offsets are tracked\n+        by the stream itself. For interoperability with Kafka monitoring tools that depend on\n+        Zookeeper, you have to update Kafka/Zookeeper yourself from the streaming application.\n+        You can access the offsets used in each batch from the generated RDDs (see\n+\n+        To recover from driver failures, you have to enable checkpointing in the StreamingContext.\n+        The information on consumed offset can be recovered from the checkpoint.\n+        See the programming guide for details (constraints, etc.).\n+\n+        :param ssc: StreamingContext object,\n+        :param locationStrategy: Strategy to schedule consumers for a given TopicPartition on an\n+               executor.\n+        :param consumerStrategy: Choices of how to create and configure underlying Kafka\n+               Consumers on driver and executors.\n+        :param keyDecoder: A function to decode key (default is utf8_decoder).\n+        :param valueDecoder: A function to decode value (default is utf8_decoder).\n+        :return: A DStream object.\n+        \"\"\"\n+\n+        helper = KafkaUtils._get_helper(ssc._sc)\n+        ser = AutoBatchedSerializer(PickleSerializer())\n+\n+        jlocationStrategy = locationStrategy._jLocationStrategy(helper)\n+        jconsumerStrategy = consumerStrategy._jConsumerStrategy(helper)\n+\n+        jstream = helper.createDirectStream(ssc._jssc, jlocationStrategy, jconsumerStrategy)\n+\n+        def func(m):\n+            m._set_key_deserializer(keyDecoder)\n+            m._set_value_deserializer(valueDecoder)\n+            return m\n+\n+        stream = DStream(jstream, ssc, ser).map(func)\n+\n+        return KafkaDStream(stream._jdstream, ssc, stream._jrdd_deserializer)\n+\n+    @staticmethod\n+    def createRDD(sc, kafkaParams, offsetRanges, locationStrategy,\n+                  keyDecoder=utf8_decoder, valueDecoder=utf8_decoder):\n+        \"\"\"\n+        .. note:: Experimental\n+\n+        Create a Kafka RDD using offset ranges and location strategy.\n+\n+        :param sc: SparkContext object.\n+        :param kafkaParams: Additional params for Kafka.\n+        :param offsetRanges: list of offsetRange to specify topic:partition:[start, end) to consume.\n+        :param locationStrategy: Strategy to schedule consumers for a given TopicPartition on an\n+               executor.\n+        :param keyDecoder: A function to decode key (default is utf8_decoder).\n+        :param valueDecoder: A function to decode value (default is utf8_decoder).\n+        :return:  A RDD object.\n+        \"\"\"\n+\n+        if not isinstance(kafkaParams, dict):\n+            raise TypeError(\"kafkaParams should be dict\")\n+\n+        helper = KafkaUtils._get_helper(sc)\n+        joffsetRanges = [o._jOffsetRange(helper) for o in offsetRanges]\n+        jlocationStrategy = locationStrategy._jLocationStrategy(helper)\n+\n+        jrdd = helper.createRDD(sc._jsc, kafkaParams, joffsetRanges, jlocationStrategy)\n+\n+        def func(m):\n+            m._set_key_deserializer(keyDecoder)\n+            m._set_value_deserializer(valueDecoder)\n+            return m\n+\n+        rdd = RDD(jrdd, sc).map(func)\n+\n+        return KafkaRDD(rdd._jrdd, sc, rdd._jrdd_deserializer)\n+\n+    @staticmethod\n+    def _get_helper(sc):\n+        try:\n+            helper = sc._jvm.org.apache.spark.streaming.kafka010.KafkaUtilsPythonHelper()\n+            KafkaRDD.set_helper(helper)\n+            return helper\n+        except TypeError as e:\n+            if str(e) == \"'JavaPackage' object is not callable\":\n+                KafkaUtils._printErrorMsg(sc)\n+            raise\n+\n+    @staticmethod\n+    def _printErrorMsg(sc):\n+        print(\"\"\"\n+________________________________________________________________________________________________\n+\n+  Spark Streaming's Kafka libraries not found in class path. Try one of the following.\n+\n+  1. Include the Kafka library and its dependencies with in the\n+     spark-submit command as\n+\n+     $ bin/spark-submit --packages org.apache.spark:spark-streaming-kafka-0-10:%s ...\n+\n+  2. Download the JAR of the artifact from Maven Central http://search.maven.org/,\n+     Group Id = org.apache.spark, Artifact Id = spark-streaming-kafka-0-10-assembly, Version = %s.\n+     Then, include the jar in the spark-submit command as\n+\n+     $ bin/spark-submit --jars <spark-streaming-kafka-0-10-assembly.jar> ...\n+\n+________________________________________________________________________________________________\n+\n+\"\"\" % (sc.version, sc.version))\n+\n+\n+class LocationStrategy(object):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    A python wrapper of Scala LocationStrategy.\n+    \"\"\"\n+\n+    def _jLocationStrategy(self, helper):\n+        pass\n+\n+\n+class PreferBrokers(LocationStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Use this only if your executors are on the same nodes as your kafka brokers.\n+\n+    \"\"\"\n+    def _jLocationStrategy(self, helper):\n+        return helper.createPreferBrokers()\n+\n+\n+class PreferConsistent(LocationStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Use this in most cases, it will consistently distribute partitions across all executors.\n+    \"\"\"\n+\n+    def _jLocationStrategy(self, helper):\n+        return helper.createPreferConsistent()\n+\n+\n+class PreferFixed(LocationStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Use this to place particular TopicPartitions on particular hosts if your load is uneven. Any\n+    TopicPartition not specified in the map will use a consistent location.\n+    \"\"\"\n+\n+    def __init__(self, hostMap):\n+        \"\"\"\n+        Python wrapper of Scala PreferFixed.\n+\n+        :param hostMap: A dict of TopicPartition to hostname.\n+        \"\"\"\n+        self.hostMap = hostMap\n+\n+    def _jLocationStrategy(self, helper):\n+        jhostMap = dict([(k._jTopicPartition(helper), v) for (k, v) in self.hostMap.items()])\n+        return helper.createPreferFixed(jhostMap)\n+\n+\n+class ConsumerStrategy(object):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    A python wrapper of Scala ConsumerStrategy.\n+    \"\"\"\n+\n+    def _jConsumerStrategy(self, helper):\n+        pass\n+\n+\n+class Subscribe(ConsumerStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Subscribe to a collection of topics.\n+    \"\"\"\n+\n+    def __init__(self, topics, kafkaParams, offsets=None):\n+        \"\"\"\n+        Subscribe to a collection of topics.\n+\n+        :param topics: List of topics to subscribe.\n+        :param kafkaParams: Kafka parameters.\n+        :param offsets: offsets to begin at on initial startup. If no offset is given for a\n+               TopicPartition, the committed offset (if applicable) or kafka param\n+               auto.offset.reset will be used.\n+        \"\"\"\n+        self.topics = set(topics)\n+        self.kafkaParams = kafkaParams\n+        self.offsets = dict() if offsets is None else offsets\n+\n+    def _jConsumerStrategy(self, helper):\n+        jOffsets = dict([k._jTopicPartition(helper), v] for (k, v) in self.offsets.items())\n+        return helper.createSubscribe(self.topics, self.kafkaParams, jOffsets)\n+\n+\n+class SubscribePattern(ConsumerStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Subscribe to all topics matching specified pattern to get dynamically assigned partitions.\n+    \"\"\"\n+\n+    def __init__(self, pattern, kafkaParams, offsets=None):\n+        \"\"\"\n+        Subscribe to all topics matching specified pattern to get dynamically assigned partitions.\n+\n+        :param pattern: pattern to subscribe to.\n+        :param kafkaParams: Kafka parameters.\n+        :param offsets: offsets to begin at on initial startup. If no offset is given for a\n+               TopicPartition, the committed offset (if applicable) or kafka param\n+               auto.offset.reset will be used.\n+        \"\"\"\n+        self.pattern = pattern\n+        self.kafkaParams = kafkaParams\n+        self.offsets = dict() if offsets is None else offsets\n+\n+    def _jConsumerStrategy(self, helper):\n+        jOffsets = dict([k._jTopicPartition(helper), v] for (k, v) in self.offsets.items())\n+        return helper.createSubscribePattern(self.pattern, self.kafkaParams, jOffsets)\n+\n+\n+class Assign(ConsumerStrategy):\n+    \"\"\"\n+    .. note:: Experimental\n+\n+    Assign a fixed collection of TopicPartitions.\n+    \"\"\"\n+\n+    def __init__(self, topicPartitions, kafkaParams, offsets=None):\n+        \"\"\"\n+        Assign a fixed collection of TopicPartitions.\n+\n+        :param topicPartitions: List of TopicPartitions to assign.\n+        :param kafkaParams: kafka parameters.\n+        :param offsets: offsets to begin at on initial startup. If no offset is given for a\n+               TopicPartition, the committed offset (if applicable) or kafka param\n+               auto.offset.reset will be used.\n+        \"\"\"\n+        self.topicPartitions = set(topicPartitions)\n+        self.kafkaParams = kafkaParams\n+        self.offsets = dict() if offsets is None else offsets\n+\n+    def _jConsumerStrategy(self, helper):\n+        jTopicPartitions = [i._jTopicPartition(helper) for i in self.topicPartitions]\n+        jOffsets = dict([k._jTopicPartition(helper), v] for (k, v) in self.offsets.items())\n+        return helper.createAssign(set(jTopicPartitions), self.kafkaParams, jOffsets)\n+\n+\n+class TopicPartition(object):\n+    \"\"\"\n+    Represents a specific top and partition for Kafka.\n+    \"\"\"\n+\n+    def __init__(self, topic, partition):\n+        \"\"\"\n+        Create a Python TopicPartition to map to the Java related object\n+        :param topic: Kafka topic name.\n+        :param partition: Kafka partition id.\n+        \"\"\"\n+        self._topic = topic\n+        self._partition = partition\n+\n+    def _jTopicPartition(self, helper):\n+        return helper.createTopicPartition(self._topic, self._partition)\n+\n+    def __eq__(self, other):\n+        if isinstance(other, self.__class__):\n+            return (self._topic == other._topic\n+                    and self._partition == other._partition)\n+        else:\n+            return False\n+\n+    def __ne__(self, other):\n+        return not self.__eq__(other)\n+\n+    def __hash__(self):\n+        return hash(self._topic) ^ hash(self._partition)\n+\n+\n+class KafkaConsumerRecord(object):\n+    \"\"\"\n+    Kafka consumer record fetch from Kafka brokers, including metadata information and message.\n+    \"\"\"\n+\n+    def __init__(self, topic, partition, offset, timestamp, timestampType, checksum,\n+                 serializedKeySize, serializedValueSize, key, value):\n+        self.topic = topic\n+        self.partition = partition\n+        self.offset = offset\n+        self.timestamp = timestamp\n+        self.timestampType = timestampType\n+        self.checksum = checksum\n+        self.serializedKeySize = serializedKeySize\n+        self.serializedValueSize = serializedValueSize\n+        self._rawKey = key\n+        self._rawValue = value\n+        self._keyDecoder = utf8_decoder\n+        self._valueDecoder = utf8_decoder\n+\n+    def __str__(self):\n+        return \"Kafka ConsumerRecords(topic: %s, partition: %d, offset: %d, timestamp: %d, \" \\"
  }],
  "prId": 14340
}]