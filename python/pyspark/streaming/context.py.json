[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "This doc string is wrong. master and appname are not parameters here.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T00:30:05Z",
    "diffHunk": "@@ -0,0 +1,243 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from py4j.java_collections import ListConverter\n+from py4j.java_gateway import java_import\n+\n+from pyspark import RDD\n+from pyspark.serializers import UTF8Deserializer\n+from pyspark.context import SparkContext\n+from pyspark.storagelevel import StorageLevel\n+from pyspark.streaming.dstream import DStream\n+from pyspark.streaming.util import RDDFunction\n+\n+__all__ = [\"StreamingContext\"]\n+\n+\n+def _daemonize_callback_server():\n+    \"\"\"\n+    Hack Py4J to daemonize callback server\n+    \"\"\"\n+    # TODO: create a patch for Py4J\n+    import socket\n+    import py4j.java_gateway\n+    logger = py4j.java_gateway.logger\n+    from py4j.java_gateway import Py4JNetworkError\n+    from threading import Thread\n+\n+    def start(self):\n+        \"\"\"Starts the CallbackServer. This method should be called by the\n+        client instead of run().\"\"\"\n+        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR,\n+                                      1)\n+        try:\n+            self.server_socket.bind((self.address, self.port))\n+            # self.port = self.server_socket.getsockname()[1]\n+        except Exception:\n+            msg = 'An error occurred while trying to start the callback server'\n+            logger.exception(msg)\n+            raise Py4JNetworkError(msg)\n+\n+        # Maybe thread needs to be cleanup up?\n+        self.thread = Thread(target=self.run)\n+        self.thread.daemon = True\n+        self.thread.start()\n+\n+    py4j.java_gateway.CallbackServer.start = start\n+\n+\n+class StreamingContext(object):\n+    \"\"\"\n+    Main entry point for Spark Streaming functionality. A StreamingContext represents the\n+    connection to a Spark cluster, and can be used to create L{DStream}s and\n+    broadcast variables on that cluster.\n+    \"\"\"\n+\n+    def __init__(self, sparkContext, duration):"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Can you add a bit more information in the docs, on how you add hack the py4j server. \n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T00:30:44Z",
    "diffHunk": "@@ -0,0 +1,243 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from py4j.java_collections import ListConverter\n+from py4j.java_gateway import java_import\n+\n+from pyspark import RDD\n+from pyspark.serializers import UTF8Deserializer\n+from pyspark.context import SparkContext\n+from pyspark.storagelevel import StorageLevel\n+from pyspark.streaming.dstream import DStream\n+from pyspark.streaming.util import RDDFunction\n+\n+__all__ = [\"StreamingContext\"]\n+\n+\n+def _daemonize_callback_server():\n+    \"\"\"\n+    Hack Py4J to daemonize callback server",
    "line": 35
  }, {
    "author": {
      "login": "davies"
    },
    "body": "done\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T07:44:35Z",
    "diffHunk": "@@ -0,0 +1,243 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from py4j.java_collections import ListConverter\n+from py4j.java_gateway import java_import\n+\n+from pyspark import RDD\n+from pyspark.serializers import UTF8Deserializer\n+from pyspark.context import SparkContext\n+from pyspark.storagelevel import StorageLevel\n+from pyspark.streaming.dstream import DStream\n+from pyspark.streaming.util import RDDFunction\n+\n+__all__ = [\"StreamingContext\"]\n+\n+\n+def _daemonize_callback_server():\n+    \"\"\"\n+    Hack Py4J to daemonize callback server",
    "line": 35
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "extra space after (\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T00:31:29Z",
    "diffHunk": "@@ -0,0 +1,243 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from py4j.java_collections import ListConverter\n+from py4j.java_gateway import java_import\n+\n+from pyspark import RDD\n+from pyspark.serializers import UTF8Deserializer\n+from pyspark.context import SparkContext\n+from pyspark.storagelevel import StorageLevel\n+from pyspark.streaming.dstream import DStream\n+from pyspark.streaming.util import RDDFunction\n+\n+__all__ = [\"StreamingContext\"]\n+\n+\n+def _daemonize_callback_server():\n+    \"\"\"\n+    Hack Py4J to daemonize callback server\n+    \"\"\"\n+    # TODO: create a patch for Py4J\n+    import socket\n+    import py4j.java_gateway\n+    logger = py4j.java_gateway.logger\n+    from py4j.java_gateway import Py4JNetworkError\n+    from threading import Thread\n+\n+    def start(self):\n+        \"\"\"Starts the CallbackServer. This method should be called by the\n+        client instead of run().\"\"\"\n+        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR,\n+                                      1)\n+        try:\n+            self.server_socket.bind((self.address, self.port))\n+            # self.port = self.server_socket.getsockname()[1]\n+        except Exception:\n+            msg = 'An error occurred while trying to start the callback server'\n+            logger.exception(msg)\n+            raise Py4JNetworkError(msg)\n+\n+        # Maybe thread needs to be cleanup up?\n+        self.thread = Thread(target=self.run)\n+        self.thread.daemon = True\n+        self.thread.start()\n+\n+    py4j.java_gateway.CallbackServer.start = start\n+\n+\n+class StreamingContext(object):\n+    \"\"\"\n+    Main entry point for Spark Streaming functionality. A StreamingContext represents the\n+    connection to a Spark cluster, and can be used to create L{DStream}s and\n+    broadcast variables on that cluster.\n+    \"\"\"\n+\n+    def __init__(self, sparkContext, duration):\n+        \"\"\"\n+        Create a new StreamingContext. At least the master and app name and duration\n+        should be set, either through the named parameters here or through C{conf}.\n+\n+        @param sparkContext: L{SparkContext} object.\n+        @param duration: seconds for SparkStreaming.\n+\n+        \"\"\"\n+        self._sc = sparkContext\n+        self._jvm = self._sc._jvm\n+        self._start_callback_server()\n+        self._jssc = self._initialize_context(self._sc, duration)\n+\n+    def _start_callback_server(self):\n+        gw = self._sc._gateway\n+        # getattr will fallback to JVM\n+        if \"_callback_server\" not in gw.__dict__:\n+            _daemonize_callback_server()\n+            gw._start_callback_server(gw._python_proxy_port)\n+            gw._python_proxy_port = gw._callback_server.port  # update port with real port\n+\n+    def _initialize_context(self, sc, duration):\n+        java_import(self._jvm, \"org.apache.spark.streaming.*\")\n+        java_import(self._jvm, \"org.apache.spark.streaming.api.java.*\")\n+        java_import(self._jvm, \"org.apache.spark.streaming.api.python.*\")\n+        return self._jvm.JavaStreamingContext(sc._jsc, self._jduration(duration))\n+\n+    def _jduration(self, seconds):\n+        \"\"\"\n+        Create Duration object given number of seconds\n+        \"\"\"\n+        return self._jvm.Duration(int(seconds * 1000))\n+\n+    @property\n+    def sparkContext(self):\n+        \"\"\"\n+        Return SparkContext which is associated with this StreamingContext.\n+        \"\"\"\n+        return self._sc\n+\n+    def start(self):\n+        \"\"\"\n+        Start the execution of the streams.\n+        \"\"\"\n+        self._jssc.start()\n+\n+    def awaitTermination(self, timeout=None):\n+        \"\"\"\n+        Wait for the execution to stop.\n+        @param timeout: time to wait in seconds\n+        \"\"\"\n+        if timeout is None:\n+            self._jssc.awaitTermination()\n+        else:\n+            self._jssc.awaitTermination(int(timeout * 1000))\n+\n+    def stop(self, stopSparkContext=True, stopGraceFully=False):\n+        \"\"\"\n+        Stop the execution of the streams immediately (does not wait for all received data\n+        to be processed).\n+        \"\"\"\n+        self._jssc.stop(stopSparkContext, stopGraceFully)\n+        if stopSparkContext:\n+            self._sc.stop()\n+\n+    def remember(self, duration):\n+        \"\"\"\n+        Set each DStreams in this context to remember RDDs it generated\n+        in the last given duration. DStreams remember RDDs only for a\n+        limited duration of time and releases them for garbage collection.\n+        This method allows the developer to specify how to long to remember\n+        the RDDs ( if the developer wishes to query old data outside the"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Wrong doc string. The Scala doc string is the following. Please add the parameter info as well. \n\n```\n/**\n   * Stop the execution of the streams, with option of ensuring all received data\n   * has been processed.\n   * @param stopSparkContext Stop the associated SparkContext or not\n   * @param stopGracefully Stop gracefully by waiting for the processing of all\n   *                       received data to be completed\n   */\n```\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-09-30T00:32:42Z",
    "diffHunk": "@@ -0,0 +1,243 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from py4j.java_collections import ListConverter\n+from py4j.java_gateway import java_import\n+\n+from pyspark import RDD\n+from pyspark.serializers import UTF8Deserializer\n+from pyspark.context import SparkContext\n+from pyspark.storagelevel import StorageLevel\n+from pyspark.streaming.dstream import DStream\n+from pyspark.streaming.util import RDDFunction\n+\n+__all__ = [\"StreamingContext\"]\n+\n+\n+def _daemonize_callback_server():\n+    \"\"\"\n+    Hack Py4J to daemonize callback server\n+    \"\"\"\n+    # TODO: create a patch for Py4J\n+    import socket\n+    import py4j.java_gateway\n+    logger = py4j.java_gateway.logger\n+    from py4j.java_gateway import Py4JNetworkError\n+    from threading import Thread\n+\n+    def start(self):\n+        \"\"\"Starts the CallbackServer. This method should be called by the\n+        client instead of run().\"\"\"\n+        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR,\n+                                      1)\n+        try:\n+            self.server_socket.bind((self.address, self.port))\n+            # self.port = self.server_socket.getsockname()[1]\n+        except Exception:\n+            msg = 'An error occurred while trying to start the callback server'\n+            logger.exception(msg)\n+            raise Py4JNetworkError(msg)\n+\n+        # Maybe thread needs to be cleanup up?\n+        self.thread = Thread(target=self.run)\n+        self.thread.daemon = True\n+        self.thread.start()\n+\n+    py4j.java_gateway.CallbackServer.start = start\n+\n+\n+class StreamingContext(object):\n+    \"\"\"\n+    Main entry point for Spark Streaming functionality. A StreamingContext represents the\n+    connection to a Spark cluster, and can be used to create L{DStream}s and\n+    broadcast variables on that cluster.\n+    \"\"\"\n+\n+    def __init__(self, sparkContext, duration):\n+        \"\"\"\n+        Create a new StreamingContext. At least the master and app name and duration\n+        should be set, either through the named parameters here or through C{conf}.\n+\n+        @param sparkContext: L{SparkContext} object.\n+        @param duration: seconds for SparkStreaming.\n+\n+        \"\"\"\n+        self._sc = sparkContext\n+        self._jvm = self._sc._jvm\n+        self._start_callback_server()\n+        self._jssc = self._initialize_context(self._sc, duration)\n+\n+    def _start_callback_server(self):\n+        gw = self._sc._gateway\n+        # getattr will fallback to JVM\n+        if \"_callback_server\" not in gw.__dict__:\n+            _daemonize_callback_server()\n+            gw._start_callback_server(gw._python_proxy_port)\n+            gw._python_proxy_port = gw._callback_server.port  # update port with real port\n+\n+    def _initialize_context(self, sc, duration):\n+        java_import(self._jvm, \"org.apache.spark.streaming.*\")\n+        java_import(self._jvm, \"org.apache.spark.streaming.api.java.*\")\n+        java_import(self._jvm, \"org.apache.spark.streaming.api.python.*\")\n+        return self._jvm.JavaStreamingContext(sc._jsc, self._jduration(duration))\n+\n+    def _jduration(self, seconds):\n+        \"\"\"\n+        Create Duration object given number of seconds\n+        \"\"\"\n+        return self._jvm.Duration(int(seconds * 1000))\n+\n+    @property\n+    def sparkContext(self):\n+        \"\"\"\n+        Return SparkContext which is associated with this StreamingContext.\n+        \"\"\"\n+        return self._sc\n+\n+    def start(self):\n+        \"\"\"\n+        Start the execution of the streams.\n+        \"\"\"\n+        self._jssc.start()\n+\n+    def awaitTermination(self, timeout=None):\n+        \"\"\"\n+        Wait for the execution to stop.\n+        @param timeout: time to wait in seconds\n+        \"\"\"\n+        if timeout is None:\n+            self._jssc.awaitTermination()\n+        else:\n+            self._jssc.awaitTermination(int(timeout * 1000))\n+\n+    def stop(self, stopSparkContext=True, stopGraceFully=False):\n+        \"\"\"\n+        Stop the execution of the streams immediately (does not wait for all received data"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Should `duration` be `batchDuration`, for parity with the Scala version?\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-02T22:41:25Z",
    "diffHunk": "@@ -0,0 +1,305 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import sys\n+\n+from py4j.java_collections import ListConverter\n+from py4j.java_gateway import java_import\n+\n+from pyspark import RDD, SparkConf\n+from pyspark.serializers import UTF8Deserializer, CloudPickleSerializer\n+from pyspark.context import SparkContext\n+from pyspark.storagelevel import StorageLevel\n+from pyspark.streaming.dstream import DStream\n+from pyspark.streaming.util import TransformFunction, TransformFunctionSerializer\n+\n+__all__ = [\"StreamingContext\"]\n+\n+\n+def _daemonize_callback_server():\n+    \"\"\"\n+    Hack Py4J to daemonize callback server\n+\n+    The thread of callback server has daemon=False, it will block the driver\n+    from exiting if it's not shutdown. The following code replace `start()`\n+    of CallbackServer with a new version, which set daemon=True for this\n+    thread.\n+    \"\"\"\n+    # TODO: create a patch for Py4J\n+    import socket\n+    import py4j.java_gateway\n+    logger = py4j.java_gateway.logger\n+    from py4j.java_gateway import Py4JNetworkError\n+    from threading import Thread\n+\n+    def start(self):\n+        \"\"\"Starts the CallbackServer. This method should be called by the\n+        client instead of run().\"\"\"\n+        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR,\n+                                      1)\n+        try:\n+            self.server_socket.bind((self.address, self.port))\n+        except Exception:\n+            msg = 'An error occurred while trying to start the callback server'\n+            logger.exception(msg)\n+            raise Py4JNetworkError(msg)\n+\n+        # Maybe thread needs to be cleanup up?\n+        self.thread = Thread(target=self.run)\n+        self.thread.daemon = True\n+        self.thread.start()\n+\n+    py4j.java_gateway.CallbackServer.start = start\n+\n+\n+class StreamingContext(object):\n+    \"\"\"\n+    Main entry point for Spark Streaming functionality. A StreamingContext\n+    represents the connection to a Spark cluster, and can be used to create\n+    L{DStream} various input sources. It can be from an existing L{SparkContext}.\n+    After creating and transforming DStreams, the streaming computation can\n+    be started and stopped using `context.start()` and `context.stop()`,\n+    respectively. `context.awaitTransformation()` allows the current thread\n+    to wait for the termination of the context by `stop()` or by an exception.\n+    \"\"\"\n+    _transformerSerializer = None\n+\n+    def __init__(self, sparkContext, duration=None, jssc=None):"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I agree. Good catch.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-03T01:36:45Z",
    "diffHunk": "@@ -0,0 +1,305 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import sys\n+\n+from py4j.java_collections import ListConverter\n+from py4j.java_gateway import java_import\n+\n+from pyspark import RDD, SparkConf\n+from pyspark.serializers import UTF8Deserializer, CloudPickleSerializer\n+from pyspark.context import SparkContext\n+from pyspark.storagelevel import StorageLevel\n+from pyspark.streaming.dstream import DStream\n+from pyspark.streaming.util import TransformFunction, TransformFunctionSerializer\n+\n+__all__ = [\"StreamingContext\"]\n+\n+\n+def _daemonize_callback_server():\n+    \"\"\"\n+    Hack Py4J to daemonize callback server\n+\n+    The thread of callback server has daemon=False, it will block the driver\n+    from exiting if it's not shutdown. The following code replace `start()`\n+    of CallbackServer with a new version, which set daemon=True for this\n+    thread.\n+    \"\"\"\n+    # TODO: create a patch for Py4J\n+    import socket\n+    import py4j.java_gateway\n+    logger = py4j.java_gateway.logger\n+    from py4j.java_gateway import Py4JNetworkError\n+    from threading import Thread\n+\n+    def start(self):\n+        \"\"\"Starts the CallbackServer. This method should be called by the\n+        client instead of run().\"\"\"\n+        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR,\n+                                      1)\n+        try:\n+            self.server_socket.bind((self.address, self.port))\n+        except Exception:\n+            msg = 'An error occurred while trying to start the callback server'\n+            logger.exception(msg)\n+            raise Py4JNetworkError(msg)\n+\n+        # Maybe thread needs to be cleanup up?\n+        self.thread = Thread(target=self.run)\n+        self.thread.daemon = True\n+        self.thread.start()\n+\n+    py4j.java_gateway.CallbackServer.start = start\n+\n+\n+class StreamingContext(object):\n+    \"\"\"\n+    Main entry point for Spark Streaming functionality. A StreamingContext\n+    represents the connection to a Spark cluster, and can be used to create\n+    L{DStream} various input sources. It can be from an existing L{SparkContext}.\n+    After creating and transforming DStreams, the streaming computation can\n+    be started and stopped using `context.start()` and `context.stop()`,\n+    respectively. `context.awaitTransformation()` allows the current thread\n+    to wait for the termination of the context by `stop()` or by an exception.\n+    \"\"\"\n+    _transformerSerializer = None\n+\n+    def __init__(self, sparkContext, duration=None, jssc=None):"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "For consistency's sake, we might just want to copy and adapt the Scala versions of these docstrings.   e.g. \n\n```\n@param batchDuration the time interval at which streaming data will be divided into batches\n```\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-02T22:42:35Z",
    "diffHunk": "@@ -0,0 +1,305 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import sys\n+\n+from py4j.java_collections import ListConverter\n+from py4j.java_gateway import java_import\n+\n+from pyspark import RDD, SparkConf\n+from pyspark.serializers import UTF8Deserializer, CloudPickleSerializer\n+from pyspark.context import SparkContext\n+from pyspark.storagelevel import StorageLevel\n+from pyspark.streaming.dstream import DStream\n+from pyspark.streaming.util import TransformFunction, TransformFunctionSerializer\n+\n+__all__ = [\"StreamingContext\"]\n+\n+\n+def _daemonize_callback_server():\n+    \"\"\"\n+    Hack Py4J to daemonize callback server\n+\n+    The thread of callback server has daemon=False, it will block the driver\n+    from exiting if it's not shutdown. The following code replace `start()`\n+    of CallbackServer with a new version, which set daemon=True for this\n+    thread.\n+    \"\"\"\n+    # TODO: create a patch for Py4J\n+    import socket\n+    import py4j.java_gateway\n+    logger = py4j.java_gateway.logger\n+    from py4j.java_gateway import Py4JNetworkError\n+    from threading import Thread\n+\n+    def start(self):\n+        \"\"\"Starts the CallbackServer. This method should be called by the\n+        client instead of run().\"\"\"\n+        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR,\n+                                      1)\n+        try:\n+            self.server_socket.bind((self.address, self.port))\n+        except Exception:\n+            msg = 'An error occurred while trying to start the callback server'\n+            logger.exception(msg)\n+            raise Py4JNetworkError(msg)\n+\n+        # Maybe thread needs to be cleanup up?\n+        self.thread = Thread(target=self.run)\n+        self.thread.daemon = True\n+        self.thread.start()\n+\n+    py4j.java_gateway.CallbackServer.start = start\n+\n+\n+class StreamingContext(object):\n+    \"\"\"\n+    Main entry point for Spark Streaming functionality. A StreamingContext\n+    represents the connection to a Spark cluster, and can be used to create\n+    L{DStream} various input sources. It can be from an existing L{SparkContext}.\n+    After creating and transforming DStreams, the streaming computation can\n+    be started and stopped using `context.start()` and `context.stop()`,\n+    respectively. `context.awaitTransformation()` allows the current thread\n+    to wait for the termination of the context by `stop()` or by an exception.\n+    \"\"\"\n+    _transformerSerializer = None\n+\n+    def __init__(self, sparkContext, duration=None, jssc=None):\n+        \"\"\"\n+        Create a new StreamingContext.\n+\n+        @param sparkContext: L{SparkContext} object.\n+        @param duration: number of seconds."
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Yeah, I had mentioned that earlier. We shoudl keep the documentation consistent.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-03T01:37:16Z",
    "diffHunk": "@@ -0,0 +1,305 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import sys\n+\n+from py4j.java_collections import ListConverter\n+from py4j.java_gateway import java_import\n+\n+from pyspark import RDD, SparkConf\n+from pyspark.serializers import UTF8Deserializer, CloudPickleSerializer\n+from pyspark.context import SparkContext\n+from pyspark.storagelevel import StorageLevel\n+from pyspark.streaming.dstream import DStream\n+from pyspark.streaming.util import TransformFunction, TransformFunctionSerializer\n+\n+__all__ = [\"StreamingContext\"]\n+\n+\n+def _daemonize_callback_server():\n+    \"\"\"\n+    Hack Py4J to daemonize callback server\n+\n+    The thread of callback server has daemon=False, it will block the driver\n+    from exiting if it's not shutdown. The following code replace `start()`\n+    of CallbackServer with a new version, which set daemon=True for this\n+    thread.\n+    \"\"\"\n+    # TODO: create a patch for Py4J\n+    import socket\n+    import py4j.java_gateway\n+    logger = py4j.java_gateway.logger\n+    from py4j.java_gateway import Py4JNetworkError\n+    from threading import Thread\n+\n+    def start(self):\n+        \"\"\"Starts the CallbackServer. This method should be called by the\n+        client instead of run().\"\"\"\n+        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR,\n+                                      1)\n+        try:\n+            self.server_socket.bind((self.address, self.port))\n+        except Exception:\n+            msg = 'An error occurred while trying to start the callback server'\n+            logger.exception(msg)\n+            raise Py4JNetworkError(msg)\n+\n+        # Maybe thread needs to be cleanup up?\n+        self.thread = Thread(target=self.run)\n+        self.thread.daemon = True\n+        self.thread.start()\n+\n+    py4j.java_gateway.CallbackServer.start = start\n+\n+\n+class StreamingContext(object):\n+    \"\"\"\n+    Main entry point for Spark Streaming functionality. A StreamingContext\n+    represents the connection to a Spark cluster, and can be used to create\n+    L{DStream} various input sources. It can be from an existing L{SparkContext}.\n+    After creating and transforming DStreams, the streaming computation can\n+    be started and stopped using `context.start()` and `context.stop()`,\n+    respectively. `context.awaitTransformation()` allows the current thread\n+    to wait for the termination of the context by `stop()` or by an exception.\n+    \"\"\"\n+    _transformerSerializer = None\n+\n+    def __init__(self, sparkContext, duration=None, jssc=None):\n+        \"\"\"\n+        Create a new StreamingContext.\n+\n+        @param sparkContext: L{SparkContext} object.\n+        @param duration: number of seconds."
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Here, too, we might want to copy the Scala docs.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-02T22:56:27Z",
    "diffHunk": "@@ -0,0 +1,305 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import sys\n+\n+from py4j.java_collections import ListConverter\n+from py4j.java_gateway import java_import\n+\n+from pyspark import RDD, SparkConf\n+from pyspark.serializers import UTF8Deserializer, CloudPickleSerializer\n+from pyspark.context import SparkContext\n+from pyspark.storagelevel import StorageLevel\n+from pyspark.streaming.dstream import DStream\n+from pyspark.streaming.util import TransformFunction, TransformFunctionSerializer\n+\n+__all__ = [\"StreamingContext\"]\n+\n+\n+def _daemonize_callback_server():\n+    \"\"\"\n+    Hack Py4J to daemonize callback server\n+\n+    The thread of callback server has daemon=False, it will block the driver\n+    from exiting if it's not shutdown. The following code replace `start()`\n+    of CallbackServer with a new version, which set daemon=True for this\n+    thread.\n+    \"\"\"\n+    # TODO: create a patch for Py4J\n+    import socket\n+    import py4j.java_gateway\n+    logger = py4j.java_gateway.logger\n+    from py4j.java_gateway import Py4JNetworkError\n+    from threading import Thread\n+\n+    def start(self):\n+        \"\"\"Starts the CallbackServer. This method should be called by the\n+        client instead of run().\"\"\"\n+        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR,\n+                                      1)\n+        try:\n+            self.server_socket.bind((self.address, self.port))\n+        except Exception:\n+            msg = 'An error occurred while trying to start the callback server'\n+            logger.exception(msg)\n+            raise Py4JNetworkError(msg)\n+\n+        # Maybe thread needs to be cleanup up?\n+        self.thread = Thread(target=self.run)\n+        self.thread.daemon = True\n+        self.thread.start()\n+\n+    py4j.java_gateway.CallbackServer.start = start\n+\n+\n+class StreamingContext(object):\n+    \"\"\"\n+    Main entry point for Spark Streaming functionality. A StreamingContext\n+    represents the connection to a Spark cluster, and can be used to create\n+    L{DStream} various input sources. It can be from an existing L{SparkContext}.\n+    After creating and transforming DStreams, the streaming computation can\n+    be started and stopped using `context.start()` and `context.stop()`,\n+    respectively. `context.awaitTransformation()` allows the current thread\n+    to wait for the termination of the context by `stop()` or by an exception.\n+    \"\"\"\n+    _transformerSerializer = None\n+\n+    def __init__(self, sparkContext, duration=None, jssc=None):\n+        \"\"\"\n+        Create a new StreamingContext.\n+\n+        @param sparkContext: L{SparkContext} object.\n+        @param duration: number of seconds.\n+        \"\"\"\n+\n+        self._sc = sparkContext\n+        self._jvm = self._sc._jvm\n+        self._jssc = jssc or self._initialize_context(self._sc, duration)\n+\n+    def _initialize_context(self, sc, duration):\n+        self._ensure_initialized()\n+        return self._jvm.JavaStreamingContext(sc._jsc, self._jduration(duration))\n+\n+    def _jduration(self, seconds):\n+        \"\"\"\n+        Create Duration object given number of seconds\n+        \"\"\"\n+        return self._jvm.Duration(int(seconds * 1000))\n+\n+    @classmethod\n+    def _ensure_initialized(cls):\n+        SparkContext._ensure_initialized()\n+        gw = SparkContext._gateway\n+        # start callback server\n+        # getattr will fallback to JVM\n+        if \"_callback_server\" not in gw.__dict__:\n+            _daemonize_callback_server()\n+            gw._start_callback_server(gw._python_proxy_port)\n+\n+        java_import(gw.jvm, \"org.apache.spark.streaming.*\")\n+        java_import(gw.jvm, \"org.apache.spark.streaming.api.java.*\")\n+        java_import(gw.jvm, \"org.apache.spark.streaming.api.python.*\")\n+        # register serializer for TransformFunction\n+        # it happens before creating SparkContext when loading from checkpointing\n+        cls._transformerSerializer = TransformFunctionSerializer(\n+            SparkContext._active_spark_context, CloudPickleSerializer(), gw)\n+\n+    @classmethod\n+    def getOrCreate(cls, path, setupFunc):\n+        \"\"\"\n+        Get the StreamingContext from checkpoint file at `path`, or setup"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "Minor typo: serialzers -> serializers.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-02T23:01:09Z",
    "diffHunk": "@@ -0,0 +1,319 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import sys\n+\n+from py4j.java_collections import ListConverter\n+from py4j.java_gateway import java_import, JavaObject\n+\n+from pyspark import RDD, SparkConf\n+from pyspark.serializers import UTF8Deserializer, CloudPickleSerializer\n+from pyspark.context import SparkContext\n+from pyspark.storagelevel import StorageLevel\n+from pyspark.streaming.dstream import DStream\n+from pyspark.streaming.util import TransformFunction, TransformFunctionSerializer\n+\n+__all__ = [\"StreamingContext\"]\n+\n+\n+def _daemonize_callback_server():\n+    \"\"\"\n+    Hack Py4J to daemonize callback server\n+\n+    The thread of callback server has daemon=False, it will block the driver\n+    from exiting if it's not shutdown. The following code replace `start()`\n+    of CallbackServer with a new version, which set daemon=True for this\n+    thread.\n+\n+    Also, it will update the port number (0) with real port\n+    \"\"\"\n+    # TODO: create a patch for Py4J\n+    import socket\n+    import py4j.java_gateway\n+    logger = py4j.java_gateway.logger\n+    from py4j.java_gateway import Py4JNetworkError\n+    from threading import Thread\n+\n+    def start(self):\n+        \"\"\"Starts the CallbackServer. This method should be called by the\n+        client instead of run().\"\"\"\n+        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR,\n+                                      1)\n+        try:\n+            self.server_socket.bind((self.address, self.port))\n+            if not self.port:\n+                # update port with real port\n+                self.port = self.server_socket.getsockname()[1]\n+        except Exception as e:\n+            msg = 'An error occurred while trying to start the callback server: %s' % e\n+            logger.exception(msg)\n+            raise Py4JNetworkError(msg)\n+\n+        # Maybe thread needs to be cleanup up?\n+        self.thread = Thread(target=self.run)\n+        self.thread.daemon = True\n+        self.thread.start()\n+\n+    py4j.java_gateway.CallbackServer.start = start\n+\n+\n+class StreamingContext(object):\n+    \"\"\"\n+    Main entry point for Spark Streaming functionality. A StreamingContext\n+    represents the connection to a Spark cluster, and can be used to create\n+    L{DStream} various input sources. It can be from an existing L{SparkContext}.\n+    After creating and transforming DStreams, the streaming computation can\n+    be started and stopped using `context.start()` and `context.stop()`,\n+    respectively. `context.awaitTransformation()` allows the current thread\n+    to wait for the termination of the context by `stop()` or by an exception.\n+    \"\"\"\n+    _transformerSerializer = None\n+\n+    def __init__(self, sparkContext, duration=None, jssc=None):\n+        \"\"\"\n+        Create a new StreamingContext.\n+\n+        @param sparkContext: L{SparkContext} object.\n+        @param duration: number of seconds.\n+        \"\"\"\n+\n+        self._sc = sparkContext\n+        self._jvm = self._sc._jvm\n+        self._jssc = jssc or self._initialize_context(self._sc, duration)\n+\n+    def _initialize_context(self, sc, duration):\n+        self._ensure_initialized()\n+        return self._jvm.JavaStreamingContext(sc._jsc, self._jduration(duration))\n+\n+    def _jduration(self, seconds):\n+        \"\"\"\n+        Create Duration object given number of seconds\n+        \"\"\"\n+        return self._jvm.Duration(int(seconds * 1000))\n+\n+    @classmethod\n+    def _ensure_initialized(cls):\n+        SparkContext._ensure_initialized()\n+        gw = SparkContext._gateway\n+\n+        java_import(gw.jvm, \"org.apache.spark.streaming.*\")\n+        java_import(gw.jvm, \"org.apache.spark.streaming.api.java.*\")\n+        java_import(gw.jvm, \"org.apache.spark.streaming.api.python.*\")\n+\n+        # start callback server\n+        # getattr will fallback to JVM, so we cannot test by hasattr()\n+        if \"_callback_server\" not in gw.__dict__:\n+            _daemonize_callback_server()\n+            # use random port\n+            gw._start_callback_server(0)\n+            # gateway with real port\n+            gw._python_proxy_port = gw._callback_server.port\n+            # get the GatewayServer object in JVM by ID\n+            jgws = JavaObject(\"GATEWAY_SERVER\", gw._gateway_client)\n+            # update the port of CallbackClient with real port\n+            gw.jvm.PythonDStream.updatePythonGatewayPort(jgws, gw._python_proxy_port)\n+\n+        # register serializer for TransformFunction\n+        # it happens before creating SparkContext when loading from checkpointing\n+        cls._transformerSerializer = TransformFunctionSerializer(\n+            SparkContext._active_spark_context, CloudPickleSerializer(), gw)\n+\n+    @classmethod\n+    def getOrCreate(cls, path, setupFunc):\n+        \"\"\"\n+        Get the StreamingContext from checkpoint file at `path`, or setup\n+        it by `setupFunc`.\n+\n+        :param path: directory of checkpoint\n+        :param setupFunc: a function used to create StreamingContext and\n+                          setup DStreams.\n+        :return: a StreamingContext\n+        \"\"\"\n+        if not os.path.exists(path) or not os.path.isdir(path) or not os.listdir(path):\n+            ssc = setupFunc()\n+            ssc.checkpoint(path)\n+            return ssc\n+\n+        cls._ensure_initialized()\n+        gw = SparkContext._gateway\n+\n+        try:\n+            jssc = gw.jvm.JavaStreamingContext(path)\n+        except Exception:\n+            print >>sys.stderr, \"failed to load StreamingContext from checkpoint\"\n+            raise\n+\n+        jsc = jssc.sparkContext()\n+        conf = SparkConf(_jconf=jsc.getConf())\n+        sc = SparkContext(conf=conf, gateway=gw, jsc=jsc)\n+        # update ctx in serializer\n+        SparkContext._active_spark_context = sc\n+        cls._transformerSerializer.ctx = sc\n+        return StreamingContext(sc, None, jssc)\n+\n+    @property\n+    def sparkContext(self):\n+        \"\"\"\n+        Return SparkContext which is associated with this StreamingContext.\n+        \"\"\"\n+        return self._sc\n+\n+    def start(self):\n+        \"\"\"\n+        Start the execution of the streams.\n+        \"\"\"\n+        self._jssc.start()\n+\n+    def awaitTermination(self, timeout=None):\n+        \"\"\"\n+        Wait for the execution to stop.\n+        @param timeout: time to wait in seconds\n+        \"\"\"\n+        if timeout is None:\n+            self._jssc.awaitTermination()\n+        else:\n+            self._jssc.awaitTermination(int(timeout * 1000))\n+\n+    def stop(self, stopSparkContext=True, stopGraceFully=False):\n+        \"\"\"\n+        Stop the execution of the streams, with option of ensuring all\n+        received data has been processed.\n+\n+        @param stopSparkContext: Stop the associated SparkContext or not\n+        @param stopGracefully: Stop gracefully by waiting for the processing\n+                              of all received data to be completed\n+        \"\"\"\n+        self._jssc.stop(stopSparkContext, stopGraceFully)\n+        if stopSparkContext:\n+            self._sc.stop()\n+\n+    def remember(self, duration):\n+        \"\"\"\n+        Set each DStreams in this context to remember RDDs it generated\n+        in the last given duration. DStreams remember RDDs only for a\n+        limited duration of time and releases them for garbage collection.\n+        This method allows the developer to specify how to long to remember\n+        the RDDs (if the developer wishes to query old data outside the\n+        DStream computation).\n+\n+        @param duration: Minimum duration (in seconds) that each DStream\n+                        should remember its RDDs\n+        \"\"\"\n+        self._jssc.remember(self._jduration(duration))\n+\n+    def checkpoint(self, directory):\n+        \"\"\"\n+        Sets the context to periodically checkpoint the DStream operations for master\n+        fault-tolerance. The graph will be checkpointed every batch interval.\n+\n+        @param directory: HDFS-compatible directory where the checkpoint data\n+                         will be reliably stored\n+        \"\"\"\n+        self._jssc.checkpoint(directory)\n+\n+    def socketTextStream(self, hostname, port, storageLevel=StorageLevel.MEMORY_AND_DISK_SER_2):\n+        \"\"\"\n+        Create an input from TCP source hostname:port. Data is received using\n+        a TCP socket and receive byte is interpreted as UTF8 encoded ``\\\\n`` delimited\n+        lines.\n+\n+        @param hostname:      Hostname to connect to for receiving data\n+        @param port:          Port to connect to for receiving data\n+        @param storageLevel:  Storage level to use for storing the received objects\n+        \"\"\"\n+        jlevel = self._sc._getJavaStorageLevel(storageLevel)\n+        return DStream(self._jssc.socketTextStream(hostname, port, jlevel), self,\n+                       UTF8Deserializer())\n+\n+    def textFileStream(self, directory):\n+        \"\"\"\n+        Create an input stream that monitors a Hadoop-compatible file system\n+        for new files and reads them as text files. Files must be wrriten to the\n+        monitored directory by \"moving\" them from another location within the same\n+        file system. File names starting with . are ignored.\n+        \"\"\"\n+        return DStream(self._jssc.textFileStream(directory), self, UTF8Deserializer())\n+\n+    def _check_serialzers(self, rdds):"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "I think you can use `rdds[i]._reserialize()` here, which will only perform the reserialization if necessary.\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-02T23:03:16Z",
    "diffHunk": "@@ -0,0 +1,319 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import sys\n+\n+from py4j.java_collections import ListConverter\n+from py4j.java_gateway import java_import, JavaObject\n+\n+from pyspark import RDD, SparkConf\n+from pyspark.serializers import UTF8Deserializer, CloudPickleSerializer\n+from pyspark.context import SparkContext\n+from pyspark.storagelevel import StorageLevel\n+from pyspark.streaming.dstream import DStream\n+from pyspark.streaming.util import TransformFunction, TransformFunctionSerializer\n+\n+__all__ = [\"StreamingContext\"]\n+\n+\n+def _daemonize_callback_server():\n+    \"\"\"\n+    Hack Py4J to daemonize callback server\n+\n+    The thread of callback server has daemon=False, it will block the driver\n+    from exiting if it's not shutdown. The following code replace `start()`\n+    of CallbackServer with a new version, which set daemon=True for this\n+    thread.\n+\n+    Also, it will update the port number (0) with real port\n+    \"\"\"\n+    # TODO: create a patch for Py4J\n+    import socket\n+    import py4j.java_gateway\n+    logger = py4j.java_gateway.logger\n+    from py4j.java_gateway import Py4JNetworkError\n+    from threading import Thread\n+\n+    def start(self):\n+        \"\"\"Starts the CallbackServer. This method should be called by the\n+        client instead of run().\"\"\"\n+        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR,\n+                                      1)\n+        try:\n+            self.server_socket.bind((self.address, self.port))\n+            if not self.port:\n+                # update port with real port\n+                self.port = self.server_socket.getsockname()[1]\n+        except Exception as e:\n+            msg = 'An error occurred while trying to start the callback server: %s' % e\n+            logger.exception(msg)\n+            raise Py4JNetworkError(msg)\n+\n+        # Maybe thread needs to be cleanup up?\n+        self.thread = Thread(target=self.run)\n+        self.thread.daemon = True\n+        self.thread.start()\n+\n+    py4j.java_gateway.CallbackServer.start = start\n+\n+\n+class StreamingContext(object):\n+    \"\"\"\n+    Main entry point for Spark Streaming functionality. A StreamingContext\n+    represents the connection to a Spark cluster, and can be used to create\n+    L{DStream} various input sources. It can be from an existing L{SparkContext}.\n+    After creating and transforming DStreams, the streaming computation can\n+    be started and stopped using `context.start()` and `context.stop()`,\n+    respectively. `context.awaitTransformation()` allows the current thread\n+    to wait for the termination of the context by `stop()` or by an exception.\n+    \"\"\"\n+    _transformerSerializer = None\n+\n+    def __init__(self, sparkContext, duration=None, jssc=None):\n+        \"\"\"\n+        Create a new StreamingContext.\n+\n+        @param sparkContext: L{SparkContext} object.\n+        @param duration: number of seconds.\n+        \"\"\"\n+\n+        self._sc = sparkContext\n+        self._jvm = self._sc._jvm\n+        self._jssc = jssc or self._initialize_context(self._sc, duration)\n+\n+    def _initialize_context(self, sc, duration):\n+        self._ensure_initialized()\n+        return self._jvm.JavaStreamingContext(sc._jsc, self._jduration(duration))\n+\n+    def _jduration(self, seconds):\n+        \"\"\"\n+        Create Duration object given number of seconds\n+        \"\"\"\n+        return self._jvm.Duration(int(seconds * 1000))\n+\n+    @classmethod\n+    def _ensure_initialized(cls):\n+        SparkContext._ensure_initialized()\n+        gw = SparkContext._gateway\n+\n+        java_import(gw.jvm, \"org.apache.spark.streaming.*\")\n+        java_import(gw.jvm, \"org.apache.spark.streaming.api.java.*\")\n+        java_import(gw.jvm, \"org.apache.spark.streaming.api.python.*\")\n+\n+        # start callback server\n+        # getattr will fallback to JVM, so we cannot test by hasattr()\n+        if \"_callback_server\" not in gw.__dict__:\n+            _daemonize_callback_server()\n+            # use random port\n+            gw._start_callback_server(0)\n+            # gateway with real port\n+            gw._python_proxy_port = gw._callback_server.port\n+            # get the GatewayServer object in JVM by ID\n+            jgws = JavaObject(\"GATEWAY_SERVER\", gw._gateway_client)\n+            # update the port of CallbackClient with real port\n+            gw.jvm.PythonDStream.updatePythonGatewayPort(jgws, gw._python_proxy_port)\n+\n+        # register serializer for TransformFunction\n+        # it happens before creating SparkContext when loading from checkpointing\n+        cls._transformerSerializer = TransformFunctionSerializer(\n+            SparkContext._active_spark_context, CloudPickleSerializer(), gw)\n+\n+    @classmethod\n+    def getOrCreate(cls, path, setupFunc):\n+        \"\"\"\n+        Get the StreamingContext from checkpoint file at `path`, or setup\n+        it by `setupFunc`.\n+\n+        :param path: directory of checkpoint\n+        :param setupFunc: a function used to create StreamingContext and\n+                          setup DStreams.\n+        :return: a StreamingContext\n+        \"\"\"\n+        if not os.path.exists(path) or not os.path.isdir(path) or not os.listdir(path):\n+            ssc = setupFunc()\n+            ssc.checkpoint(path)\n+            return ssc\n+\n+        cls._ensure_initialized()\n+        gw = SparkContext._gateway\n+\n+        try:\n+            jssc = gw.jvm.JavaStreamingContext(path)\n+        except Exception:\n+            print >>sys.stderr, \"failed to load StreamingContext from checkpoint\"\n+            raise\n+\n+        jsc = jssc.sparkContext()\n+        conf = SparkConf(_jconf=jsc.getConf())\n+        sc = SparkContext(conf=conf, gateway=gw, jsc=jsc)\n+        # update ctx in serializer\n+        SparkContext._active_spark_context = sc\n+        cls._transformerSerializer.ctx = sc\n+        return StreamingContext(sc, None, jssc)\n+\n+    @property\n+    def sparkContext(self):\n+        \"\"\"\n+        Return SparkContext which is associated with this StreamingContext.\n+        \"\"\"\n+        return self._sc\n+\n+    def start(self):\n+        \"\"\"\n+        Start the execution of the streams.\n+        \"\"\"\n+        self._jssc.start()\n+\n+    def awaitTermination(self, timeout=None):\n+        \"\"\"\n+        Wait for the execution to stop.\n+        @param timeout: time to wait in seconds\n+        \"\"\"\n+        if timeout is None:\n+            self._jssc.awaitTermination()\n+        else:\n+            self._jssc.awaitTermination(int(timeout * 1000))\n+\n+    def stop(self, stopSparkContext=True, stopGraceFully=False):\n+        \"\"\"\n+        Stop the execution of the streams, with option of ensuring all\n+        received data has been processed.\n+\n+        @param stopSparkContext: Stop the associated SparkContext or not\n+        @param stopGracefully: Stop gracefully by waiting for the processing\n+                              of all received data to be completed\n+        \"\"\"\n+        self._jssc.stop(stopSparkContext, stopGraceFully)\n+        if stopSparkContext:\n+            self._sc.stop()\n+\n+    def remember(self, duration):\n+        \"\"\"\n+        Set each DStreams in this context to remember RDDs it generated\n+        in the last given duration. DStreams remember RDDs only for a\n+        limited duration of time and releases them for garbage collection.\n+        This method allows the developer to specify how to long to remember\n+        the RDDs (if the developer wishes to query old data outside the\n+        DStream computation).\n+\n+        @param duration: Minimum duration (in seconds) that each DStream\n+                        should remember its RDDs\n+        \"\"\"\n+        self._jssc.remember(self._jduration(duration))\n+\n+    def checkpoint(self, directory):\n+        \"\"\"\n+        Sets the context to periodically checkpoint the DStream operations for master\n+        fault-tolerance. The graph will be checkpointed every batch interval.\n+\n+        @param directory: HDFS-compatible directory where the checkpoint data\n+                         will be reliably stored\n+        \"\"\"\n+        self._jssc.checkpoint(directory)\n+\n+    def socketTextStream(self, hostname, port, storageLevel=StorageLevel.MEMORY_AND_DISK_SER_2):\n+        \"\"\"\n+        Create an input from TCP source hostname:port. Data is received using\n+        a TCP socket and receive byte is interpreted as UTF8 encoded ``\\\\n`` delimited\n+        lines.\n+\n+        @param hostname:      Hostname to connect to for receiving data\n+        @param port:          Port to connect to for receiving data\n+        @param storageLevel:  Storage level to use for storing the received objects\n+        \"\"\"\n+        jlevel = self._sc._getJavaStorageLevel(storageLevel)\n+        return DStream(self._jssc.socketTextStream(hostname, port, jlevel), self,\n+                       UTF8Deserializer())\n+\n+    def textFileStream(self, directory):\n+        \"\"\"\n+        Create an input stream that monitors a Hadoop-compatible file system\n+        for new files and reads them as text files. Files must be wrriten to the\n+        monitored directory by \"moving\" them from another location within the same\n+        file system. File names starting with . are ignored.\n+        \"\"\"\n+        return DStream(self._jssc.textFileStream(directory), self, UTF8Deserializer())\n+\n+    def _check_serialzers(self, rdds):\n+        # make sure they have same serializer\n+        if len(set(rdd._jrdd_deserializer for rdd in rdds)) > 1:\n+            for i in range(len(rdds)):\n+                # reset them to sc.serializer\n+                rdds[i] = rdds[i].map(lambda x: x, preservesPartitioning=True)"
  }],
  "prId": 2538
}, {
  "comments": [{
    "author": {
      "login": "JoshRosen"
    },
    "body": "In `DStream.union`, we check the input to verify that the streams have the same slide duration (https://github.com/apache/spark/pull/2538/files#diff-6c576c52abc0624ccb6a2f45828dc6a7R326).  Can we do the same sort of input validation here?\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-02T23:16:19Z",
    "diffHunk": "@@ -0,0 +1,319 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import sys\n+\n+from py4j.java_collections import ListConverter\n+from py4j.java_gateway import java_import, JavaObject\n+\n+from pyspark import RDD, SparkConf\n+from pyspark.serializers import UTF8Deserializer, CloudPickleSerializer\n+from pyspark.context import SparkContext\n+from pyspark.storagelevel import StorageLevel\n+from pyspark.streaming.dstream import DStream\n+from pyspark.streaming.util import TransformFunction, TransformFunctionSerializer\n+\n+__all__ = [\"StreamingContext\"]\n+\n+\n+def _daemonize_callback_server():\n+    \"\"\"\n+    Hack Py4J to daemonize callback server\n+\n+    The thread of callback server has daemon=False, it will block the driver\n+    from exiting if it's not shutdown. The following code replace `start()`\n+    of CallbackServer with a new version, which set daemon=True for this\n+    thread.\n+\n+    Also, it will update the port number (0) with real port\n+    \"\"\"\n+    # TODO: create a patch for Py4J\n+    import socket\n+    import py4j.java_gateway\n+    logger = py4j.java_gateway.logger\n+    from py4j.java_gateway import Py4JNetworkError\n+    from threading import Thread\n+\n+    def start(self):\n+        \"\"\"Starts the CallbackServer. This method should be called by the\n+        client instead of run().\"\"\"\n+        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR,\n+                                      1)\n+        try:\n+            self.server_socket.bind((self.address, self.port))\n+            if not self.port:\n+                # update port with real port\n+                self.port = self.server_socket.getsockname()[1]\n+        except Exception as e:\n+            msg = 'An error occurred while trying to start the callback server: %s' % e\n+            logger.exception(msg)\n+            raise Py4JNetworkError(msg)\n+\n+        # Maybe thread needs to be cleanup up?\n+        self.thread = Thread(target=self.run)\n+        self.thread.daemon = True\n+        self.thread.start()\n+\n+    py4j.java_gateway.CallbackServer.start = start\n+\n+\n+class StreamingContext(object):\n+    \"\"\"\n+    Main entry point for Spark Streaming functionality. A StreamingContext\n+    represents the connection to a Spark cluster, and can be used to create\n+    L{DStream} various input sources. It can be from an existing L{SparkContext}.\n+    After creating and transforming DStreams, the streaming computation can\n+    be started and stopped using `context.start()` and `context.stop()`,\n+    respectively. `context.awaitTransformation()` allows the current thread\n+    to wait for the termination of the context by `stop()` or by an exception.\n+    \"\"\"\n+    _transformerSerializer = None\n+\n+    def __init__(self, sparkContext, duration=None, jssc=None):\n+        \"\"\"\n+        Create a new StreamingContext.\n+\n+        @param sparkContext: L{SparkContext} object.\n+        @param duration: number of seconds.\n+        \"\"\"\n+\n+        self._sc = sparkContext\n+        self._jvm = self._sc._jvm\n+        self._jssc = jssc or self._initialize_context(self._sc, duration)\n+\n+    def _initialize_context(self, sc, duration):\n+        self._ensure_initialized()\n+        return self._jvm.JavaStreamingContext(sc._jsc, self._jduration(duration))\n+\n+    def _jduration(self, seconds):\n+        \"\"\"\n+        Create Duration object given number of seconds\n+        \"\"\"\n+        return self._jvm.Duration(int(seconds * 1000))\n+\n+    @classmethod\n+    def _ensure_initialized(cls):\n+        SparkContext._ensure_initialized()\n+        gw = SparkContext._gateway\n+\n+        java_import(gw.jvm, \"org.apache.spark.streaming.*\")\n+        java_import(gw.jvm, \"org.apache.spark.streaming.api.java.*\")\n+        java_import(gw.jvm, \"org.apache.spark.streaming.api.python.*\")\n+\n+        # start callback server\n+        # getattr will fallback to JVM, so we cannot test by hasattr()\n+        if \"_callback_server\" not in gw.__dict__:\n+            _daemonize_callback_server()\n+            # use random port\n+            gw._start_callback_server(0)\n+            # gateway with real port\n+            gw._python_proxy_port = gw._callback_server.port\n+            # get the GatewayServer object in JVM by ID\n+            jgws = JavaObject(\"GATEWAY_SERVER\", gw._gateway_client)\n+            # update the port of CallbackClient with real port\n+            gw.jvm.PythonDStream.updatePythonGatewayPort(jgws, gw._python_proxy_port)\n+\n+        # register serializer for TransformFunction\n+        # it happens before creating SparkContext when loading from checkpointing\n+        cls._transformerSerializer = TransformFunctionSerializer(\n+            SparkContext._active_spark_context, CloudPickleSerializer(), gw)\n+\n+    @classmethod\n+    def getOrCreate(cls, path, setupFunc):\n+        \"\"\"\n+        Get the StreamingContext from checkpoint file at `path`, or setup\n+        it by `setupFunc`.\n+\n+        :param path: directory of checkpoint\n+        :param setupFunc: a function used to create StreamingContext and\n+                          setup DStreams.\n+        :return: a StreamingContext\n+        \"\"\"\n+        if not os.path.exists(path) or not os.path.isdir(path) or not os.listdir(path):\n+            ssc = setupFunc()\n+            ssc.checkpoint(path)\n+            return ssc\n+\n+        cls._ensure_initialized()\n+        gw = SparkContext._gateway\n+\n+        try:\n+            jssc = gw.jvm.JavaStreamingContext(path)\n+        except Exception:\n+            print >>sys.stderr, \"failed to load StreamingContext from checkpoint\"\n+            raise\n+\n+        jsc = jssc.sparkContext()\n+        conf = SparkConf(_jconf=jsc.getConf())\n+        sc = SparkContext(conf=conf, gateway=gw, jsc=jsc)\n+        # update ctx in serializer\n+        SparkContext._active_spark_context = sc\n+        cls._transformerSerializer.ctx = sc\n+        return StreamingContext(sc, None, jssc)\n+\n+    @property\n+    def sparkContext(self):\n+        \"\"\"\n+        Return SparkContext which is associated with this StreamingContext.\n+        \"\"\"\n+        return self._sc\n+\n+    def start(self):\n+        \"\"\"\n+        Start the execution of the streams.\n+        \"\"\"\n+        self._jssc.start()\n+\n+    def awaitTermination(self, timeout=None):\n+        \"\"\"\n+        Wait for the execution to stop.\n+        @param timeout: time to wait in seconds\n+        \"\"\"\n+        if timeout is None:\n+            self._jssc.awaitTermination()\n+        else:\n+            self._jssc.awaitTermination(int(timeout * 1000))\n+\n+    def stop(self, stopSparkContext=True, stopGraceFully=False):\n+        \"\"\"\n+        Stop the execution of the streams, with option of ensuring all\n+        received data has been processed.\n+\n+        @param stopSparkContext: Stop the associated SparkContext or not\n+        @param stopGracefully: Stop gracefully by waiting for the processing\n+                              of all received data to be completed\n+        \"\"\"\n+        self._jssc.stop(stopSparkContext, stopGraceFully)\n+        if stopSparkContext:\n+            self._sc.stop()\n+\n+    def remember(self, duration):\n+        \"\"\"\n+        Set each DStreams in this context to remember RDDs it generated\n+        in the last given duration. DStreams remember RDDs only for a\n+        limited duration of time and releases them for garbage collection.\n+        This method allows the developer to specify how to long to remember\n+        the RDDs (if the developer wishes to query old data outside the\n+        DStream computation).\n+\n+        @param duration: Minimum duration (in seconds) that each DStream\n+                        should remember its RDDs\n+        \"\"\"\n+        self._jssc.remember(self._jduration(duration))\n+\n+    def checkpoint(self, directory):\n+        \"\"\"\n+        Sets the context to periodically checkpoint the DStream operations for master\n+        fault-tolerance. The graph will be checkpointed every batch interval.\n+\n+        @param directory: HDFS-compatible directory where the checkpoint data\n+                         will be reliably stored\n+        \"\"\"\n+        self._jssc.checkpoint(directory)\n+\n+    def socketTextStream(self, hostname, port, storageLevel=StorageLevel.MEMORY_AND_DISK_SER_2):\n+        \"\"\"\n+        Create an input from TCP source hostname:port. Data is received using\n+        a TCP socket and receive byte is interpreted as UTF8 encoded ``\\\\n`` delimited\n+        lines.\n+\n+        @param hostname:      Hostname to connect to for receiving data\n+        @param port:          Port to connect to for receiving data\n+        @param storageLevel:  Storage level to use for storing the received objects\n+        \"\"\"\n+        jlevel = self._sc._getJavaStorageLevel(storageLevel)\n+        return DStream(self._jssc.socketTextStream(hostname, port, jlevel), self,\n+                       UTF8Deserializer())\n+\n+    def textFileStream(self, directory):\n+        \"\"\"\n+        Create an input stream that monitors a Hadoop-compatible file system\n+        for new files and reads them as text files. Files must be wrriten to the\n+        monitored directory by \"moving\" them from another location within the same\n+        file system. File names starting with . are ignored.\n+        \"\"\"\n+        return DStream(self._jssc.textFileStream(directory), self, UTF8Deserializer())\n+\n+    def _check_serialzers(self, rdds):\n+        # make sure they have same serializer\n+        if len(set(rdd._jrdd_deserializer for rdd in rdds)) > 1:\n+            for i in range(len(rdds)):\n+                # reset them to sc.serializer\n+                rdds[i] = rdds[i].map(lambda x: x, preservesPartitioning=True)\n+\n+    def queueStream(self, rdds, oneAtATime=True, default=None):\n+        \"\"\"\n+        Create an input stream from an queue of RDDs or list. In each batch,\n+        it will process either one or all of the RDDs returned by the queue.\n+\n+        NOTE: changes to the queue after the stream is created will not be recognized.\n+\n+        @param rdds:       Queue of RDDs\n+        @param oneAtATime: pick one rdd each time or pick all of them once.\n+        @param default:    The default rdd if no more in rdds\n+        \"\"\"\n+        if default and not isinstance(default, RDD):\n+            default = self._sc.parallelize(default)\n+\n+        if not rdds and default:\n+            rdds = [rdds]\n+\n+        if rdds and not isinstance(rdds[0], RDD):\n+            rdds = [self._sc.parallelize(input) for input in rdds]\n+        self._check_serialzers(rdds)\n+\n+        jrdds = ListConverter().convert([r._jrdd for r in rdds],\n+                                        SparkContext._gateway._gateway_client)\n+        queue = self._jvm.PythonDStream.toRDDQueue(jrdds)\n+        if default:\n+            default = default._reserialize(rdds[0]._jrdd_deserializer)\n+            jdstream = self._jssc.queueStream(queue, oneAtATime, default._jrdd)\n+        else:\n+            jdstream = self._jssc.queueStream(queue, oneAtATime)\n+        return DStream(jdstream, self, rdds[0]._jrdd_deserializer)\n+\n+    def transform(self, dstreams, transformFunc):\n+        \"\"\"\n+        Create a new DStream in which each RDD is generated by applying\n+        a function on RDDs of the DStreams. The order of the JavaRDDs in\n+        the transform function parameter will be the same as the order\n+        of corresponding DStreams in the list.\n+        \"\"\"\n+        jdstreams = ListConverter().convert([d._jdstream for d in dstreams],\n+                                            SparkContext._gateway._gateway_client)\n+        # change the final serializer to sc.serializer\n+        func = TransformFunction(self._sc,\n+                                 lambda t, *rdds: transformFunc(rdds).map(lambda x: x),\n+                                 *[d._jrdd_deserializer for d in dstreams])\n+        jfunc = self._jvm.TransformFunction(func)\n+        jdstream = self._jssc.transform(jdstreams, jfunc)\n+        return DStream(jdstream, self, self._sc.serializer)\n+\n+    def union(self, *dstreams):\n+        \"\"\"\n+        Create a unified DStream from multiple DStreams of the same\n+        type and same slide duration.",
    "line": 312
  }, {
    "author": {
      "login": "davies"
    },
    "body": "done\n",
    "commit": "64561e4e503eafb958f6769383ba3b37edbe5fa2",
    "createdAt": "2014-10-07T23:42:59Z",
    "diffHunk": "@@ -0,0 +1,319 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+import os\n+import sys\n+\n+from py4j.java_collections import ListConverter\n+from py4j.java_gateway import java_import, JavaObject\n+\n+from pyspark import RDD, SparkConf\n+from pyspark.serializers import UTF8Deserializer, CloudPickleSerializer\n+from pyspark.context import SparkContext\n+from pyspark.storagelevel import StorageLevel\n+from pyspark.streaming.dstream import DStream\n+from pyspark.streaming.util import TransformFunction, TransformFunctionSerializer\n+\n+__all__ = [\"StreamingContext\"]\n+\n+\n+def _daemonize_callback_server():\n+    \"\"\"\n+    Hack Py4J to daemonize callback server\n+\n+    The thread of callback server has daemon=False, it will block the driver\n+    from exiting if it's not shutdown. The following code replace `start()`\n+    of CallbackServer with a new version, which set daemon=True for this\n+    thread.\n+\n+    Also, it will update the port number (0) with real port\n+    \"\"\"\n+    # TODO: create a patch for Py4J\n+    import socket\n+    import py4j.java_gateway\n+    logger = py4j.java_gateway.logger\n+    from py4j.java_gateway import Py4JNetworkError\n+    from threading import Thread\n+\n+    def start(self):\n+        \"\"\"Starts the CallbackServer. This method should be called by the\n+        client instead of run().\"\"\"\n+        self.server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+        self.server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR,\n+                                      1)\n+        try:\n+            self.server_socket.bind((self.address, self.port))\n+            if not self.port:\n+                # update port with real port\n+                self.port = self.server_socket.getsockname()[1]\n+        except Exception as e:\n+            msg = 'An error occurred while trying to start the callback server: %s' % e\n+            logger.exception(msg)\n+            raise Py4JNetworkError(msg)\n+\n+        # Maybe thread needs to be cleanup up?\n+        self.thread = Thread(target=self.run)\n+        self.thread.daemon = True\n+        self.thread.start()\n+\n+    py4j.java_gateway.CallbackServer.start = start\n+\n+\n+class StreamingContext(object):\n+    \"\"\"\n+    Main entry point for Spark Streaming functionality. A StreamingContext\n+    represents the connection to a Spark cluster, and can be used to create\n+    L{DStream} various input sources. It can be from an existing L{SparkContext}.\n+    After creating and transforming DStreams, the streaming computation can\n+    be started and stopped using `context.start()` and `context.stop()`,\n+    respectively. `context.awaitTransformation()` allows the current thread\n+    to wait for the termination of the context by `stop()` or by an exception.\n+    \"\"\"\n+    _transformerSerializer = None\n+\n+    def __init__(self, sparkContext, duration=None, jssc=None):\n+        \"\"\"\n+        Create a new StreamingContext.\n+\n+        @param sparkContext: L{SparkContext} object.\n+        @param duration: number of seconds.\n+        \"\"\"\n+\n+        self._sc = sparkContext\n+        self._jvm = self._sc._jvm\n+        self._jssc = jssc or self._initialize_context(self._sc, duration)\n+\n+    def _initialize_context(self, sc, duration):\n+        self._ensure_initialized()\n+        return self._jvm.JavaStreamingContext(sc._jsc, self._jduration(duration))\n+\n+    def _jduration(self, seconds):\n+        \"\"\"\n+        Create Duration object given number of seconds\n+        \"\"\"\n+        return self._jvm.Duration(int(seconds * 1000))\n+\n+    @classmethod\n+    def _ensure_initialized(cls):\n+        SparkContext._ensure_initialized()\n+        gw = SparkContext._gateway\n+\n+        java_import(gw.jvm, \"org.apache.spark.streaming.*\")\n+        java_import(gw.jvm, \"org.apache.spark.streaming.api.java.*\")\n+        java_import(gw.jvm, \"org.apache.spark.streaming.api.python.*\")\n+\n+        # start callback server\n+        # getattr will fallback to JVM, so we cannot test by hasattr()\n+        if \"_callback_server\" not in gw.__dict__:\n+            _daemonize_callback_server()\n+            # use random port\n+            gw._start_callback_server(0)\n+            # gateway with real port\n+            gw._python_proxy_port = gw._callback_server.port\n+            # get the GatewayServer object in JVM by ID\n+            jgws = JavaObject(\"GATEWAY_SERVER\", gw._gateway_client)\n+            # update the port of CallbackClient with real port\n+            gw.jvm.PythonDStream.updatePythonGatewayPort(jgws, gw._python_proxy_port)\n+\n+        # register serializer for TransformFunction\n+        # it happens before creating SparkContext when loading from checkpointing\n+        cls._transformerSerializer = TransformFunctionSerializer(\n+            SparkContext._active_spark_context, CloudPickleSerializer(), gw)\n+\n+    @classmethod\n+    def getOrCreate(cls, path, setupFunc):\n+        \"\"\"\n+        Get the StreamingContext from checkpoint file at `path`, or setup\n+        it by `setupFunc`.\n+\n+        :param path: directory of checkpoint\n+        :param setupFunc: a function used to create StreamingContext and\n+                          setup DStreams.\n+        :return: a StreamingContext\n+        \"\"\"\n+        if not os.path.exists(path) or not os.path.isdir(path) or not os.listdir(path):\n+            ssc = setupFunc()\n+            ssc.checkpoint(path)\n+            return ssc\n+\n+        cls._ensure_initialized()\n+        gw = SparkContext._gateway\n+\n+        try:\n+            jssc = gw.jvm.JavaStreamingContext(path)\n+        except Exception:\n+            print >>sys.stderr, \"failed to load StreamingContext from checkpoint\"\n+            raise\n+\n+        jsc = jssc.sparkContext()\n+        conf = SparkConf(_jconf=jsc.getConf())\n+        sc = SparkContext(conf=conf, gateway=gw, jsc=jsc)\n+        # update ctx in serializer\n+        SparkContext._active_spark_context = sc\n+        cls._transformerSerializer.ctx = sc\n+        return StreamingContext(sc, None, jssc)\n+\n+    @property\n+    def sparkContext(self):\n+        \"\"\"\n+        Return SparkContext which is associated with this StreamingContext.\n+        \"\"\"\n+        return self._sc\n+\n+    def start(self):\n+        \"\"\"\n+        Start the execution of the streams.\n+        \"\"\"\n+        self._jssc.start()\n+\n+    def awaitTermination(self, timeout=None):\n+        \"\"\"\n+        Wait for the execution to stop.\n+        @param timeout: time to wait in seconds\n+        \"\"\"\n+        if timeout is None:\n+            self._jssc.awaitTermination()\n+        else:\n+            self._jssc.awaitTermination(int(timeout * 1000))\n+\n+    def stop(self, stopSparkContext=True, stopGraceFully=False):\n+        \"\"\"\n+        Stop the execution of the streams, with option of ensuring all\n+        received data has been processed.\n+\n+        @param stopSparkContext: Stop the associated SparkContext or not\n+        @param stopGracefully: Stop gracefully by waiting for the processing\n+                              of all received data to be completed\n+        \"\"\"\n+        self._jssc.stop(stopSparkContext, stopGraceFully)\n+        if stopSparkContext:\n+            self._sc.stop()\n+\n+    def remember(self, duration):\n+        \"\"\"\n+        Set each DStreams in this context to remember RDDs it generated\n+        in the last given duration. DStreams remember RDDs only for a\n+        limited duration of time and releases them for garbage collection.\n+        This method allows the developer to specify how to long to remember\n+        the RDDs (if the developer wishes to query old data outside the\n+        DStream computation).\n+\n+        @param duration: Minimum duration (in seconds) that each DStream\n+                        should remember its RDDs\n+        \"\"\"\n+        self._jssc.remember(self._jduration(duration))\n+\n+    def checkpoint(self, directory):\n+        \"\"\"\n+        Sets the context to periodically checkpoint the DStream operations for master\n+        fault-tolerance. The graph will be checkpointed every batch interval.\n+\n+        @param directory: HDFS-compatible directory where the checkpoint data\n+                         will be reliably stored\n+        \"\"\"\n+        self._jssc.checkpoint(directory)\n+\n+    def socketTextStream(self, hostname, port, storageLevel=StorageLevel.MEMORY_AND_DISK_SER_2):\n+        \"\"\"\n+        Create an input from TCP source hostname:port. Data is received using\n+        a TCP socket and receive byte is interpreted as UTF8 encoded ``\\\\n`` delimited\n+        lines.\n+\n+        @param hostname:      Hostname to connect to for receiving data\n+        @param port:          Port to connect to for receiving data\n+        @param storageLevel:  Storage level to use for storing the received objects\n+        \"\"\"\n+        jlevel = self._sc._getJavaStorageLevel(storageLevel)\n+        return DStream(self._jssc.socketTextStream(hostname, port, jlevel), self,\n+                       UTF8Deserializer())\n+\n+    def textFileStream(self, directory):\n+        \"\"\"\n+        Create an input stream that monitors a Hadoop-compatible file system\n+        for new files and reads them as text files. Files must be wrriten to the\n+        monitored directory by \"moving\" them from another location within the same\n+        file system. File names starting with . are ignored.\n+        \"\"\"\n+        return DStream(self._jssc.textFileStream(directory), self, UTF8Deserializer())\n+\n+    def _check_serialzers(self, rdds):\n+        # make sure they have same serializer\n+        if len(set(rdd._jrdd_deserializer for rdd in rdds)) > 1:\n+            for i in range(len(rdds)):\n+                # reset them to sc.serializer\n+                rdds[i] = rdds[i].map(lambda x: x, preservesPartitioning=True)\n+\n+    def queueStream(self, rdds, oneAtATime=True, default=None):\n+        \"\"\"\n+        Create an input stream from an queue of RDDs or list. In each batch,\n+        it will process either one or all of the RDDs returned by the queue.\n+\n+        NOTE: changes to the queue after the stream is created will not be recognized.\n+\n+        @param rdds:       Queue of RDDs\n+        @param oneAtATime: pick one rdd each time or pick all of them once.\n+        @param default:    The default rdd if no more in rdds\n+        \"\"\"\n+        if default and not isinstance(default, RDD):\n+            default = self._sc.parallelize(default)\n+\n+        if not rdds and default:\n+            rdds = [rdds]\n+\n+        if rdds and not isinstance(rdds[0], RDD):\n+            rdds = [self._sc.parallelize(input) for input in rdds]\n+        self._check_serialzers(rdds)\n+\n+        jrdds = ListConverter().convert([r._jrdd for r in rdds],\n+                                        SparkContext._gateway._gateway_client)\n+        queue = self._jvm.PythonDStream.toRDDQueue(jrdds)\n+        if default:\n+            default = default._reserialize(rdds[0]._jrdd_deserializer)\n+            jdstream = self._jssc.queueStream(queue, oneAtATime, default._jrdd)\n+        else:\n+            jdstream = self._jssc.queueStream(queue, oneAtATime)\n+        return DStream(jdstream, self, rdds[0]._jrdd_deserializer)\n+\n+    def transform(self, dstreams, transformFunc):\n+        \"\"\"\n+        Create a new DStream in which each RDD is generated by applying\n+        a function on RDDs of the DStreams. The order of the JavaRDDs in\n+        the transform function parameter will be the same as the order\n+        of corresponding DStreams in the list.\n+        \"\"\"\n+        jdstreams = ListConverter().convert([d._jdstream for d in dstreams],\n+                                            SparkContext._gateway._gateway_client)\n+        # change the final serializer to sc.serializer\n+        func = TransformFunction(self._sc,\n+                                 lambda t, *rdds: transformFunc(rdds).map(lambda x: x),\n+                                 *[d._jrdd_deserializer for d in dstreams])\n+        jfunc = self._jvm.TransformFunction(func)\n+        jdstream = self._jssc.transform(jdstreams, jfunc)\n+        return DStream(jdstream, self, self._sc.serializer)\n+\n+    def union(self, *dstreams):\n+        \"\"\"\n+        Create a unified DStream from multiple DStreams of the same\n+        type and same slide duration.",
    "line": 312
  }],
  "prId": 2538
}]