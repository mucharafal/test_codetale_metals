[{
  "comments": [{
    "author": {
      "login": "mariusvniekerk"
    },
    "body": "assume this sign is just flipped\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-27T12:14:39Z",
    "diffHunk": "@@ -0,0 +1,169 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+exec(open('pyspark/version.py').read())\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):\n+        print(\"Python versions prior to 2.7 are not supported.\", file=sys.stderr)\n+        exit(-1)\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists %s\" % TEMP_PATH, file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    if (in_spark):\n+        # Construct the symlink farm\n+        if getattr(os, \"symlink\", None) is not None:\n+            os.symlink(JARS_PATH, JARS_TARGET)\n+            os.symlink(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            os.symlink(EXAMPLES_PATH, EXAMPLES_TARGET)\n+        else:\n+            # For windows fall back to the slower copytree\n+            copytree(JARS_PATH, JARS_TARGET)\n+            copytree(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            copytree(EXAMPLES_PATH, EXAMPLES_TARGET)\n+    else:\n+        # We copy the shell script to be under pyspark/python/pyspark so that the launcher scripts\n+        # find it where expected. The rest of the files aren't copied because they are accessed\n+        # using Python imports instead which will be resolved correctly.\n+        try:\n+            os.makedirs(\"pyspark/python/pyspark\")\n+        except OSError:\n+            # Don't worry if the directory already exists.\n+            True\n+        copyfile(\"pyspark/shell.py\", \"pyspark/python/pyspark/shell.py\")\n+\n+    if not os.path.isdir(SCRIPTS_TARGET):\n+        print(\"You must first create a source dist and install that source dist.\", file=sys.stderr)\n+        exit(-1)\n+\n+    # Scripts directive requires a list of each script path and does not take wild cards.\n+    script_names = os.listdir(SCRIPTS_TARGET)\n+    scripts = map(lambda script: SCRIPTS_TARGET + \"/\" + script, script_names)\n+    # We add find_spark_home.py to the bin directory we install so that pip installed PySpark\n+    # will search for SPARK_HOME with Python.\n+    scripts.append(\"pyspark/find_spark_home.py\")\n+\n+    # Parse the README markdown file into rst for PyPI\n+    long_description = \"!!!!! missing pandoc do not upload to PyPI !!!!\"\n+    try:\n+        import pypandoc\n+        long_description = pypandoc.convert('README.md', 'rst')\n+    except ImportError:\n+        print(\"Could not import pypandoc - required to package PySpark\", file=sys.stderr)\n+\n+    setup(\n+        name='pyspark',\n+        version=VERSION,\n+        description='Apache Spark Python API',\n+        long_description=long_description,\n+        author='Spark Developers',\n+        author_email='dev@spark.apache.org',\n+        url='https://github.com/apache/spark/tree/master/python',\n+        packages=['pyspark',\n+                  'pyspark.mllib',\n+                  'pyspark.ml',\n+                  'pyspark.sql',\n+                  'pyspark.streaming',\n+                  'pyspark.bin',\n+                  'pyspark.jars',\n+                  'pyspark.python.lib',\n+                  'pyspark.examples.src.main.python'],\n+        include_package_data=True,\n+        package_dir={\n+            'pyspark.jars': 'deps/jars',\n+            'pyspark.bin': 'deps/bin',\n+            'pyspark.python.lib': 'lib',\n+            'pyspark.examples.src.main.python': 'deps/examples',\n+        },\n+        package_data={\n+            'pyspark.jars': ['*.jar'],\n+            'pyspark.bin': ['*'],\n+            'pyspark.python.lib': ['*.zip'],\n+            'pyspark.examples.src.main.python': ['*.py', '*/*.py']},\n+        scripts=scripts,\n+        license='http://www.apache.org/licenses/LICENSE-2.0',\n+        install_requires=['py4j==0.10.4'],\n+        setup_requires=['pypandoc'],\n+        extras_require={\n+            'ml': ['numpy>=1.7'],\n+            'mllib': ['numpy<=1.7'],"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "ah yes, good catch.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-27T12:34:53Z",
    "diffHunk": "@@ -0,0 +1,169 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+exec(open('pyspark/version.py').read())\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):\n+        print(\"Python versions prior to 2.7 are not supported.\", file=sys.stderr)\n+        exit(-1)\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists %s\" % TEMP_PATH, file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    if (in_spark):\n+        # Construct the symlink farm\n+        if getattr(os, \"symlink\", None) is not None:\n+            os.symlink(JARS_PATH, JARS_TARGET)\n+            os.symlink(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            os.symlink(EXAMPLES_PATH, EXAMPLES_TARGET)\n+        else:\n+            # For windows fall back to the slower copytree\n+            copytree(JARS_PATH, JARS_TARGET)\n+            copytree(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            copytree(EXAMPLES_PATH, EXAMPLES_TARGET)\n+    else:\n+        # We copy the shell script to be under pyspark/python/pyspark so that the launcher scripts\n+        # find it where expected. The rest of the files aren't copied because they are accessed\n+        # using Python imports instead which will be resolved correctly.\n+        try:\n+            os.makedirs(\"pyspark/python/pyspark\")\n+        except OSError:\n+            # Don't worry if the directory already exists.\n+            True\n+        copyfile(\"pyspark/shell.py\", \"pyspark/python/pyspark/shell.py\")\n+\n+    if not os.path.isdir(SCRIPTS_TARGET):\n+        print(\"You must first create a source dist and install that source dist.\", file=sys.stderr)\n+        exit(-1)\n+\n+    # Scripts directive requires a list of each script path and does not take wild cards.\n+    script_names = os.listdir(SCRIPTS_TARGET)\n+    scripts = map(lambda script: SCRIPTS_TARGET + \"/\" + script, script_names)\n+    # We add find_spark_home.py to the bin directory we install so that pip installed PySpark\n+    # will search for SPARK_HOME with Python.\n+    scripts.append(\"pyspark/find_spark_home.py\")\n+\n+    # Parse the README markdown file into rst for PyPI\n+    long_description = \"!!!!! missing pandoc do not upload to PyPI !!!!\"\n+    try:\n+        import pypandoc\n+        long_description = pypandoc.convert('README.md', 'rst')\n+    except ImportError:\n+        print(\"Could not import pypandoc - required to package PySpark\", file=sys.stderr)\n+\n+    setup(\n+        name='pyspark',\n+        version=VERSION,\n+        description='Apache Spark Python API',\n+        long_description=long_description,\n+        author='Spark Developers',\n+        author_email='dev@spark.apache.org',\n+        url='https://github.com/apache/spark/tree/master/python',\n+        packages=['pyspark',\n+                  'pyspark.mllib',\n+                  'pyspark.ml',\n+                  'pyspark.sql',\n+                  'pyspark.streaming',\n+                  'pyspark.bin',\n+                  'pyspark.jars',\n+                  'pyspark.python.lib',\n+                  'pyspark.examples.src.main.python'],\n+        include_package_data=True,\n+        package_dir={\n+            'pyspark.jars': 'deps/jars',\n+            'pyspark.bin': 'deps/bin',\n+            'pyspark.python.lib': 'lib',\n+            'pyspark.examples.src.main.python': 'deps/examples',\n+        },\n+        package_data={\n+            'pyspark.jars': ['*.jar'],\n+            'pyspark.bin': ['*'],\n+            'pyspark.python.lib': ['*.zip'],\n+            'pyspark.examples.src.main.python': ['*.py', '*/*.py']},\n+        scripts=scripts,\n+        license='http://www.apache.org/licenses/LICENSE-2.0',\n+        install_requires=['py4j==0.10.4'],\n+        setup_requires=['pypandoc'],\n+        extras_require={\n+            'ml': ['numpy>=1.7'],\n+            'mllib': ['numpy<=1.7'],"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "What's the purpose of these symlinks?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-27T14:59:24Z",
    "diffHunk": "@@ -0,0 +1,169 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+exec(open('pyspark/version.py').read())\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):\n+        print(\"Python versions prior to 2.7 are not supported.\", file=sys.stderr)\n+        exit(-1)\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists %s\" % TEMP_PATH, file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    if (in_spark):\n+        # Construct the symlink farm"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "We need the symlinks so that we can reference these files, it isn't possible to refer to the directory above. I'll add a comment about this.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-27T15:13:34Z",
    "diffHunk": "@@ -0,0 +1,169 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+exec(open('pyspark/version.py').read())\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):\n+        print(\"Python versions prior to 2.7 are not supported.\", file=sys.stderr)\n+        exit(-1)\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists %s\" % TEMP_PATH, file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    if (in_spark):\n+        # Construct the symlink farm"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "If we are not in spark, what will happen do `python setup.py sdist`? Looks like we need to be in spark in order to packaging. Do we need to show some error message for that?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-28T12:57:05Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+exec(open('pyspark/version.py').read())\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):\n+        print(\"Python versions prior to 2.7 are not supported.\", file=sys.stderr)\n+        exit(-1)\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or",
    "line": 81
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "Why and how would we expect that to happen?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-28T14:15:34Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+exec(open('pyspark/version.py').read())\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):\n+        print(\"Python versions prior to 2.7 are not supported.\", file=sys.stderr)\n+        exit(-1)\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or",
    "line": 81
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "So what is the difference between in_spark is true and false?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-28T14:19:28Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+exec(open('pyspark/version.py').read())\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):\n+        print(\"Python versions prior to 2.7 are not supported.\", file=sys.stderr)\n+        exit(-1)\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or",
    "line": 81
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "We would expect in_spark to be false during the installation rather than during packaging.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-28T23:10:10Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+exec(open('pyspark/version.py').read())\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):\n+        print(\"Python versions prior to 2.7 are not supported.\", file=sys.stderr)\n+        exit(-1)\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or",
    "line": 81
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "I've added an error message for if someone runs sdist from an unpexpected location (ends up showing up earlier during version check).\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-31T01:12:10Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+exec(open('pyspark/version.py').read())\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):\n+        print(\"Python versions prior to 2.7 are not supported.\", file=sys.stderr)\n+        exit(-1)\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or",
    "line": 81
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "OK. Looks good.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-31T01:47:07Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+exec(open('pyspark/version.py').read())\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):\n+        print(\"Python versions prior to 2.7 are not supported.\", file=sys.stderr)\n+        exit(-1)\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or",
    "line": 81
  }, {
    "author": {
      "login": "felixcheung"
    },
    "body": "This seems rather fragile to depend on source file path?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-02T04:46:26Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+exec(open('pyspark/version.py').read())\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):\n+        print(\"Python versions prior to 2.7 are not supported.\", file=sys.stderr)\n+        exit(-1)\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or",
    "line": 81
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "I mean we haven't moved SparkContext.scala and it's the clearest thing which will only exist in Spark. I'm open to checking for something else if people have a suggestion.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-02T04:58:37Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+exec(open('pyspark/version.py').read())\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):\n+        print(\"Python versions prior to 2.7 are not supported.\", file=sys.stderr)\n+        exit(-1)\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or",
    "line": 81
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Is `setuptools` installed on every machine? I checked on ubuntu, installed python doesn't have `setuptools` module. Is it a problem?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-28T13:06:05Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages",
    "line": 23
  }, {
    "author": {
      "login": "nchammas"
    },
    "body": "pip bundles setuptools, so if you have pip you have setuptools. Specifically, I think if this script is being invoked because the user ran pip, this will work. \n\nIf it is invoked as `python setup.py`, though, it is possible for this to fail because the user doesn't have setuptools. \n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-28T13:36:59Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages",
    "line": 23
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "In `dev/make-distribution.sh`, we do `python setup.py sdist`. As this change lists the packaging as part of making distribution. I think it would possibly fail the procedure.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-28T13:54:05Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages",
    "line": 23
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "Interesting, we can add pip to the requirements.txt file under dev/ so people can install it along with our other Python packages required for the dev & packaging tools.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-28T14:10:17Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages",
    "line": 23
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "Also I've tested install on a centos system without pip - so install side seems reasonable (not having setuptools would block many Python packages from being able to be installed).\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-10-28T14:18:28Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages",
    "line": 23
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "I don't think that the `setuptools` requirement is a big deal; my impression is that there are many Python packages whose installation will fail if `setuptools` is missing and it's not hard for users to figure out what went wrong and fix this.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-12T00:20:35Z",
    "diffHunk": "@@ -0,0 +1,170 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages",
    "line": 23
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Indentation :).\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-01T14:10:57Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging you must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):\n+        print(\"Python versions prior to 2.7 are not supported.\", file=sys.stderr)\n+        exit(-1)"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "nit: move this to the top of the file?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-02T04:44:04Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging you must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "I can move it up, not quite to the top (need sys import of course first)\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-02T04:57:46Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging you must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "felixcheung"
    },
    "body": "is there a way to detect \"new packages\" that we have forgotten to update this list with?\nOr to generate this list automatically?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-02T04:47:23Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging you must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists %s\" % TEMP_PATH, file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    if (in_spark):\n+        # Construct the symlink farm - this is necessary since we can't refer to the path above the\n+        # package root and we need to copy the jars and scripts which are up above the python root.\n+        if getattr(os, \"symlink\", None) is not None:\n+            os.symlink(JARS_PATH, JARS_TARGET)\n+            os.symlink(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            os.symlink(EXAMPLES_PATH, EXAMPLES_TARGET)\n+        else:\n+            # For windows fall back to the slower copytree\n+            copytree(JARS_PATH, JARS_TARGET)\n+            copytree(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            copytree(EXAMPLES_PATH, EXAMPLES_TARGET)\n+    else:\n+        # If we are not inside of SPARK_HOME verify we have the required symlink farm\n+        if not os.path.exists(JARS_TARGET):\n+            print(\"To build packaging must be in the python directory under the SPARK_HOME.\",\n+                  file=sys.stderr)\n+        # We copy the shell script to be under pyspark/python/pyspark so that the launcher scripts\n+        # find it where expected. The rest of the files aren't copied because they are accessed\n+        # using Python imports instead which will be resolved correctly.\n+        try:\n+            os.makedirs(\"pyspark/python/pyspark\")\n+        except OSError:\n+            # Don't worry if the directory already exists.\n+            True\n+        copyfile(\"pyspark/shell.py\", \"pyspark/python/pyspark/shell.py\")\n+\n+    if not os.path.isdir(SCRIPTS_TARGET):\n+        print(\"You must first create a source dist and install that source dist.\", file=sys.stderr)\n+        exit(-1)\n+\n+    # Scripts directive requires a list of each script path and does not take wild cards.\n+    script_names = os.listdir(SCRIPTS_TARGET)\n+    scripts = map(lambda script: os.path.join(SCRIPTS_TARGET, script), script_names)\n+    # We add find_spark_home.py to the bin directory we install so that pip installed PySpark\n+    # will search for SPARK_HOME with Python.\n+    scripts.append(\"pyspark/find_spark_home.py\")\n+\n+    # Parse the README markdown file into rst for PyPI\n+    long_description = \"!!!!! missing pandoc do not upload to PyPI !!!!\"\n+    try:\n+        import pypandoc\n+        long_description = pypandoc.convert('README.md', 'rst')\n+    except ImportError:\n+        print(\"Could not import pypandoc - required to package PySpark\", file=sys.stderr)\n+\n+    setup(\n+        name='pyspark',\n+        version=VERSION,\n+        description='Apache Spark Python API',\n+        long_description=long_description,\n+        author='Spark Developers',\n+        author_email='dev@spark.apache.org',\n+        url='https://github.com/apache/spark/tree/master/python',\n+        packages=['pyspark',\n+                  'pyspark.mllib',\n+                  'pyspark.ml',\n+                  'pyspark.sql',\n+                  'pyspark.streaming',\n+                  'pyspark.bin',\n+                  'pyspark.jars',\n+                  'pyspark.python.lib',\n+                  'pyspark.examples.src.main.python'],"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "I'd rather not automate this - I think its more likely to go wrong than right. We also need some custom magic for the Python libs and the examples - so it would be partially manual anyways.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-02T05:00:48Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging you must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists %s\" % TEMP_PATH, file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    if (in_spark):\n+        # Construct the symlink farm - this is necessary since we can't refer to the path above the\n+        # package root and we need to copy the jars and scripts which are up above the python root.\n+        if getattr(os, \"symlink\", None) is not None:\n+            os.symlink(JARS_PATH, JARS_TARGET)\n+            os.symlink(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            os.symlink(EXAMPLES_PATH, EXAMPLES_TARGET)\n+        else:\n+            # For windows fall back to the slower copytree\n+            copytree(JARS_PATH, JARS_TARGET)\n+            copytree(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            copytree(EXAMPLES_PATH, EXAMPLES_TARGET)\n+    else:\n+        # If we are not inside of SPARK_HOME verify we have the required symlink farm\n+        if not os.path.exists(JARS_TARGET):\n+            print(\"To build packaging must be in the python directory under the SPARK_HOME.\",\n+                  file=sys.stderr)\n+        # We copy the shell script to be under pyspark/python/pyspark so that the launcher scripts\n+        # find it where expected. The rest of the files aren't copied because they are accessed\n+        # using Python imports instead which will be resolved correctly.\n+        try:\n+            os.makedirs(\"pyspark/python/pyspark\")\n+        except OSError:\n+            # Don't worry if the directory already exists.\n+            True\n+        copyfile(\"pyspark/shell.py\", \"pyspark/python/pyspark/shell.py\")\n+\n+    if not os.path.isdir(SCRIPTS_TARGET):\n+        print(\"You must first create a source dist and install that source dist.\", file=sys.stderr)\n+        exit(-1)\n+\n+    # Scripts directive requires a list of each script path and does not take wild cards.\n+    script_names = os.listdir(SCRIPTS_TARGET)\n+    scripts = map(lambda script: os.path.join(SCRIPTS_TARGET, script), script_names)\n+    # We add find_spark_home.py to the bin directory we install so that pip installed PySpark\n+    # will search for SPARK_HOME with Python.\n+    scripts.append(\"pyspark/find_spark_home.py\")\n+\n+    # Parse the README markdown file into rst for PyPI\n+    long_description = \"!!!!! missing pandoc do not upload to PyPI !!!!\"\n+    try:\n+        import pypandoc\n+        long_description = pypandoc.convert('README.md', 'rst')\n+    except ImportError:\n+        print(\"Could not import pypandoc - required to package PySpark\", file=sys.stderr)\n+\n+    setup(\n+        name='pyspark',\n+        version=VERSION,\n+        description='Apache Spark Python API',\n+        long_description=long_description,\n+        author='Spark Developers',\n+        author_email='dev@spark.apache.org',\n+        url='https://github.com/apache/spark/tree/master/python',\n+        packages=['pyspark',\n+                  'pyspark.mllib',\n+                  'pyspark.ml',\n+                  'pyspark.sql',\n+                  'pyspark.streaming',\n+                  'pyspark.bin',\n+                  'pyspark.jars',\n+                  'pyspark.python.lib',\n+                  'pyspark.examples.src.main.python'],"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "This line causes error in python3.\n\n```\nTraceback (most recent call last):\n  File \"setup.py\", line 105, in <module>\n    scripts.append(\"pyspark/find_spark_home.py\")\n    AttributeError: 'map' object has no attribute 'append'\n```\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-02T09:03:03Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging you must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists %s\" % TEMP_PATH, file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    if (in_spark):\n+        # Construct the symlink farm - this is necessary since we can't refer to the path above the\n+        # package root and we need to copy the jars and scripts which are up above the python root.\n+        if getattr(os, \"symlink\", None) is not None:\n+            os.symlink(JARS_PATH, JARS_TARGET)\n+            os.symlink(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            os.symlink(EXAMPLES_PATH, EXAMPLES_TARGET)\n+        else:\n+            # For windows fall back to the slower copytree\n+            copytree(JARS_PATH, JARS_TARGET)\n+            copytree(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            copytree(EXAMPLES_PATH, EXAMPLES_TARGET)\n+    else:\n+        # If we are not inside of SPARK_HOME verify we have the required symlink farm\n+        if not os.path.exists(JARS_TARGET):\n+            print(\"To build packaging must be in the python directory under the SPARK_HOME.\",\n+                  file=sys.stderr)\n+        # We copy the shell script to be under pyspark/python/pyspark so that the launcher scripts\n+        # find it where expected. The rest of the files aren't copied because they are accessed\n+        # using Python imports instead which will be resolved correctly.\n+        try:\n+            os.makedirs(\"pyspark/python/pyspark\")\n+        except OSError:\n+            # Don't worry if the directory already exists.\n+            True\n+        copyfile(\"pyspark/shell.py\", \"pyspark/python/pyspark/shell.py\")\n+\n+    if not os.path.isdir(SCRIPTS_TARGET):\n+        print(\"You must first create a source dist and install that source dist.\", file=sys.stderr)\n+        exit(-1)\n+\n+    # Scripts directive requires a list of each script path and does not take wild cards.\n+    script_names = os.listdir(SCRIPTS_TARGET)\n+    scripts = map(lambda script: os.path.join(SCRIPTS_TARGET, script), script_names)\n+    # We add find_spark_home.py to the bin directory we install so that pip installed PySpark\n+    # will search for SPARK_HOME with Python.\n+    scripts.append(\"pyspark/find_spark_home.py\")"
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "Ah good point, I'll fix this and check packaging under Python 3 today.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-02T15:07:28Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging you must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists %s\" % TEMP_PATH, file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    if (in_spark):\n+        # Construct the symlink farm - this is necessary since we can't refer to the path above the\n+        # package root and we need to copy the jars and scripts which are up above the python root.\n+        if getattr(os, \"symlink\", None) is not None:\n+            os.symlink(JARS_PATH, JARS_TARGET)\n+            os.symlink(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            os.symlink(EXAMPLES_PATH, EXAMPLES_TARGET)\n+        else:\n+            # For windows fall back to the slower copytree\n+            copytree(JARS_PATH, JARS_TARGET)\n+            copytree(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            copytree(EXAMPLES_PATH, EXAMPLES_TARGET)\n+    else:\n+        # If we are not inside of SPARK_HOME verify we have the required symlink farm\n+        if not os.path.exists(JARS_TARGET):\n+            print(\"To build packaging must be in the python directory under the SPARK_HOME.\",\n+                  file=sys.stderr)\n+        # We copy the shell script to be under pyspark/python/pyspark so that the launcher scripts\n+        # find it where expected. The rest of the files aren't copied because they are accessed\n+        # using Python imports instead which will be resolved correctly.\n+        try:\n+            os.makedirs(\"pyspark/python/pyspark\")\n+        except OSError:\n+            # Don't worry if the directory already exists.\n+            True\n+        copyfile(\"pyspark/shell.py\", \"pyspark/python/pyspark/shell.py\")\n+\n+    if not os.path.isdir(SCRIPTS_TARGET):\n+        print(\"You must first create a source dist and install that source dist.\", file=sys.stderr)\n+        exit(-1)\n+\n+    # Scripts directive requires a list of each script path and does not take wild cards.\n+    script_names = os.listdir(SCRIPTS_TARGET)\n+    scripts = map(lambda script: os.path.join(SCRIPTS_TARGET, script), script_names)\n+    # We add find_spark_home.py to the bin directory we install so that pip installed PySpark\n+    # will search for SPARK_HOME with Python.\n+    scripts.append(\"pyspark/find_spark_home.py\")"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "Seems like there is a missing sentence break somewhere here. :)\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-05T16:00:04Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging you must be in Spark's python dir.\","
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "If this is required, should we not `exit()` here?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-05T16:01:15Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging you must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists %s\" % TEMP_PATH, file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    if (in_spark):\n+        # Construct the symlink farm - this is necessary since we can't refer to the path above the\n+        # package root and we need to copy the jars and scripts which are up above the python root.\n+        if getattr(os, \"symlink\", None) is not None:\n+            os.symlink(JARS_PATH, JARS_TARGET)\n+            os.symlink(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            os.symlink(EXAMPLES_PATH, EXAMPLES_TARGET)\n+        else:\n+            # For windows fall back to the slower copytree\n+            copytree(JARS_PATH, JARS_TARGET)\n+            copytree(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            copytree(EXAMPLES_PATH, EXAMPLES_TARGET)\n+    else:\n+        # If we are not inside of SPARK_HOME verify we have the required symlink farm\n+        if not os.path.exists(JARS_TARGET):\n+            print(\"To build packaging must be in the python directory under the SPARK_HOME.\",\n+                  file=sys.stderr)\n+        # We copy the shell script to be under pyspark/python/pyspark so that the launcher scripts\n+        # find it where expected. The rest of the files aren't copied because they are accessed\n+        # using Python imports instead which will be resolved correctly.\n+        try:\n+            os.makedirs(\"pyspark/python/pyspark\")\n+        except OSError:\n+            # Don't worry if the directory already exists.\n+            True\n+        copyfile(\"pyspark/shell.py\", \"pyspark/python/pyspark/shell.py\")\n+\n+    if not os.path.isdir(SCRIPTS_TARGET):\n+        print(\"You must first create a source dist and install that source dist.\", file=sys.stderr)\n+        exit(-1)\n+\n+    # Scripts directive requires a list of each script path and does not take wild cards.\n+    script_names = os.listdir(SCRIPTS_TARGET)\n+    scripts = list(map(lambda script: os.path.join(SCRIPTS_TARGET, script), script_names))\n+    # We add find_spark_home.py to the bin directory we install so that pip installed PySpark\n+    # will search for SPARK_HOME with Python.\n+    scripts.append(\"pyspark/find_spark_home.py\")\n+\n+    # Parse the README markdown file into rst for PyPI\n+    long_description = \"!!!!! missing pandoc do not upload to PyPI !!!!\"\n+    try:\n+        import pypandoc\n+        long_description = pypandoc.convert('README.md', 'rst')\n+    except ImportError:\n+        print(\"Could not import pypandoc - required to package PySpark\", file=sys.stderr)",
    "line": 145
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "Maybe? You can still build a perfectly installable pip package but the long_description won't be fully present. Generally most setup scripts seem to print an error message but continue on - but I'm open to hard exiting if we think that would be better (depends on if we expect users to build their own pip install packages from source or not).\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-05T22:55:58Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging you must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists %s\" % TEMP_PATH, file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    if (in_spark):\n+        # Construct the symlink farm - this is necessary since we can't refer to the path above the\n+        # package root and we need to copy the jars and scripts which are up above the python root.\n+        if getattr(os, \"symlink\", None) is not None:\n+            os.symlink(JARS_PATH, JARS_TARGET)\n+            os.symlink(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            os.symlink(EXAMPLES_PATH, EXAMPLES_TARGET)\n+        else:\n+            # For windows fall back to the slower copytree\n+            copytree(JARS_PATH, JARS_TARGET)\n+            copytree(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            copytree(EXAMPLES_PATH, EXAMPLES_TARGET)\n+    else:\n+        # If we are not inside of SPARK_HOME verify we have the required symlink farm\n+        if not os.path.exists(JARS_TARGET):\n+            print(\"To build packaging must be in the python directory under the SPARK_HOME.\",\n+                  file=sys.stderr)\n+        # We copy the shell script to be under pyspark/python/pyspark so that the launcher scripts\n+        # find it where expected. The rest of the files aren't copied because they are accessed\n+        # using Python imports instead which will be resolved correctly.\n+        try:\n+            os.makedirs(\"pyspark/python/pyspark\")\n+        except OSError:\n+            # Don't worry if the directory already exists.\n+            True\n+        copyfile(\"pyspark/shell.py\", \"pyspark/python/pyspark/shell.py\")\n+\n+    if not os.path.isdir(SCRIPTS_TARGET):\n+        print(\"You must first create a source dist and install that source dist.\", file=sys.stderr)\n+        exit(-1)\n+\n+    # Scripts directive requires a list of each script path and does not take wild cards.\n+    script_names = os.listdir(SCRIPTS_TARGET)\n+    scripts = list(map(lambda script: os.path.join(SCRIPTS_TARGET, script), script_names))\n+    # We add find_spark_home.py to the bin directory we install so that pip installed PySpark\n+    # will search for SPARK_HOME with Python.\n+    scripts.append(\"pyspark/find_spark_home.py\")\n+\n+    # Parse the README markdown file into rst for PyPI\n+    long_description = \"!!!!! missing pandoc do not upload to PyPI !!!!\"\n+    try:\n+        import pypandoc\n+        long_description = pypandoc.convert('README.md', 'rst')\n+    except ImportError:\n+        print(\"Could not import pypandoc - required to package PySpark\", file=sys.stderr)",
    "line": 145
  }, {
    "author": {
      "login": "JoshRosen"
    },
    "body": "Pandoc is kind of painful to install on some OS's since it may have to compile from source which can take _forever_; this doesn't seem like a huge deal to me, so I'm fine wth this being best-effort. We should definitely make sure that `pypandoc` and `pandoc` are installed in Jenkins for release-packaging, though (although I think I can just containerize that packaging within Jenkins, but that's another story).\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-12T00:25:57Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging you must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists %s\" % TEMP_PATH, file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    if (in_spark):\n+        # Construct the symlink farm - this is necessary since we can't refer to the path above the\n+        # package root and we need to copy the jars and scripts which are up above the python root.\n+        if getattr(os, \"symlink\", None) is not None:\n+            os.symlink(JARS_PATH, JARS_TARGET)\n+            os.symlink(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            os.symlink(EXAMPLES_PATH, EXAMPLES_TARGET)\n+        else:\n+            # For windows fall back to the slower copytree\n+            copytree(JARS_PATH, JARS_TARGET)\n+            copytree(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            copytree(EXAMPLES_PATH, EXAMPLES_TARGET)\n+    else:\n+        # If we are not inside of SPARK_HOME verify we have the required symlink farm\n+        if not os.path.exists(JARS_TARGET):\n+            print(\"To build packaging must be in the python directory under the SPARK_HOME.\",\n+                  file=sys.stderr)\n+        # We copy the shell script to be under pyspark/python/pyspark so that the launcher scripts\n+        # find it where expected. The rest of the files aren't copied because they are accessed\n+        # using Python imports instead which will be resolved correctly.\n+        try:\n+            os.makedirs(\"pyspark/python/pyspark\")\n+        except OSError:\n+            # Don't worry if the directory already exists.\n+            True\n+        copyfile(\"pyspark/shell.py\", \"pyspark/python/pyspark/shell.py\")\n+\n+    if not os.path.isdir(SCRIPTS_TARGET):\n+        print(\"You must first create a source dist and install that source dist.\", file=sys.stderr)\n+        exit(-1)\n+\n+    # Scripts directive requires a list of each script path and does not take wild cards.\n+    script_names = os.listdir(SCRIPTS_TARGET)\n+    scripts = list(map(lambda script: os.path.join(SCRIPTS_TARGET, script), script_names))\n+    # We add find_spark_home.py to the bin directory we install so that pip installed PySpark\n+    # will search for SPARK_HOME with Python.\n+    scripts.append(\"pyspark/find_spark_home.py\")\n+\n+    # Parse the README markdown file into rst for PyPI\n+    long_description = \"!!!!! missing pandoc do not upload to PyPI !!!!\"\n+    try:\n+        import pypandoc\n+        long_description = pypandoc.convert('README.md', 'rst')\n+    except ImportError:\n+        print(\"Could not import pypandoc - required to package PySpark\", file=sys.stderr)",
    "line": 145
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "nchammas"
    },
    "body": "We don't support Python 3.0 - 3.3. [It's 2.7 or 3.4+.](http://spark.apache.org/docs/latest/#downloading) (This is common. Very, very few people support 3.0 - 3.3.)\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-05T16:04:52Z",
    "diffHunk": "@@ -0,0 +1,180 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging you must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = \"%s/assembly/target/scala-2.11/jars/\" % SPARK_HOME\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = \"%s/jars/\" % SPARK_HOME\n+\n+EXAMPLES_PATH = \"%s/examples/src/main/python\" % SPARK_HOME\n+SCRIPTS_PATH = \"%s/bin\" % SPARK_HOME\n+SCRIPTS_TARGET = \"%s/bin\" % TEMP_PATH\n+JARS_TARGET = \"%s/jars\" % TEMP_PATH\n+EXAMPLES_TARGET = \"%s/examples\" % TEMP_PATH\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists %s\" % TEMP_PATH, file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    if (in_spark):\n+        # Construct the symlink farm - this is necessary since we can't refer to the path above the\n+        # package root and we need to copy the jars and scripts which are up above the python root.\n+        if getattr(os, \"symlink\", None) is not None:\n+            os.symlink(JARS_PATH, JARS_TARGET)\n+            os.symlink(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            os.symlink(EXAMPLES_PATH, EXAMPLES_TARGET)\n+        else:\n+            # For windows fall back to the slower copytree\n+            copytree(JARS_PATH, JARS_TARGET)\n+            copytree(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            copytree(EXAMPLES_PATH, EXAMPLES_TARGET)\n+    else:\n+        # If we are not inside of SPARK_HOME verify we have the required symlink farm\n+        if not os.path.exists(JARS_TARGET):\n+            print(\"To build packaging must be in the python directory under the SPARK_HOME.\",\n+                  file=sys.stderr)\n+        # We copy the shell script to be under pyspark/python/pyspark so that the launcher scripts\n+        # find it where expected. The rest of the files aren't copied because they are accessed\n+        # using Python imports instead which will be resolved correctly.\n+        try:\n+            os.makedirs(\"pyspark/python/pyspark\")\n+        except OSError:\n+            # Don't worry if the directory already exists.\n+            True\n+        copyfile(\"pyspark/shell.py\", \"pyspark/python/pyspark/shell.py\")\n+\n+    if not os.path.isdir(SCRIPTS_TARGET):\n+        print(\"You must first create a source dist and install that source dist.\", file=sys.stderr)\n+        exit(-1)\n+\n+    # Scripts directive requires a list of each script path and does not take wild cards.\n+    script_names = os.listdir(SCRIPTS_TARGET)\n+    scripts = list(map(lambda script: os.path.join(SCRIPTS_TARGET, script), script_names))\n+    # We add find_spark_home.py to the bin directory we install so that pip installed PySpark\n+    # will search for SPARK_HOME with Python.\n+    scripts.append(\"pyspark/find_spark_home.py\")\n+\n+    # Parse the README markdown file into rst for PyPI\n+    long_description = \"!!!!! missing pandoc do not upload to PyPI !!!!\"\n+    try:\n+        import pypandoc\n+        long_description = pypandoc.convert('README.md', 'rst')\n+    except ImportError:\n+        print(\"Could not import pypandoc - required to package PySpark\", file=sys.stderr)\n+\n+    setup(\n+        name='pyspark',\n+        version=VERSION,\n+        description='Apache Spark Python API',\n+        long_description=long_description,\n+        author='Spark Developers',\n+        author_email='dev@spark.apache.org',\n+        url='https://github.com/apache/spark/tree/master/python',\n+        packages=['pyspark',\n+                  'pyspark.mllib',\n+                  'pyspark.ml',\n+                  'pyspark.sql',\n+                  'pyspark.streaming',\n+                  'pyspark.bin',\n+                  'pyspark.jars',\n+                  'pyspark.python.lib',\n+                  'pyspark.examples.src.main.python'],\n+        include_package_data=True,\n+        package_dir={\n+            'pyspark.jars': 'deps/jars',\n+            'pyspark.bin': 'deps/bin',\n+            'pyspark.python.lib': 'lib',\n+            'pyspark.examples.src.main.python': 'deps/examples',\n+        },\n+        package_data={\n+            'pyspark.jars': ['*.jar'],\n+            'pyspark.bin': ['*'],\n+            'pyspark.python.lib': ['*.zip'],\n+            'pyspark.examples.src.main.python': ['*.py', '*/*.py']},\n+        scripts=scripts,\n+        license='http://www.apache.org/licenses/LICENSE-2.0',\n+        install_requires=['py4j==0.10.4'],\n+        setup_requires=['pypandoc'],\n+        extras_require={\n+            'ml': ['numpy>=1.7'],\n+            'mllib': ['numpy>=1.7'],\n+            'sql': ['pandas']\n+        },\n+        classifiers=[\n+            'Development Status :: 5 - Production/Stable',\n+            'License :: OSI Approved :: Apache Software License',\n+            'Programming Language :: Python :: 2.7',\n+            'Programming Language :: Python :: 3',\n+            'Programming Language :: Python :: 3.0',\n+            'Programming Language :: Python :: 3.1',\n+            'Programming Language :: Python :: 3.2',\n+            'Programming Language :: Python :: 3.3',"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "Should we not hardcode scala 2.11 here?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-07T22:26:40Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging. You must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = os.path.join(SPARK_HOME, \"assembly/target/scala-2.11/jars/\")"
  }, {
    "author": {
      "login": "rgbkrk"
    },
    "body": "Probably `glob.glob` would be good to use here if we're not going to match the current spark setup.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-07T23:41:14Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging. You must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = os.path.join(SPARK_HOME, \"assembly/target/scala-2.11/jars/\")"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "create a viable or function for this?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-07T22:32:08Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging. You must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = os.path.join(SPARK_HOME, \"assembly/target/scala-2.11/jars/\")\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = os.path.join(SPARK_HOME, \"jars\")\n+\n+EXAMPLES_PATH = os.path.join(SPARK_HOME, \"examples/src/main/python\")\n+SCRIPTS_PATH = os.path.join(SPARK_HOME, \"bin\")\n+SCRIPTS_TARGET = os.path.join(TEMP_PATH, \"bin\")\n+JARS_TARGET = os.path.join(TEMP_PATH, \"jars\")\n+EXAMPLES_TARGET = os.path.join(TEMP_PATH, \"examples\")\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists {0}\".format(TEMP_PATH),\n+              file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    # We copy the shell script to be under pyspark/python/pyspark so that the launcher scripts\n+    # find it where expected. The rest of the files aren't copied because they are accessed\n+    # using Python imports instead which will be resolved correctly.\n+    try:\n+        os.makedirs(\"pyspark/python/pyspark\")\n+    except OSError:\n+        # Don't worry if the directory already exists.\n+        pass\n+    copyfile(\"pyspark/shell.py\", \"pyspark/python/pyspark/shell.py\")\n+\n+    if (in_spark):\n+        # Construct the symlink farm - this is necessary since we can't refer to the path above the\n+        # package root and we need to copy the jars and scripts which are up above the python root.\n+        if getattr(os, \"symlink\", None) is not None:\n+            os.symlink(JARS_PATH, JARS_TARGET)\n+            os.symlink(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            os.symlink(EXAMPLES_PATH, EXAMPLES_TARGET)\n+        else:\n+            # For windows fall back to the slower copytree\n+            copytree(JARS_PATH, JARS_TARGET)\n+            copytree(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            copytree(EXAMPLES_PATH, EXAMPLES_TARGET)\n+    else:\n+        # If we are not inside of SPARK_HOME verify we have the required symlink farm\n+        if not os.path.exists(JARS_TARGET):\n+            print(\"To build packaging must be in the python directory under the SPARK_HOME.\",\n+                  file=sys.stderr)\n+\n+    if not os.path.isdir(SCRIPTS_TARGET):\n+        print(\"You must first create a source dist and install that source dist.\", file=sys.stderr)\n+        exit(-1)\n+\n+    # Scripts directive requires a list of each script path and does not take wild cards.\n+    script_names = os.listdir(SCRIPTS_TARGET)\n+    scripts = list(map(lambda script: os.path.join(SCRIPTS_TARGET, script), script_names))\n+    # We add find_spark_home.py to the bin directory we install so that pip installed PySpark\n+    # will search for SPARK_HOME with Python.\n+    scripts.append(\"pyspark/find_spark_home.py\")\n+\n+    # Parse the README markdown file into rst for PyPI\n+    long_description = \"!!!!! missing pandoc do not upload to PyPI !!!!\"\n+    try:\n+        import pypandoc\n+        long_description = pypandoc.convert('README.md', 'rst')\n+    except ImportError:\n+        print(\"Could not import pypandoc - required to package PySpark\", file=sys.stderr)\n+\n+    setup(\n+        name='pyspark',\n+        version=VERSION,\n+        description='Apache Spark Python API',\n+        long_description=long_description,\n+        author='Spark Developers',\n+        author_email='dev@spark.apache.org',\n+        url='https://github.com/apache/spark/tree/master/python',\n+        packages=['pyspark',\n+                  'pyspark.mllib',\n+                  'pyspark.ml',\n+                  'pyspark.sql',\n+                  'pyspark.streaming',\n+                  'pyspark.bin',\n+                  'pyspark.jars',\n+                  'pyspark.python.pyspark',\n+                  'pyspark.python.lib',\n+                  'pyspark.examples.src.main.python'],\n+        include_package_data=True,\n+        package_dir={\n+            'pyspark.jars': 'deps/jars',\n+            'pyspark.bin': 'deps/bin',\n+            'pyspark.python.lib': 'lib',\n+            'pyspark.examples.src.main.python': 'deps/examples',\n+        },\n+        package_data={\n+            'pyspark.jars': ['*.jar'],\n+            'pyspark.bin': ['*'],\n+            'pyspark.python.lib': ['*.zip'],\n+            'pyspark.examples.src.main.python': ['*.py', '*/*.py']},\n+        scripts=scripts,\n+        license='http://www.apache.org/licenses/LICENSE-2.0',\n+        install_requires=['py4j==0.10.4'],\n+        setup_requires=['pypandoc'],\n+        extras_require={\n+            'ml': ['numpy>=1.7'],\n+            'mllib': ['numpy>=1.7'],\n+            'sql': ['pandas']\n+        },\n+        classifiers=[\n+            'Development Status :: 5 - Production/Stable',\n+            'License :: OSI Approved :: Apache Software License',\n+            'Programming Language :: Python :: 2.7',\n+            'Programming Language :: Python :: 3',\n+            'Programming Language :: Python :: 3.4',\n+            'Programming Language :: Python :: 3.5',\n+            'Programming Language :: Python :: Implementation :: CPython',\n+            'Programming Language :: Python :: Implementation :: PyPy']\n+    )\n+finally:\n+    # We only cleanup the symlink farm if we were in Spark, otherwise we are installing rather than\n+    # packaging.\n+    if (in_spark):\n+        # Depending on cleaning up the symlink farm or copied version\n+        if getattr(os, \"symlink\", None) is not None:"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "What's this?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-07T22:49:20Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging. You must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = os.path.join(SPARK_HOME, \"assembly/target/scala-2.11/jars/\")\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = os.path.join(SPARK_HOME, \"jars\")\n+\n+EXAMPLES_PATH = os.path.join(SPARK_HOME, \"examples/src/main/python\")\n+SCRIPTS_PATH = os.path.join(SPARK_HOME, \"bin\")\n+SCRIPTS_TARGET = os.path.join(TEMP_PATH, \"bin\")\n+JARS_TARGET = os.path.join(TEMP_PATH, \"jars\")\n+EXAMPLES_TARGET = os.path.join(TEMP_PATH, \"examples\")\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists {0}\".format(TEMP_PATH),\n+              file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    # We copy the shell script to be under pyspark/python/pyspark so that the launcher scripts\n+    # find it where expected. The rest of the files aren't copied because they are accessed\n+    # using Python imports instead which will be resolved correctly.\n+    try:\n+        os.makedirs(\"pyspark/python/pyspark\")\n+    except OSError:\n+        # Don't worry if the directory already exists.\n+        pass\n+    copyfile(\"pyspark/shell.py\", \"pyspark/python/pyspark/shell.py\")\n+\n+    if (in_spark):\n+        # Construct the symlink farm - this is necessary since we can't refer to the path above the\n+        # package root and we need to copy the jars and scripts which are up above the python root.\n+        if getattr(os, \"symlink\", None) is not None:\n+            os.symlink(JARS_PATH, JARS_TARGET)\n+            os.symlink(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            os.symlink(EXAMPLES_PATH, EXAMPLES_TARGET)\n+        else:\n+            # For windows fall back to the slower copytree\n+            copytree(JARS_PATH, JARS_TARGET)\n+            copytree(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            copytree(EXAMPLES_PATH, EXAMPLES_TARGET)\n+    else:\n+        # If we are not inside of SPARK_HOME verify we have the required symlink farm\n+        if not os.path.exists(JARS_TARGET):\n+            print(\"To build packaging must be in the python directory under the SPARK_HOME.\",\n+                  file=sys.stderr)\n+\n+    if not os.path.isdir(SCRIPTS_TARGET):\n+        print(\"You must first create a source dist and install that source dist.\", file=sys.stderr)\n+        exit(-1)\n+\n+    # Scripts directive requires a list of each script path and does not take wild cards.\n+    script_names = os.listdir(SCRIPTS_TARGET)\n+    scripts = list(map(lambda script: os.path.join(SCRIPTS_TARGET, script), script_names))\n+    # We add find_spark_home.py to the bin directory we install so that pip installed PySpark\n+    # will search for SPARK_HOME with Python.\n+    scripts.append(\"pyspark/find_spark_home.py\")\n+\n+    # Parse the README markdown file into rst for PyPI\n+    long_description = \"!!!!! missing pandoc do not upload to PyPI !!!!\"\n+    try:\n+        import pypandoc\n+        long_description = pypandoc.convert('README.md', 'rst')\n+    except ImportError:\n+        print(\"Could not import pypandoc - required to package PySpark\", file=sys.stderr)\n+\n+    setup(\n+        name='pyspark',\n+        version=VERSION,\n+        description='Apache Spark Python API',\n+        long_description=long_description,\n+        author='Spark Developers',\n+        author_email='dev@spark.apache.org',\n+        url='https://github.com/apache/spark/tree/master/python',\n+        packages=['pyspark',\n+                  'pyspark.mllib',\n+                  'pyspark.ml',\n+                  'pyspark.sql',\n+                  'pyspark.streaming',\n+                  'pyspark.bin',\n+                  'pyspark.jars',\n+                  'pyspark.python.pyspark',",
    "line": 162
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "I think it is for shell.py which is copied to pyspark/python/pyspark so that the launcher scripts can find it where expected.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-08T01:26:51Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging. You must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = os.path.join(SPARK_HOME, \"assembly/target/scala-2.11/jars/\")\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = os.path.join(SPARK_HOME, \"jars\")\n+\n+EXAMPLES_PATH = os.path.join(SPARK_HOME, \"examples/src/main/python\")\n+SCRIPTS_PATH = os.path.join(SPARK_HOME, \"bin\")\n+SCRIPTS_TARGET = os.path.join(TEMP_PATH, \"bin\")\n+JARS_TARGET = os.path.join(TEMP_PATH, \"jars\")\n+EXAMPLES_TARGET = os.path.join(TEMP_PATH, \"examples\")\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists {0}\".format(TEMP_PATH),\n+              file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    # We copy the shell script to be under pyspark/python/pyspark so that the launcher scripts\n+    # find it where expected. The rest of the files aren't copied because they are accessed\n+    # using Python imports instead which will be resolved correctly.\n+    try:\n+        os.makedirs(\"pyspark/python/pyspark\")\n+    except OSError:\n+        # Don't worry if the directory already exists.\n+        pass\n+    copyfile(\"pyspark/shell.py\", \"pyspark/python/pyspark/shell.py\")\n+\n+    if (in_spark):\n+        # Construct the symlink farm - this is necessary since we can't refer to the path above the\n+        # package root and we need to copy the jars and scripts which are up above the python root.\n+        if getattr(os, \"symlink\", None) is not None:\n+            os.symlink(JARS_PATH, JARS_TARGET)\n+            os.symlink(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            os.symlink(EXAMPLES_PATH, EXAMPLES_TARGET)\n+        else:\n+            # For windows fall back to the slower copytree\n+            copytree(JARS_PATH, JARS_TARGET)\n+            copytree(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            copytree(EXAMPLES_PATH, EXAMPLES_TARGET)\n+    else:\n+        # If we are not inside of SPARK_HOME verify we have the required symlink farm\n+        if not os.path.exists(JARS_TARGET):\n+            print(\"To build packaging must be in the python directory under the SPARK_HOME.\",\n+                  file=sys.stderr)\n+\n+    if not os.path.isdir(SCRIPTS_TARGET):\n+        print(\"You must first create a source dist and install that source dist.\", file=sys.stderr)\n+        exit(-1)\n+\n+    # Scripts directive requires a list of each script path and does not take wild cards.\n+    script_names = os.listdir(SCRIPTS_TARGET)\n+    scripts = list(map(lambda script: os.path.join(SCRIPTS_TARGET, script), script_names))\n+    # We add find_spark_home.py to the bin directory we install so that pip installed PySpark\n+    # will search for SPARK_HOME with Python.\n+    scripts.append(\"pyspark/find_spark_home.py\")\n+\n+    # Parse the README markdown file into rst for PyPI\n+    long_description = \"!!!!! missing pandoc do not upload to PyPI !!!!\"\n+    try:\n+        import pypandoc\n+        long_description = pypandoc.convert('README.md', 'rst')\n+    except ImportError:\n+        print(\"Could not import pypandoc - required to package PySpark\", file=sys.stderr)\n+\n+    setup(\n+        name='pyspark',\n+        version=VERSION,\n+        description='Apache Spark Python API',\n+        long_description=long_description,\n+        author='Spark Developers',\n+        author_email='dev@spark.apache.org',\n+        url='https://github.com/apache/spark/tree/master/python',\n+        packages=['pyspark',\n+                  'pyspark.mllib',\n+                  'pyspark.ml',\n+                  'pyspark.sql',\n+                  'pyspark.streaming',\n+                  'pyspark.bin',\n+                  'pyspark.jars',\n+                  'pyspark.python.pyspark',",
    "line": 162
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "Yup @viirya's explanation is correct.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-08T07:58:34Z",
    "diffHunk": "@@ -0,0 +1,179 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging. You must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+JARS_PATH = os.path.join(SPARK_HOME, \"assembly/target/scala-2.11/jars/\")\n+\n+# Use the release jars path if we are in release mode.\n+if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    JARS_PATH = os.path.join(SPARK_HOME, \"jars\")\n+\n+EXAMPLES_PATH = os.path.join(SPARK_HOME, \"examples/src/main/python\")\n+SCRIPTS_PATH = os.path.join(SPARK_HOME, \"bin\")\n+SCRIPTS_TARGET = os.path.join(TEMP_PATH, \"bin\")\n+JARS_TARGET = os.path.join(TEMP_PATH, \"jars\")\n+EXAMPLES_TARGET = os.path.join(TEMP_PATH, \"examples\")\n+\n+# Check and see if we are under the spark path in which case we need to build the symlink farm.\n+# This is important because we only want to build the symlink farm while under Spark otherwise we\n+# want to use the symlink farm. And if the symlink farm exists under while under Spark (e.g. a\n+# partially built sdist) we should error and have the user sort it out.\n+in_spark = (os.path.isfile(\"../core/src/main/scala/org/apache/spark/SparkContext.scala\") or\n+            (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1))\n+\n+if (in_spark):\n+    # Construct links for setup\n+    try:\n+        os.mkdir(TEMP_PATH)\n+    except:\n+        print(\"Temp path for symlink to parent already exists {0}\".format(TEMP_PATH),\n+              file=sys.stderr)\n+        exit(-1)\n+\n+try:\n+    # We copy the shell script to be under pyspark/python/pyspark so that the launcher scripts\n+    # find it where expected. The rest of the files aren't copied because they are accessed\n+    # using Python imports instead which will be resolved correctly.\n+    try:\n+        os.makedirs(\"pyspark/python/pyspark\")\n+    except OSError:\n+        # Don't worry if the directory already exists.\n+        pass\n+    copyfile(\"pyspark/shell.py\", \"pyspark/python/pyspark/shell.py\")\n+\n+    if (in_spark):\n+        # Construct the symlink farm - this is necessary since we can't refer to the path above the\n+        # package root and we need to copy the jars and scripts which are up above the python root.\n+        if getattr(os, \"symlink\", None) is not None:\n+            os.symlink(JARS_PATH, JARS_TARGET)\n+            os.symlink(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            os.symlink(EXAMPLES_PATH, EXAMPLES_TARGET)\n+        else:\n+            # For windows fall back to the slower copytree\n+            copytree(JARS_PATH, JARS_TARGET)\n+            copytree(SCRIPTS_PATH, SCRIPTS_TARGET)\n+            copytree(EXAMPLES_PATH, EXAMPLES_TARGET)\n+    else:\n+        # If we are not inside of SPARK_HOME verify we have the required symlink farm\n+        if not os.path.exists(JARS_TARGET):\n+            print(\"To build packaging must be in the python directory under the SPARK_HOME.\",\n+                  file=sys.stderr)\n+\n+    if not os.path.isdir(SCRIPTS_TARGET):\n+        print(\"You must first create a source dist and install that source dist.\", file=sys.stderr)\n+        exit(-1)\n+\n+    # Scripts directive requires a list of each script path and does not take wild cards.\n+    script_names = os.listdir(SCRIPTS_TARGET)\n+    scripts = list(map(lambda script: os.path.join(SCRIPTS_TARGET, script), script_names))\n+    # We add find_spark_home.py to the bin directory we install so that pip installed PySpark\n+    # will search for SPARK_HOME with Python.\n+    scripts.append(\"pyspark/find_spark_home.py\")\n+\n+    # Parse the README markdown file into rst for PyPI\n+    long_description = \"!!!!! missing pandoc do not upload to PyPI !!!!\"\n+    try:\n+        import pypandoc\n+        long_description = pypandoc.convert('README.md', 'rst')\n+    except ImportError:\n+        print(\"Could not import pypandoc - required to package PySpark\", file=sys.stderr)\n+\n+    setup(\n+        name='pyspark',\n+        version=VERSION,\n+        description='Apache Spark Python API',\n+        long_description=long_description,\n+        author='Spark Developers',\n+        author_email='dev@spark.apache.org',\n+        url='https://github.com/apache/spark/tree/master/python',\n+        packages=['pyspark',\n+                  'pyspark.mllib',\n+                  'pyspark.ml',\n+                  'pyspark.sql',\n+                  'pyspark.streaming',\n+                  'pyspark.bin',\n+                  'pyspark.jars',\n+                  'pyspark.python.pyspark',",
    "line": 162
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "nit: \"deps\" -> TEMP_PATH, I think?\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-09T02:38:26Z",
    "diffHunk": "@@ -38,11 +38,22 @@\n # A temporary path so we can access above the Python project root and fetch scripts and jars we need\n TEMP_PATH = \"deps\"\n SPARK_HOME = os.path.abspath(\"../\")\n-JARS_PATH = os.path.join(SPARK_HOME, \"assembly/target/scala-2.11/jars/\")\n \n-# Use the release jars path if we are in release mode.\n-if (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+# Figure out where the jars are we need to package with PySpark.\n+JARS_PATH = glob.glob(os.path.join(SPARK_HOME, \"assembly/target/scala-*/jars/\"))\n+\n+if len(JARS_PATH) == 1:\n+    JARS_PATH = JARS_PATH[0]\n+elif (os.path.isfile(\"../RELEASE\") and len(glob.glob(\"../jars/spark*core*.jar\")) == 1):\n+    # Release mode puts the jars in a jars directory\n     JARS_PATH = os.path.join(SPARK_HOME, \"jars\")\n+elif len(JARS_PATH) > 1:\n+    print(\"Assembly jars exist for multiple scalas, please cleanup assembly/target\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+elif len(JARS_PATH) == 0 and not os.path.exists(\"deps\"):"
  }],
  "prId": 15659
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "Should we respect and use `$SPARK_SCALA_VERSION` here if defined? We do that in `bin/spark-class`.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-16T08:12:03Z",
    "diffHunk": "@@ -0,0 +1,209 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging. You must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+\n+# Provide guidance about how to use setup.py\n+incorrect_invocation_message = \"\"\"\n+If you are installing pyspark from spark source, you must first build Spark and\n+run sdist.\n+\n+    To build Spark with maven you can run:\n+      ./build/mvn -DskipTests clean package\n+    Building the source dist is done in the Python directory:\n+      cd python\n+      python setup.py sdist\n+      pip install dist/*.tar.gz\"\"\"\n+\n+# Figure out where the jars are we need to package with PySpark.\n+JARS_PATH = glob.glob(os.path.join(SPARK_HOME, \"assembly/target/scala-*/jars/\"))",
    "line": 55
  }, {
    "author": {
      "login": "holdenk"
    },
    "body": "It might not be defined if someone is just building there own sdist or manually installing from source rather than with the packaging scripts so I'd rather avoid assuming `$SPARK_SCALA_VERSION` is present.\n",
    "commit": "e1398552469288de3829eb889fec0de2ba568f15",
    "createdAt": "2016-11-16T15:12:37Z",
    "diffHunk": "@@ -0,0 +1,209 @@\n+#!/usr/bin/env python\n+\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from __future__ import print_function\n+import glob\n+import os\n+import sys\n+from setuptools import setup, find_packages\n+from shutil import copyfile, copytree, rmtree\n+\n+if sys.version_info < (2, 7):\n+    print(\"Python versions prior to 2.7 are not supported for pip installed PySpark.\",\n+          file=sys.stderr)\n+    exit(-1)\n+\n+try:\n+    exec(open('pyspark/version.py').read())\n+except IOError:\n+    print(\"Failed to load PySpark version file for packaging. You must be in Spark's python dir.\",\n+          file=sys.stderr)\n+    sys.exit(-1)\n+VERSION = __version__\n+# A temporary path so we can access above the Python project root and fetch scripts and jars we need\n+TEMP_PATH = \"deps\"\n+SPARK_HOME = os.path.abspath(\"../\")\n+\n+# Provide guidance about how to use setup.py\n+incorrect_invocation_message = \"\"\"\n+If you are installing pyspark from spark source, you must first build Spark and\n+run sdist.\n+\n+    To build Spark with maven you can run:\n+      ./build/mvn -DskipTests clean package\n+    Building the source dist is done in the Python directory:\n+      cd python\n+      python setup.py sdist\n+      pip install dist/*.tar.gz\"\"\"\n+\n+# Figure out where the jars are we need to package with PySpark.\n+JARS_PATH = glob.glob(os.path.join(SPARK_HOME, \"assembly/target/scala-*/jars/\"))",
    "line": 55
  }],
  "prId": 15659
}]