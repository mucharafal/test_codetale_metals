[{
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "We should `collect()` here so this doesn't run on the executors.\n",
    "commit": "ca7f099cf1a6f16d6f89bd91fe76b2ce114ab49e",
    "createdAt": "2015-11-02T16:40:20Z",
    "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.sql\n+\n+import org.apache.spark.sql.{Dataset, SQLContext}\n+import org.apache.spark.{SparkContext, SparkConf}\n+\n+object DatasetWordCount {\n+  def main(args: Array[String]): Unit = {\n+    val sparkConf = new SparkConf().setAppName(\"DatasetWordCount\")\n+    val sc = new SparkContext(sparkConf)\n+    val sqlContext = new SQLContext(sc)\n+\n+    // Importing the SQL context gives access to all the SQL functions and implicit conversions.\n+    import sqlContext.implicits._\n+\n+    val lines: Dataset[String] = Seq(\"hello world\", \"say hello to the world\").toDS()\n+    val words: Dataset[(String, Int)] = lines.flatMap(_.split(\" \")).map(word => word -> 1)\n+    val counts: Dataset[(String, Int)] = words.groupBy(_._1).mapGroups {\n+      case (word, iter) => Iterator(word -> iter.length)\n+    }\n+\n+    counts.foreach { case (word, count) => println(s\"$word: $count\") }"
  }, {
    "author": {
      "login": "mateiz"
    },
    "body": "It can be just counts.collect.foreach(println)\n",
    "commit": "ca7f099cf1a6f16d6f89bd91fe76b2ce114ab49e",
    "createdAt": "2015-11-03T02:55:13Z",
    "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.sql\n+\n+import org.apache.spark.sql.{Dataset, SQLContext}\n+import org.apache.spark.{SparkContext, SparkConf}\n+\n+object DatasetWordCount {\n+  def main(args: Array[String]): Unit = {\n+    val sparkConf = new SparkConf().setAppName(\"DatasetWordCount\")\n+    val sc = new SparkContext(sparkConf)\n+    val sqlContext = new SQLContext(sc)\n+\n+    // Importing the SQL context gives access to all the SQL functions and implicit conversions.\n+    import sqlContext.implicits._\n+\n+    val lines: Dataset[String] = Seq(\"hello world\", \"say hello to the world\").toDS()\n+    val words: Dataset[(String, Int)] = lines.flatMap(_.split(\" \")).map(word => word -> 1)\n+    val counts: Dataset[(String, Int)] = words.groupBy(_._1).mapGroups {\n+      case (word, iter) => Iterator(word -> iter.length)\n+    }\n+\n+    counts.foreach { case (word, count) => println(s\"$word: $count\") }"
  }],
  "prId": 9415
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "I'm questioning if GroupedDataset should just have `map` and `flatMap` to match the RDD API.  It is kind of annoying to have to wrap things in `Iterator` when I think the common case might be to return a single item per group.\n",
    "commit": "ca7f099cf1a6f16d6f89bd91fe76b2ce114ab49e",
    "createdAt": "2015-11-02T16:42:22Z",
    "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.sql\n+\n+import org.apache.spark.sql.{Dataset, SQLContext}\n+import org.apache.spark.{SparkContext, SparkConf}\n+\n+object DatasetWordCount {\n+  def main(args: Array[String]): Unit = {\n+    val sparkConf = new SparkConf().setAppName(\"DatasetWordCount\")\n+    val sc = new SparkContext(sparkConf)\n+    val sqlContext = new SQLContext(sc)\n+\n+    // Importing the SQL context gives access to all the SQL functions and implicit conversions.\n+    import sqlContext.implicits._\n+\n+    val lines: Dataset[String] = Seq(\"hello world\", \"say hello to the world\").toDS()\n+    val words: Dataset[(String, Int)] = lines.flatMap(_.split(\" \")).map(word => word -> 1)\n+    val counts: Dataset[(String, Int)] = words.groupBy(_._1).mapGroups {\n+      case (word, iter) => Iterator(word -> iter.length)",
    "line": 40
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "/cc @rxin, @mateiz\n",
    "commit": "ca7f099cf1a6f16d6f89bd91fe76b2ce114ab49e",
    "createdAt": "2015-11-02T16:45:04Z",
    "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.sql\n+\n+import org.apache.spark.sql.{Dataset, SQLContext}\n+import org.apache.spark.{SparkContext, SparkConf}\n+\n+object DatasetWordCount {\n+  def main(args: Array[String]): Unit = {\n+    val sparkConf = new SparkConf().setAppName(\"DatasetWordCount\")\n+    val sc = new SparkContext(sparkConf)\n+    val sqlContext = new SQLContext(sc)\n+\n+    // Importing the SQL context gives access to all the SQL functions and implicit conversions.\n+    import sqlContext.implicits._\n+\n+    val lines: Dataset[String] = Seq(\"hello world\", \"say hello to the world\").toDS()\n+    val words: Dataset[(String, Int)] = lines.flatMap(_.split(\" \")).map(word => word -> 1)\n+    val counts: Dataset[(String, Int)] = words.groupBy(_._1).mapGroups {\n+      case (word, iter) => Iterator(word -> iter.length)",
    "line": 40
  }, {
    "author": {
      "login": "mateiz"
    },
    "body": "For this particular example, why are we using mapGroups? We should be using aggregation, otherwise it' not a great example.\n\nI agree it would be nice to have methods that don't return an Iterator though.\n",
    "commit": "ca7f099cf1a6f16d6f89bd91fe76b2ce114ab49e",
    "createdAt": "2015-11-03T02:54:53Z",
    "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.sql\n+\n+import org.apache.spark.sql.{Dataset, SQLContext}\n+import org.apache.spark.{SparkContext, SparkConf}\n+\n+object DatasetWordCount {\n+  def main(args: Array[String]): Unit = {\n+    val sparkConf = new SparkConf().setAppName(\"DatasetWordCount\")\n+    val sc = new SparkContext(sparkConf)\n+    val sqlContext = new SQLContext(sc)\n+\n+    // Importing the SQL context gives access to all the SQL functions and implicit conversions.\n+    import sqlContext.implicits._\n+\n+    val lines: Dataset[String] = Seq(\"hello world\", \"say hello to the world\").toDS()\n+    val words: Dataset[(String, Int)] = lines.flatMap(_.split(\" \")).map(word => word -> 1)\n+    val counts: Dataset[(String, Int)] = words.groupBy(_._1).mapGroups {\n+      case (word, iter) => Iterator(word -> iter.length)",
    "line": 40
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "Actually `mapGroups` and `cogroup` are the only 2 aggregations we have in `Dataset`...\n",
    "commit": "ca7f099cf1a6f16d6f89bd91fe76b2ce114ab49e",
    "createdAt": "2015-11-03T03:07:24Z",
    "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.sql\n+\n+import org.apache.spark.sql.{Dataset, SQLContext}\n+import org.apache.spark.{SparkContext, SparkConf}\n+\n+object DatasetWordCount {\n+  def main(args: Array[String]): Unit = {\n+    val sparkConf = new SparkConf().setAppName(\"DatasetWordCount\")\n+    val sc = new SparkContext(sparkConf)\n+    val sqlContext = new SQLContext(sc)\n+\n+    // Importing the SQL context gives access to all the SQL functions and implicit conversions.\n+    import sqlContext.implicits._\n+\n+    val lines: Dataset[String] = Seq(\"hello world\", \"say hello to the world\").toDS()\n+    val words: Dataset[(String, Int)] = lines.flatMap(_.split(\" \")).map(word => word -> 1)\n+    val counts: Dataset[(String, Int)] = words.groupBy(_._1).mapGroups {\n+      case (word, iter) => Iterator(word -> iter.length)",
    "line": 40
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "It's best not to use this example, otherwise you might find a lot of users writing wordcounts in this inefficient way. If you can't come up with a better one, maybe put a big warning in the example code saying this is a really inefficient way to do wordcount, and is used here only for illustrative purpose.\n",
    "commit": "ca7f099cf1a6f16d6f89bd91fe76b2ce114ab49e",
    "createdAt": "2015-11-03T15:37:16Z",
    "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.sql\n+\n+import org.apache.spark.sql.{Dataset, SQLContext}\n+import org.apache.spark.{SparkContext, SparkConf}\n+\n+object DatasetWordCount {\n+  def main(args: Array[String]): Unit = {\n+    val sparkConf = new SparkConf().setAppName(\"DatasetWordCount\")\n+    val sc = new SparkContext(sparkConf)\n+    val sqlContext = new SQLContext(sc)\n+\n+    // Importing the SQL context gives access to all the SQL functions and implicit conversions.\n+    import sqlContext.implicits._\n+\n+    val lines: Dataset[String] = Seq(\"hello world\", \"say hello to the world\").toDS()\n+    val words: Dataset[(String, Int)] = lines.flatMap(_.split(\" \")).map(word => word -> 1)\n+    val counts: Dataset[(String, Int)] = words.groupBy(_._1).mapGroups {\n+      case (word, iter) => Iterator(word -> iter.length)",
    "line": 40
  }, {
    "author": {
      "login": "marmbrus"
    },
    "body": "Yeah, agree.  I wasn't planning to commit all of these unless they end up being good examples of the API.  I mostly just think it super helpful to see various ways that people might try to implement common tasks.\n\nAnyway, this will be much easier once #9499 is merged.\n",
    "commit": "ca7f099cf1a6f16d6f89bd91fe76b2ce114ab49e",
    "createdAt": "2015-11-05T18:26:02Z",
    "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.sql\n+\n+import org.apache.spark.sql.{Dataset, SQLContext}\n+import org.apache.spark.{SparkContext, SparkConf}\n+\n+object DatasetWordCount {\n+  def main(args: Array[String]): Unit = {\n+    val sparkConf = new SparkConf().setAppName(\"DatasetWordCount\")\n+    val sc = new SparkContext(sparkConf)\n+    val sqlContext = new SQLContext(sc)\n+\n+    // Importing the SQL context gives access to all the SQL functions and implicit conversions.\n+    import sqlContext.implicits._\n+\n+    val lines: Dataset[String] = Seq(\"hello world\", \"say hello to the world\").toDS()\n+    val words: Dataset[(String, Int)] = lines.flatMap(_.split(\" \")).map(word => word -> 1)\n+    val counts: Dataset[(String, Int)] = words.groupBy(_._1).mapGroups {\n+      case (word, iter) => Iterator(word -> iter.length)",
    "line": 40
  }],
  "prId": 9415
}, {
  "comments": [{
    "author": {
      "login": "marmbrus"
    },
    "body": "We should add some scaladoc to explain what the example is going.\n\n```\nGiven a Dataset of Strings, we tokenize by splitting on whitespace and count the number of occurrences of each unique word.\n```\n",
    "commit": "ca7f099cf1a6f16d6f89bd91fe76b2ce114ab49e",
    "createdAt": "2015-11-02T16:47:56Z",
    "diffHunk": "@@ -0,0 +1,42 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.sql\n+\n+import org.apache.spark.sql.{Dataset, SQLContext}\n+import org.apache.spark.{SparkContext, SparkConf}\n+\n+object DatasetWordCount {"
  }],
  "prId": 9415
}]