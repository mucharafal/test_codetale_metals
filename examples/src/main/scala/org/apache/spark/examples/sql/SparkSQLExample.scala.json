[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "I don't think this is correct because there's also not a reason to assume this is the location. Just write `file:examples/...`? does that not work?\n",
    "commit": "0f8fac6079592fef2c6d605c2a26701414c32fb4",
    "createdAt": "2016-11-10T11:03:47Z",
    "diffHunk": "@@ -60,7 +60,8 @@ object SparkSQLExample {\n \n   private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n     // $example on:create_df$\n-    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+    val df = spark.read.json(\n+      \"file:///\" + System.getProperty(\"user.dir\") + \"examples/src/main/resources/people.json\")"
  }, {
    "author": {
      "login": "oza"
    },
    "body": "@srowen thanks for taking a look! Unfortunately, your suggestion doesn't work well in this case. I tried it on my local spark shell:\n\n```\nscala> val df = spark.read.json(\"file:examples/src/main/resources/people.json\")\n16/11/10 20:12:25 WARN datasources.DataSource: Error while looking for metadata directory.\njava.lang.IllegalArgumentException: java.net.URISyntaxException: Relative path in absolute URI: file:examples/src/main/resources/people.json\n  at org.apache.hadoop.fs.Path.initialize(Path.java:206)\n  at org.apache.hadoop.fs.Path.<init>(Path.java:172)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:283)\n  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$8.apply(DataSource.scala:282)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n  at scala.collection.immutable.List.foreach(List.scala:381)\n  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n  at scala.collection.immutable.List.flatMap(List.scala:344)\n  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:282)\n  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:152)\n  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:297)\n  at org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:250)\n```\n\nAs you mentioned, the fix is not correct if we are not in spark homw directory. How about fixing the documentation to do the comments in spark directory?\n",
    "commit": "0f8fac6079592fef2c6d605c2a26701414c32fb4",
    "createdAt": "2016-11-10T11:17:43Z",
    "diffHunk": "@@ -60,7 +60,8 @@ object SparkSQLExample {\n \n   private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n     // $example on:create_df$\n-    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+    val df = spark.read.json(\n+      \"file:///\" + System.getProperty(\"user.dir\") + \"examples/src/main/resources/people.json\")"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Isn't this supposed to be ran via https://github.com/apache/spark#example-programs ? I just ran some of them after manually building and it seems fine.\nFor example,\n\n```\n./bin/run-example sql.SparkSQLExample\n```\n",
    "commit": "0f8fac6079592fef2c6d605c2a26701414c32fb4",
    "createdAt": "2016-11-10T12:25:20Z",
    "diffHunk": "@@ -60,7 +60,8 @@ object SparkSQLExample {\n \n   private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n     // $example on:create_df$\n-    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+    val df = spark.read.json(\n+      \"file:///\" + System.getProperty(\"user.dir\") + \"examples/src/main/resources/people.json\")"
  }, {
    "author": {
      "login": "oza"
    },
    "body": "@HyukjinKwon Thanks for your comment! Without HDFS configuration(including HADOOP_HOME), it result in failure.\n\n```\n./bin/run-example sql.SparkSQLExample\n...\n16/11/10 22:48:07 WARN datasources.DataSource: Error while looking for metadata directory.\nException in thread \"main\" java.net.ConnectException: Call From ozamac-2.local/10.0.1.20 to localhost:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n    at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n    at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n```\n\nAfter applying the patch, it works correctly.\n",
    "commit": "0f8fac6079592fef2c6d605c2a26701414c32fb4",
    "createdAt": "2016-11-10T13:53:20Z",
    "diffHunk": "@@ -60,7 +60,8 @@ object SparkSQLExample {\n \n   private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n     // $example on:create_df$\n-    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+    val df = spark.read.json(\n+      \"file:///\" + System.getProperty(\"user.dir\") + \"examples/src/main/resources/people.json\")"
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "I just downloaded Spark 2.0.1 and executed `./bin/run-example sql.SparkSQLExample` in the spark dir and that runs fine. I am on a Mac (on which platform are you?). It only becomes an issue when you try to run the examples from a different location; then it will fail. \n",
    "commit": "0f8fac6079592fef2c6d605c2a26701414c32fb4",
    "createdAt": "2016-11-10T14:21:02Z",
    "diffHunk": "@@ -60,7 +60,8 @@ object SparkSQLExample {\n \n   private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n     // $example on:create_df$\n-    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+    val df = spark.read.json(\n+      \"file:///\" + System.getProperty(\"user.dir\") + \"examples/src/main/resources/people.json\")"
  }],
  "prId": 15841
}, {
  "comments": [{
    "author": {
      "login": "hvanhovell"
    },
    "body": "This makes a very strong assumption about the location of the files right?\n",
    "commit": "0f8fac6079592fef2c6d605c2a26701414c32fb4",
    "createdAt": "2016-11-10T13:48:53Z",
    "diffHunk": "@@ -60,7 +60,8 @@ object SparkSQLExample {\n \n   private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n     // $example on:create_df$\n-    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+    val df = spark.read.json(\n+      \"file:///\" + System.getProperty(\"user.dir\") + \"/examples/src/main/resources/people.json\")",
    "line": 6
  }, {
    "author": {
      "login": "oza"
    },
    "body": "Yes. This also assumes that user's current directory is $SPARK_HOME. However, without this change, `./bin/run-example sql.SparkSQLExample` goes failure with default configuration.\n\nAnother option is to give a path of people.json/people.text via CLI. I would like to hear your thought.\n",
    "commit": "0f8fac6079592fef2c6d605c2a26701414c32fb4",
    "createdAt": "2016-11-10T13:57:33Z",
    "diffHunk": "@@ -60,7 +60,8 @@ object SparkSQLExample {\n \n   private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n     // $example on:create_df$\n-    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+    val df = spark.read.json(\n+      \"file:///\" + System.getProperty(\"user.dir\") + \"/examples/src/main/resources/people.json\")",
    "line": 6
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "No it does not, the `user.dir` is simply the location java is started from. That can be anywhere.\n",
    "commit": "0f8fac6079592fef2c6d605c2a26701414c32fb4",
    "createdAt": "2016-11-10T14:00:53Z",
    "diffHunk": "@@ -60,7 +60,8 @@ object SparkSQLExample {\n \n   private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n     // $example on:create_df$\n-    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+    val df = spark.read.json(\n+      \"file:///\" + System.getProperty(\"user.dir\") + \"/examples/src/main/resources/people.json\")",
    "line": 6
  }, {
    "author": {
      "login": "oza"
    },
    "body": "Oh, my bad. Thanks for pointing out. Then, how about passing the path of people.json/people.text via CLI?\n",
    "commit": "0f8fac6079592fef2c6d605c2a26701414c32fb4",
    "createdAt": "2016-11-10T14:07:56Z",
    "diffHunk": "@@ -60,7 +60,8 @@ object SparkSQLExample {\n \n   private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n     // $example on:create_df$\n-    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+    val df = spark.read.json(\n+      \"file:///\" + System.getProperty(\"user.dir\") + \"/examples/src/main/resources/people.json\")",
    "line": 6
  }, {
    "author": {
      "login": "oza"
    },
    "body": "> This makes a very strong assumption about the location of the files right?\n\nI thought so, but it results in failure because client tries to access HDFS with the default configuration. I think it should work without any configuration, since the sample is on the page of [getting started](http://spark.apache.org/docs/latest/sql-programming-guide.html#getting-started).\n\nThe stack trace is attached here:\nhttps://issues.apache.org/jira/browse/SPARK-18399?focusedCommentId=15653722&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15653722\n",
    "commit": "0f8fac6079592fef2c6d605c2a26701414c32fb4",
    "createdAt": "2016-11-10T14:13:22Z",
    "diffHunk": "@@ -60,7 +60,8 @@ object SparkSQLExample {\n \n   private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n     // $example on:create_df$\n-    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+    val df = spark.read.json(\n+      \"file:///\" + System.getProperty(\"user.dir\") + \"/examples/src/main/resources/people.json\")",
    "line": 6
  }, {
    "author": {
      "login": "hvanhovell"
    },
    "body": "You wouldn't happen to some hadoop configuration on your system?\n",
    "commit": "0f8fac6079592fef2c6d605c2a26701414c32fb4",
    "createdAt": "2016-11-10T14:22:22Z",
    "diffHunk": "@@ -60,7 +60,8 @@ object SparkSQLExample {\n \n   private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n     // $example on:create_df$\n-    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+    val df = spark.read.json(\n+      \"file:///\" + System.getProperty(\"user.dir\") + \"/examples/src/main/resources/people.json\")",
    "line": 6
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Please check if you are referring Hadoop configuration somewhere. If the path does not specify the file system in the schema, then, I believe the property will be referred. If it is not found, it will use `file:///`[1].\n\n[1]https://github.com/apache/hadoop/blob/7e521c5a49fbcf88285c102051ea2522edc847b9/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/fs/CommonConfigurationKeysPublic.java#L58\n",
    "commit": "0f8fac6079592fef2c6d605c2a26701414c32fb4",
    "createdAt": "2016-11-10T14:34:33Z",
    "diffHunk": "@@ -60,7 +60,8 @@ object SparkSQLExample {\n \n   private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n     // $example on:create_df$\n-    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+    val df = spark.read.json(\n+      \"file:///\" + System.getProperty(\"user.dir\") + \"/examples/src/main/resources/people.json\")",
    "line": 6
  }, {
    "author": {
      "login": "oza"
    },
    "body": "@hvanhovell @HyukjinKwon oh, it's completely my bad! You're right, I had a hadoop configuration. Closing this as not a problem.\n",
    "commit": "0f8fac6079592fef2c6d605c2a26701414c32fb4",
    "createdAt": "2016-11-10T14:39:11Z",
    "diffHunk": "@@ -60,7 +60,8 @@ object SparkSQLExample {\n \n   private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n     // $example on:create_df$\n-    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+    val df = spark.read.json(\n+      \"file:///\" + System.getProperty(\"user.dir\") + \"/examples/src/main/resources/people.json\")",
    "line": 6
  }],
  "prId": 15841
}]