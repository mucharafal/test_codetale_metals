[{
  "comments": [{
    "author": {
      "login": "hhbyyh"
    },
    "body": "Maybe OnlineLDAIncrementalExample ?",
    "commit": "31cd11b94c751c42347e1c0e5a2cb707d1cd907c",
    "createdAt": "2017-08-26T05:54:49Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml\n+\n+// scalastyle:off println\n+// $example on$\n+import java.io.File\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.ml.{Pipeline, PipelineModel}\n+import org.apache.spark.ml.clustering.{LDA, LDAModel}\n+import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, RegexTokenizer, StopWordsRemover}\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{length, udf}\n+// $example off$\n+import org.apache.spark.sql.SparkSession\n+\n+/**\n+ * An example demonstrating incremental update of LDA, with setInitialModel parameter.\n+ * Run with\n+ * {{{\n+ * bin/run-example ml.LDAIncrementalExample\n+ * }}}\n+ */\n+object LDAIncrementalExample {"
  }],
  "prId": 17461
}, {
  "comments": [{
    "author": {
      "login": "hhbyyh"
    },
    "body": "Other example usually will not specify master",
    "commit": "31cd11b94c751c42347e1c0e5a2cb707d1cd907c",
    "createdAt": "2017-08-26T05:56:17Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml\n+\n+// scalastyle:off println\n+// $example on$\n+import java.io.File\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.ml.{Pipeline, PipelineModel}\n+import org.apache.spark.ml.clustering.{LDA, LDAModel}\n+import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, RegexTokenizer, StopWordsRemover}\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{length, udf}\n+// $example off$\n+import org.apache.spark.sql.SparkSession\n+\n+/**\n+ * An example demonstrating incremental update of LDA, with setInitialModel parameter.\n+ * Run with\n+ * {{{\n+ * bin/run-example ml.LDAIncrementalExample\n+ * }}}\n+ */\n+object LDAIncrementalExample {\n+\n+  val modelPath = File.createTempFile(\"./incrModel\", null).getPath\n+\n+  def main(args: Array[String]): Unit = {\n+\n+    val spark = SparkSession\n+        .builder()\n+        .master(\"local[*]\")"
  }],
  "prId": 17461
}, {
  "comments": [{
    "author": {
      "login": "hhbyyh"
    },
    "body": "Maybe remove this line.",
    "commit": "31cd11b94c751c42347e1c0e5a2cb707d1cd907c",
    "createdAt": "2017-08-26T05:56:50Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml\n+\n+// scalastyle:off println\n+// $example on$\n+import java.io.File\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.ml.{Pipeline, PipelineModel}\n+import org.apache.spark.ml.clustering.{LDA, LDAModel}\n+import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, RegexTokenizer, StopWordsRemover}\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{length, udf}\n+// $example off$\n+import org.apache.spark.sql.SparkSession\n+\n+/**\n+ * An example demonstrating incremental update of LDA, with setInitialModel parameter.\n+ * Run with\n+ * {{{\n+ * bin/run-example ml.LDAIncrementalExample\n+ * }}}\n+ */\n+object LDAIncrementalExample {\n+\n+  val modelPath = File.createTempFile(\"./incrModel\", null).getPath\n+\n+  def main(args: Array[String]): Unit = {\n+\n+    val spark = SparkSession\n+        .builder()\n+        .master(\"local[*]\")\n+        .appName(s\"${this.getClass.getSimpleName}\")\n+        .getOrCreate()\n+    spark.sparkContext.setLogLevel(\"ERROR\")"
  }],
  "prId": 17461
}, {
  "comments": [{
    "author": {
      "login": "hhbyyh"
    },
    "body": "can we use LDA sample data in Spark?",
    "commit": "31cd11b94c751c42347e1c0e5a2cb707d1cd907c",
    "createdAt": "2017-08-26T05:58:00Z",
    "diffHunk": "@@ -0,0 +1,175 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml\n+\n+// scalastyle:off println\n+// $example on$\n+import java.io.File\n+\n+import scala.collection.mutable\n+\n+import org.apache.spark.ml.{Pipeline, PipelineModel}\n+import org.apache.spark.ml.clustering.{LDA, LDAModel}\n+import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, RegexTokenizer, StopWordsRemover}\n+import org.apache.spark.sql.DataFrame\n+import org.apache.spark.sql.functions.{length, udf}\n+// $example off$\n+import org.apache.spark.sql.SparkSession\n+\n+/**\n+ * An example demonstrating incremental update of LDA, with setInitialModel parameter.\n+ * Run with\n+ * {{{\n+ * bin/run-example ml.LDAIncrementalExample\n+ * }}}\n+ */\n+object LDAIncrementalExample {\n+\n+  val modelPath = File.createTempFile(\"./incrModel\", null).getPath\n+\n+  def main(args: Array[String]): Unit = {\n+\n+    val spark = SparkSession\n+        .builder()\n+        .master(\"local[*]\")\n+        .appName(s\"${this.getClass.getSimpleName}\")\n+        .getOrCreate()\n+    spark.sparkContext.setLogLevel(\"ERROR\")\n+\n+    import spark.implicits._\n+\n+    val dataset = spark.read.text(\"/home/mde/workspaces/spark-project/spark/docs/*md\").toDF(\"doc\")"
  }],
  "prId": 17461
}]