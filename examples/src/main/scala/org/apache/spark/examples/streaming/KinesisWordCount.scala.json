[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "I know you simplified this, but I would prefer this to be even more simpler. Its better to have small examples which shows one very simple thing. And then people can combine this examples / components. Take a look at \nhttps://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/streaming/KafkaWordCount.scala \n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-07-31T20:37:09Z",
    "diffHunk": "@@ -0,0 +1,369 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming\n+\n+import java.nio.ByteBuffer\n+\n+import scala.util.Random\n+\n+import org.apache.log4j.Level\n+import org.apache.log4j.Logger\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.SparkContext.rddToOrderedRDDFunctions\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.Milliseconds\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.streaming.StreamingContext.toPairDStreamFunctions\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.kinesis.KinesisStringRecordSerializer\n+import org.apache.spark.streaming.kinesis.KinesisUtils\n+\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n+import com.amazonaws.services.kinesis.AmazonKinesisClient\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream\n+import com.amazonaws.services.kinesis.model.PutRecordRequest\n+\n+/**\n+ * Kinesis Spark Streaming WordCount example.\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-programming-guide.html for more details on\n+ *   the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receivers) per shard of the\n+ *   given stream.\n+ * It then starts pulling from the last checkpointed sequence number of the given \n+ *   <stream-name> and <endpoint-url>. \n+ *\n+ * Valid endpoint urls:  http://docs.aws.amazon.com/general/latest/gr/rande.html#ak_region\n+ * \n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials\n+ *   in the following order of precedence:\n+ * Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ * Java System Properties - aws.accessKeyId and aws.secretKey\n+ * Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ * Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: KinesisWordCount <stream-name> <endpoint-url>\n+ *   <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *   <endpoint-url> is the endpoint of the Kinesis service\n+ *     (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *\n+ * Example:\n+ *    $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *    $ export AWS_SECRET_KEY=<your-secret-key>\n+ *    $ $SPARK_HOME/bin/run-example \\\n+ *        org.apache.spark.examples.streaming.KinesisWordCount mySparkStream \\\n+ *        https://kinesis.us-east-1.amazonaws.com\n+ *\n+ * There is a companion helper class below called KinesisWordCountProducer which puts\n+ *   dummy data onto the Kinesis stream.\n+ * Usage instructions for KinesisWordCountProducer are provided in that class definition.\n+ */\n+object KinesisWordCount extends Logging {\n+  val WordSeparator = \" \"\n+\n+  def main(args: Array[String]) {\n+/**\n+ * Check that all required args were passed in.\n+ */\n+    if (args.length < 2) {\n+      System.err.println(\"Usage: KinesisWordCount <stream-name> <endpoint-url>\")\n+      System.exit(1)\n+    }\n+\n+    /**\n+     * (This was lifted from the StreamingExamples.scala in order to avoid the dependency\n+     *   on the spark-examples artifact.)\n+     * Set reasonable logging levels for streaming if the user has not configured log4j.\n+     */\n+    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements()\n+    if (!log4jInitialized) {\n+      /** \n+       *  We first log something to initialize Spark's default logging, \n+       *  then we override the logging level. \n+       *  */\n+      logInfo(\"Setting log level to [INFO] for streaming example.\" +\n+        \" To override add a custom log4j.properties to the classpath.\")\n+\n+      Logger.getRootLogger().setLevel(Level.INFO)\n+      Logger.getLogger(\"org.apache.spark.examples.streaming\").setLevel(Level.DEBUG);\n+    }\n+\n+    /** Populate the appropriate variables from the given args */\n+    val Array(stream, endpoint) = args\n+    val batchIntervalMillis = 2000\n+\n+    /** Create a Kinesis client in order to determine the number of shards for the given stream */\n+    val kinesisClient = new AmazonKinesisClient(new DefaultAWSCredentialsProviderChain())\n+    kinesisClient.setEndpoint(endpoint)\n+\n+    /** Determine the number of shards from the stream */\n+    val numShards = kinesisClient.describeStream(stream).getStreamDescription().getShards().size()\n+\n+    /** In this example, we're going to create 1 Kinesis Worker/Receiver/DStream for each shard.*/\n+    val numStreams = numShards\n+\n+    /** \n+     *  Must add 1 more thread than the number of receivers or the output won't show properly\n+     *   from the driver \n+     */\n+    val numSparkThreads = numStreams + 1\n+\n+    /** Set the app name */\n+    val appName = \"KinesisWordCount\"\n+\n+    /** Setup the Spark config. */\n+    val sparkConfig = new SparkConf().setAppName(appName).setMaster(s\"local[$numSparkThreads]\")\n+\n+    /**\n+     * Set the batch interval.\n+     * Records will be pulled from the Kinesis stream and stored as a single DStream within Spark\n+     *   every batch interval.\n+     */\n+    val batchInterval = Milliseconds(batchIntervalMillis)\n+\n+    /**\n+     * It's recommended that you perform a Spark checkpoint between 5 and 10 times the batch\n+     *   interval.\n+     * While this is the Spark checkpoint interval, we're going to use it for the Kinesis\n+     *   checkpoint interval, as well.\n+     * For example purposes, we'll just use the batchInterval.\n+     */\n+    val checkpointInterval = batchInterval\n+\n+    /** Setup the StreamingContext */\n+    val ssc = new StreamingContext(sparkConfig, batchInterval)\n+\n+    /** Setup the checkpoint directory used by Spark Streaming */\n+    ssc.checkpoint(\"/tmp/checkpoint\");\n+\n+    /** \n+     *  Create the same number of Kinesis Receivers/DStreams as stream shards, then union\n+     *   them all. \n+     */\n+    var allStreams: DStream[Array[Byte]] = KinesisUtils.createStream(ssc, appName, stream, \n+        endpoint, checkpointInterval.milliseconds, InitialPositionInStream.LATEST)\n+    /** Set the checkpoint interval */\n+    allStreams.checkpoint(checkpointInterval)\n+    for (i <- 1 until numStreams) {\n+      /** Create a new Receiver/DStream for each stream shard */\n+      val dStream = KinesisUtils.createStream(ssc, appName, stream, endpoint, \n+          checkpointInterval.milliseconds, InitialPositionInStream.LATEST)\n+      /** Set the Spark checkpoint interval */\n+      dStream.checkpoint(checkpointInterval)\n+\n+      /** Union with the existing streams */\n+      allStreams = allStreams.union(dStream)\n+    }\n+\n+    /** This implementation uses the String-based KinesisRecordSerializer impl */\n+    val recordSerializer = new KinesisStringRecordSerializer()\n+\n+    /**\n+     * Sort and print the given dstream.\n+     * This is an Output Operation that will materialize the underlying DStream.\n+     * Everything up to this point is a lazy Transformation Operation.\n+     * \n+     * @param description of the dstream for logging purposes\n+     * @param dstream to sort and print\n+     */\n+    def sortAndPrint(description: String, dstream: DStream[(String,Int)]) = {\n+      dstream.foreachRDD((batch, endOfWindowTime) => {\n+        val sortedBatch = batch.sortByKey(true)\n+          logInfo(s\"$description @ $endOfWindowTime\")\n+          sortedBatch.collect().foreach(\n+            wordCount => logInfo(s\"$wordCount\"))\n+        }\n+      )\n+    }\n+\n+    /**\n+     * Split each line of the union'd DStreams into multiple words using flatMap\n+     *   to produce the collection."
  }, {
    "author": {
      "login": "cfregly"
    },
    "body": "simplified similar to KafkaWordCount\n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-08-01T08:12:12Z",
    "diffHunk": "@@ -0,0 +1,369 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming\n+\n+import java.nio.ByteBuffer\n+\n+import scala.util.Random\n+\n+import org.apache.log4j.Level\n+import org.apache.log4j.Logger\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.SparkContext.rddToOrderedRDDFunctions\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.Milliseconds\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.streaming.StreamingContext.toPairDStreamFunctions\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.kinesis.KinesisStringRecordSerializer\n+import org.apache.spark.streaming.kinesis.KinesisUtils\n+\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n+import com.amazonaws.services.kinesis.AmazonKinesisClient\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream\n+import com.amazonaws.services.kinesis.model.PutRecordRequest\n+\n+/**\n+ * Kinesis Spark Streaming WordCount example.\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-programming-guide.html for more details on\n+ *   the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receivers) per shard of the\n+ *   given stream.\n+ * It then starts pulling from the last checkpointed sequence number of the given \n+ *   <stream-name> and <endpoint-url>. \n+ *\n+ * Valid endpoint urls:  http://docs.aws.amazon.com/general/latest/gr/rande.html#ak_region\n+ * \n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials\n+ *   in the following order of precedence:\n+ * Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ * Java System Properties - aws.accessKeyId and aws.secretKey\n+ * Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ * Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: KinesisWordCount <stream-name> <endpoint-url>\n+ *   <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *   <endpoint-url> is the endpoint of the Kinesis service\n+ *     (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *\n+ * Example:\n+ *    $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *    $ export AWS_SECRET_KEY=<your-secret-key>\n+ *    $ $SPARK_HOME/bin/run-example \\\n+ *        org.apache.spark.examples.streaming.KinesisWordCount mySparkStream \\\n+ *        https://kinesis.us-east-1.amazonaws.com\n+ *\n+ * There is a companion helper class below called KinesisWordCountProducer which puts\n+ *   dummy data onto the Kinesis stream.\n+ * Usage instructions for KinesisWordCountProducer are provided in that class definition.\n+ */\n+object KinesisWordCount extends Logging {\n+  val WordSeparator = \" \"\n+\n+  def main(args: Array[String]) {\n+/**\n+ * Check that all required args were passed in.\n+ */\n+    if (args.length < 2) {\n+      System.err.println(\"Usage: KinesisWordCount <stream-name> <endpoint-url>\")\n+      System.exit(1)\n+    }\n+\n+    /**\n+     * (This was lifted from the StreamingExamples.scala in order to avoid the dependency\n+     *   on the spark-examples artifact.)\n+     * Set reasonable logging levels for streaming if the user has not configured log4j.\n+     */\n+    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements()\n+    if (!log4jInitialized) {\n+      /** \n+       *  We first log something to initialize Spark's default logging, \n+       *  then we override the logging level. \n+       *  */\n+      logInfo(\"Setting log level to [INFO] for streaming example.\" +\n+        \" To override add a custom log4j.properties to the classpath.\")\n+\n+      Logger.getRootLogger().setLevel(Level.INFO)\n+      Logger.getLogger(\"org.apache.spark.examples.streaming\").setLevel(Level.DEBUG);\n+    }\n+\n+    /** Populate the appropriate variables from the given args */\n+    val Array(stream, endpoint) = args\n+    val batchIntervalMillis = 2000\n+\n+    /** Create a Kinesis client in order to determine the number of shards for the given stream */\n+    val kinesisClient = new AmazonKinesisClient(new DefaultAWSCredentialsProviderChain())\n+    kinesisClient.setEndpoint(endpoint)\n+\n+    /** Determine the number of shards from the stream */\n+    val numShards = kinesisClient.describeStream(stream).getStreamDescription().getShards().size()\n+\n+    /** In this example, we're going to create 1 Kinesis Worker/Receiver/DStream for each shard.*/\n+    val numStreams = numShards\n+\n+    /** \n+     *  Must add 1 more thread than the number of receivers or the output won't show properly\n+     *   from the driver \n+     */\n+    val numSparkThreads = numStreams + 1\n+\n+    /** Set the app name */\n+    val appName = \"KinesisWordCount\"\n+\n+    /** Setup the Spark config. */\n+    val sparkConfig = new SparkConf().setAppName(appName).setMaster(s\"local[$numSparkThreads]\")\n+\n+    /**\n+     * Set the batch interval.\n+     * Records will be pulled from the Kinesis stream and stored as a single DStream within Spark\n+     *   every batch interval.\n+     */\n+    val batchInterval = Milliseconds(batchIntervalMillis)\n+\n+    /**\n+     * It's recommended that you perform a Spark checkpoint between 5 and 10 times the batch\n+     *   interval.\n+     * While this is the Spark checkpoint interval, we're going to use it for the Kinesis\n+     *   checkpoint interval, as well.\n+     * For example purposes, we'll just use the batchInterval.\n+     */\n+    val checkpointInterval = batchInterval\n+\n+    /** Setup the StreamingContext */\n+    val ssc = new StreamingContext(sparkConfig, batchInterval)\n+\n+    /** Setup the checkpoint directory used by Spark Streaming */\n+    ssc.checkpoint(\"/tmp/checkpoint\");\n+\n+    /** \n+     *  Create the same number of Kinesis Receivers/DStreams as stream shards, then union\n+     *   them all. \n+     */\n+    var allStreams: DStream[Array[Byte]] = KinesisUtils.createStream(ssc, appName, stream, \n+        endpoint, checkpointInterval.milliseconds, InitialPositionInStream.LATEST)\n+    /** Set the checkpoint interval */\n+    allStreams.checkpoint(checkpointInterval)\n+    for (i <- 1 until numStreams) {\n+      /** Create a new Receiver/DStream for each stream shard */\n+      val dStream = KinesisUtils.createStream(ssc, appName, stream, endpoint, \n+          checkpointInterval.milliseconds, InitialPositionInStream.LATEST)\n+      /** Set the Spark checkpoint interval */\n+      dStream.checkpoint(checkpointInterval)\n+\n+      /** Union with the existing streams */\n+      allStreams = allStreams.union(dStream)\n+    }\n+\n+    /** This implementation uses the String-based KinesisRecordSerializer impl */\n+    val recordSerializer = new KinesisStringRecordSerializer()\n+\n+    /**\n+     * Sort and print the given dstream.\n+     * This is an Output Operation that will materialize the underlying DStream.\n+     * Everything up to this point is a lazy Transformation Operation.\n+     * \n+     * @param description of the dstream for logging purposes\n+     * @param dstream to sort and print\n+     */\n+    def sortAndPrint(description: String, dstream: DStream[(String,Int)]) = {\n+      dstream.foreachRDD((batch, endOfWindowTime) => {\n+        val sortedBatch = batch.sortByKey(true)\n+          logInfo(s\"$description @ $endOfWindowTime\")\n+          sortedBatch.collect().foreach(\n+            wordCount => logInfo(s\"$wordCount\"))\n+        }\n+      )\n+    }\n+\n+    /**\n+     * Split each line of the union'd DStreams into multiple words using flatMap\n+     *   to produce the collection."
  }],
  "prId": 1434
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Does this stream need to be checkpointed?\n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-07-31T20:38:58Z",
    "diffHunk": "@@ -0,0 +1,369 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming\n+\n+import java.nio.ByteBuffer\n+\n+import scala.util.Random\n+\n+import org.apache.log4j.Level\n+import org.apache.log4j.Logger\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.SparkContext.rddToOrderedRDDFunctions\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.Milliseconds\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.streaming.StreamingContext.toPairDStreamFunctions\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.kinesis.KinesisStringRecordSerializer\n+import org.apache.spark.streaming.kinesis.KinesisUtils\n+\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n+import com.amazonaws.services.kinesis.AmazonKinesisClient\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream\n+import com.amazonaws.services.kinesis.model.PutRecordRequest\n+\n+/**\n+ * Kinesis Spark Streaming WordCount example.\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-programming-guide.html for more details on\n+ *   the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receivers) per shard of the\n+ *   given stream.\n+ * It then starts pulling from the last checkpointed sequence number of the given \n+ *   <stream-name> and <endpoint-url>. \n+ *\n+ * Valid endpoint urls:  http://docs.aws.amazon.com/general/latest/gr/rande.html#ak_region\n+ * \n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials\n+ *   in the following order of precedence:\n+ * Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ * Java System Properties - aws.accessKeyId and aws.secretKey\n+ * Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ * Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: KinesisWordCount <stream-name> <endpoint-url>\n+ *   <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *   <endpoint-url> is the endpoint of the Kinesis service\n+ *     (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *\n+ * Example:\n+ *    $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *    $ export AWS_SECRET_KEY=<your-secret-key>\n+ *    $ $SPARK_HOME/bin/run-example \\\n+ *        org.apache.spark.examples.streaming.KinesisWordCount mySparkStream \\\n+ *        https://kinesis.us-east-1.amazonaws.com\n+ *\n+ * There is a companion helper class below called KinesisWordCountProducer which puts\n+ *   dummy data onto the Kinesis stream.\n+ * Usage instructions for KinesisWordCountProducer are provided in that class definition.\n+ */\n+object KinesisWordCount extends Logging {\n+  val WordSeparator = \" \"\n+\n+  def main(args: Array[String]) {\n+/**\n+ * Check that all required args were passed in.\n+ */\n+    if (args.length < 2) {\n+      System.err.println(\"Usage: KinesisWordCount <stream-name> <endpoint-url>\")\n+      System.exit(1)\n+    }\n+\n+    /**\n+     * (This was lifted from the StreamingExamples.scala in order to avoid the dependency\n+     *   on the spark-examples artifact.)\n+     * Set reasonable logging levels for streaming if the user has not configured log4j.\n+     */\n+    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements()\n+    if (!log4jInitialized) {\n+      /** \n+       *  We first log something to initialize Spark's default logging, \n+       *  then we override the logging level. \n+       *  */\n+      logInfo(\"Setting log level to [INFO] for streaming example.\" +\n+        \" To override add a custom log4j.properties to the classpath.\")\n+\n+      Logger.getRootLogger().setLevel(Level.INFO)\n+      Logger.getLogger(\"org.apache.spark.examples.streaming\").setLevel(Level.DEBUG);\n+    }\n+\n+    /** Populate the appropriate variables from the given args */\n+    val Array(stream, endpoint) = args\n+    val batchIntervalMillis = 2000\n+\n+    /** Create a Kinesis client in order to determine the number of shards for the given stream */\n+    val kinesisClient = new AmazonKinesisClient(new DefaultAWSCredentialsProviderChain())\n+    kinesisClient.setEndpoint(endpoint)\n+\n+    /** Determine the number of shards from the stream */\n+    val numShards = kinesisClient.describeStream(stream).getStreamDescription().getShards().size()\n+\n+    /** In this example, we're going to create 1 Kinesis Worker/Receiver/DStream for each shard.*/\n+    val numStreams = numShards\n+\n+    /** \n+     *  Must add 1 more thread than the number of receivers or the output won't show properly\n+     *   from the driver \n+     */\n+    val numSparkThreads = numStreams + 1\n+\n+    /** Set the app name */\n+    val appName = \"KinesisWordCount\"\n+\n+    /** Setup the Spark config. */\n+    val sparkConfig = new SparkConf().setAppName(appName).setMaster(s\"local[$numSparkThreads]\")\n+\n+    /**\n+     * Set the batch interval.\n+     * Records will be pulled from the Kinesis stream and stored as a single DStream within Spark\n+     *   every batch interval.\n+     */\n+    val batchInterval = Milliseconds(batchIntervalMillis)\n+\n+    /**\n+     * It's recommended that you perform a Spark checkpoint between 5 and 10 times the batch\n+     *   interval.\n+     * While this is the Spark checkpoint interval, we're going to use it for the Kinesis\n+     *   checkpoint interval, as well.\n+     * For example purposes, we'll just use the batchInterval.\n+     */\n+    val checkpointInterval = batchInterval\n+\n+    /** Setup the StreamingContext */\n+    val ssc = new StreamingContext(sparkConfig, batchInterval)\n+\n+    /** Setup the checkpoint directory used by Spark Streaming */\n+    ssc.checkpoint(\"/tmp/checkpoint\");\n+\n+    /** \n+     *  Create the same number of Kinesis Receivers/DStreams as stream shards, then union\n+     *   them all. \n+     */\n+    var allStreams: DStream[Array[Byte]] = KinesisUtils.createStream(ssc, appName, stream, \n+        endpoint, checkpointInterval.milliseconds, InitialPositionInStream.LATEST)\n+    /** Set the checkpoint interval */\n+    allStreams.checkpoint(checkpointInterval)\n+    for (i <- 1 until numStreams) {\n+      /** Create a new Receiver/DStream for each stream shard */\n+      val dStream = KinesisUtils.createStream(ssc, appName, stream, endpoint, \n+          checkpointInterval.milliseconds, InitialPositionInStream.LATEST)\n+      /** Set the Spark checkpoint interval */\n+      dStream.checkpoint(checkpointInterval)"
  }, {
    "author": {
      "login": "cfregly"
    },
    "body": "i removed the call to dstream.checkpoint()\n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-08-01T08:12:34Z",
    "diffHunk": "@@ -0,0 +1,369 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming\n+\n+import java.nio.ByteBuffer\n+\n+import scala.util.Random\n+\n+import org.apache.log4j.Level\n+import org.apache.log4j.Logger\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.SparkContext.rddToOrderedRDDFunctions\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.Milliseconds\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.streaming.StreamingContext.toPairDStreamFunctions\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.kinesis.KinesisStringRecordSerializer\n+import org.apache.spark.streaming.kinesis.KinesisUtils\n+\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n+import com.amazonaws.services.kinesis.AmazonKinesisClient\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream\n+import com.amazonaws.services.kinesis.model.PutRecordRequest\n+\n+/**\n+ * Kinesis Spark Streaming WordCount example.\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-programming-guide.html for more details on\n+ *   the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receivers) per shard of the\n+ *   given stream.\n+ * It then starts pulling from the last checkpointed sequence number of the given \n+ *   <stream-name> and <endpoint-url>. \n+ *\n+ * Valid endpoint urls:  http://docs.aws.amazon.com/general/latest/gr/rande.html#ak_region\n+ * \n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials\n+ *   in the following order of precedence:\n+ * Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ * Java System Properties - aws.accessKeyId and aws.secretKey\n+ * Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ * Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: KinesisWordCount <stream-name> <endpoint-url>\n+ *   <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *   <endpoint-url> is the endpoint of the Kinesis service\n+ *     (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *\n+ * Example:\n+ *    $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *    $ export AWS_SECRET_KEY=<your-secret-key>\n+ *    $ $SPARK_HOME/bin/run-example \\\n+ *        org.apache.spark.examples.streaming.KinesisWordCount mySparkStream \\\n+ *        https://kinesis.us-east-1.amazonaws.com\n+ *\n+ * There is a companion helper class below called KinesisWordCountProducer which puts\n+ *   dummy data onto the Kinesis stream.\n+ * Usage instructions for KinesisWordCountProducer are provided in that class definition.\n+ */\n+object KinesisWordCount extends Logging {\n+  val WordSeparator = \" \"\n+\n+  def main(args: Array[String]) {\n+/**\n+ * Check that all required args were passed in.\n+ */\n+    if (args.length < 2) {\n+      System.err.println(\"Usage: KinesisWordCount <stream-name> <endpoint-url>\")\n+      System.exit(1)\n+    }\n+\n+    /**\n+     * (This was lifted from the StreamingExamples.scala in order to avoid the dependency\n+     *   on the spark-examples artifact.)\n+     * Set reasonable logging levels for streaming if the user has not configured log4j.\n+     */\n+    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements()\n+    if (!log4jInitialized) {\n+      /** \n+       *  We first log something to initialize Spark's default logging, \n+       *  then we override the logging level. \n+       *  */\n+      logInfo(\"Setting log level to [INFO] for streaming example.\" +\n+        \" To override add a custom log4j.properties to the classpath.\")\n+\n+      Logger.getRootLogger().setLevel(Level.INFO)\n+      Logger.getLogger(\"org.apache.spark.examples.streaming\").setLevel(Level.DEBUG);\n+    }\n+\n+    /** Populate the appropriate variables from the given args */\n+    val Array(stream, endpoint) = args\n+    val batchIntervalMillis = 2000\n+\n+    /** Create a Kinesis client in order to determine the number of shards for the given stream */\n+    val kinesisClient = new AmazonKinesisClient(new DefaultAWSCredentialsProviderChain())\n+    kinesisClient.setEndpoint(endpoint)\n+\n+    /** Determine the number of shards from the stream */\n+    val numShards = kinesisClient.describeStream(stream).getStreamDescription().getShards().size()\n+\n+    /** In this example, we're going to create 1 Kinesis Worker/Receiver/DStream for each shard.*/\n+    val numStreams = numShards\n+\n+    /** \n+     *  Must add 1 more thread than the number of receivers or the output won't show properly\n+     *   from the driver \n+     */\n+    val numSparkThreads = numStreams + 1\n+\n+    /** Set the app name */\n+    val appName = \"KinesisWordCount\"\n+\n+    /** Setup the Spark config. */\n+    val sparkConfig = new SparkConf().setAppName(appName).setMaster(s\"local[$numSparkThreads]\")\n+\n+    /**\n+     * Set the batch interval.\n+     * Records will be pulled from the Kinesis stream and stored as a single DStream within Spark\n+     *   every batch interval.\n+     */\n+    val batchInterval = Milliseconds(batchIntervalMillis)\n+\n+    /**\n+     * It's recommended that you perform a Spark checkpoint between 5 and 10 times the batch\n+     *   interval.\n+     * While this is the Spark checkpoint interval, we're going to use it for the Kinesis\n+     *   checkpoint interval, as well.\n+     * For example purposes, we'll just use the batchInterval.\n+     */\n+    val checkpointInterval = batchInterval\n+\n+    /** Setup the StreamingContext */\n+    val ssc = new StreamingContext(sparkConfig, batchInterval)\n+\n+    /** Setup the checkpoint directory used by Spark Streaming */\n+    ssc.checkpoint(\"/tmp/checkpoint\");\n+\n+    /** \n+     *  Create the same number of Kinesis Receivers/DStreams as stream shards, then union\n+     *   them all. \n+     */\n+    var allStreams: DStream[Array[Byte]] = KinesisUtils.createStream(ssc, appName, stream, \n+        endpoint, checkpointInterval.milliseconds, InitialPositionInStream.LATEST)\n+    /** Set the checkpoint interval */\n+    allStreams.checkpoint(checkpointInterval)\n+    for (i <- 1 until numStreams) {\n+      /** Create a new Receiver/DStream for each stream shard */\n+      val dStream = KinesisUtils.createStream(ssc, appName, stream, endpoint, \n+          checkpointInterval.milliseconds, InitialPositionInStream.LATEST)\n+      /** Set the Spark checkpoint interval */\n+      dStream.checkpoint(checkpointInterval)"
  }],
  "prId": 1434
}, {
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "The better way to do this Union is to do the followings.\n\n```\nval kafkaStreams  = (0 until numStream).map { i =>\n    KinesisUtils.createStream(ssc, ...\n}\n\nval unifiedStream = ssc.union(kafkaStreams)\n```\n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-07-31T20:40:41Z",
    "diffHunk": "@@ -0,0 +1,369 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming\n+\n+import java.nio.ByteBuffer\n+\n+import scala.util.Random\n+\n+import org.apache.log4j.Level\n+import org.apache.log4j.Logger\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.SparkContext.rddToOrderedRDDFunctions\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.Milliseconds\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.streaming.StreamingContext.toPairDStreamFunctions\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.kinesis.KinesisStringRecordSerializer\n+import org.apache.spark.streaming.kinesis.KinesisUtils\n+\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n+import com.amazonaws.services.kinesis.AmazonKinesisClient\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream\n+import com.amazonaws.services.kinesis.model.PutRecordRequest\n+\n+/**\n+ * Kinesis Spark Streaming WordCount example.\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-programming-guide.html for more details on\n+ *   the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receivers) per shard of the\n+ *   given stream.\n+ * It then starts pulling from the last checkpointed sequence number of the given \n+ *   <stream-name> and <endpoint-url>. \n+ *\n+ * Valid endpoint urls:  http://docs.aws.amazon.com/general/latest/gr/rande.html#ak_region\n+ * \n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials\n+ *   in the following order of precedence:\n+ * Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ * Java System Properties - aws.accessKeyId and aws.secretKey\n+ * Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ * Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: KinesisWordCount <stream-name> <endpoint-url>\n+ *   <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *   <endpoint-url> is the endpoint of the Kinesis service\n+ *     (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *\n+ * Example:\n+ *    $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *    $ export AWS_SECRET_KEY=<your-secret-key>\n+ *    $ $SPARK_HOME/bin/run-example \\\n+ *        org.apache.spark.examples.streaming.KinesisWordCount mySparkStream \\\n+ *        https://kinesis.us-east-1.amazonaws.com\n+ *\n+ * There is a companion helper class below called KinesisWordCountProducer which puts\n+ *   dummy data onto the Kinesis stream.\n+ * Usage instructions for KinesisWordCountProducer are provided in that class definition.\n+ */\n+object KinesisWordCount extends Logging {\n+  val WordSeparator = \" \"\n+\n+  def main(args: Array[String]) {\n+/**\n+ * Check that all required args were passed in.\n+ */\n+    if (args.length < 2) {\n+      System.err.println(\"Usage: KinesisWordCount <stream-name> <endpoint-url>\")\n+      System.exit(1)\n+    }\n+\n+    /**\n+     * (This was lifted from the StreamingExamples.scala in order to avoid the dependency\n+     *   on the spark-examples artifact.)\n+     * Set reasonable logging levels for streaming if the user has not configured log4j.\n+     */\n+    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements()\n+    if (!log4jInitialized) {\n+      /** \n+       *  We first log something to initialize Spark's default logging, \n+       *  then we override the logging level. \n+       *  */\n+      logInfo(\"Setting log level to [INFO] for streaming example.\" +\n+        \" To override add a custom log4j.properties to the classpath.\")\n+\n+      Logger.getRootLogger().setLevel(Level.INFO)\n+      Logger.getLogger(\"org.apache.spark.examples.streaming\").setLevel(Level.DEBUG);\n+    }\n+\n+    /** Populate the appropriate variables from the given args */\n+    val Array(stream, endpoint) = args\n+    val batchIntervalMillis = 2000\n+\n+    /** Create a Kinesis client in order to determine the number of shards for the given stream */\n+    val kinesisClient = new AmazonKinesisClient(new DefaultAWSCredentialsProviderChain())\n+    kinesisClient.setEndpoint(endpoint)\n+\n+    /** Determine the number of shards from the stream */\n+    val numShards = kinesisClient.describeStream(stream).getStreamDescription().getShards().size()\n+\n+    /** In this example, we're going to create 1 Kinesis Worker/Receiver/DStream for each shard.*/\n+    val numStreams = numShards\n+\n+    /** \n+     *  Must add 1 more thread than the number of receivers or the output won't show properly\n+     *   from the driver \n+     */\n+    val numSparkThreads = numStreams + 1\n+\n+    /** Set the app name */\n+    val appName = \"KinesisWordCount\"\n+\n+    /** Setup the Spark config. */\n+    val sparkConfig = new SparkConf().setAppName(appName).setMaster(s\"local[$numSparkThreads]\")\n+\n+    /**\n+     * Set the batch interval.\n+     * Records will be pulled from the Kinesis stream and stored as a single DStream within Spark\n+     *   every batch interval.\n+     */\n+    val batchInterval = Milliseconds(batchIntervalMillis)\n+\n+    /**\n+     * It's recommended that you perform a Spark checkpoint between 5 and 10 times the batch\n+     *   interval.\n+     * While this is the Spark checkpoint interval, we're going to use it for the Kinesis\n+     *   checkpoint interval, as well.\n+     * For example purposes, we'll just use the batchInterval.\n+     */\n+    val checkpointInterval = batchInterval\n+\n+    /** Setup the StreamingContext */\n+    val ssc = new StreamingContext(sparkConfig, batchInterval)\n+\n+    /** Setup the checkpoint directory used by Spark Streaming */\n+    ssc.checkpoint(\"/tmp/checkpoint\");\n+\n+    /** \n+     *  Create the same number of Kinesis Receivers/DStreams as stream shards, then union\n+     *   them all. \n+     */\n+    var allStreams: DStream[Array[Byte]] = KinesisUtils.createStream(ssc, appName, stream, \n+        endpoint, checkpointInterval.milliseconds, InitialPositionInStream.LATEST)\n+    /** Set the checkpoint interval */\n+    allStreams.checkpoint(checkpointInterval)\n+    for (i <- 1 until numStreams) {\n+      /** Create a new Receiver/DStream for each stream shard */\n+      val dStream = KinesisUtils.createStream(ssc, appName, stream, endpoint, \n+          checkpointInterval.milliseconds, InitialPositionInStream.LATEST)\n+      /** Set the Spark checkpoint interval */\n+      dStream.checkpoint(checkpointInterval)\n+\n+      /** Union with the existing streams */\n+      allStreams = allStreams.union(dStream)"
  }, {
    "author": {
      "login": "cfregly"
    },
    "body": "very cool.  however, as part of the simplification effort, i remove this altogether.  i'm only creating 1 dstream\n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-08-01T08:13:11Z",
    "diffHunk": "@@ -0,0 +1,369 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming\n+\n+import java.nio.ByteBuffer\n+\n+import scala.util.Random\n+\n+import org.apache.log4j.Level\n+import org.apache.log4j.Logger\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.SparkContext.rddToOrderedRDDFunctions\n+import org.apache.spark.annotation.Experimental\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.Milliseconds\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.streaming.StreamingContext.toPairDStreamFunctions\n+import org.apache.spark.streaming.dstream.DStream\n+import org.apache.spark.streaming.kinesis.KinesisStringRecordSerializer\n+import org.apache.spark.streaming.kinesis.KinesisUtils\n+\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n+import com.amazonaws.services.kinesis.AmazonKinesisClient\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream\n+import com.amazonaws.services.kinesis.model.PutRecordRequest\n+\n+/**\n+ * Kinesis Spark Streaming WordCount example.\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-programming-guide.html for more details on\n+ *   the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receivers) per shard of the\n+ *   given stream.\n+ * It then starts pulling from the last checkpointed sequence number of the given \n+ *   <stream-name> and <endpoint-url>. \n+ *\n+ * Valid endpoint urls:  http://docs.aws.amazon.com/general/latest/gr/rande.html#ak_region\n+ * \n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials\n+ *   in the following order of precedence:\n+ * Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ * Java System Properties - aws.accessKeyId and aws.secretKey\n+ * Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ * Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: KinesisWordCount <stream-name> <endpoint-url>\n+ *   <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *   <endpoint-url> is the endpoint of the Kinesis service\n+ *     (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *\n+ * Example:\n+ *    $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *    $ export AWS_SECRET_KEY=<your-secret-key>\n+ *    $ $SPARK_HOME/bin/run-example \\\n+ *        org.apache.spark.examples.streaming.KinesisWordCount mySparkStream \\\n+ *        https://kinesis.us-east-1.amazonaws.com\n+ *\n+ * There is a companion helper class below called KinesisWordCountProducer which puts\n+ *   dummy data onto the Kinesis stream.\n+ * Usage instructions for KinesisWordCountProducer are provided in that class definition.\n+ */\n+object KinesisWordCount extends Logging {\n+  val WordSeparator = \" \"\n+\n+  def main(args: Array[String]) {\n+/**\n+ * Check that all required args were passed in.\n+ */\n+    if (args.length < 2) {\n+      System.err.println(\"Usage: KinesisWordCount <stream-name> <endpoint-url>\")\n+      System.exit(1)\n+    }\n+\n+    /**\n+     * (This was lifted from the StreamingExamples.scala in order to avoid the dependency\n+     *   on the spark-examples artifact.)\n+     * Set reasonable logging levels for streaming if the user has not configured log4j.\n+     */\n+    val log4jInitialized = Logger.getRootLogger.getAllAppenders.hasMoreElements()\n+    if (!log4jInitialized) {\n+      /** \n+       *  We first log something to initialize Spark's default logging, \n+       *  then we override the logging level. \n+       *  */\n+      logInfo(\"Setting log level to [INFO] for streaming example.\" +\n+        \" To override add a custom log4j.properties to the classpath.\")\n+\n+      Logger.getRootLogger().setLevel(Level.INFO)\n+      Logger.getLogger(\"org.apache.spark.examples.streaming\").setLevel(Level.DEBUG);\n+    }\n+\n+    /** Populate the appropriate variables from the given args */\n+    val Array(stream, endpoint) = args\n+    val batchIntervalMillis = 2000\n+\n+    /** Create a Kinesis client in order to determine the number of shards for the given stream */\n+    val kinesisClient = new AmazonKinesisClient(new DefaultAWSCredentialsProviderChain())\n+    kinesisClient.setEndpoint(endpoint)\n+\n+    /** Determine the number of shards from the stream */\n+    val numShards = kinesisClient.describeStream(stream).getStreamDescription().getShards().size()\n+\n+    /** In this example, we're going to create 1 Kinesis Worker/Receiver/DStream for each shard.*/\n+    val numStreams = numShards\n+\n+    /** \n+     *  Must add 1 more thread than the number of receivers or the output won't show properly\n+     *   from the driver \n+     */\n+    val numSparkThreads = numStreams + 1\n+\n+    /** Set the app name */\n+    val appName = \"KinesisWordCount\"\n+\n+    /** Setup the Spark config. */\n+    val sparkConfig = new SparkConf().setAppName(appName).setMaster(s\"local[$numSparkThreads]\")\n+\n+    /**\n+     * Set the batch interval.\n+     * Records will be pulled from the Kinesis stream and stored as a single DStream within Spark\n+     *   every batch interval.\n+     */\n+    val batchInterval = Milliseconds(batchIntervalMillis)\n+\n+    /**\n+     * It's recommended that you perform a Spark checkpoint between 5 and 10 times the batch\n+     *   interval.\n+     * While this is the Spark checkpoint interval, we're going to use it for the Kinesis\n+     *   checkpoint interval, as well.\n+     * For example purposes, we'll just use the batchInterval.\n+     */\n+    val checkpointInterval = batchInterval\n+\n+    /** Setup the StreamingContext */\n+    val ssc = new StreamingContext(sparkConfig, batchInterval)\n+\n+    /** Setup the checkpoint directory used by Spark Streaming */\n+    ssc.checkpoint(\"/tmp/checkpoint\");\n+\n+    /** \n+     *  Create the same number of Kinesis Receivers/DStreams as stream shards, then union\n+     *   them all. \n+     */\n+    var allStreams: DStream[Array[Byte]] = KinesisUtils.createStream(ssc, appName, stream, \n+        endpoint, checkpointInterval.milliseconds, InitialPositionInStream.LATEST)\n+    /** Set the checkpoint interval */\n+    allStreams.checkpoint(checkpointInterval)\n+    for (i <- 1 until numStreams) {\n+      /** Create a new Receiver/DStream for each stream shard */\n+      val dStream = KinesisUtils.createStream(ssc, appName, stream, endpoint, \n+          checkpointInterval.milliseconds, InitialPositionInStream.LATEST)\n+      /** Set the Spark checkpoint interval */\n+      dStream.checkpoint(checkpointInterval)\n+\n+      /** Union with the existing streams */\n+      allStreams = allStreams.union(dStream)"
  }],
  "prId": 1434
}]