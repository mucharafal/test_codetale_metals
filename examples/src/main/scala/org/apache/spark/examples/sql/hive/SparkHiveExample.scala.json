[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Do you not want the code below to render in the docs as part of the example? maybe not, just checking if that's intentional.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-19T13:44:00Z",
    "diffHunk": "@@ -104,6 +103,60 @@ object SparkHiveExample {\n     // ...\n     // $example off:spark_hive$"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@srowen Thank you for valueable feedback review, I have added that so it can help other develoeprs.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-19T16:02:17Z",
    "diffHunk": "@@ -104,6 +103,60 @@ object SparkHiveExample {\n     // ...\n     // $example off:spark_hive$"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@srowen Can you please review this cc\\ @holdenk @sameeragarwal ",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-20T06:04:03Z",
    "diffHunk": "@@ -104,6 +103,60 @@ object SparkHiveExample {\n     // ...\n     // $example off:spark_hive$"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@srowen I have updated DDL when storing data with parititoning in Hive.\r\ncc\\ @HyukjinKwon @mgaido91 @markgrover @markhamstra ",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-20T09:30:53Z",
    "diffHunk": "@@ -104,6 +103,60 @@ object SparkHiveExample {\n     // ...\n     // $example off:spark_hive$"
  }, {
    "author": {
      "login": "srowen"
    },
    "body": "Why do you turn the example listing off then on again? just remove those two lines",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-20T18:30:42Z",
    "diffHunk": "@@ -104,6 +103,60 @@ object SparkHiveExample {\n     // ...\n     // $example off:spark_hive$"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@srowen I mis-understood your first comment. I have reverted as suggested. Please check now",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-20T19:26:20Z",
    "diffHunk": "@@ -104,6 +103,60 @@ object SparkHiveExample {\n     // ...\n     // $example off:spark_hive$"
  }],
  "prId": 20018
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Oh just noticed this. You're using javadoc style comments here, but they won't have effect.\r\njust use the `//` block style for comments that you see above, for consistency.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-20T20:56:33Z",
    "diffHunk": "@@ -102,8 +101,63 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    /*"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "+1",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-21T07:20:25Z",
    "diffHunk": "@@ -102,8 +101,63 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    /*"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@srowen Done, changes addressed",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-21T20:05:22Z",
    "diffHunk": "@@ -102,8 +101,63 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    /*"
  }],
  "prId": 20018
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "This is more stuff that should go in docs, not comments in an example. It kind of duplicates existing documentation. Is this commentary really needed to illustrate usage of the API? that's the only goal right here. \r\n\r\nWhat are small-small files? You have some inconsistent capitalization; Parquet should be capitalized but not file, bandwidth, etc.\r\n",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-20T20:58:03Z",
    "diffHunk": "@@ -102,8 +101,63 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    /*\n+     * Save DataFrame to Hive Managed table as Parquet format\n+     * 1. Create Hive Database / Schema with location at HDFS if you want to mentioned explicitly else default\n+     * warehouse location will be used to store Hive table Data.\n+     * Ex: CREATE DATABASE IF NOT EXISTS database_name LOCATION hdfs_path;\n+     * You don't have to explicitly give location for each table, every tables under specified schema will be located at\n+     * location given while creating schema.\n+     * 2. Create Hive Managed table with storage format as 'Parquet'\n+     * Ex: CREATE TABLE records(key int, value string) STORED AS PARQUET;\n+     */\n+    val hiveTableDF = sql(\"SELECT * FROM records\").toDF()\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+\n+    /*\n+     * Save DataFrame to Hive External table as compatible parquet format.\n+     * 1. Create Hive External table with storage format as parquet.\n+     * Ex: CREATE EXTERNAL TABLE records(key int, value string) STORED AS PARQUET;\n+     * Since we are not explicitly providing hive database location, it automatically takes default warehouse location\n+     * given to 'spark.sql.warehouse.dir' while creating SparkSession with enableHiveSupport().\n+     * For example, we have given '/user/hive/warehouse/' as a Hive Warehouse location. It will create schema directories\n+     * under '/user/hive/warehouse/' as '/user/hive/warehouse/database_name.db' and '/user/hive/warehouse/database_name'.\n+     */\n+\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+    // Multiple parquet files could be created accordingly to volume of data under directory given.\n+    val hiveExternalTableLocation = s\"/user/hive/warehouse/database_name.db/records\"\n+    hiveTableDF.write.mode(SaveMode.Overwrite).parquet(hiveExternalTableLocation)\n+\n+    // turn on flag for Dynamic Partitioning\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition\", \"true\")\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n+    // You can create partitions in Hive table, so downstream queries run much faster.\n+    hiveTableDF.write.mode(SaveMode.Overwrite).partitionBy(\"key\")\n+      .parquet(hiveExternalTableLocation)\n+    /*\n+    If Data volume is very huge, then every partitions would have many small-small files which may harm"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@srowen I totally agree with you. I will rephrase content for docs. from here: i have removed as of now. please check and do needful.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-21T20:22:51Z",
    "diffHunk": "@@ -102,8 +101,63 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    /*\n+     * Save DataFrame to Hive Managed table as Parquet format\n+     * 1. Create Hive Database / Schema with location at HDFS if you want to mentioned explicitly else default\n+     * warehouse location will be used to store Hive table Data.\n+     * Ex: CREATE DATABASE IF NOT EXISTS database_name LOCATION hdfs_path;\n+     * You don't have to explicitly give location for each table, every tables under specified schema will be located at\n+     * location given while creating schema.\n+     * 2. Create Hive Managed table with storage format as 'Parquet'\n+     * Ex: CREATE TABLE records(key int, value string) STORED AS PARQUET;\n+     */\n+    val hiveTableDF = sql(\"SELECT * FROM records\").toDF()\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+\n+    /*\n+     * Save DataFrame to Hive External table as compatible parquet format.\n+     * 1. Create Hive External table with storage format as parquet.\n+     * Ex: CREATE EXTERNAL TABLE records(key int, value string) STORED AS PARQUET;\n+     * Since we are not explicitly providing hive database location, it automatically takes default warehouse location\n+     * given to 'spark.sql.warehouse.dir' while creating SparkSession with enableHiveSupport().\n+     * For example, we have given '/user/hive/warehouse/' as a Hive Warehouse location. It will create schema directories\n+     * under '/user/hive/warehouse/' as '/user/hive/warehouse/database_name.db' and '/user/hive/warehouse/database_name'.\n+     */\n+\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+    // Multiple parquet files could be created accordingly to volume of data under directory given.\n+    val hiveExternalTableLocation = s\"/user/hive/warehouse/database_name.db/records\"\n+    hiveTableDF.write.mode(SaveMode.Overwrite).parquet(hiveExternalTableLocation)\n+\n+    // turn on flag for Dynamic Partitioning\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition\", \"true\")\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n+    // You can create partitions in Hive table, so downstream queries run much faster.\n+    hiveTableDF.write.mode(SaveMode.Overwrite).partitionBy(\"key\")\n+      .parquet(hiveExternalTableLocation)\n+    /*\n+    If Data volume is very huge, then every partitions would have many small-small files which may harm"
  }],
  "prId": 20018
}, {
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "Sentences need some cleanup here. What do you mean by 'Int' argument? maybe it's best to point people to the API docs rather than incompletely repeat it.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-20T20:58:49Z",
    "diffHunk": "@@ -102,8 +101,63 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    /*\n+     * Save DataFrame to Hive Managed table as Parquet format\n+     * 1. Create Hive Database / Schema with location at HDFS if you want to mentioned explicitly else default\n+     * warehouse location will be used to store Hive table Data.\n+     * Ex: CREATE DATABASE IF NOT EXISTS database_name LOCATION hdfs_path;\n+     * You don't have to explicitly give location for each table, every tables under specified schema will be located at\n+     * location given while creating schema.\n+     * 2. Create Hive Managed table with storage format as 'Parquet'\n+     * Ex: CREATE TABLE records(key int, value string) STORED AS PARQUET;\n+     */\n+    val hiveTableDF = sql(\"SELECT * FROM records\").toDF()\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+\n+    /*\n+     * Save DataFrame to Hive External table as compatible parquet format.\n+     * 1. Create Hive External table with storage format as parquet.\n+     * Ex: CREATE EXTERNAL TABLE records(key int, value string) STORED AS PARQUET;\n+     * Since we are not explicitly providing hive database location, it automatically takes default warehouse location\n+     * given to 'spark.sql.warehouse.dir' while creating SparkSession with enableHiveSupport().\n+     * For example, we have given '/user/hive/warehouse/' as a Hive Warehouse location. It will create schema directories\n+     * under '/user/hive/warehouse/' as '/user/hive/warehouse/database_name.db' and '/user/hive/warehouse/database_name'.\n+     */\n+\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+    // Multiple parquet files could be created accordingly to volume of data under directory given.\n+    val hiveExternalTableLocation = s\"/user/hive/warehouse/database_name.db/records\"\n+    hiveTableDF.write.mode(SaveMode.Overwrite).parquet(hiveExternalTableLocation)\n+\n+    // turn on flag for Dynamic Partitioning\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition\", \"true\")\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n+    // You can create partitions in Hive table, so downstream queries run much faster.\n+    hiveTableDF.write.mode(SaveMode.Overwrite).partitionBy(\"key\")\n+      .parquet(hiveExternalTableLocation)\n+    /*\n+    If Data volume is very huge, then every partitions would have many small-small files which may harm\n+    downstream query performance due to File I/O, Bandwidth I/O, Network I/O, Disk I/O.\n+    To improve performance you can create single parquet file under each partition directory using 'repartition'\n+    on partitioned key for Hive table. When you add partition to table, there will be change in table DDL.\n+    Ex: CREATE TABLE records(value string) PARTITIONED BY(key int) STORED AS PARQUET;\n+     */\n+    hiveTableDF.repartition($\"key\").write.mode(SaveMode.Overwrite)\n+      .partitionBy(\"key\").parquet(hiveExternalTableLocation)\n+\n+    /*\n+     You can also do coalesce to control number of files under each partitions, repartition does full shuffle and equal\n+     data distribution to all partitions. here coalesce can reduce number of files to given 'Int' argument without"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@srowen done.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-21T20:24:50Z",
    "diffHunk": "@@ -102,8 +101,63 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    /*\n+     * Save DataFrame to Hive Managed table as Parquet format\n+     * 1. Create Hive Database / Schema with location at HDFS if you want to mentioned explicitly else default\n+     * warehouse location will be used to store Hive table Data.\n+     * Ex: CREATE DATABASE IF NOT EXISTS database_name LOCATION hdfs_path;\n+     * You don't have to explicitly give location for each table, every tables under specified schema will be located at\n+     * location given while creating schema.\n+     * 2. Create Hive Managed table with storage format as 'Parquet'\n+     * Ex: CREATE TABLE records(key int, value string) STORED AS PARQUET;\n+     */\n+    val hiveTableDF = sql(\"SELECT * FROM records\").toDF()\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+\n+    /*\n+     * Save DataFrame to Hive External table as compatible parquet format.\n+     * 1. Create Hive External table with storage format as parquet.\n+     * Ex: CREATE EXTERNAL TABLE records(key int, value string) STORED AS PARQUET;\n+     * Since we are not explicitly providing hive database location, it automatically takes default warehouse location\n+     * given to 'spark.sql.warehouse.dir' while creating SparkSession with enableHiveSupport().\n+     * For example, we have given '/user/hive/warehouse/' as a Hive Warehouse location. It will create schema directories\n+     * under '/user/hive/warehouse/' as '/user/hive/warehouse/database_name.db' and '/user/hive/warehouse/database_name'.\n+     */\n+\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+    // Multiple parquet files could be created accordingly to volume of data under directory given.\n+    val hiveExternalTableLocation = s\"/user/hive/warehouse/database_name.db/records\"\n+    hiveTableDF.write.mode(SaveMode.Overwrite).parquet(hiveExternalTableLocation)\n+\n+    // turn on flag for Dynamic Partitioning\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition\", \"true\")\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n+    // You can create partitions in Hive table, so downstream queries run much faster.\n+    hiveTableDF.write.mode(SaveMode.Overwrite).partitionBy(\"key\")\n+      .parquet(hiveExternalTableLocation)\n+    /*\n+    If Data volume is very huge, then every partitions would have many small-small files which may harm\n+    downstream query performance due to File I/O, Bandwidth I/O, Network I/O, Disk I/O.\n+    To improve performance you can create single parquet file under each partition directory using 'repartition'\n+    on partitioned key for Hive table. When you add partition to table, there will be change in table DDL.\n+    Ex: CREATE TABLE records(value string) PARTITIONED BY(key int) STORED AS PARQUET;\n+     */\n+    hiveTableDF.repartition($\"key\").write.mode(SaveMode.Overwrite)\n+      .partitionBy(\"key\").parquet(hiveExternalTableLocation)\n+\n+    /*\n+     You can also do coalesce to control number of files under each partitions, repartition does full shuffle and equal\n+     data distribution to all partitions. here coalesce can reduce number of files to given 'Int' argument without"
  }],
  "prId": 20018
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "`.toDF` is not needed",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-21T07:22:37Z",
    "diffHunk": "@@ -102,8 +101,63 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    /*\n+     * Save DataFrame to Hive Managed table as Parquet format\n+     * 1. Create Hive Database / Schema with location at HDFS if you want to mentioned explicitly else default\n+     * warehouse location will be used to store Hive table Data.\n+     * Ex: CREATE DATABASE IF NOT EXISTS database_name LOCATION hdfs_path;\n+     * You don't have to explicitly give location for each table, every tables under specified schema will be located at\n+     * location given while creating schema.\n+     * 2. Create Hive Managed table with storage format as 'Parquet'\n+     * Ex: CREATE TABLE records(key int, value string) STORED AS PARQUET;\n+     */\n+    val hiveTableDF = sql(\"SELECT * FROM records\").toDF()"
  }, {
    "author": {
      "login": "cloud-fan"
    },
    "body": "actually, I think `spark.table(\"records\")` is a better example.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-21T07:23:07Z",
    "diffHunk": "@@ -102,8 +101,63 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    /*\n+     * Save DataFrame to Hive Managed table as Parquet format\n+     * 1. Create Hive Database / Schema with location at HDFS if you want to mentioned explicitly else default\n+     * warehouse location will be used to store Hive table Data.\n+     * Ex: CREATE DATABASE IF NOT EXISTS database_name LOCATION hdfs_path;\n+     * You don't have to explicitly give location for each table, every tables under specified schema will be located at\n+     * location given while creating schema.\n+     * 2. Create Hive Managed table with storage format as 'Parquet'\n+     * Ex: CREATE TABLE records(key int, value string) STORED AS PARQUET;\n+     */\n+    val hiveTableDF = sql(\"SELECT * FROM records\").toDF()"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@srowen done cc\\ @cloud-fan removed toDF() ",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-21T20:14:10Z",
    "diffHunk": "@@ -102,8 +101,63 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    /*\n+     * Save DataFrame to Hive Managed table as Parquet format\n+     * 1. Create Hive Database / Schema with location at HDFS if you want to mentioned explicitly else default\n+     * warehouse location will be used to store Hive table Data.\n+     * Ex: CREATE DATABASE IF NOT EXISTS database_name LOCATION hdfs_path;\n+     * You don't have to explicitly give location for each table, every tables under specified schema will be located at\n+     * location given while creating schema.\n+     * 2. Create Hive Managed table with storage format as 'Parquet'\n+     * Ex: CREATE TABLE records(key int, value string) STORED AS PARQUET;\n+     */\n+    val hiveTableDF = sql(\"SELECT * FROM records\").toDF()"
  }],
  "prId": 20018
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "it's weird to create an external table without a location. User may be confused between the difference between managed table and external table.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-21T07:25:19Z",
    "diffHunk": "@@ -102,8 +101,63 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    /*\n+     * Save DataFrame to Hive Managed table as Parquet format\n+     * 1. Create Hive Database / Schema with location at HDFS if you want to mentioned explicitly else default\n+     * warehouse location will be used to store Hive table Data.\n+     * Ex: CREATE DATABASE IF NOT EXISTS database_name LOCATION hdfs_path;\n+     * You don't have to explicitly give location for each table, every tables under specified schema will be located at\n+     * location given while creating schema.\n+     * 2. Create Hive Managed table with storage format as 'Parquet'\n+     * Ex: CREATE TABLE records(key int, value string) STORED AS PARQUET;\n+     */\n+    val hiveTableDF = sql(\"SELECT * FROM records\").toDF()\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+\n+    /*\n+     * Save DataFrame to Hive External table as compatible parquet format.\n+     * 1. Create Hive External table with storage format as parquet.\n+     * Ex: CREATE EXTERNAL TABLE records(key int, value string) STORED AS PARQUET;"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@cloud-fan we'll keep all comments description at documentation with user friendly lines. I have added location also.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-21T20:14:59Z",
    "diffHunk": "@@ -102,8 +101,63 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    /*\n+     * Save DataFrame to Hive Managed table as Parquet format\n+     * 1. Create Hive Database / Schema with location at HDFS if you want to mentioned explicitly else default\n+     * warehouse location will be used to store Hive table Data.\n+     * Ex: CREATE DATABASE IF NOT EXISTS database_name LOCATION hdfs_path;\n+     * You don't have to explicitly give location for each table, every tables under specified schema will be located at\n+     * location given while creating schema.\n+     * 2. Create Hive Managed table with storage format as 'Parquet'\n+     * Ex: CREATE TABLE records(key int, value string) STORED AS PARQUET;\n+     */\n+    val hiveTableDF = sql(\"SELECT * FROM records\").toDF()\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+\n+    /*\n+     * Save DataFrame to Hive External table as compatible parquet format.\n+     * 1. Create Hive External table with storage format as parquet.\n+     * Ex: CREATE EXTERNAL TABLE records(key int, value string) STORED AS PARQUET;"
  }],
  "prId": 20018
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "This is not a standard usage, let's not put it in the example.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-21T07:25:47Z",
    "diffHunk": "@@ -102,8 +101,63 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    /*\n+     * Save DataFrame to Hive Managed table as Parquet format\n+     * 1. Create Hive Database / Schema with location at HDFS if you want to mentioned explicitly else default\n+     * warehouse location will be used to store Hive table Data.\n+     * Ex: CREATE DATABASE IF NOT EXISTS database_name LOCATION hdfs_path;\n+     * You don't have to explicitly give location for each table, every tables under specified schema will be located at\n+     * location given while creating schema.\n+     * 2. Create Hive Managed table with storage format as 'Parquet'\n+     * Ex: CREATE TABLE records(key int, value string) STORED AS PARQUET;\n+     */\n+    val hiveTableDF = sql(\"SELECT * FROM records\").toDF()\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+\n+    /*\n+     * Save DataFrame to Hive External table as compatible parquet format.\n+     * 1. Create Hive External table with storage format as parquet.\n+     * Ex: CREATE EXTERNAL TABLE records(key int, value string) STORED AS PARQUET;\n+     * Since we are not explicitly providing hive database location, it automatically takes default warehouse location\n+     * given to 'spark.sql.warehouse.dir' while creating SparkSession with enableHiveSupport().\n+     * For example, we have given '/user/hive/warehouse/' as a Hive Warehouse location. It will create schema directories\n+     * under '/user/hive/warehouse/' as '/user/hive/warehouse/database_name.db' and '/user/hive/warehouse/database_name'.\n+     */\n+\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+    // Multiple parquet files could be created accordingly to volume of data under directory given.\n+    val hiveExternalTableLocation = s\"/user/hive/warehouse/database_name.db/records\"\n+    hiveTableDF.write.mode(SaveMode.Overwrite).parquet(hiveExternalTableLocation)\n+\n+    // turn on flag for Dynamic Partitioning\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition\", \"true\")\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n+    // You can create partitions in Hive table, so downstream queries run much faster.\n+    hiveTableDF.write.mode(SaveMode.Overwrite).partitionBy(\"key\")\n+      .parquet(hiveExternalTableLocation)\n+    /*\n+    If Data volume is very huge, then every partitions would have many small-small files which may harm\n+    downstream query performance due to File I/O, Bandwidth I/O, Network I/O, Disk I/O.\n+    To improve performance you can create single parquet file under each partition directory using 'repartition'\n+    on partitioned key for Hive table. When you add partition to table, there will be change in table DDL.\n+    Ex: CREATE TABLE records(value string) PARTITIONED BY(key int) STORED AS PARQUET;\n+     */\n+    hiveTableDF.repartition($\"key\").write.mode(SaveMode.Overwrite)"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@cloud-fan removed all comments , as discussed with @srowen it does really make sense to have at docs with removed inconsitency.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-21T20:24:24Z",
    "diffHunk": "@@ -102,8 +101,63 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    /*\n+     * Save DataFrame to Hive Managed table as Parquet format\n+     * 1. Create Hive Database / Schema with location at HDFS if you want to mentioned explicitly else default\n+     * warehouse location will be used to store Hive table Data.\n+     * Ex: CREATE DATABASE IF NOT EXISTS database_name LOCATION hdfs_path;\n+     * You don't have to explicitly give location for each table, every tables under specified schema will be located at\n+     * location given while creating schema.\n+     * 2. Create Hive Managed table with storage format as 'Parquet'\n+     * Ex: CREATE TABLE records(key int, value string) STORED AS PARQUET;\n+     */\n+    val hiveTableDF = sql(\"SELECT * FROM records\").toDF()\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+\n+    /*\n+     * Save DataFrame to Hive External table as compatible parquet format.\n+     * 1. Create Hive External table with storage format as parquet.\n+     * Ex: CREATE EXTERNAL TABLE records(key int, value string) STORED AS PARQUET;\n+     * Since we are not explicitly providing hive database location, it automatically takes default warehouse location\n+     * given to 'spark.sql.warehouse.dir' while creating SparkSession with enableHiveSupport().\n+     * For example, we have given '/user/hive/warehouse/' as a Hive Warehouse location. It will create schema directories\n+     * under '/user/hive/warehouse/' as '/user/hive/warehouse/database_name.db' and '/user/hive/warehouse/database_name'.\n+     */\n+\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+    // Multiple parquet files could be created accordingly to volume of data under directory given.\n+    val hiveExternalTableLocation = s\"/user/hive/warehouse/database_name.db/records\"\n+    hiveTableDF.write.mode(SaveMode.Overwrite).parquet(hiveExternalTableLocation)\n+\n+    // turn on flag for Dynamic Partitioning\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition\", \"true\")\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n+    // You can create partitions in Hive table, so downstream queries run much faster.\n+    hiveTableDF.write.mode(SaveMode.Overwrite).partitionBy(\"key\")\n+      .parquet(hiveExternalTableLocation)\n+    /*\n+    If Data volume is very huge, then every partitions would have many small-small files which may harm\n+    downstream query performance due to File I/O, Bandwidth I/O, Network I/O, Disk I/O.\n+    To improve performance you can create single parquet file under each partition directory using 'repartition'\n+    on partitioned key for Hive table. When you add partition to table, there will be change in table DDL.\n+    Ex: CREATE TABLE records(value string) PARTITIONED BY(key int) STORED AS PARQUET;\n+     */\n+    hiveTableDF.repartition($\"key\").write.mode(SaveMode.Overwrite)"
  }],
  "prId": 20018
}, {
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "ditto",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-21T07:26:06Z",
    "diffHunk": "@@ -102,8 +101,63 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    /*\n+     * Save DataFrame to Hive Managed table as Parquet format\n+     * 1. Create Hive Database / Schema with location at HDFS if you want to mentioned explicitly else default\n+     * warehouse location will be used to store Hive table Data.\n+     * Ex: CREATE DATABASE IF NOT EXISTS database_name LOCATION hdfs_path;\n+     * You don't have to explicitly give location for each table, every tables under specified schema will be located at\n+     * location given while creating schema.\n+     * 2. Create Hive Managed table with storage format as 'Parquet'\n+     * Ex: CREATE TABLE records(key int, value string) STORED AS PARQUET;\n+     */\n+    val hiveTableDF = sql(\"SELECT * FROM records\").toDF()\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+\n+    /*\n+     * Save DataFrame to Hive External table as compatible parquet format.\n+     * 1. Create Hive External table with storage format as parquet.\n+     * Ex: CREATE EXTERNAL TABLE records(key int, value string) STORED AS PARQUET;\n+     * Since we are not explicitly providing hive database location, it automatically takes default warehouse location\n+     * given to 'spark.sql.warehouse.dir' while creating SparkSession with enableHiveSupport().\n+     * For example, we have given '/user/hive/warehouse/' as a Hive Warehouse location. It will create schema directories\n+     * under '/user/hive/warehouse/' as '/user/hive/warehouse/database_name.db' and '/user/hive/warehouse/database_name'.\n+     */\n+\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+    // Multiple parquet files could be created accordingly to volume of data under directory given.\n+    val hiveExternalTableLocation = s\"/user/hive/warehouse/database_name.db/records\"\n+    hiveTableDF.write.mode(SaveMode.Overwrite).parquet(hiveExternalTableLocation)\n+\n+    // turn on flag for Dynamic Partitioning\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition\", \"true\")\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n+    // You can create partitions in Hive table, so downstream queries run much faster.\n+    hiveTableDF.write.mode(SaveMode.Overwrite).partitionBy(\"key\")\n+      .parquet(hiveExternalTableLocation)\n+    /*\n+    If Data volume is very huge, then every partitions would have many small-small files which may harm\n+    downstream query performance due to File I/O, Bandwidth I/O, Network I/O, Disk I/O.\n+    To improve performance you can create single parquet file under each partition directory using 'repartition'\n+    on partitioned key for Hive table. When you add partition to table, there will be change in table DDL.\n+    Ex: CREATE TABLE records(value string) PARTITIONED BY(key int) STORED AS PARQUET;\n+     */\n+    hiveTableDF.repartition($\"key\").write.mode(SaveMode.Overwrite)\n+      .partitionBy(\"key\").parquet(hiveExternalTableLocation)\n+\n+    /*\n+     You can also do coalesce to control number of files under each partitions, repartition does full shuffle and equal\n+     data distribution to all partitions. here coalesce can reduce number of files to given 'Int' argument without\n+     full data shuffle.\n+     */\n+    // coalesce of 10 could create 10 parquet files under each partitions,\n+    // if data is huge and make sense to do partitioning.\n+    hiveTableDF.coalesce(10).write.mode(SaveMode.Overwrite)"
  }],
  "prId": 20018
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "`parquet` -> `Parquet`",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T00:25:04Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@HyukjinKwon Thanks for highlight, improved the same.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T09:17:50Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet"
  }],
  "prId": 20018
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "`Managed` -> `managed`",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T00:25:21Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet\n+    sql(\"CREATE TABLE records(key int, value string) STORED AS PARQUET\")\n+    // Save DataFrame to Hive Managed table as Parquet format"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@HyukjinKwon Thanks for highlight, improved the same.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T09:17:53Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet\n+    sql(\"CREATE TABLE records(key int, value string) STORED AS PARQUET\")\n+    // Save DataFrame to Hive Managed table as Parquet format"
  }],
  "prId": 20018
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "`parquet` ->`Parquet`.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T00:26:22Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet\n+    sql(\"CREATE TABLE records(key int, value string) STORED AS PARQUET\")\n+    // Save DataFrame to Hive Managed table as Parquet format\n+    val hiveTableDF = sql(\"SELECT * FROM records\")\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+    // Create External Hive table with parquet\n+    sql(\"CREATE EXTERNAL TABLE records(key int, value string) \" +\n+      \"STORED AS PARQUET LOCATION '/user/hive/warehouse/'\")\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+\n+    // Multiple parquet files could be created accordingly to volume of data under directory given.\n+    val hiveExternalTableLocation = \"/user/hive/warehouse/database_name.db/records\"\n+\n+    // Save DataFrame to Hive External table as compatible parquet format"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@HyukjinKwon Thanks for highlight, improved the same.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T09:18:06Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet\n+    sql(\"CREATE TABLE records(key int, value string) STORED AS PARQUET\")\n+    // Save DataFrame to Hive Managed table as Parquet format\n+    val hiveTableDF = sql(\"SELECT * FROM records\")\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+    // Create External Hive table with parquet\n+    sql(\"CREATE EXTERNAL TABLE records(key int, value string) \" +\n+      \"STORED AS PARQUET LOCATION '/user/hive/warehouse/'\")\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+\n+    // Multiple parquet files could be created accordingly to volume of data under directory given.\n+    val hiveExternalTableLocation = \"/user/hive/warehouse/database_name.db/records\"\n+\n+    // Save DataFrame to Hive External table as compatible parquet format"
  }],
  "prId": 20018
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "`parquet` ->`Parquet`",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T00:26:35Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet\n+    sql(\"CREATE TABLE records(key int, value string) STORED AS PARQUET\")\n+    // Save DataFrame to Hive Managed table as Parquet format\n+    val hiveTableDF = sql(\"SELECT * FROM records\")\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+    // Create External Hive table with parquet\n+    sql(\"CREATE EXTERNAL TABLE records(key int, value string) \" +\n+      \"STORED AS PARQUET LOCATION '/user/hive/warehouse/'\")\n+    // to make Hive parquet format compatible with spark parquet format"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "`spark` -> `Spark`",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T00:34:31Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet\n+    sql(\"CREATE TABLE records(key int, value string) STORED AS PARQUET\")\n+    // Save DataFrame to Hive Managed table as Parquet format\n+    val hiveTableDF = sql(\"SELECT * FROM records\")\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+    // Create External Hive table with parquet\n+    sql(\"CREATE EXTERNAL TABLE records(key int, value string) \" +\n+      \"STORED AS PARQUET LOCATION '/user/hive/warehouse/'\")\n+    // to make Hive parquet format compatible with spark parquet format"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@HyukjinKwon Thanks for highlight, improved the same.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T09:17:58Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet\n+    sql(\"CREATE TABLE records(key int, value string) STORED AS PARQUET\")\n+    // Save DataFrame to Hive Managed table as Parquet format\n+    val hiveTableDF = sql(\"SELECT * FROM records\")\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+    // Create External Hive table with parquet\n+    sql(\"CREATE EXTERNAL TABLE records(key int, value string) \" +\n+      \"STORED AS PARQUET LOCATION '/user/hive/warehouse/'\")\n+    // to make Hive parquet format compatible with spark parquet format"
  }],
  "prId": 20018
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "`parquet` -> `Parquet`.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T00:26:52Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet\n+    sql(\"CREATE TABLE records(key int, value string) STORED AS PARQUET\")\n+    // Save DataFrame to Hive Managed table as Parquet format\n+    val hiveTableDF = sql(\"SELECT * FROM records\")\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+    // Create External Hive table with parquet\n+    sql(\"CREATE EXTERNAL TABLE records(key int, value string) \" +\n+      \"STORED AS PARQUET LOCATION '/user/hive/warehouse/'\")\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+\n+    // Multiple parquet files could be created accordingly to volume of data under directory given."
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@HyukjinKwon Thanks for highlight, improved the same.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T09:18:02Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet\n+    sql(\"CREATE TABLE records(key int, value string) STORED AS PARQUET\")\n+    // Save DataFrame to Hive Managed table as Parquet format\n+    val hiveTableDF = sql(\"SELECT * FROM records\")\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+    // Create External Hive table with parquet\n+    sql(\"CREATE EXTERNAL TABLE records(key int, value string) \" +\n+      \"STORED AS PARQUET LOCATION '/user/hive/warehouse/'\")\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+\n+    // Multiple parquet files could be created accordingly to volume of data under directory given."
  }],
  "prId": 20018
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "`turn` -> `Turn`.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T00:27:47Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet\n+    sql(\"CREATE TABLE records(key int, value string) STORED AS PARQUET\")\n+    // Save DataFrame to Hive Managed table as Parquet format\n+    val hiveTableDF = sql(\"SELECT * FROM records\")\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+    // Create External Hive table with parquet\n+    sql(\"CREATE EXTERNAL TABLE records(key int, value string) \" +\n+      \"STORED AS PARQUET LOCATION '/user/hive/warehouse/'\")\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+\n+    // Multiple parquet files could be created accordingly to volume of data under directory given.\n+    val hiveExternalTableLocation = \"/user/hive/warehouse/database_name.db/records\"\n+\n+    // Save DataFrame to Hive External table as compatible parquet format\n+    hiveTableDF.write.mode(SaveMode.Overwrite).parquet(hiveExternalTableLocation)\n+\n+    // turn on flag for Dynamic Partitioning"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@HyukjinKwon Thanks for highlight, improved the same.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T09:18:10Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet\n+    sql(\"CREATE TABLE records(key int, value string) STORED AS PARQUET\")\n+    // Save DataFrame to Hive Managed table as Parquet format\n+    val hiveTableDF = sql(\"SELECT * FROM records\")\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+    // Create External Hive table with parquet\n+    sql(\"CREATE EXTERNAL TABLE records(key int, value string) \" +\n+      \"STORED AS PARQUET LOCATION '/user/hive/warehouse/'\")\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+\n+    // Multiple parquet files could be created accordingly to volume of data under directory given.\n+    val hiveExternalTableLocation = \"/user/hive/warehouse/database_name.db/records\"\n+\n+    // Save DataFrame to Hive External table as compatible parquet format\n+    hiveTableDF.write.mode(SaveMode.Overwrite).parquet(hiveExternalTableLocation)\n+\n+    // turn on flag for Dynamic Partitioning"
  }],
  "prId": 20018
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "`reduce` -> `Reduce`.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T00:28:10Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet\n+    sql(\"CREATE TABLE records(key int, value string) STORED AS PARQUET\")\n+    // Save DataFrame to Hive Managed table as Parquet format\n+    val hiveTableDF = sql(\"SELECT * FROM records\")\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+    // Create External Hive table with parquet\n+    sql(\"CREATE EXTERNAL TABLE records(key int, value string) \" +\n+      \"STORED AS PARQUET LOCATION '/user/hive/warehouse/'\")\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+\n+    // Multiple parquet files could be created accordingly to volume of data under directory given.\n+    val hiveExternalTableLocation = \"/user/hive/warehouse/database_name.db/records\"\n+\n+    // Save DataFrame to Hive External table as compatible parquet format\n+    hiveTableDF.write.mode(SaveMode.Overwrite).parquet(hiveExternalTableLocation)\n+\n+    // turn on flag for Dynamic Partitioning\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition\", \"true\")\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n+\n+    // You can create partitions in Hive table, so downstream queries run much faster.\n+    hiveTableDF.write.mode(SaveMode.Overwrite).partitionBy(\"key\")\n+      .parquet(hiveExternalTableLocation)\n+\n+    // reduce number of files for each partition by repartition"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@HyukjinKwon Thanks for highlight, improved the same.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T09:18:14Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet\n+    sql(\"CREATE TABLE records(key int, value string) STORED AS PARQUET\")\n+    // Save DataFrame to Hive Managed table as Parquet format\n+    val hiveTableDF = sql(\"SELECT * FROM records\")\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+    // Create External Hive table with parquet\n+    sql(\"CREATE EXTERNAL TABLE records(key int, value string) \" +\n+      \"STORED AS PARQUET LOCATION '/user/hive/warehouse/'\")\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+\n+    // Multiple parquet files could be created accordingly to volume of data under directory given.\n+    val hiveExternalTableLocation = \"/user/hive/warehouse/database_name.db/records\"\n+\n+    // Save DataFrame to Hive External table as compatible parquet format\n+    hiveTableDF.write.mode(SaveMode.Overwrite).parquet(hiveExternalTableLocation)\n+\n+    // turn on flag for Dynamic Partitioning\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition\", \"true\")\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n+\n+    // You can create partitions in Hive table, so downstream queries run much faster.\n+    hiveTableDF.write.mode(SaveMode.Overwrite).partitionBy(\"key\")\n+      .parquet(hiveExternalTableLocation)\n+\n+    // reduce number of files for each partition by repartition"
  }],
  "prId": 20018
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "` Control number of files` -> ` Control the number of files`",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T00:28:28Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet\n+    sql(\"CREATE TABLE records(key int, value string) STORED AS PARQUET\")\n+    // Save DataFrame to Hive Managed table as Parquet format\n+    val hiveTableDF = sql(\"SELECT * FROM records\")\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+    // Create External Hive table with parquet\n+    sql(\"CREATE EXTERNAL TABLE records(key int, value string) \" +\n+      \"STORED AS PARQUET LOCATION '/user/hive/warehouse/'\")\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+\n+    // Multiple parquet files could be created accordingly to volume of data under directory given.\n+    val hiveExternalTableLocation = \"/user/hive/warehouse/database_name.db/records\"\n+\n+    // Save DataFrame to Hive External table as compatible parquet format\n+    hiveTableDF.write.mode(SaveMode.Overwrite).parquet(hiveExternalTableLocation)\n+\n+    // turn on flag for Dynamic Partitioning\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition\", \"true\")\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n+\n+    // You can create partitions in Hive table, so downstream queries run much faster.\n+    hiveTableDF.write.mode(SaveMode.Overwrite).partitionBy(\"key\")\n+      .parquet(hiveExternalTableLocation)\n+\n+    // reduce number of files for each partition by repartition\n+    hiveTableDF.repartition($\"key\").write.mode(SaveMode.Overwrite)\n+      .partitionBy(\"key\").parquet(hiveExternalTableLocation)\n+\n+    // Control number of files in each partition by coalesce"
  }, {
    "author": {
      "login": "chetkhatri"
    },
    "body": "@HyukjinKwon Thanks for highlight, improved the same.",
    "commit": "c3dda1bd3445dc34e9c980d3f19ecd7abfc2ccc5",
    "createdAt": "2017-12-22T09:18:18Z",
    "diffHunk": "@@ -102,8 +101,41 @@ object SparkHiveExample {\n     // |  4| val_4|  4| val_4|\n     // |  5| val_5|  5| val_5|\n     // ...\n-    // $example off:spark_hive$\n \n+    // Create Hive managed table with parquet\n+    sql(\"CREATE TABLE records(key int, value string) STORED AS PARQUET\")\n+    // Save DataFrame to Hive Managed table as Parquet format\n+    val hiveTableDF = sql(\"SELECT * FROM records\")\n+    hiveTableDF.write.mode(SaveMode.Overwrite).saveAsTable(\"database_name.records\")\n+    // Create External Hive table with parquet\n+    sql(\"CREATE EXTERNAL TABLE records(key int, value string) \" +\n+      \"STORED AS PARQUET LOCATION '/user/hive/warehouse/'\")\n+    // to make Hive parquet format compatible with spark parquet format\n+    spark.sqlContext.setConf(\"spark.sql.parquet.writeLegacyFormat\", \"true\")\n+\n+    // Multiple parquet files could be created accordingly to volume of data under directory given.\n+    val hiveExternalTableLocation = \"/user/hive/warehouse/database_name.db/records\"\n+\n+    // Save DataFrame to Hive External table as compatible parquet format\n+    hiveTableDF.write.mode(SaveMode.Overwrite).parquet(hiveExternalTableLocation)\n+\n+    // turn on flag for Dynamic Partitioning\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition\", \"true\")\n+    spark.sqlContext.setConf(\"hive.exec.dynamic.partition.mode\", \"nonstrict\")\n+\n+    // You can create partitions in Hive table, so downstream queries run much faster.\n+    hiveTableDF.write.mode(SaveMode.Overwrite).partitionBy(\"key\")\n+      .parquet(hiveExternalTableLocation)\n+\n+    // reduce number of files for each partition by repartition\n+    hiveTableDF.repartition($\"key\").write.mode(SaveMode.Overwrite)\n+      .partitionBy(\"key\").parquet(hiveExternalTableLocation)\n+\n+    // Control number of files in each partition by coalesce"
  }],
  "prId": 20018
}]