[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Dont need checkpoint directory in this example.\n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-08-01T22:43:46Z",
    "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming\n+\n+import java.nio.ByteBuffer\n+\n+import scala.util.Random\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.Milliseconds\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.streaming.StreamingContext.toPairDStreamFunctions\n+import org.apache.spark.streaming.kinesis.KinesisUtils\n+\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n+import com.amazonaws.services.kinesis.AmazonKinesisClient\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream\n+import com.amazonaws.services.kinesis.model.PutRecordRequest\n+\n+/**\n+ * Kinesis Spark Streaming WordCount example.\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-kinesis.html for more details on\n+ *   the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receiver) per shard \n+ *   for the given stream.\n+ * It then starts pulling from the last checkpointed sequence number of the given \n+ *   <stream-name> and <endpoint-url>. \n+ *\n+ * Valid endpoint urls:  http://docs.aws.amazon.com/general/latest/gr/rande.html#ak_region\n+ * \n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials\n+ *   in the following order of precedence:\n+ * Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ * Java System Properties - aws.accessKeyId and aws.secretKey\n+ * Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ * Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: KinesisWordCountASL <stream-name> <endpoint-url>\n+ *   <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *   <endpoint-url> is the endpoint of the Kinesis service\n+ *     (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *\n+ * Example:\n+ *    $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *    $ export AWS_SECRET_KEY=<your-secret-key>\n+ *    $ $SPARK_HOME/bin/run-example \\\n+ *        org.apache.spark.examples.streaming.KinesisWordCountASL mySparkStream \\\n+ *        https://kinesis.us-east-1.amazonaws.com\n+ *\n+ * There is a companion helper class below called KinesisWordCountProducerASL which puts\n+ *   dummy data onto the Kinesis stream.\n+ * Usage instructions for KinesisWordCountProducerASL are provided in that class definition.\n+ */\n+object KinesisWordCountASL extends Logging {\n+  def main(args: Array[String]) {\n+/**\n+ * Check that all required args were passed in.\n+ */\n+    if (args.length < 2) {\n+      System.err.println(\n+        \"\"\"\n+          |Usage: KinesisWordCount <stream-name> <endpoint-url>\n+          |    <stream-name> is the name of the Kinesis stream\n+          |    <endpoint-url> is the endpoint of the Kinesis service\n+          |                   (e.g. https://kinesis.us-east-1.amazonaws.com)\n+        \"\"\".stripMargin)\n+      System.exit(1)\n+    }\n+\n+    StreamingExamples.setStreamingLogLevels()\n+    \n+    /** Populate the appropriate variables from the given args */\n+    val Array(streamName, endpointUrl) = args\n+\n+    /** Determine the number of shards from the stream */\n+    val kinesisClient = new AmazonKinesisClient(new DefaultAWSCredentialsProviderChain())\n+    kinesisClient.setEndpoint(endpointUrl)\n+    val numShards = kinesisClient.describeStream(streamName).getStreamDescription().getShards()\n+      .size()\n+\n+    /** In this example, we're going to create 1 Kinesis Worker/Receiver/DStream for each shard. */\n+    val numStreams = numShards\n+\n+    /** \n+     *  numSparkThreads should be 1 more thread than the number of receivers.\n+     *  This leaves one thread available for actually processing the data.\n+     */\n+    val numSparkThreads = numStreams + 1\n+\n+    /** Setup the and SparkConfig and StreamingContext */\n+    /** Spark Streaming batch interval */\n+    val batchInterval = Milliseconds(2000)    \n+    val sparkConfig = new SparkConf().setAppName(\"KinesisWordCount\")\n+      .setMaster(s\"local[$numSparkThreads]\")\n+    val ssc = new StreamingContext(sparkConfig, batchInterval)\n+    /** Setup the checkpoint directory used by Spark Streaming */\n+    ssc.checkpoint(\"/tmp/checkpoint\");"
  }, {
    "author": {
      "login": "cfregly"
    },
    "body": "removed\n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-08-02T06:45:36Z",
    "diffHunk": "@@ -0,0 +1,237 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming\n+\n+import java.nio.ByteBuffer\n+\n+import scala.util.Random\n+\n+import org.apache.spark.Logging\n+import org.apache.spark.SparkConf\n+import org.apache.spark.storage.StorageLevel\n+import org.apache.spark.streaming.Milliseconds\n+import org.apache.spark.streaming.StreamingContext\n+import org.apache.spark.streaming.StreamingContext.toPairDStreamFunctions\n+import org.apache.spark.streaming.kinesis.KinesisUtils\n+\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain\n+import com.amazonaws.services.kinesis.AmazonKinesisClient\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream\n+import com.amazonaws.services.kinesis.model.PutRecordRequest\n+\n+/**\n+ * Kinesis Spark Streaming WordCount example.\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-kinesis.html for more details on\n+ *   the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receiver) per shard \n+ *   for the given stream.\n+ * It then starts pulling from the last checkpointed sequence number of the given \n+ *   <stream-name> and <endpoint-url>. \n+ *\n+ * Valid endpoint urls:  http://docs.aws.amazon.com/general/latest/gr/rande.html#ak_region\n+ * \n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials\n+ *   in the following order of precedence:\n+ * Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ * Java System Properties - aws.accessKeyId and aws.secretKey\n+ * Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ * Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: KinesisWordCountASL <stream-name> <endpoint-url>\n+ *   <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *   <endpoint-url> is the endpoint of the Kinesis service\n+ *     (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *\n+ * Example:\n+ *    $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *    $ export AWS_SECRET_KEY=<your-secret-key>\n+ *    $ $SPARK_HOME/bin/run-example \\\n+ *        org.apache.spark.examples.streaming.KinesisWordCountASL mySparkStream \\\n+ *        https://kinesis.us-east-1.amazonaws.com\n+ *\n+ * There is a companion helper class below called KinesisWordCountProducerASL which puts\n+ *   dummy data onto the Kinesis stream.\n+ * Usage instructions for KinesisWordCountProducerASL are provided in that class definition.\n+ */\n+object KinesisWordCountASL extends Logging {\n+  def main(args: Array[String]) {\n+/**\n+ * Check that all required args were passed in.\n+ */\n+    if (args.length < 2) {\n+      System.err.println(\n+        \"\"\"\n+          |Usage: KinesisWordCount <stream-name> <endpoint-url>\n+          |    <stream-name> is the name of the Kinesis stream\n+          |    <endpoint-url> is the endpoint of the Kinesis service\n+          |                   (e.g. https://kinesis.us-east-1.amazonaws.com)\n+        \"\"\".stripMargin)\n+      System.exit(1)\n+    }\n+\n+    StreamingExamples.setStreamingLogLevels()\n+    \n+    /** Populate the appropriate variables from the given args */\n+    val Array(streamName, endpointUrl) = args\n+\n+    /** Determine the number of shards from the stream */\n+    val kinesisClient = new AmazonKinesisClient(new DefaultAWSCredentialsProviderChain())\n+    kinesisClient.setEndpoint(endpointUrl)\n+    val numShards = kinesisClient.describeStream(streamName).getStreamDescription().getShards()\n+      .size()\n+\n+    /** In this example, we're going to create 1 Kinesis Worker/Receiver/DStream for each shard. */\n+    val numStreams = numShards\n+\n+    /** \n+     *  numSparkThreads should be 1 more thread than the number of receivers.\n+     *  This leaves one thread available for actually processing the data.\n+     */\n+    val numSparkThreads = numStreams + 1\n+\n+    /** Setup the and SparkConfig and StreamingContext */\n+    /** Spark Streaming batch interval */\n+    val batchInterval = Milliseconds(2000)    \n+    val sparkConfig = new SparkConf().setAppName(\"KinesisWordCount\")\n+      .setMaster(s\"local[$numSparkThreads]\")\n+    val ssc = new StreamingContext(sparkConfig, batchInterval)\n+    /** Setup the checkpoint directory used by Spark Streaming */\n+    ssc.checkpoint(\"/tmp/checkpoint\");"
  }],
  "prId": 1434
}]