[{
  "comments": [{
    "author": {
      "login": "Tagar"
    },
    "body": "Does it make assumption that both `ms` and `us` can fit on each of the executors? \r\nHow well does it scale?\r\nThanks!\r\n",
    "commit": "d21d93be8c4541a62383b973efd5ce5b61eab989",
    "createdAt": "2018-06-27T22:42:47Z",
    "diffHunk": "@@ -0,0 +1,223 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regPenarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples\n+\n+import breeze.linalg.{DenseVector => BDV}\n+import org.apache.spark.SparkContext._\n+import org.apache.spark.broadcast.Broadcast\n+import org.apache.spark.mllib.linalg.distributed.MatrixEntry\n+import org.apache.spark.rdd.RDD\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+import scala.collection.BitSet\n+\n+/**\n+ * Generalized Low Rank Models for Spark\n+ *\n+ * Run these commands from the spark root directory.\n+ *\n+ * Compile with:\n+ * sbt/sbt assembly\n+ *\n+ * Run with:\n+ * ./bin/spark-submit  --class org.apache.spark.examples.SparkGLRM  \\\n+ * ./examples/target/scala-2.10/spark-examples-1.1.0-SNAPSHOT-hadoop1.0.4.jar \\\n+ * --executor-memory 1G \\\n+ * --driver-memory 1G\n+ */\n+\n+object SparkGLRM {\n+  /*********************************\n+   * GLRM: Bank of loss functions\n+   *********************************/\n+  def lossL2squaredGrad(i: Int, j: Int, prediction: Double, actual: Double): Double = {\n+    prediction - actual\n+  }\n+\n+  def lossL1Grad(i: Int, j: Int, prediction: Double, actual: Double): Double = {\n+    // a subgradient of L1\n+    math.signum(prediction - actual)\n+  }\n+\n+  def mixedLossGrad(i: Int, j: Int, prediction: Double, actual: Double): Double = {\n+    // weird loss function subgradient for demonstration\n+    if (i + j % 2 == 0) lossL1Grad(i, j, prediction, actual) else lossL2squaredGrad(i, j, prediction, actual)\n+  }\n+\n+  /***********************************\n+   * GLRM: Bank of prox functions\n+   **********************************/\n+  // L2 prox\n+  def proxL2(v:BDV[Double], stepSize:Double, regPen:Double): BDV[Double] = {\n+    val arr = v.toArray.map(x => x / (1.0 + stepSize * regPen))\n+    new BDV[Double](arr)\n+  }\n+\n+  // L1 prox\n+  def proxL1(v:BDV[Double], stepSize:Double, regPen:Double): BDV[Double] = {\n+    val sr = regPen * stepSize\n+    val arr = v.toArray.map(x =>\n+      if (math.abs(x) < sr) 0\n+      else if (x < -sr) x + sr\n+      else x - sr\n+    )\n+    new BDV[Double](arr)\n+  }\n+\n+  // Non-negative prox\n+  def proxNonneg(v:BDV[Double], stepSize:Double, regPen:Double): BDV[Double] = {\n+    val arr = v.toArray.map(x => math.max(x, 0))\n+    new BDV[Double](arr)\n+  }\n+\n+  /* End of GLRM libarry */\n+\n+\n+  // Helper functions for updating\n+  def computeLossGrads(ms: Broadcast[Array[BDV[Double]]], us: Broadcast[Array[BDV[Double]]],\n+                       R: RDD[(Int, Int, Double)],\n+                       lossGrad: (Int, Int, Double, Double) => Double) : RDD[(Int, Int, Double)] = {\n+    R.map { case (i, j, rij) => (i, j, lossGrad(i, j, ms.value(i).dot(us.value(j)), rij))}\n+  }\n+\n+  // Update factors\n+  def update(us: Broadcast[Array[BDV[Double]]], ms: Broadcast[Array[BDV[Double]]],\n+             lossGrads: RDD[(Int, Int, Double)], stepSize: Double,\n+             nnz: Array[Double],\n+             prox: (BDV[Double], Double, Double) => BDV[Double], regPen: Double)\n+  : Array[BDV[Double]] = {\n+    val rank = ms.value(0).length\n+    val ret = Array.fill(ms.value.size)(BDV.zeros[Double](rank))\n+\n+    val retu = lossGrads.map { case (i, j, lossij) => (i, us.value(j) * lossij) } // vector/scalar multiply\n+                .reduceByKey(_ + _).collect() // vector addition through breeze\n+\n+    for (entry <- retu) {\n+      val idx = entry._1\n+      val g = entry._2\n+      val alpha = (stepSize / (nnz(idx) + 1))\n+\n+      ret(idx) = prox(ms.value(idx) - g * alpha, alpha, regPen)\n+    }\n+\n+    ret\n+  }\n+\n+  def fitGLRM(R: RDD[(Int, Int, Double)], M:Int, U:Int,\n+              lossFunctionGrad: (Int, Int, Double, Double) => Double,\n+              moviesProx: (BDV[Double], Double, Double) => BDV[Double],\n+              usersProx: (BDV[Double], Double, Double) => BDV[Double],\n+              rank: Int,\n+              numIterations: Int,\n+              regPen: Double) : (Array[BDV[Double]], Array[BDV[Double]], Array[Double]) = {\n+    // Transpose data\n+    val RT = R.map { case (i, j, rij) => (j, i, rij) }.cache()\n+\n+    val sc = R.context\n+\n+    // Compute number of nonzeros per row and column\n+    val mCountRDD = R.map { case (i, j, rij) => (i, 1) }.reduceByKey(_ + _).collect()\n+    val mCount = Array.ofDim[Double](M)\n+    for (entry <- mCountRDD)\n+      mCount(entry._1) = entry._2\n+    val maxM = mCount.max\n+    val uCountRDD = R.map { case (i, j, rij) => (j, 1) }.reduceByKey(_ + _).collect()\n+    val uCount = Array.ofDim[Double](U)\n+    for (entry <- uCountRDD)\n+      uCount(entry._1) = entry._2\n+    val maxU = uCount.max\n+\n+    // Initialize m and u\n+    var ms = Array.fill(M)(BDV[Double](Array.tabulate(rank)(x => math.random / (M * U))))\n+    var us = Array.fill(U)(BDV[Double](Array.tabulate(rank)(x => math.random / (M * U))))\n+\n+    // Iteratively update movies then users\n+    var msb = sc.broadcast(ms)\n+    var usb = sc.broadcast(us)",
    "line": 151
  }],
  "prId": 13274
}]