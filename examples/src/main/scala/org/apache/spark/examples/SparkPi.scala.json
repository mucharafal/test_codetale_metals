[{
  "comments": [{
    "author": {
      "login": "srowen"
    },
    "body": "How about something like `math.min(100000L * slices, Int.MaxValue).toInt`? Or even just cap `slices` with `val slices = math.min(..., 21474)`, with a comment. I know it's a demo but maybe it just makes sense to cap the value of `slices` to something much lower anyway.\n",
    "commit": "62d7cd798a92b95bc686dc647a7b96ecd8db0bc2",
    "createdAt": "2014-10-21T09:46:26Z",
    "diffHunk": "@@ -27,7 +27,7 @@ object SparkPi {\n     val conf = new SparkConf().setAppName(\"Spark Pi\")\n     val spark = new SparkContext(conf)\n     val slices = if (args.length > 0) args(0).toInt else 2\n-    val n = 100000 * slices\n+    val n = if (100000.toLong * slices > Int.MaxValue) Int.MaxValue - 1 else 100000 * slices"
  }, {
    "author": {
      "login": "SaintBacchus"
    },
    "body": "@srowen when the value n is larger than the Int.MaxValue, it's better to set just a small value for slices.But user inputting a big slices may just wish that the SparkPi can run for more time.So I just  limit the size of Seqs and do not modify the parallelism of the app.\n",
    "commit": "62d7cd798a92b95bc686dc647a7b96ecd8db0bc2",
    "createdAt": "2014-10-21T15:31:01Z",
    "diffHunk": "@@ -27,7 +27,7 @@ object SparkPi {\n     val conf = new SparkConf().setAppName(\"Spark Pi\")\n     val spark = new SparkContext(conf)\n     val slices = if (args.length > 0) args(0).toInt else 2\n-    val n = 100000 * slices\n+    val n = if (100000.toLong * slices > Int.MaxValue) Int.MaxValue - 1 else 100000 * slices"
  }],
  "prId": 2874
}, {
  "comments": [{
    "author": {
      "login": "advancedxy"
    },
    "body": "Why Int.MaxValue - 1? I believe (1 to Int.MaxValue) will just work fine.\n",
    "commit": "62d7cd798a92b95bc686dc647a7b96ecd8db0bc2",
    "createdAt": "2014-10-22T06:42:02Z",
    "diffHunk": "@@ -27,7 +27,7 @@ object SparkPi {\n     val conf = new SparkConf().setAppName(\"Spark Pi\")\n     val spark = new SparkContext(conf)\n     val slices = if (args.length > 0) args(0).toInt else 2\n-    val n = 100000 * slices\n+    val n = math.min(100000L * slices, Int.MaxValue -1).toInt"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "yes, this can just be `Int.MaxValue`\n",
    "commit": "62d7cd798a92b95bc686dc647a7b96ecd8db0bc2",
    "createdAt": "2015-01-09T19:48:37Z",
    "diffHunk": "@@ -27,7 +27,7 @@ object SparkPi {\n     val conf = new SparkConf().setAppName(\"Spark Pi\")\n     val spark = new SparkContext(conf)\n     val slices = if (args.length > 0) args(0).toInt else 2\n-    val n = 100000 * slices\n+    val n = math.min(100000L * slices, Int.MaxValue -1).toInt"
  }, {
    "author": {
      "login": "andrewor14"
    },
    "body": "maybe also add a quick comment at the end to explain why we're doing this:\n\n```\nval n = math.min(100000L * slices, Int.MaxValue).toInt // avoid overflow\n```\n",
    "commit": "62d7cd798a92b95bc686dc647a7b96ecd8db0bc2",
    "createdAt": "2015-01-09T19:56:39Z",
    "diffHunk": "@@ -27,7 +27,7 @@ object SparkPi {\n     val conf = new SparkConf().setAppName(\"Spark Pi\")\n     val spark = new SparkContext(conf)\n     val slices = if (args.length > 0) args(0).toInt else 2\n-    val n = 100000 * slices\n+    val n = math.min(100000L * slices, Int.MaxValue -1).toInt"
  }],
  "prId": 2874
}, {
  "comments": [{
    "author": {
      "login": "advancedxy"
    },
    "body": "I think you need to supply a new slices number if slices is too large. \n something like math.min(slices, n / 100000).\n\nSo, a better solution may be :\n\n```\nval realSlices = math.min(slices, Int.MaxValue/100000)\nval n = 100000 * realSlices\n```\n",
    "commit": "62d7cd798a92b95bc686dc647a7b96ecd8db0bc2",
    "createdAt": "2014-10-22T06:46:25Z",
    "diffHunk": "@@ -27,7 +27,7 @@ object SparkPi {\n     val conf = new SparkConf().setAppName(\"Spark Pi\")\n     val spark = new SparkContext(conf)\n     val slices = if (args.length > 0) args(0).toInt else 2\n-    val n = 100000 * slices\n+    val n = math.min(100000L * slices, Int.MaxValue -1).toInt\n     val count = spark.parallelize(1 to n, slices).map { i =>"
  }],
  "prId": 2874
}]