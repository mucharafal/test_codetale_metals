[{
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "I'd prefer `$\"name\"` instead of `df(\"name\")` through out all Scala examples. The latter is not recommended because it may introduce ambiguous query plans when dealing with self-joins. The `$`-notation needs `import spark.implicits._` though.\n",
    "commit": "7451fc784d5c8f87c37f7707c4323a280d52417b",
    "createdAt": "2016-07-11T13:35:36Z",
    "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.sql\n+\n+// $example on:schema_inferring$\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.Encoder\n+// $example off:schema_inferring$\n+import org.apache.spark.sql.Row\n+// $example on:init_session$\n+import org.apache.spark.sql.SparkSession\n+// $example off:init_session$\n+// $example on:programmatic_schema$\n+import org.apache.spark.sql.types.StringType\n+import org.apache.spark.sql.types.StructField\n+import org.apache.spark.sql.types.StructType\n+// $example off:programmatic_schema$\n+\n+object SparkSqlExample {\n+\n+  // $example on:create_ds$\n+  // Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,\n+  // you can use custom classes that implement the Product interface\n+  case class Person(name: String, age: Long)\n+  // $example off:create_ds$\n+\n+  def main(args: Array[String]) {\n+    // $example on:init_session$\n+    val spark = SparkSession\n+        .builder()\n+        .appName(\"Spark SQL Example\")\n+        .config(\"spark.some.config.option\", \"some-value\")\n+        .getOrCreate()\n+\n+    // For implicit conversions like converting RDDs to DataFrames\n+    import spark.implicits._\n+    // $example off:init_session$\n+\n+    runBasicDataFrameExample(spark)\n+    runDatasetCreationExample(spark)\n+    runInferSchemaExample(spark)\n+    runProgrammaticSchemaExample(spark)\n+\n+    spark.stop()\n+  }\n+\n+  private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n+    // $example on:create_df$\n+    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+\n+    // Displays the content of the DataFrame to stdout\n+    df.show()\n+    // age  name\n+    // null Michael\n+    // 30   Andy\n+    // 19   Justin\n+    // $example off:create_df$\n+\n+    // $example on:untyped_ops$\n+    // Print the schema in a tree format\n+    df.printSchema()\n+    // root\n+    // |-- age: long (nullable = true)\n+    // |-- name: string (nullable = true)\n+\n+    // Select only the \"name\" column\n+    df.select(\"name\").show()\n+    // name\n+    // Michael\n+    // Andy\n+    // Justin\n+\n+    // Select everybody, but increment the age by 1\n+    df.select(df(\"name\"), df(\"age\") + 1).show()"
  }],
  "prId": 14119
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Nit: Use 2-space indentation here.\n",
    "commit": "7451fc784d5c8f87c37f7707c4323a280d52417b",
    "createdAt": "2016-07-11T14:07:15Z",
    "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.sql\n+\n+// $example on:schema_inferring$\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.Encoder\n+// $example off:schema_inferring$\n+import org.apache.spark.sql.Row\n+// $example on:init_session$\n+import org.apache.spark.sql.SparkSession\n+// $example off:init_session$\n+// $example on:programmatic_schema$\n+import org.apache.spark.sql.types.StringType\n+import org.apache.spark.sql.types.StructField\n+import org.apache.spark.sql.types.StructType\n+// $example off:programmatic_schema$\n+\n+object SparkSqlExample {\n+\n+  // $example on:create_ds$\n+  // Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,\n+  // you can use custom classes that implement the Product interface\n+  case class Person(name: String, age: Long)\n+  // $example off:create_ds$\n+\n+  def main(args: Array[String]) {\n+    // $example on:init_session$\n+    val spark = SparkSession\n+        .builder()\n+        .appName(\"Spark SQL Example\")\n+        .config(\"spark.some.config.option\", \"some-value\")\n+        .getOrCreate()"
  }],
  "prId": 14119
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Nit: Use 2-space indentation here.\n",
    "commit": "7451fc784d5c8f87c37f7707c4323a280d52417b",
    "createdAt": "2016-07-11T14:07:23Z",
    "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.sql\n+\n+// $example on:schema_inferring$\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.Encoder\n+// $example off:schema_inferring$\n+import org.apache.spark.sql.Row\n+// $example on:init_session$\n+import org.apache.spark.sql.SparkSession\n+// $example off:init_session$\n+// $example on:programmatic_schema$\n+import org.apache.spark.sql.types.StringType\n+import org.apache.spark.sql.types.StructField\n+import org.apache.spark.sql.types.StructType\n+// $example off:programmatic_schema$\n+\n+object SparkSqlExample {\n+\n+  // $example on:create_ds$\n+  // Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,\n+  // you can use custom classes that implement the Product interface\n+  case class Person(name: String, age: Long)\n+  // $example off:create_ds$\n+\n+  def main(args: Array[String]) {\n+    // $example on:init_session$\n+    val spark = SparkSession\n+        .builder()\n+        .appName(\"Spark SQL Example\")\n+        .config(\"spark.some.config.option\", \"some-value\")\n+        .getOrCreate()\n+\n+    // For implicit conversions like converting RDDs to DataFrames\n+    import spark.implicits._\n+    // $example off:init_session$\n+\n+    runBasicDataFrameExample(spark)\n+    runDatasetCreationExample(spark)\n+    runInferSchemaExample(spark)\n+    runProgrammaticSchemaExample(spark)\n+\n+    spark.stop()\n+  }\n+\n+  private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n+    // $example on:create_df$\n+    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+\n+    // Displays the content of the DataFrame to stdout\n+    df.show()\n+    // age  name\n+    // null Michael\n+    // 30   Andy\n+    // 19   Justin\n+    // $example off:create_df$\n+\n+    // $example on:untyped_ops$\n+    // Print the schema in a tree format\n+    df.printSchema()\n+    // root\n+    // |-- age: long (nullable = true)\n+    // |-- name: string (nullable = true)\n+\n+    // Select only the \"name\" column\n+    df.select(\"name\").show()\n+    // name\n+    // Michael\n+    // Andy\n+    // Justin\n+\n+    // Select everybody, but increment the age by 1\n+    df.select(df(\"name\"), df(\"age\") + 1).show()\n+    // name    (age + 1)\n+    // Michael null\n+    // Andy    31\n+    // Justin  20\n+\n+    // Select people older than 21\n+    df.filter(df(\"age\") > 21).show()\n+    // age name\n+    // 30  Andy\n+\n+    // Count people by age\n+    df.groupBy(\"age\").count().show()\n+    // age  count\n+    // null 1\n+    // 19   1\n+    // 30   1\n+    // $example off:untyped_ops$\n+\n+    // $example on:run_sql$\n+    // Register the DataFrame as a SQL temporary view\n+    df.createOrReplaceTempView(\"people\")\n+\n+    val sqlDF = spark.sql(\"SELECT * FROM people\")\n+    sqlDF.show()\n+    // $example off:run_sql$\n+  }\n+\n+  private def runDatasetCreationExample(spark: SparkSession): Unit = {\n+    import spark.implicits._\n+    // $example on:create_ds$\n+    // Encoders are created for case classes\n+    val caseClassDS = Seq(Person(\"Andy\", 32)).toDS()\n+    caseClassDS.show()\n+\n+    // Encoders for most common types are automatically provided by importing spark.implicits._\n+    val primitiveDS = Seq(1, 2, 3).toDS()\n+    primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)\n+\n+    // DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name\n+    val path = \"examples/src/main/resources/people.json\"\n+    val peopleDS = spark.read.json(path).as[Person]\n+    peopleDS.show()\n+    // $example off:create_ds$\n+  }\n+\n+  private def runInferSchemaExample(spark: SparkSession): Unit = {\n+    // $example on:schema_inferring$\n+    // For implicit conversions from RDDs to DataFrames\n+    import spark.implicits._\n+\n+    // Create an RDD of Person objects from a text file, convert it to a Dataframe\n+    val peopleDF = spark.sparkContext\n+        .textFile(\"examples/src/main/resources/people.txt\")\n+        .map(_.split(\",\"))\n+        .map(attributes => Person(attributes(0), attributes(1).trim.toInt))\n+        .toDF()"
  }],
  "prId": 14119
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Nit: Use 2-space indentation here.\n",
    "commit": "7451fc784d5c8f87c37f7707c4323a280d52417b",
    "createdAt": "2016-07-11T14:07:29Z",
    "diffHunk": "@@ -0,0 +1,201 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.sql\n+\n+// $example on:schema_inferring$\n+import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder\n+import org.apache.spark.sql.Encoder\n+// $example off:schema_inferring$\n+import org.apache.spark.sql.Row\n+// $example on:init_session$\n+import org.apache.spark.sql.SparkSession\n+// $example off:init_session$\n+// $example on:programmatic_schema$\n+import org.apache.spark.sql.types.StringType\n+import org.apache.spark.sql.types.StructField\n+import org.apache.spark.sql.types.StructType\n+// $example off:programmatic_schema$\n+\n+object SparkSqlExample {\n+\n+  // $example on:create_ds$\n+  // Note: Case classes in Scala 2.10 can support only up to 22 fields. To work around this limit,\n+  // you can use custom classes that implement the Product interface\n+  case class Person(name: String, age: Long)\n+  // $example off:create_ds$\n+\n+  def main(args: Array[String]) {\n+    // $example on:init_session$\n+    val spark = SparkSession\n+        .builder()\n+        .appName(\"Spark SQL Example\")\n+        .config(\"spark.some.config.option\", \"some-value\")\n+        .getOrCreate()\n+\n+    // For implicit conversions like converting RDDs to DataFrames\n+    import spark.implicits._\n+    // $example off:init_session$\n+\n+    runBasicDataFrameExample(spark)\n+    runDatasetCreationExample(spark)\n+    runInferSchemaExample(spark)\n+    runProgrammaticSchemaExample(spark)\n+\n+    spark.stop()\n+  }\n+\n+  private def runBasicDataFrameExample(spark: SparkSession): Unit = {\n+    // $example on:create_df$\n+    val df = spark.read.json(\"examples/src/main/resources/people.json\")\n+\n+    // Displays the content of the DataFrame to stdout\n+    df.show()\n+    // age  name\n+    // null Michael\n+    // 30   Andy\n+    // 19   Justin\n+    // $example off:create_df$\n+\n+    // $example on:untyped_ops$\n+    // Print the schema in a tree format\n+    df.printSchema()\n+    // root\n+    // |-- age: long (nullable = true)\n+    // |-- name: string (nullable = true)\n+\n+    // Select only the \"name\" column\n+    df.select(\"name\").show()\n+    // name\n+    // Michael\n+    // Andy\n+    // Justin\n+\n+    // Select everybody, but increment the age by 1\n+    df.select(df(\"name\"), df(\"age\") + 1).show()\n+    // name    (age + 1)\n+    // Michael null\n+    // Andy    31\n+    // Justin  20\n+\n+    // Select people older than 21\n+    df.filter(df(\"age\") > 21).show()\n+    // age name\n+    // 30  Andy\n+\n+    // Count people by age\n+    df.groupBy(\"age\").count().show()\n+    // age  count\n+    // null 1\n+    // 19   1\n+    // 30   1\n+    // $example off:untyped_ops$\n+\n+    // $example on:run_sql$\n+    // Register the DataFrame as a SQL temporary view\n+    df.createOrReplaceTempView(\"people\")\n+\n+    val sqlDF = spark.sql(\"SELECT * FROM people\")\n+    sqlDF.show()\n+    // $example off:run_sql$\n+  }\n+\n+  private def runDatasetCreationExample(spark: SparkSession): Unit = {\n+    import spark.implicits._\n+    // $example on:create_ds$\n+    // Encoders are created for case classes\n+    val caseClassDS = Seq(Person(\"Andy\", 32)).toDS()\n+    caseClassDS.show()\n+\n+    // Encoders for most common types are automatically provided by importing spark.implicits._\n+    val primitiveDS = Seq(1, 2, 3).toDS()\n+    primitiveDS.map(_ + 1).collect() // Returns: Array(2, 3, 4)\n+\n+    // DataFrames can be converted to a Dataset by providing a class. Mapping will be done by name\n+    val path = \"examples/src/main/resources/people.json\"\n+    val peopleDS = spark.read.json(path).as[Person]\n+    peopleDS.show()\n+    // $example off:create_ds$\n+  }\n+\n+  private def runInferSchemaExample(spark: SparkSession): Unit = {\n+    // $example on:schema_inferring$\n+    // For implicit conversions from RDDs to DataFrames\n+    import spark.implicits._\n+\n+    // Create an RDD of Person objects from a text file, convert it to a Dataframe\n+    val peopleDF = spark.sparkContext\n+        .textFile(\"examples/src/main/resources/people.txt\")\n+        .map(_.split(\",\"))\n+        .map(attributes => Person(attributes(0), attributes(1).trim.toInt))\n+        .toDF()\n+    // Register the DataFrame as a temporary view\n+    peopleDF.createOrReplaceTempView(\"people\")\n+\n+    // SQL statements can be run by using the sql methods provided by Spark\n+    val teenagersDF = spark.sql(\"SELECT name, age FROM people WHERE age BETWEEN 13 AND 19\")\n+\n+    // The columns of a row in the result can be accessed by field index\n+    teenagersDF.map(teenager => \"Name: \" + teenager(0)).show()\n+\n+    // or by field name\n+    teenagersDF.map(teenager => \"Name: \" + teenager.getAs[String](\"name\")).show()\n+\n+    // No pre-defined encoders for Dataset[Map[K,V]], define explicitly\n+    implicit val mapEncoder = org.apache.spark.sql.Encoders.kryo[Map[String, Any]]\n+    // Primitive types and case classes can be also defined as\n+    implicit val stringIntMapEncoder: Encoder[Map[String, Int]] = ExpressionEncoder()\n+\n+    // row.getValuesMap[T] retrieves multiple columns at once into a Map[String, T]\n+    teenagersDF.map(teenager => teenager.getValuesMap[Any](List(\"name\", \"age\"))).collect()\n+    // Array(Map(\"name\" -> \"Justin\", \"age\" -> 19))\n+    // $example off:schema_inferring$\n+  }\n+\n+  private def runProgrammaticSchemaExample(spark: SparkSession): Unit = {\n+    import spark.implicits._\n+    // $example on:programmatic_schema$\n+    // Create an RDD\n+    val peopleRDD = spark.sparkContext.textFile(\"examples/src/main/resources/people.txt\")\n+\n+    // The schema is encoded in a string\n+    val schemaString = \"name age\"\n+\n+    // Generate the schema based on the string of schema\n+    val fields = schemaString.split(\" \")\n+        .map(fieldName => StructField(fieldName, StringType, nullable = true))\n+    val schema = StructType(fields)\n+\n+    // Convert records of the RDD (people) to Rows\n+    val rowRDD = peopleRDD\n+        .map(_.split(\",\"))\n+        .map(attributes => Row(attributes(0), attributes(1).trim))"
  }],
  "prId": 14119
}]