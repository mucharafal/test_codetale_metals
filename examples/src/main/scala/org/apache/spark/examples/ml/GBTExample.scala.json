[{
  "comments": [{
    "author": {
      "login": "mengxr"
    },
    "body": "`>= 1` or document the behavior when `fracTest  == 1`.\n",
    "commit": "729167ab86e406a452509cb4d081efe378d8cacd",
    "createdAt": "2015-04-24T05:37:19Z",
    "diffHunk": "@@ -0,0 +1,238 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml\n+\n+import scala.collection.mutable\n+import scala.language.reflectiveCalls\n+\n+import scopt.OptionParser\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.examples.mllib.AbstractParams\n+import org.apache.spark.ml.{Pipeline, PipelineStage}\n+import org.apache.spark.ml.classification.{GBTClassificationModel, GBTClassifier}\n+import org.apache.spark.ml.feature.{StringIndexer, VectorIndexer}\n+import org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}\n+import org.apache.spark.sql.DataFrame\n+\n+\n+/**\n+ * An example runner for decision trees. Run with\n+ * {{{\n+ * ./bin/run-example ml.GBTExample [options]\n+ * }}}\n+ * Decision Trees and ensembles can take a large amount of memory.  If the run-example command\n+ * above fails, try running via spark-submit and specifying the amount of memory as at least 1g.\n+ * For local mode, run\n+ * {{{\n+ * ./bin/spark-submit --class org.apache.spark.examples.ml.GBTExample --driver-memory 1g\n+ *   [examples JAR path] [options]\n+ * }}}\n+ * If you use it as a template to create your own app, please use `spark-submit` to submit your app.\n+ */\n+object GBTExample {\n+\n+  case class Params(\n+      input: String = null,\n+      testInput: String = \"\",\n+      dataFormat: String = \"libsvm\",\n+      algo: String = \"classification\",\n+      maxDepth: Int = 5,\n+      maxBins: Int = 32,\n+      minInstancesPerNode: Int = 1,\n+      minInfoGain: Double = 0.0,\n+      maxIter: Int = 10,\n+      fracTest: Double = 0.2,\n+      cacheNodeIds: Boolean = false,\n+      checkpointDir: Option[String] = None,\n+      checkpointInterval: Int = 10) extends AbstractParams[Params]\n+\n+  def main(args: Array[String]) {\n+    val defaultParams = Params()\n+\n+    val parser = new OptionParser[Params](\"GBTExample\") {\n+      head(\"GBTExample: an example Gradient-Boosted Trees app.\")\n+      opt[String](\"algo\")\n+        .text(s\"algorithm (classification, regression), default: ${defaultParams.algo}\")\n+        .action((x, c) => c.copy(algo = x))\n+      opt[Int](\"maxDepth\")\n+        .text(s\"max depth of the tree, default: ${defaultParams.maxDepth}\")\n+        .action((x, c) => c.copy(maxDepth = x))\n+      opt[Int](\"maxBins\")\n+        .text(s\"max number of bins, default: ${defaultParams.maxBins}\")\n+        .action((x, c) => c.copy(maxBins = x))\n+      opt[Int](\"minInstancesPerNode\")\n+        .text(s\"min number of instances required at child nodes to create the parent split,\" +\n+        s\" default: ${defaultParams.minInstancesPerNode}\")\n+        .action((x, c) => c.copy(minInstancesPerNode = x))\n+      opt[Double](\"minInfoGain\")\n+        .text(s\"min info gain required to create a split, default: ${defaultParams.minInfoGain}\")\n+        .action((x, c) => c.copy(minInfoGain = x))\n+      opt[Int](\"maxIter\")\n+        .text(s\"number of trees in ensemble, default: ${defaultParams.maxIter}\")\n+        .action((x, c) => c.copy(maxIter = x))\n+      opt[Double](\"fracTest\")\n+        .text(s\"fraction of data to hold out for testing.  If given option testInput, \" +\n+        s\"this option is ignored. default: ${defaultParams.fracTest}\")\n+        .action((x, c) => c.copy(fracTest = x))\n+      opt[Boolean](\"cacheNodeIds\")\n+        .text(s\"whether to use node Id cache during training, \" +\n+        s\"default: ${defaultParams.cacheNodeIds}\")\n+        .action((x, c) => c.copy(cacheNodeIds = x))\n+      opt[String](\"checkpointDir\")\n+        .text(s\"checkpoint directory where intermediate node Id caches will be stored, \" +\n+        s\"default: ${\n+          defaultParams.checkpointDir match {\n+            case Some(strVal) => strVal\n+            case None => \"None\"\n+          }\n+        }\")\n+        .action((x, c) => c.copy(checkpointDir = Some(x)))\n+      opt[Int](\"checkpointInterval\")\n+        .text(s\"how often to checkpoint the node Id cache, \" +\n+        s\"default: ${defaultParams.checkpointInterval}\")\n+        .action((x, c) => c.copy(checkpointInterval = x))\n+      opt[String](\"testInput\")\n+        .text(s\"input path to test dataset.  If given, option fracTest is ignored.\" +\n+        s\" default: ${defaultParams.testInput}\")\n+        .action((x, c) => c.copy(testInput = x))\n+      opt[String](\"<dataFormat>\")\n+        .text(\"data format: libsvm (default), dense (deprecated in Spark v1.1)\")\n+        .action((x, c) => c.copy(dataFormat = x))\n+      arg[String](\"<input>\")\n+        .text(\"input path to labeled examples\")\n+        .required()\n+        .action((x, c) => c.copy(input = x))\n+      checkConfig { params =>\n+        if (params.fracTest < 0 || params.fracTest > 1) {"
  }],
  "prId": 5626
}]