[{
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "Ditto about input format being a text file of count vectors\n",
    "commit": "c794096d7ae52a91b577640d27f480860f4b31d0",
    "createdAt": "2015-11-18T14:51:57Z",
    "diffHunk": "@@ -0,0 +1,82 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml\n+\n+import org.apache.spark.{SparkContext, SparkConf}\n+import org.apache.spark.mllib.linalg.{VectorUDT, Vectors}\n+// $example on$\n+import org.apache.spark.ml.clustering.LDA\n+import org.apache.spark.sql.{Row, SQLContext}\n+import org.apache.spark.sql.types.{StructField, StructType}\n+// $example off$\n+\n+/**\n+ * An example demonstrating a LDA of ML pipeline.\n+ * Run with\n+ * {{{\n+ * bin/run-example ml.LDAExample <file> <k>\n+ * }}}\n+ */\n+object LDAExample {\n+\n+  final val FEATURES_COL = \"features\"\n+\n+  def main(args: Array[String]): Unit = {\n+    if (args.length != 2) {\n+      // scalastyle:off println\n+      System.err.println(\"Usage: ml.LDAExample <file> <k>\")\n+      // scalastyle:on println\n+      System.exit(1)\n+    }\n+    val input = args(0)\n+    val k = args(1).toInt\n+\n+    // Creates a Spark context and a SQL context\n+    val conf = new SparkConf().setAppName(s\"${this.getClass.getSimpleName}\")\n+    val sc = new SparkContext(conf)\n+    val sqlContext = new SQLContext(sc)\n+\n+    // $example on$\n+    // Loads data\n+    val rowRDD = sc.textFile(input).filter(_.nonEmpty)\n+      .map(_.split(\" \").map(_.toDouble)).map(Vectors.dense).map(Row(_))",
    "line": 51
  }],
  "prId": 9722
}]