[{
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "Can we revert these NOOP changes (here and `stopwordFile`) since there's no style guide violation here?\n",
    "commit": "672ce97892c974994630527e0e724090b3ac52e5",
    "createdAt": "2015-09-14T23:43:08Z",
    "diffHunk": "@@ -186,121 +186,52 @@ object LDAExample {\n    * Load documents, tokenize them, create vocabulary, and prepare documents as term count vectors.\n    * @return (corpus, vocabulary as array, total token count in corpus)\n    */\n-  private def preprocess(\n+  private def preProcess("
  }],
  "prId": 8551
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "`SQLContext.getOrCreate`\n",
    "commit": "672ce97892c974994630527e0e724090b3ac52e5",
    "createdAt": "2015-09-14T23:43:45Z",
    "diffHunk": "@@ -186,121 +186,52 @@ object LDAExample {\n    * Load documents, tokenize them, create vocabulary, and prepare documents as term count vectors.\n    * @return (corpus, vocabulary as array, total token count in corpus)\n    */\n-  private def preprocess(\n+  private def preProcess(\n       sc: SparkContext,\n       paths: Seq[String],\n       vocabSize: Int,\n-      stopwordFile: String): (RDD[(Long, Vector)], Array[String], Long) = {\n+      stopWordFile: String): (RDD[(Long, Vector)], Array[String], Long) = {\n \n     // Get dataset of document texts\n     // One document per line in each text file. If the input consists of many small files,\n     // this can result in a large number of small partitions, which can degrade performance.\n     // In this case, consider using coalesce() to create fewer, larger partitions.\n     val textRDD: RDD[String] = sc.textFile(paths.mkString(\",\"))\n-\n-    // Split text into words\n-    val tokenizer = new SimpleTokenizer(sc, stopwordFile)\n-    val tokenized: RDD[(Long, IndexedSeq[String])] = textRDD.zipWithIndex().map { case (text, id) =>\n-      id -> tokenizer.getWords(text)\n-    }\n-    tokenized.cache()\n-\n-    // Counts words: RDD[(word, wordCount)]\n-    val wordCounts: RDD[(String, Long)] = tokenized\n-      .flatMap { case (_, tokens) => tokens.map(_ -> 1L) }\n-      .reduceByKey(_ + _)\n-    wordCounts.cache()\n-    val fullVocabSize = wordCounts.count()\n-    // Select vocab\n-    //  (vocab: Map[word -> id], total tokens after selecting vocab)\n-    val (vocab: Map[String, Int], selectedTokenCount: Long) = {\n-      val tmpSortedWC: Array[(String, Long)] = if (vocabSize == -1 || fullVocabSize <= vocabSize) {\n-        // Use all terms\n-        wordCounts.collect().sortBy(-_._2)\n-      } else {\n-        // Sort terms to select vocab\n-        wordCounts.sortBy(_._2, ascending = false).take(vocabSize)\n-      }\n-      (tmpSortedWC.map(_._1).zipWithIndex.toMap, tmpSortedWC.map(_._2).sum)\n+    val sqlContext = new SQLContext(sc)"
  }],
  "prId": 8551
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "nit: I would put this sqlContext instantiation and implicits import right at the start of the method (on L194)\n",
    "commit": "672ce97892c974994630527e0e724090b3ac52e5",
    "createdAt": "2015-09-14T23:44:16Z",
    "diffHunk": "@@ -186,121 +186,52 @@ object LDAExample {\n    * Load documents, tokenize them, create vocabulary, and prepare documents as term count vectors.\n    * @return (corpus, vocabulary as array, total token count in corpus)\n    */\n-  private def preprocess(\n+  private def preProcess(\n       sc: SparkContext,\n       paths: Seq[String],\n       vocabSize: Int,\n-      stopwordFile: String): (RDD[(Long, Vector)], Array[String], Long) = {\n+      stopWordFile: String): (RDD[(Long, Vector)], Array[String], Long) = {\n \n     // Get dataset of document texts\n     // One document per line in each text file. If the input consists of many small files,\n     // this can result in a large number of small partitions, which can degrade performance.\n     // In this case, consider using coalesce() to create fewer, larger partitions.\n     val textRDD: RDD[String] = sc.textFile(paths.mkString(\",\"))\n-\n-    // Split text into words\n-    val tokenizer = new SimpleTokenizer(sc, stopwordFile)\n-    val tokenized: RDD[(Long, IndexedSeq[String])] = textRDD.zipWithIndex().map { case (text, id) =>\n-      id -> tokenizer.getWords(text)\n-    }\n-    tokenized.cache()\n-\n-    // Counts words: RDD[(word, wordCount)]\n-    val wordCounts: RDD[(String, Long)] = tokenized\n-      .flatMap { case (_, tokens) => tokens.map(_ -> 1L) }\n-      .reduceByKey(_ + _)\n-    wordCounts.cache()\n-    val fullVocabSize = wordCounts.count()\n-    // Select vocab\n-    //  (vocab: Map[word -> id], total tokens after selecting vocab)\n-    val (vocab: Map[String, Int], selectedTokenCount: Long) = {\n-      val tmpSortedWC: Array[(String, Long)] = if (vocabSize == -1 || fullVocabSize <= vocabSize) {\n-        // Use all terms\n-        wordCounts.collect().sortBy(-_._2)\n-      } else {\n-        // Sort terms to select vocab\n-        wordCounts.sortBy(_._2, ascending = false).take(vocabSize)\n-      }\n-      (tmpSortedWC.map(_._1).zipWithIndex.toMap, tmpSortedWC.map(_._2).sum)\n+    val sqlContext = new SQLContext(sc)\n+    import sqlContext.implicits._"
  }],
  "prId": 8551
}, {
  "comments": [{
    "author": {
      "login": "feynmanliang"
    },
    "body": "After moving where implicit import is, this can just be `sc.textFile(paths.mkString(\",\")).toDF(\"texts\")`.\n",
    "commit": "672ce97892c974994630527e0e724090b3ac52e5",
    "createdAt": "2015-09-14T23:44:54Z",
    "diffHunk": "@@ -186,121 +186,52 @@ object LDAExample {\n    * Load documents, tokenize them, create vocabulary, and prepare documents as term count vectors.\n    * @return (corpus, vocabulary as array, total token count in corpus)\n    */\n-  private def preprocess(\n+  private def preProcess(\n       sc: SparkContext,\n       paths: Seq[String],\n       vocabSize: Int,\n-      stopwordFile: String): (RDD[(Long, Vector)], Array[String], Long) = {\n+      stopWordFile: String): (RDD[(Long, Vector)], Array[String], Long) = {\n \n     // Get dataset of document texts\n     // One document per line in each text file. If the input consists of many small files,\n     // this can result in a large number of small partitions, which can degrade performance.\n     // In this case, consider using coalesce() to create fewer, larger partitions.\n     val textRDD: RDD[String] = sc.textFile(paths.mkString(\",\"))\n-\n-    // Split text into words\n-    val tokenizer = new SimpleTokenizer(sc, stopwordFile)\n-    val tokenized: RDD[(Long, IndexedSeq[String])] = textRDD.zipWithIndex().map { case (text, id) =>\n-      id -> tokenizer.getWords(text)\n-    }\n-    tokenized.cache()\n-\n-    // Counts words: RDD[(word, wordCount)]\n-    val wordCounts: RDD[(String, Long)] = tokenized\n-      .flatMap { case (_, tokens) => tokens.map(_ -> 1L) }\n-      .reduceByKey(_ + _)\n-    wordCounts.cache()\n-    val fullVocabSize = wordCounts.count()\n-    // Select vocab\n-    //  (vocab: Map[word -> id], total tokens after selecting vocab)\n-    val (vocab: Map[String, Int], selectedTokenCount: Long) = {\n-      val tmpSortedWC: Array[(String, Long)] = if (vocabSize == -1 || fullVocabSize <= vocabSize) {\n-        // Use all terms\n-        wordCounts.collect().sortBy(-_._2)\n-      } else {\n-        // Sort terms to select vocab\n-        wordCounts.sortBy(_._2, ascending = false).take(vocabSize)\n-      }\n-      (tmpSortedWC.map(_._1).zipWithIndex.toMap, tmpSortedWC.map(_._2).sum)\n+    val sqlContext = new SQLContext(sc)\n+    import sqlContext.implicits._\n+\n+    val df = textRDD.toDF(\"texts\")"
  }, {
    "author": {
      "login": "feynmanliang"
    },
    "body": "Also a small nit: I would call them `docs` instead of `texts` to be consistent with user guide\n",
    "commit": "672ce97892c974994630527e0e724090b3ac52e5",
    "createdAt": "2015-09-14T23:45:10Z",
    "diffHunk": "@@ -186,121 +186,52 @@ object LDAExample {\n    * Load documents, tokenize them, create vocabulary, and prepare documents as term count vectors.\n    * @return (corpus, vocabulary as array, total token count in corpus)\n    */\n-  private def preprocess(\n+  private def preProcess(\n       sc: SparkContext,\n       paths: Seq[String],\n       vocabSize: Int,\n-      stopwordFile: String): (RDD[(Long, Vector)], Array[String], Long) = {\n+      stopWordFile: String): (RDD[(Long, Vector)], Array[String], Long) = {\n \n     // Get dataset of document texts\n     // One document per line in each text file. If the input consists of many small files,\n     // this can result in a large number of small partitions, which can degrade performance.\n     // In this case, consider using coalesce() to create fewer, larger partitions.\n     val textRDD: RDD[String] = sc.textFile(paths.mkString(\",\"))\n-\n-    // Split text into words\n-    val tokenizer = new SimpleTokenizer(sc, stopwordFile)\n-    val tokenized: RDD[(Long, IndexedSeq[String])] = textRDD.zipWithIndex().map { case (text, id) =>\n-      id -> tokenizer.getWords(text)\n-    }\n-    tokenized.cache()\n-\n-    // Counts words: RDD[(word, wordCount)]\n-    val wordCounts: RDD[(String, Long)] = tokenized\n-      .flatMap { case (_, tokens) => tokens.map(_ -> 1L) }\n-      .reduceByKey(_ + _)\n-    wordCounts.cache()\n-    val fullVocabSize = wordCounts.count()\n-    // Select vocab\n-    //  (vocab: Map[word -> id], total tokens after selecting vocab)\n-    val (vocab: Map[String, Int], selectedTokenCount: Long) = {\n-      val tmpSortedWC: Array[(String, Long)] = if (vocabSize == -1 || fullVocabSize <= vocabSize) {\n-        // Use all terms\n-        wordCounts.collect().sortBy(-_._2)\n-      } else {\n-        // Sort terms to select vocab\n-        wordCounts.sortBy(_._2, ascending = false).take(vocabSize)\n-      }\n-      (tmpSortedWC.map(_._1).zipWithIndex.toMap, tmpSortedWC.map(_._2).sum)\n+    val sqlContext = new SQLContext(sc)\n+    import sqlContext.implicits._\n+\n+    val df = textRDD.toDF(\"texts\")"
  }],
  "prId": 8551
}, {
  "comments": [{
    "author": {
      "login": "BenFradet"
    },
    "body": "Are you sure about removing the scalastyle line?\n",
    "commit": "672ce97892c974994630527e0e724090b3ac52e5",
    "createdAt": "2015-12-07T20:40:31Z",
    "diffHunk": "@@ -192,115 +192,45 @@ object LDAExample {\n       vocabSize: Int,\n       stopwordFile: String): (RDD[(Long, Vector)], Array[String], Long) = {\n \n+    val sqlContext = SQLContext.getOrCreate(sc)\n+    import sqlContext.implicits._\n+\n     // Get dataset of document texts\n     // One document per line in each text file. If the input consists of many small files,\n     // this can result in a large number of small partitions, which can degrade performance.\n     // In this case, consider using coalesce() to create fewer, larger partitions.\n-    val textRDD: RDD[String] = sc.textFile(paths.mkString(\",\"))\n-\n-    // Split text into words\n-    val tokenizer = new SimpleTokenizer(sc, stopwordFile)\n-    val tokenized: RDD[(Long, IndexedSeq[String])] = textRDD.zipWithIndex().map { case (text, id) =>\n-      id -> tokenizer.getWords(text)\n-    }\n-    tokenized.cache()\n-\n-    // Counts words: RDD[(word, wordCount)]\n-    val wordCounts: RDD[(String, Long)] = tokenized\n-      .flatMap { case (_, tokens) => tokens.map(_ -> 1L) }\n-      .reduceByKey(_ + _)\n-    wordCounts.cache()\n-    val fullVocabSize = wordCounts.count()\n-    // Select vocab\n-    //  (vocab: Map[word -> id], total tokens after selecting vocab)\n-    val (vocab: Map[String, Int], selectedTokenCount: Long) = {\n-      val tmpSortedWC: Array[(String, Long)] = if (vocabSize == -1 || fullVocabSize <= vocabSize) {\n-        // Use all terms\n-        wordCounts.collect().sortBy(-_._2)\n-      } else {\n-        // Sort terms to select vocab\n-        wordCounts.sortBy(_._2, ascending = false).take(vocabSize)\n-      }\n-      (tmpSortedWC.map(_._1).zipWithIndex.toMap, tmpSortedWC.map(_._2).sum)\n+    val df = sc.textFile(paths.mkString(\",\")).toDF(\"docs\")\n+    val customizedStopWords: Array[String] = if (stopwordFile.isEmpty) {\n+      Array.empty[String]\n+    } else {\n+      val stopWordText = sc.textFile(stopwordFile).collect()\n+      stopWordText.flatMap(_.stripMargin.split(\"\\\\s+\"))\n     }\n-\n-    val documents = tokenized.map { case (id, tokens) =>\n-      // Filter tokens by vocabulary, and create word count vector representation of document.\n-      val wc = new mutable.HashMap[Int, Int]()\n-      tokens.foreach { term =>\n-        if (vocab.contains(term)) {\n-          val termIndex = vocab(term)\n-          wc(termIndex) = wc.getOrElse(termIndex, 0) + 1\n-        }\n-      }\n-      val indices = wc.keys.toArray.sorted\n-      val values = indices.map(i => wc(i).toDouble)\n-\n-      val sb = Vectors.sparse(vocab.size, indices, values)\n-      (id, sb)\n-    }\n-\n-    val vocabArray = new Array[String](vocab.size)\n-    vocab.foreach { case (term, i) => vocabArray(i) = term }\n-\n-    (documents, vocabArray, selectedTokenCount)\n+    val tokenizer = new RegexTokenizer()\n+      .setInputCol(\"docs\")\n+      .setOutputCol(\"rawTokens\")\n+    val stopWordsRemover = new StopWordsRemover()\n+      .setInputCol(\"rawTokens\")\n+      .setOutputCol(\"tokens\")\n+    stopWordsRemover.setStopWords(stopWordsRemover.getStopWords ++ customizedStopWords)\n+    val countVectorizer = new CountVectorizer()\n+      .setVocabSize(vocabSize)\n+      .setInputCol(\"tokens\")\n+      .setOutputCol(\"vectors\")\n+\n+    val pipeline = new Pipeline()\n+      .setStages(Array(tokenizer, stopWordsRemover, countVectorizer))\n+\n+    val model = pipeline.fit(df)\n+    val documents = model.transform(df)\n+      .select(\"vectors\")\n+      .map { case Row(features: Vector) => features }\n+      .zipWithIndex()\n+      .map(_.swap)\n+\n+    (documents,\n+      model.stages(2).asInstanceOf[CountVectorizerModel].vocabulary,  // vocabulary\n+      documents.map(_._2.numActives).sum().toLong) // total token count\n   }\n }\n \n-/**\n- * Simple Tokenizer.\n- *\n- * TODO: Formalize the interface, and make this a public class in mllib.feature\n- */\n-private class SimpleTokenizer(sc: SparkContext, stopwordFile: String) extends Serializable {\n-\n-  private val stopwords: Set[String] = if (stopwordFile.isEmpty) {\n-    Set.empty[String]\n-  } else {\n-    val stopwordText = sc.textFile(stopwordFile).collect()\n-    stopwordText.flatMap(_.stripMargin.split(\"\\\\s+\")).toSet\n-  }\n-\n-  // Matches sequences of Unicode letters\n-  private val allWordRegex = \"^(\\\\p{L}*)$\".r\n-\n-  // Ignore words shorter than this length.\n-  private val minWordLength = 3\n-\n-  def getWords(text: String): IndexedSeq[String] = {\n-\n-    val words = new mutable.ArrayBuffer[String]()\n-\n-    // Use Java BreakIterator to tokenize text into words.\n-    val wb = BreakIterator.getWordInstance\n-    wb.setText(text)\n-\n-    // current,end index start,end of each word\n-    var current = wb.first()\n-    var end = wb.next()\n-    while (end != BreakIterator.DONE) {\n-      // Convert to lowercase\n-      val word: String = text.substring(current, end).toLowerCase\n-      // Remove short words and strings that aren't only letters\n-      word match {\n-        case allWordRegex(w) if w.length >= minWordLength && !stopwords.contains(w) =>\n-          words += w\n-        case _ =>\n-      }\n-\n-      current = end\n-      try {\n-        end = wb.next()\n-      } catch {\n-        case e: Exception =>\n-          // Ignore remaining text in line.\n-          // This is a known bug in BreakIterator (for some Java versions),\n-          // which fails when it sees certain characters.\n-          end = BreakIterator.DONE\n-      }\n-    }\n-    words\n-  }\n-\n-}\n-// scalastyle:on println"
  }],
  "prId": 8551
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Organize imports\n",
    "commit": "672ce97892c974994630527e0e724090b3ac52e5",
    "createdAt": "2015-12-07T20:43:30Z",
    "diffHunk": "@@ -18,9 +18,9 @@\n // scalastyle:off println\n package org.apache.spark.examples.mllib\n \n-import java.text.BreakIterator\n-\n-import scala.collection.mutable\n+import org.apache.spark.ml.Pipeline"
  }],
  "prId": 8551
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "How about calling these \"features\" instead of \"vectors\"?\n",
    "commit": "672ce97892c974994630527e0e724090b3ac52e5",
    "createdAt": "2015-12-07T20:45:02Z",
    "diffHunk": "@@ -192,115 +192,45 @@ object LDAExample {\n       vocabSize: Int,\n       stopwordFile: String): (RDD[(Long, Vector)], Array[String], Long) = {\n \n+    val sqlContext = SQLContext.getOrCreate(sc)\n+    import sqlContext.implicits._\n+\n     // Get dataset of document texts\n     // One document per line in each text file. If the input consists of many small files,\n     // this can result in a large number of small partitions, which can degrade performance.\n     // In this case, consider using coalesce() to create fewer, larger partitions.\n-    val textRDD: RDD[String] = sc.textFile(paths.mkString(\",\"))\n-\n-    // Split text into words\n-    val tokenizer = new SimpleTokenizer(sc, stopwordFile)\n-    val tokenized: RDD[(Long, IndexedSeq[String])] = textRDD.zipWithIndex().map { case (text, id) =>\n-      id -> tokenizer.getWords(text)\n-    }\n-    tokenized.cache()\n-\n-    // Counts words: RDD[(word, wordCount)]\n-    val wordCounts: RDD[(String, Long)] = tokenized\n-      .flatMap { case (_, tokens) => tokens.map(_ -> 1L) }\n-      .reduceByKey(_ + _)\n-    wordCounts.cache()\n-    val fullVocabSize = wordCounts.count()\n-    // Select vocab\n-    //  (vocab: Map[word -> id], total tokens after selecting vocab)\n-    val (vocab: Map[String, Int], selectedTokenCount: Long) = {\n-      val tmpSortedWC: Array[(String, Long)] = if (vocabSize == -1 || fullVocabSize <= vocabSize) {\n-        // Use all terms\n-        wordCounts.collect().sortBy(-_._2)\n-      } else {\n-        // Sort terms to select vocab\n-        wordCounts.sortBy(_._2, ascending = false).take(vocabSize)\n-      }\n-      (tmpSortedWC.map(_._1).zipWithIndex.toMap, tmpSortedWC.map(_._2).sum)\n+    val df = sc.textFile(paths.mkString(\",\")).toDF(\"docs\")\n+    val customizedStopWords: Array[String] = if (stopwordFile.isEmpty) {\n+      Array.empty[String]\n+    } else {\n+      val stopWordText = sc.textFile(stopwordFile).collect()\n+      stopWordText.flatMap(_.stripMargin.split(\"\\\\s+\"))\n     }\n-\n-    val documents = tokenized.map { case (id, tokens) =>\n-      // Filter tokens by vocabulary, and create word count vector representation of document.\n-      val wc = new mutable.HashMap[Int, Int]()\n-      tokens.foreach { term =>\n-        if (vocab.contains(term)) {\n-          val termIndex = vocab(term)\n-          wc(termIndex) = wc.getOrElse(termIndex, 0) + 1\n-        }\n-      }\n-      val indices = wc.keys.toArray.sorted\n-      val values = indices.map(i => wc(i).toDouble)\n-\n-      val sb = Vectors.sparse(vocab.size, indices, values)\n-      (id, sb)\n-    }\n-\n-    val vocabArray = new Array[String](vocab.size)\n-    vocab.foreach { case (term, i) => vocabArray(i) = term }\n-\n-    (documents, vocabArray, selectedTokenCount)\n+    val tokenizer = new RegexTokenizer()\n+      .setInputCol(\"docs\")\n+      .setOutputCol(\"rawTokens\")\n+    val stopWordsRemover = new StopWordsRemover()\n+      .setInputCol(\"rawTokens\")\n+      .setOutputCol(\"tokens\")\n+    stopWordsRemover.setStopWords(stopWordsRemover.getStopWords ++ customizedStopWords)\n+    val countVectorizer = new CountVectorizer()\n+      .setVocabSize(vocabSize)\n+      .setInputCol(\"tokens\")\n+      .setOutputCol(\"vectors\")"
  }],
  "prId": 8551
}]