[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Spark's convention is either `.map { s=> s.toString }` or `.map(_.toString)`. I noticed this in a few other places too.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-09-05T21:13:21Z",
    "diffHunk": "@@ -41,6 +41,8 @@ object HdfsTest {\n       val end = System.currentTimeMillis()\n       println(s\"Iteration $iter took ${end-start} ms\")\n     }\n+    println(s\"File contents: ${file.map(s => s.toString).collect().mkString(\",\")}\")"
  }],
  "prId": 21669
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I actually ran this and this output is really noisy. You're also concatenating the contents of the whole input directory into a really long string in memory.\r\n\r\nIf you want to print something I'd somehow limit the amount of data to be shown here.",
    "commit": "dd95fcab754e71e9465f4e46818c3cef09e86c8b",
    "createdAt": "2018-10-08T22:23:27Z",
    "diffHunk": "@@ -41,6 +41,8 @@ object HdfsTest {\n       val end = System.currentTimeMillis()\n       println(s\"Iteration $iter took ${end-start} ms\")\n     }\n+    println(s\"File contents: ${file.map(_.toString).collect().mkString(\",\")}\")"
  }],
  "prId": 21669
}]