[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Do you know how stable this URL is?  Is the UCI dataset ok to use too, if that's more stable?\n",
    "commit": "76551f853cbb7f7ca27306f28a0ce6828cdf078b",
    "createdAt": "2015-06-17T22:53:14Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml\n+\n+import org.apache.spark.ml.Pipeline\n+import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}\n+import org.apache.spark.ml.feature.{HashingTF, StringIndexer, Tokenizer}\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.hive.HiveContext\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+/**\n+ * An example of an end to end machine learning pipeline that classifies text\n+ * into one of twenty possible news categories. The dataset is the 20newsgroups\n+ * dataset (http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz)",
    "line": 31
  }],
  "prId": 6654
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "one should be \"test\"\n",
    "commit": "76551f853cbb7f7ca27306f28a0ce6828cdf078b",
    "createdAt": "2015-06-17T22:53:16Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml\n+\n+import org.apache.spark.ml.Pipeline\n+import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}\n+import org.apache.spark.ml.feature.{HashingTF, StringIndexer, Tokenizer}\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.hive.HiveContext\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+/**\n+ * An example of an end to end machine learning pipeline that classifies text\n+ * into one of twenty possible news categories. The dataset is the 20newsgroups\n+ * dataset (http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz)\n+ *\n+ * We assume some minimal preprocessing of this dataset has been done to unzip the dataset and\n+ * load the data into HDFS as follows:\n+ *  wget http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz\n+ *  tar -xvzf 20news-bydate.tar.gz\n+ *  hadoop fs -mkdir ${20news.root.dir}\n+ *  hadoop fs -copyFromLocal 20news-bydate-train/ ${20news.root.dir}\n+ *  hadoop fs -copyFromLocal 20news-bydate-test/ ${20news.root.dir}\n+ *\n+ * This example uses Hive to schematize the data as tables, in order to map the folder\n+ * structure ${20news.root.dir}/{20news-bydate-train, 20news-bydate-train}/{newsgroup}/",
    "line": 42
  }],
  "prId": 6654
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "A comment about what this is doing would help people not used to *QL\n",
    "commit": "76551f853cbb7f7ca27306f28a0ce6828cdf078b",
    "createdAt": "2015-06-17T22:53:18Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml\n+\n+import org.apache.spark.ml.Pipeline\n+import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}\n+import org.apache.spark.ml.feature.{HashingTF, StringIndexer, Tokenizer}\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.hive.HiveContext\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+/**\n+ * An example of an end to end machine learning pipeline that classifies text\n+ * into one of twenty possible news categories. The dataset is the 20newsgroups\n+ * dataset (http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz)\n+ *\n+ * We assume some minimal preprocessing of this dataset has been done to unzip the dataset and\n+ * load the data into HDFS as follows:\n+ *  wget http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz\n+ *  tar -xvzf 20news-bydate.tar.gz\n+ *  hadoop fs -mkdir ${20news.root.dir}\n+ *  hadoop fs -copyFromLocal 20news-bydate-train/ ${20news.root.dir}\n+ *  hadoop fs -copyFromLocal 20news-bydate-test/ ${20news.root.dir}\n+ *\n+ * This example uses Hive to schematize the data as tables, in order to map the folder\n+ * structure ${20news.root.dir}/{20news-bydate-train, 20news-bydate-train}/{newsgroup}/\n+ * to partition columns type, newsgroup resulting in a dataset with three columns:\n+ *  type, newsgroup, text\n+ *\n+ * In order to run this example, Spark needs to be build with hive, and at runtime there\n+ * should be a valid hive-site.xml in $SPARK_HOME/conf with at minimal the following\n+ * configuration:\n+ * <configuration>\n+ *   <property>\n+ *     <name>hive.metastore.uris</name>\n+ * <!-- Ensure that the following statement points to the Hive Metastore URI in your cluster -->\n+ *     <value>thrift://${thriftserver.host}:${thriftserver.port}</value>\n+ *   <description>URI for client to contact metastore server</description>\n+ *   </property>\n+ * </configuration>\n+ *\n+ * Run with\n+ * {{{\n+ * bin/spark-submit --class org.apache.spark.examples.ml.ComplexPipelineExample\n+ *   --driver-memory 4g [examples JAR path] ${20news.root.dir}\n+ * }}}\n+ */\n+object ComplexPipelineExample {\n+\n+  def main(args: Array[String]): Unit = {\n+    val conf = new SparkConf().setAppName(\"ComplexPipelineExample\")\n+    val sc = new SparkContext(conf)\n+    val sqlContext = new HiveContext(sc)\n+    val path = args(0)\n+\n+    sqlContext.sql(s\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS 20NEWS(text String)",
    "line": 72
  }],
  "prId": 6654
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "I assume one of these should be \"test\"\n\nA comment on what this is doing would help\n",
    "commit": "76551f853cbb7f7ca27306f28a0ce6828cdf078b",
    "createdAt": "2015-06-17T22:53:20Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml\n+\n+import org.apache.spark.ml.Pipeline\n+import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}\n+import org.apache.spark.ml.feature.{HashingTF, StringIndexer, Tokenizer}\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.hive.HiveContext\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+/**\n+ * An example of an end to end machine learning pipeline that classifies text\n+ * into one of twenty possible news categories. The dataset is the 20newsgroups\n+ * dataset (http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz)\n+ *\n+ * We assume some minimal preprocessing of this dataset has been done to unzip the dataset and\n+ * load the data into HDFS as follows:\n+ *  wget http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz\n+ *  tar -xvzf 20news-bydate.tar.gz\n+ *  hadoop fs -mkdir ${20news.root.dir}\n+ *  hadoop fs -copyFromLocal 20news-bydate-train/ ${20news.root.dir}\n+ *  hadoop fs -copyFromLocal 20news-bydate-test/ ${20news.root.dir}\n+ *\n+ * This example uses Hive to schematize the data as tables, in order to map the folder\n+ * structure ${20news.root.dir}/{20news-bydate-train, 20news-bydate-train}/{newsgroup}/\n+ * to partition columns type, newsgroup resulting in a dataset with three columns:\n+ *  type, newsgroup, text\n+ *\n+ * In order to run this example, Spark needs to be build with hive, and at runtime there\n+ * should be a valid hive-site.xml in $SPARK_HOME/conf with at minimal the following\n+ * configuration:\n+ * <configuration>\n+ *   <property>\n+ *     <name>hive.metastore.uris</name>\n+ * <!-- Ensure that the following statement points to the Hive Metastore URI in your cluster -->\n+ *     <value>thrift://${thriftserver.host}:${thriftserver.port}</value>\n+ *   <description>URI for client to contact metastore server</description>\n+ *   </property>\n+ * </configuration>\n+ *\n+ * Run with\n+ * {{{\n+ * bin/spark-submit --class org.apache.spark.examples.ml.ComplexPipelineExample\n+ *   --driver-memory 4g [examples JAR path] ${20news.root.dir}\n+ * }}}\n+ */\n+object ComplexPipelineExample {\n+\n+  def main(args: Array[String]): Unit = {\n+    val conf = new SparkConf().setAppName(\"ComplexPipelineExample\")\n+    val sc = new SparkContext(conf)\n+    val sqlContext = new HiveContext(sc)\n+    val path = args(0)\n+\n+    sqlContext.sql(s\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS 20NEWS(text String)\n+      PARTITIONED BY (type String, newsgroup String)\n+      STORED AS TEXTFILE location '$path'\"\"\")\n+\n+    val newsgroups = Array(\"alt.atheism\", \"comp.graphics\",\n+      \"comp.os.ms-windows.misc\", \"comp.sys.ibm.pc.hardware\",\n+      \"comp.sys.mac.hardware\", \"comp.windows.x\", \"misc.forsale\",\n+      \"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\",\n+      \"rec.sport.hockey\", \"sci.crypt\", \"sci.electronics\",\n+      \"sci.med\", \"sci.space\", \"soc.religion.christian\",\n+      \"talk.politics.guns\", \"talk.politics.mideast\",\n+      \"talk.politics.misc\", \"talk.religion.misc\")\n+\n+    for (t <- Array(\"20news-bydate-train\", \"20news-bydate-train\")) {",
    "line": 85
  }],
  "prId": 6654
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "Do you need this?\n",
    "commit": "76551f853cbb7f7ca27306f28a0ce6828cdf078b",
    "createdAt": "2015-06-17T22:53:22Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml\n+\n+import org.apache.spark.ml.Pipeline\n+import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}\n+import org.apache.spark.ml.feature.{HashingTF, StringIndexer, Tokenizer}\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.hive.HiveContext\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+/**\n+ * An example of an end to end machine learning pipeline that classifies text\n+ * into one of twenty possible news categories. The dataset is the 20newsgroups\n+ * dataset (http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz)\n+ *\n+ * We assume some minimal preprocessing of this dataset has been done to unzip the dataset and\n+ * load the data into HDFS as follows:\n+ *  wget http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz\n+ *  tar -xvzf 20news-bydate.tar.gz\n+ *  hadoop fs -mkdir ${20news.root.dir}\n+ *  hadoop fs -copyFromLocal 20news-bydate-train/ ${20news.root.dir}\n+ *  hadoop fs -copyFromLocal 20news-bydate-test/ ${20news.root.dir}\n+ *\n+ * This example uses Hive to schematize the data as tables, in order to map the folder\n+ * structure ${20news.root.dir}/{20news-bydate-train, 20news-bydate-train}/{newsgroup}/\n+ * to partition columns type, newsgroup resulting in a dataset with three columns:\n+ *  type, newsgroup, text\n+ *\n+ * In order to run this example, Spark needs to be build with hive, and at runtime there\n+ * should be a valid hive-site.xml in $SPARK_HOME/conf with at minimal the following\n+ * configuration:\n+ * <configuration>\n+ *   <property>\n+ *     <name>hive.metastore.uris</name>\n+ * <!-- Ensure that the following statement points to the Hive Metastore URI in your cluster -->\n+ *     <value>thrift://${thriftserver.host}:${thriftserver.port}</value>\n+ *   <description>URI for client to contact metastore server</description>\n+ *   </property>\n+ * </configuration>\n+ *\n+ * Run with\n+ * {{{\n+ * bin/spark-submit --class org.apache.spark.examples.ml.ComplexPipelineExample\n+ *   --driver-memory 4g [examples JAR path] ${20news.root.dir}\n+ * }}}\n+ */\n+object ComplexPipelineExample {\n+\n+  def main(args: Array[String]): Unit = {\n+    val conf = new SparkConf().setAppName(\"ComplexPipelineExample\")\n+    val sc = new SparkContext(conf)\n+    val sqlContext = new HiveContext(sc)\n+    val path = args(0)\n+\n+    sqlContext.sql(s\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS 20NEWS(text String)\n+      PARTITIONED BY (type String, newsgroup String)\n+      STORED AS TEXTFILE location '$path'\"\"\")\n+\n+    val newsgroups = Array(\"alt.atheism\", \"comp.graphics\",\n+      \"comp.os.ms-windows.misc\", \"comp.sys.ibm.pc.hardware\",\n+      \"comp.sys.mac.hardware\", \"comp.windows.x\", \"misc.forsale\",\n+      \"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\",\n+      \"rec.sport.hockey\", \"sci.crypt\", \"sci.electronics\",\n+      \"sci.med\", \"sci.space\", \"soc.religion.christian\",\n+      \"talk.politics.guns\", \"talk.politics.mideast\",\n+      \"talk.politics.misc\", \"talk.religion.misc\")\n+\n+    for (t <- Array(\"20news-bydate-train\", \"20news-bydate-train\")) {\n+      for (newsgroup <- newsgroups) {\n+        sqlContext.sql(\n+          s\"\"\"ALTER TABLE 20NEWS ADD IF NOT EXISTS\n+             | PARTITION(type='$t', newsgroup='$newsgroup') LOCATION '$path/$t/$newsgroup/'\"\"\"\n+          .stripMargin)\n+      }\n+    }\n+\n+    // shuffle the data\n+    val partitions = 100\n+    val data = sqlContext.sql(\"SELECT * FROM 20NEWS\")\n+      .coalesce(partitions)  // by default we have over 19k partitions\n+      .repartition(partitions)\n+      .cache()\n+\n+    import sqlContext.implicits._",
    "line": 101
  }],
  "prId": 6654
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "It might be nice to include saving a DataFrame with results to HDFS.\n",
    "commit": "76551f853cbb7f7ca27306f28a0ce6828cdf078b",
    "createdAt": "2015-06-17T22:53:25Z",
    "diffHunk": "@@ -0,0 +1,156 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml\n+\n+import org.apache.spark.ml.Pipeline\n+import org.apache.spark.ml.classification.{LogisticRegression, OneVsRest}\n+import org.apache.spark.ml.feature.{HashingTF, StringIndexer, Tokenizer}\n+import org.apache.spark.mllib.evaluation.MulticlassMetrics\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.hive.HiveContext\n+import org.apache.spark.{SparkConf, SparkContext}\n+\n+/**\n+ * An example of an end to end machine learning pipeline that classifies text\n+ * into one of twenty possible news categories. The dataset is the 20newsgroups\n+ * dataset (http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz)\n+ *\n+ * We assume some minimal preprocessing of this dataset has been done to unzip the dataset and\n+ * load the data into HDFS as follows:\n+ *  wget http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz\n+ *  tar -xvzf 20news-bydate.tar.gz\n+ *  hadoop fs -mkdir ${20news.root.dir}\n+ *  hadoop fs -copyFromLocal 20news-bydate-train/ ${20news.root.dir}\n+ *  hadoop fs -copyFromLocal 20news-bydate-test/ ${20news.root.dir}\n+ *\n+ * This example uses Hive to schematize the data as tables, in order to map the folder\n+ * structure ${20news.root.dir}/{20news-bydate-train, 20news-bydate-train}/{newsgroup}/\n+ * to partition columns type, newsgroup resulting in a dataset with three columns:\n+ *  type, newsgroup, text\n+ *\n+ * In order to run this example, Spark needs to be build with hive, and at runtime there\n+ * should be a valid hive-site.xml in $SPARK_HOME/conf with at minimal the following\n+ * configuration:\n+ * <configuration>\n+ *   <property>\n+ *     <name>hive.metastore.uris</name>\n+ * <!-- Ensure that the following statement points to the Hive Metastore URI in your cluster -->\n+ *     <value>thrift://${thriftserver.host}:${thriftserver.port}</value>\n+ *   <description>URI for client to contact metastore server</description>\n+ *   </property>\n+ * </configuration>\n+ *\n+ * Run with\n+ * {{{\n+ * bin/spark-submit --class org.apache.spark.examples.ml.ComplexPipelineExample\n+ *   --driver-memory 4g [examples JAR path] ${20news.root.dir}\n+ * }}}\n+ */\n+object ComplexPipelineExample {\n+\n+  def main(args: Array[String]): Unit = {\n+    val conf = new SparkConf().setAppName(\"ComplexPipelineExample\")\n+    val sc = new SparkContext(conf)\n+    val sqlContext = new HiveContext(sc)\n+    val path = args(0)\n+\n+    sqlContext.sql(s\"\"\"CREATE EXTERNAL TABLE IF NOT EXISTS 20NEWS(text String)\n+      PARTITIONED BY (type String, newsgroup String)\n+      STORED AS TEXTFILE location '$path'\"\"\")\n+\n+    val newsgroups = Array(\"alt.atheism\", \"comp.graphics\",\n+      \"comp.os.ms-windows.misc\", \"comp.sys.ibm.pc.hardware\",\n+      \"comp.sys.mac.hardware\", \"comp.windows.x\", \"misc.forsale\",\n+      \"rec.autos\", \"rec.motorcycles\", \"rec.sport.baseball\",\n+      \"rec.sport.hockey\", \"sci.crypt\", \"sci.electronics\",\n+      \"sci.med\", \"sci.space\", \"soc.religion.christian\",\n+      \"talk.politics.guns\", \"talk.politics.mideast\",\n+      \"talk.politics.misc\", \"talk.religion.misc\")\n+\n+    for (t <- Array(\"20news-bydate-train\", \"20news-bydate-train\")) {\n+      for (newsgroup <- newsgroups) {\n+        sqlContext.sql(\n+          s\"\"\"ALTER TABLE 20NEWS ADD IF NOT EXISTS\n+             | PARTITION(type='$t', newsgroup='$newsgroup') LOCATION '$path/$t/$newsgroup/'\"\"\"\n+          .stripMargin)\n+      }\n+    }\n+\n+    // shuffle the data\n+    val partitions = 100\n+    val data = sqlContext.sql(\"SELECT * FROM 20NEWS\")\n+      .coalesce(partitions)  // by default we have over 19k partitions\n+      .repartition(partitions)\n+      .cache()\n+\n+    import sqlContext.implicits._\n+    val train = data.filter($\"type\" === \"20news-bydate-train\").cache()\n+    val test = data.filter($\"type\" === \"20news-bydate-train\").cache()\n+\n+    // convert string labels into numeric\n+    val labelIndexer = new StringIndexer()\n+      .setInputCol(\"newsgroup\")\n+      .setOutputCol(\"label\")\n+\n+    // tokenize text into words\n+    val tokenizer = new Tokenizer()\n+      .setInputCol(\"text\")\n+      .setOutputCol(\"words\")\n+\n+    // extract hash based TF-IDF features\n+    val hashingTF = new HashingTF()\n+      .setNumFeatures(1000)\n+      .setInputCol(tokenizer.getOutputCol)\n+      .setOutputCol(\"features\")\n+\n+    val lr = new LogisticRegression()\n+      .setLabelCol(labelIndexer.getOutputCol)\n+      .setMaxIter(10)\n+      .setRegParam(0.001)\n+\n+    // learn multiclass classifier with Logistic Regression as base classifier\n+    val ovr = new OneVsRest()\n+      .setClassifier(lr)\n+\n+    val pipeline = new Pipeline()\n+      .setStages(Array(labelIndexer, tokenizer, hashingTF, ovr))\n+\n+    val model = pipeline.fit(train)\n+    val predictions = model.transform(test).cache()\n+    val predictionAndLabels = predictions.select($\"prediction\", $\"label\")\n+      .map { case Row(prediction: Double, label: Double) => (prediction, label)}\n+\n+    // compute multiclass metrics\n+    val metrics = new MulticlassMetrics(predictionAndLabels)\n+\n+    val labelToIndexMap = predictions.select($\"label\", $\"newsgroup\").distinct\n+      .map {case Row(x: Double, y: String) => (y, x)}\n+      .collectAsMap()\n+\n+    val fprs = labelToIndexMap.map { case (newsgroup, label) =>\n+      (newsgroup, metrics.falsePositiveRate(label))\n+    }\n+\n+    println(metrics.confusionMatrix)\n+    println(\"label\\tfpr\")\n+    println(fprs.map {case (label, fpr) => label + \"\\t\" + fpr}.mkString(\"\\n\"))",
    "line": 151
  }],
  "prId": 6654
}]