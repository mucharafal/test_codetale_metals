[{
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "add imports with `// $example on$` and `// $example off$`\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-17T19:37:45Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.ml\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.ml.{Pipeline, PipelineModel}"
  }],
  "prId": 11053
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "now -> Now\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-17T19:38:26Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.ml\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.ml.{Pipeline, PipelineModel}\n+import org.apache.spark.ml.classification.LogisticRegression\n+import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.SQLContext\n+\n+object PipelineExample {\n+\n+  def main(args: Array[String]): Unit = {\n+    val conf = new SparkConf().setAppName(\"PipelineExample\")\n+    val sc = new SparkContext(conf)\n+    val sqlContext = new SQLContext(sc)\n+\n+    // $example on$\n+    // Prepare training documents from a list of (id, text, label) tuples.\n+    val training = sqlContext.createDataFrame(Seq(\n+      (0L, \"a b c d e spark\", 1.0),\n+      (1L, \"b d\", 0.0),\n+      (2L, \"spark f g h\", 1.0),\n+      (3L, \"hadoop mapreduce\", 0.0)\n+    )).toDF(\"id\", \"text\", \"label\")\n+\n+    // Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n+    val tokenizer = new Tokenizer()\n+      .setInputCol(\"text\")\n+      .setOutputCol(\"words\")\n+    val hashingTF = new HashingTF()\n+      .setNumFeatures(1000)\n+      .setInputCol(tokenizer.getOutputCol)\n+      .setOutputCol(\"features\")\n+    val lr = new LogisticRegression()\n+      .setMaxIter(10)\n+      .setRegParam(0.01)\n+    val pipeline = new Pipeline()\n+      .setStages(Array(tokenizer, hashingTF, lr))\n+\n+    // Fit the pipeline to training documents.\n+    val model = pipeline.fit(training)\n+\n+    // now we can optionally save the fitted pipeline to disk"
  }],
  "prId": 11053
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "we -> We\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-17T19:38:36Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.ml\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.ml.{Pipeline, PipelineModel}\n+import org.apache.spark.ml.classification.LogisticRegression\n+import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.SQLContext\n+\n+object PipelineExample {\n+\n+  def main(args: Array[String]): Unit = {\n+    val conf = new SparkConf().setAppName(\"PipelineExample\")\n+    val sc = new SparkContext(conf)\n+    val sqlContext = new SQLContext(sc)\n+\n+    // $example on$\n+    // Prepare training documents from a list of (id, text, label) tuples.\n+    val training = sqlContext.createDataFrame(Seq(\n+      (0L, \"a b c d e spark\", 1.0),\n+      (1L, \"b d\", 0.0),\n+      (2L, \"spark f g h\", 1.0),\n+      (3L, \"hadoop mapreduce\", 0.0)\n+    )).toDF(\"id\", \"text\", \"label\")\n+\n+    // Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n+    val tokenizer = new Tokenizer()\n+      .setInputCol(\"text\")\n+      .setOutputCol(\"words\")\n+    val hashingTF = new HashingTF()\n+      .setNumFeatures(1000)\n+      .setInputCol(tokenizer.getOutputCol)\n+      .setOutputCol(\"features\")\n+    val lr = new LogisticRegression()\n+      .setMaxIter(10)\n+      .setRegParam(0.01)\n+    val pipeline = new Pipeline()\n+      .setStages(Array(tokenizer, hashingTF, lr))\n+\n+    // Fit the pipeline to training documents.\n+    val model = pipeline.fit(training)\n+\n+    // now we can optionally save the fitted pipeline to disk\n+    model.save(\"/tmp/spark-logistic-regression-model\")\n+\n+    // we can also save this unfit pipeline to disk"
  }],
  "prId": 11053
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "and -> And\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-17T19:38:44Z",
    "diffHunk": "@@ -0,0 +1,91 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.ml\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.ml.{Pipeline, PipelineModel}\n+import org.apache.spark.ml.classification.LogisticRegression\n+import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.Row\n+import org.apache.spark.sql.SQLContext\n+\n+object PipelineExample {\n+\n+  def main(args: Array[String]): Unit = {\n+    val conf = new SparkConf().setAppName(\"PipelineExample\")\n+    val sc = new SparkContext(conf)\n+    val sqlContext = new SQLContext(sc)\n+\n+    // $example on$\n+    // Prepare training documents from a list of (id, text, label) tuples.\n+    val training = sqlContext.createDataFrame(Seq(\n+      (0L, \"a b c d e spark\", 1.0),\n+      (1L, \"b d\", 0.0),\n+      (2L, \"spark f g h\", 1.0),\n+      (3L, \"hadoop mapreduce\", 0.0)\n+    )).toDF(\"id\", \"text\", \"label\")\n+\n+    // Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n+    val tokenizer = new Tokenizer()\n+      .setInputCol(\"text\")\n+      .setOutputCol(\"words\")\n+    val hashingTF = new HashingTF()\n+      .setNumFeatures(1000)\n+      .setInputCol(tokenizer.getOutputCol)\n+      .setOutputCol(\"features\")\n+    val lr = new LogisticRegression()\n+      .setMaxIter(10)\n+      .setRegParam(0.01)\n+    val pipeline = new Pipeline()\n+      .setStages(Array(tokenizer, hashingTF, lr))\n+\n+    // Fit the pipeline to training documents.\n+    val model = pipeline.fit(training)\n+\n+    // now we can optionally save the fitted pipeline to disk\n+    model.save(\"/tmp/spark-logistic-regression-model\")\n+\n+    // we can also save this unfit pipeline to disk\n+    pipeline.save(\"/tmp/unfit-lr-model\")\n+\n+    // and load it back in during production"
  }],
  "prId": 11053
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "One more thing, it's better to make it overridable: `model.write.overwrite().save(\"/tmp/spark-logistic-regression-model\")`, otherwise the example can only run once.\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-20T00:52:09Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.ml\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+// $example on$\n+import org.apache.spark.ml.{Pipeline, PipelineModel}\n+import org.apache.spark.ml.classification.LogisticRegression\n+import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.Row\n+// $example off$\n+import org.apache.spark.sql.SQLContext\n+\n+object PipelineExample {\n+\n+  def main(args: Array[String]): Unit = {\n+    val conf = new SparkConf().setAppName(\"PipelineExample\")\n+    val sc = new SparkContext(conf)\n+    val sqlContext = new SQLContext(sc)\n+\n+    // $example on$\n+    // Prepare training documents from a list of (id, text, label) tuples.\n+    val training = sqlContext.createDataFrame(Seq(\n+      (0L, \"a b c d e spark\", 1.0),\n+      (1L, \"b d\", 0.0),\n+      (2L, \"spark f g h\", 1.0),\n+      (3L, \"hadoop mapreduce\", 0.0)\n+    )).toDF(\"id\", \"text\", \"label\")\n+\n+    // Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n+    val tokenizer = new Tokenizer()\n+      .setInputCol(\"text\")\n+      .setOutputCol(\"words\")\n+    val hashingTF = new HashingTF()\n+      .setNumFeatures(1000)\n+      .setInputCol(tokenizer.getOutputCol)\n+      .setOutputCol(\"features\")\n+    val lr = new LogisticRegression()\n+      .setMaxIter(10)\n+      .setRegParam(0.01)\n+    val pipeline = new Pipeline()\n+      .setStages(Array(tokenizer, hashingTF, lr))\n+\n+    // Fit the pipeline to training documents.\n+    val model = pipeline.fit(training)\n+\n+    // Now we can optionally save the fitted pipeline to disk\n+    model.save(\"/tmp/spark-logistic-regression-model\")"
  }],
  "prId": 11053
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "Ditto: `pipeline.write.overwrite().save(\"/tmp/unfit-lr-model\")`\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-20T00:52:34Z",
    "diffHunk": "@@ -0,0 +1,93 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.ml\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+// $example on$\n+import org.apache.spark.ml.{Pipeline, PipelineModel}\n+import org.apache.spark.ml.classification.LogisticRegression\n+import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\n+import org.apache.spark.mllib.linalg.Vector\n+import org.apache.spark.sql.Row\n+// $example off$\n+import org.apache.spark.sql.SQLContext\n+\n+object PipelineExample {\n+\n+  def main(args: Array[String]): Unit = {\n+    val conf = new SparkConf().setAppName(\"PipelineExample\")\n+    val sc = new SparkContext(conf)\n+    val sqlContext = new SQLContext(sc)\n+\n+    // $example on$\n+    // Prepare training documents from a list of (id, text, label) tuples.\n+    val training = sqlContext.createDataFrame(Seq(\n+      (0L, \"a b c d e spark\", 1.0),\n+      (1L, \"b d\", 0.0),\n+      (2L, \"spark f g h\", 1.0),\n+      (3L, \"hadoop mapreduce\", 0.0)\n+    )).toDF(\"id\", \"text\", \"label\")\n+\n+    // Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n+    val tokenizer = new Tokenizer()\n+      .setInputCol(\"text\")\n+      .setOutputCol(\"words\")\n+    val hashingTF = new HashingTF()\n+      .setNumFeatures(1000)\n+      .setInputCol(tokenizer.getOutputCol)\n+      .setOutputCol(\"features\")\n+    val lr = new LogisticRegression()\n+      .setMaxIter(10)\n+      .setRegParam(0.01)\n+    val pipeline = new Pipeline()\n+      .setStages(Array(tokenizer, hashingTF, lr))\n+\n+    // Fit the pipeline to training documents.\n+    val model = pipeline.fit(training)\n+\n+    // Now we can optionally save the fitted pipeline to disk\n+    model.save(\"/tmp/spark-logistic-regression-model\")\n+\n+    // We can also save this unfit pipeline to disk\n+    pipeline.save(\"/tmp/unfit-lr-model\")"
  }],
  "prId": 11053
}]