[{
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "minor spelling error of `columnFamily` here\n",
    "commit": "d2153df5f5472f34b427a39e8fd450f7d37d2fc6",
    "createdAt": "2015-01-22T19:37:49Z",
    "diffHunk": "@@ -23,15 +23,27 @@ import org.apache.spark.api.python.Converter\n import org.apache.hadoop.hbase.client.{Put, Result}\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.KeyValue.Type\n+import org.apache.hadoop.hbase.CellUtil\n \n /**\n- * Implementation of [[org.apache.spark.api.python.Converter]] that converts an\n- * HBase Result to a String\n+ * Implementation of [[org.apache.spark.api.python.Converter]] that converts all\n+ * the records in an HBase Result to a String\n  */\n class HBaseResultToStringConverter extends Converter[Any, String] {\n   override def convert(obj: Any): String = {\n+    import collection.JavaConverters._\n     val result = obj.asInstanceOf[Result]\n-    Bytes.toStringBinary(result.value())\n+    val output = result.listCells.asScala.map(cell =>\n+        \"{'columnFamliy':'%s','qualifier':'%s','timestamp':'%s','type':'%s','value':'%s'}\".format("
  }],
  "prId": 3920
}, {
  "comments": [{
    "author": {
      "login": "MLnick"
    },
    "body": "I'm not too familiar with HBase `Result` - are multiple rows in a result, separated by whitespace (in this case)? or is the idea one record per column family in a row?\n",
    "commit": "d2153df5f5472f34b427a39e8fd450f7d37d2fc6",
    "createdAt": "2015-01-22T19:39:36Z",
    "diffHunk": "@@ -23,15 +23,27 @@ import org.apache.spark.api.python.Converter\n import org.apache.hadoop.hbase.client.{Put, Result}\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.KeyValue.Type\n+import org.apache.hadoop.hbase.CellUtil\n \n /**\n- * Implementation of [[org.apache.spark.api.python.Converter]] that converts an\n- * HBase Result to a String\n+ * Implementation of [[org.apache.spark.api.python.Converter]] that converts all\n+ * the records in an HBase Result to a String\n  */\n class HBaseResultToStringConverter extends Converter[Any, String] {\n   override def convert(obj: Any): String = {\n+    import collection.JavaConverters._\n     val result = obj.asInstanceOf[Result]\n-    Bytes.toStringBinary(result.value())\n+    val output = result.listCells.asScala.map(cell =>\n+        \"{'columnFamliy':'%s','qualifier':'%s','timestamp':'%s','type':'%s','value':'%s'}\".format(\n+          Bytes.toStringBinary(CellUtil.cloneFamily(cell)),\n+          Bytes.toStringBinary(CellUtil.cloneQualifier(cell)),\n+          cell.getTimestamp.toString,\n+          Type.codeToType(cell.getTypeByte),\n+          Bytes.toStringBinary(CellUtil.cloneValue(cell))\n+        )\n+    )   \n+    output.mkString(\" \")"
  }, {
    "author": {
      "login": "GenTang"
    },
    "body": "In fact, in HBase a A _{row, columnFamily:qualifier, version}_ tuple exactly specifies a cell (a record) in HBase. So there are usually several records per column family in a row. \nIn fact, HBase `Result` contains all the information about a HBase table and `Result.listCells` returns all the cells as a java list of `hbase.Cell` which contains all the information about a cell.\n",
    "commit": "d2153df5f5472f34b427a39e8fd450f7d37d2fc6",
    "createdAt": "2015-01-22T21:59:33Z",
    "diffHunk": "@@ -23,15 +23,27 @@ import org.apache.spark.api.python.Converter\n import org.apache.hadoop.hbase.client.{Put, Result}\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.KeyValue.Type\n+import org.apache.hadoop.hbase.CellUtil\n \n /**\n- * Implementation of [[org.apache.spark.api.python.Converter]] that converts an\n- * HBase Result to a String\n+ * Implementation of [[org.apache.spark.api.python.Converter]] that converts all\n+ * the records in an HBase Result to a String\n  */\n class HBaseResultToStringConverter extends Converter[Any, String] {\n   override def convert(obj: Any): String = {\n+    import collection.JavaConverters._\n     val result = obj.asInstanceOf[Result]\n-    Bytes.toStringBinary(result.value())\n+    val output = result.listCells.asScala.map(cell =>\n+        \"{'columnFamliy':'%s','qualifier':'%s','timestamp':'%s','type':'%s','value':'%s'}\".format(\n+          Bytes.toStringBinary(CellUtil.cloneFamily(cell)),\n+          Bytes.toStringBinary(CellUtil.cloneQualifier(cell)),\n+          cell.getTimestamp.toString,\n+          Type.codeToType(cell.getTypeByte),\n+          Bytes.toStringBinary(CellUtil.cloneValue(cell))\n+        )\n+    )   \n+    output.mkString(\" \")"
  }],
  "prId": 3920
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "Is it possible that `cell` will have `'` in it? Then the string will be broken (can not be parsed by ast. literal_eval).\n\nIs it any easy and safe to do the serialization? Json?\n",
    "commit": "d2153df5f5472f34b427a39e8fd450f7d37d2fc6",
    "createdAt": "2015-02-05T18:14:54Z",
    "diffHunk": "@@ -23,15 +23,27 @@ import org.apache.spark.api.python.Converter\n import org.apache.hadoop.hbase.client.{Put, Result}\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.KeyValue.Type\n+import org.apache.hadoop.hbase.CellUtil\n \n /**\n- * Implementation of [[org.apache.spark.api.python.Converter]] that converts an\n- * HBase Result to a String\n+ * Implementation of [[org.apache.spark.api.python.Converter]] that converts all\n+ * the records in an HBase Result to a String\n  */\n class HBaseResultToStringConverter extends Converter[Any, String] {\n   override def convert(obj: Any): String = {\n+    import collection.JavaConverters._\n     val result = obj.asInstanceOf[Result]\n-    Bytes.toStringBinary(result.value())\n+    val output = result.listCells.asScala.map(cell =>\n+        \"{'columnFamily':'%s','qualifier':'%s','timestamp':'%s','type':'%s','value':'%s'}\".format("
  }, {
    "author": {
      "login": "GenTang"
    },
    "body": "Aha,... I didn't consider that. \nYou are right. It is not safe to use ast.literal_val.\nFor your suggestion, you mean that we transform string into json in python, or the converter returns a json? \n",
    "commit": "d2153df5f5472f34b427a39e8fd450f7d37d2fc6",
    "createdAt": "2015-02-05T18:51:24Z",
    "diffHunk": "@@ -23,15 +23,27 @@ import org.apache.spark.api.python.Converter\n import org.apache.hadoop.hbase.client.{Put, Result}\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.KeyValue.Type\n+import org.apache.hadoop.hbase.CellUtil\n \n /**\n- * Implementation of [[org.apache.spark.api.python.Converter]] that converts an\n- * HBase Result to a String\n+ * Implementation of [[org.apache.spark.api.python.Converter]] that converts all\n+ * the records in an HBase Result to a String\n  */\n class HBaseResultToStringConverter extends Converter[Any, String] {\n   override def convert(obj: Any): String = {\n+    import collection.JavaConverters._\n     val result = obj.asInstanceOf[Result]\n-    Bytes.toStringBinary(result.value())\n+    val output = result.listCells.asScala.map(cell =>\n+        \"{'columnFamily':'%s','qualifier':'%s','timestamp':'%s','type':'%s','value':'%s'}\".format("
  }, {
    "author": {
      "login": "davies"
    },
    "body": "We could the converter still return a string, but in json format, then we could decode them in Python using simplejson. Json library will do the escaping for us.\n",
    "commit": "d2153df5f5472f34b427a39e8fd450f7d37d2fc6",
    "createdAt": "2015-02-05T18:54:07Z",
    "diffHunk": "@@ -23,15 +23,27 @@ import org.apache.spark.api.python.Converter\n import org.apache.hadoop.hbase.client.{Put, Result}\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.KeyValue.Type\n+import org.apache.hadoop.hbase.CellUtil\n \n /**\n- * Implementation of [[org.apache.spark.api.python.Converter]] that converts an\n- * HBase Result to a String\n+ * Implementation of [[org.apache.spark.api.python.Converter]] that converts all\n+ * the records in an HBase Result to a String\n  */\n class HBaseResultToStringConverter extends Converter[Any, String] {\n   override def convert(obj: Any): String = {\n+    import collection.JavaConverters._\n     val result = obj.asInstanceOf[Result]\n-    Bytes.toStringBinary(result.value())\n+    val output = result.listCells.asScala.map(cell =>\n+        \"{'columnFamily':'%s','qualifier':'%s','timestamp':'%s','type':'%s','value':'%s'}\".format("
  }, {
    "author": {
      "login": "davies"
    },
    "body": "Also, we can bypassing the decoding in Python, then user could get a DataFrame by call sqlCtx.JsonRDD(), or just return a DataFrame.\n",
    "commit": "d2153df5f5472f34b427a39e8fd450f7d37d2fc6",
    "createdAt": "2015-02-05T18:55:57Z",
    "diffHunk": "@@ -23,15 +23,27 @@ import org.apache.spark.api.python.Converter\n import org.apache.hadoop.hbase.client.{Put, Result}\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.KeyValue.Type\n+import org.apache.hadoop.hbase.CellUtil\n \n /**\n- * Implementation of [[org.apache.spark.api.python.Converter]] that converts an\n- * HBase Result to a String\n+ * Implementation of [[org.apache.spark.api.python.Converter]] that converts all\n+ * the records in an HBase Result to a String\n  */\n class HBaseResultToStringConverter extends Converter[Any, String] {\n   override def convert(obj: Any): String = {\n+    import collection.JavaConverters._\n     val result = obj.asInstanceOf[Result]\n-    Bytes.toStringBinary(result.value())\n+    val output = result.listCells.asScala.map(cell =>\n+        \"{'columnFamily':'%s','qualifier':'%s','timestamp':'%s','type':'%s','value':'%s'}\".format("
  }, {
    "author": {
      "login": "GenTang"
    },
    "body": "Great, I will do that.Thanks\n",
    "commit": "d2153df5f5472f34b427a39e8fd450f7d37d2fc6",
    "createdAt": "2015-02-05T18:58:00Z",
    "diffHunk": "@@ -23,15 +23,27 @@ import org.apache.spark.api.python.Converter\n import org.apache.hadoop.hbase.client.{Put, Result}\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.KeyValue.Type\n+import org.apache.hadoop.hbase.CellUtil\n \n /**\n- * Implementation of [[org.apache.spark.api.python.Converter]] that converts an\n- * HBase Result to a String\n+ * Implementation of [[org.apache.spark.api.python.Converter]] that converts all\n+ * the records in an HBase Result to a String\n  */\n class HBaseResultToStringConverter extends Converter[Any, String] {\n   override def convert(obj: Any): String = {\n+    import collection.JavaConverters._\n     val result = obj.asInstanceOf[Result]\n-    Bytes.toStringBinary(result.value())\n+    val output = result.listCells.asScala.map(cell =>\n+        \"{'columnFamily':'%s','qualifier':'%s','timestamp':'%s','type':'%s','value':'%s'}\".format("
  }, {
    "author": {
      "login": "GenTang"
    },
    "body": "@davies \nHi, Sorry for the delay.\nRight now, for returning a string in json format is done.\nHowever, I am thinking let the converter return a `Array[String]`, since there are several records for one row. Here what we are doing now is that we convert the `Buffer[String]` to `string` in the converter and then in python, we reconvert the 'string' to 'list'(or 'tuple'). \nIf the converter return directly Array[String], there are two advantages:\n1. We don't need to do conversion from 'string' to 'list' in python.\n2. Right now, we should add a specific character to aggregate the `Buffer[String]` to a `String`(I am thinking \"\\n\"). Therefore it is safer if we return `Array[String]`\n\nHow do you think about this? Thanks \n",
    "commit": "d2153df5f5472f34b427a39e8fd450f7d37d2fc6",
    "createdAt": "2015-02-11T13:55:34Z",
    "diffHunk": "@@ -23,15 +23,27 @@ import org.apache.spark.api.python.Converter\n import org.apache.hadoop.hbase.client.{Put, Result}\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.KeyValue.Type\n+import org.apache.hadoop.hbase.CellUtil\n \n /**\n- * Implementation of [[org.apache.spark.api.python.Converter]] that converts an\n- * HBase Result to a String\n+ * Implementation of [[org.apache.spark.api.python.Converter]] that converts all\n+ * the records in an HBase Result to a String\n  */\n class HBaseResultToStringConverter extends Converter[Any, String] {\n   override def convert(obj: Any): String = {\n+    import collection.JavaConverters._\n     val result = obj.asInstanceOf[Result]\n-    Bytes.toStringBinary(result.value())\n+    val output = result.listCells.asScala.map(cell =>\n+        \"{'columnFamily':'%s','qualifier':'%s','timestamp':'%s','type':'%s','value':'%s'}\".format("
  }],
  "prId": 3920
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "I'm wondering that does changing the type from String to Array[String] really work? Currently, PySpark does not support deserialize a RDD of (Any, Array[String]), only (String, String) or (Array[Byte], Array[Byte]) are supported.\n\nSo you should dump the whole `output` as single JSON String. \n",
    "commit": "d2153df5f5472f34b427a39e8fd450f7d37d2fc6",
    "createdAt": "2015-02-15T03:40:40Z",
    "diffHunk": "@@ -18,20 +18,34 @@\n package org.apache.spark.examples.pythonconverters\n \n import scala.collection.JavaConversions._\n+import scala.util.parsing.json._\n \n import org.apache.spark.api.python.Converter\n import org.apache.hadoop.hbase.client.{Put, Result}\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.KeyValue.Type\n+import org.apache.hadoop.hbase.CellUtil\n \n /**\n- * Implementation of [[org.apache.spark.api.python.Converter]] that converts an\n- * HBase Result to a String\n+ * Implementation of [[org.apache.spark.api.python.Converter]] that converts all\n+ * the records in an HBase Result to an Array[String]\n  */\n-class HBaseResultToStringConverter extends Converter[Any, String] {\n-  override def convert(obj: Any): String = {\n+class HBaseResultToStringConverter extends Converter[Any, Array[String]] {"
  }, {
    "author": {
      "login": "GenTang"
    },
    "body": "You are right.\nBefore, I test the code in local mode where the code passed. However, in cluster mode, it doesn't\n",
    "commit": "d2153df5f5472f34b427a39e8fd450f7d37d2fc6",
    "createdAt": "2015-02-15T23:06:24Z",
    "diffHunk": "@@ -18,20 +18,34 @@\n package org.apache.spark.examples.pythonconverters\n \n import scala.collection.JavaConversions._\n+import scala.util.parsing.json._\n \n import org.apache.spark.api.python.Converter\n import org.apache.hadoop.hbase.client.{Put, Result}\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.KeyValue.Type\n+import org.apache.hadoop.hbase.CellUtil\n \n /**\n- * Implementation of [[org.apache.spark.api.python.Converter]] that converts an\n- * HBase Result to a String\n+ * Implementation of [[org.apache.spark.api.python.Converter]] that converts all\n+ * the records in an HBase Result to an Array[String]\n  */\n-class HBaseResultToStringConverter extends Converter[Any, String] {\n-  override def convert(obj: Any): String = {\n+class HBaseResultToStringConverter extends Converter[Any, Array[String]] {"
  }],
  "prId": 3920
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "How about  `JSONObject(output).toString()`\n",
    "commit": "d2153df5f5472f34b427a39e8fd450f7d37d2fc6",
    "createdAt": "2015-02-23T06:26:39Z",
    "diffHunk": "@@ -18,20 +18,34 @@\n package org.apache.spark.examples.pythonconverters\n \n import scala.collection.JavaConversions._\n+import scala.util.parsing.json._\n \n import org.apache.spark.api.python.Converter\n import org.apache.hadoop.hbase.client.{Put, Result}\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.KeyValue.Type\n+import org.apache.hadoop.hbase.CellUtil\n \n /**\n- * Implementation of [[org.apache.spark.api.python.Converter]] that converts an\n- * HBase Result to a String\n+ * Implementation of [[org.apache.spark.api.python.Converter]] that converts all\n+ * the records in an HBase Result to a String\n  */\n class HBaseResultToStringConverter extends Converter[Any, String] {\n   override def convert(obj: Any): String = {\n+    import collection.JavaConverters._\n     val result = obj.asInstanceOf[Result]\n-    Bytes.toStringBinary(result.value())\n+    val output = result.listCells.asScala.map(cell =>\n+        Map(\n+          \"row\" -> Bytes.toStringBinary(CellUtil.cloneRow(cell)),\n+          \"columnFamily\" -> Bytes.toStringBinary(CellUtil.cloneFamily(cell)),\n+          \"qualifier\" -> Bytes.toStringBinary(CellUtil.cloneQualifier(cell)),\n+          \"timestamp\" -> cell.getTimestamp.toString,\n+          \"type\" -> Type.codeToType(cell.getTypeByte).toString,\n+          \"value\" -> Bytes.toStringBinary(CellUtil.cloneValue(cell))\n+        )\n+    )\n+    output.map(JSONObject(_).toString()).mkString(\"\\n\")",
    "line": 34
  }, {
    "author": {
      "login": "GenTang"
    },
    "body": "Output is a `Buffer[Map[String, String]]`, since there are several records in an HBase Result.  \nHowever `JSONObject` has the only constructor `JSONObject(obj: Map[String, Any])`. So `JSONObject(output).toString()` will cause compilation failure. ^^\n",
    "commit": "d2153df5f5472f34b427a39e8fd450f7d37d2fc6",
    "createdAt": "2015-02-23T10:59:02Z",
    "diffHunk": "@@ -18,20 +18,34 @@\n package org.apache.spark.examples.pythonconverters\n \n import scala.collection.JavaConversions._\n+import scala.util.parsing.json._\n \n import org.apache.spark.api.python.Converter\n import org.apache.hadoop.hbase.client.{Put, Result}\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.KeyValue.Type\n+import org.apache.hadoop.hbase.CellUtil\n \n /**\n- * Implementation of [[org.apache.spark.api.python.Converter]] that converts an\n- * HBase Result to a String\n+ * Implementation of [[org.apache.spark.api.python.Converter]] that converts all\n+ * the records in an HBase Result to a String\n  */\n class HBaseResultToStringConverter extends Converter[Any, String] {\n   override def convert(obj: Any): String = {\n+    import collection.JavaConverters._\n     val result = obj.asInstanceOf[Result]\n-    Bytes.toStringBinary(result.value())\n+    val output = result.listCells.asScala.map(cell =>\n+        Map(\n+          \"row\" -> Bytes.toStringBinary(CellUtil.cloneRow(cell)),\n+          \"columnFamily\" -> Bytes.toStringBinary(CellUtil.cloneFamily(cell)),\n+          \"qualifier\" -> Bytes.toStringBinary(CellUtil.cloneQualifier(cell)),\n+          \"timestamp\" -> cell.getTimestamp.toString,\n+          \"type\" -> Type.codeToType(cell.getTypeByte).toString,\n+          \"value\" -> Bytes.toStringBinary(CellUtil.cloneValue(cell))\n+        )\n+    )\n+    output.map(JSONObject(_).toString()).mkString(\"\\n\")",
    "line": 34
  }, {
    "author": {
      "login": "davies"
    },
    "body": "That make sense. JSON will escape the `\\n` in String, so it's safe to have `\\n` as separator.\n",
    "commit": "d2153df5f5472f34b427a39e8fd450f7d37d2fc6",
    "createdAt": "2015-02-23T16:56:38Z",
    "diffHunk": "@@ -18,20 +18,34 @@\n package org.apache.spark.examples.pythonconverters\n \n import scala.collection.JavaConversions._\n+import scala.util.parsing.json._\n \n import org.apache.spark.api.python.Converter\n import org.apache.hadoop.hbase.client.{Put, Result}\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.KeyValue.Type\n+import org.apache.hadoop.hbase.CellUtil\n \n /**\n- * Implementation of [[org.apache.spark.api.python.Converter]] that converts an\n- * HBase Result to a String\n+ * Implementation of [[org.apache.spark.api.python.Converter]] that converts all\n+ * the records in an HBase Result to a String\n  */\n class HBaseResultToStringConverter extends Converter[Any, String] {\n   override def convert(obj: Any): String = {\n+    import collection.JavaConverters._\n     val result = obj.asInstanceOf[Result]\n-    Bytes.toStringBinary(result.value())\n+    val output = result.listCells.asScala.map(cell =>\n+        Map(\n+          \"row\" -> Bytes.toStringBinary(CellUtil.cloneRow(cell)),\n+          \"columnFamily\" -> Bytes.toStringBinary(CellUtil.cloneFamily(cell)),\n+          \"qualifier\" -> Bytes.toStringBinary(CellUtil.cloneQualifier(cell)),\n+          \"timestamp\" -> cell.getTimestamp.toString,\n+          \"type\" -> Type.codeToType(cell.getTypeByte).toString,\n+          \"value\" -> Bytes.toStringBinary(CellUtil.cloneValue(cell))\n+        )\n+    )\n+    output.map(JSONObject(_).toString()).mkString(\"\\n\")",
    "line": 34
  }, {
    "author": {
      "login": "GenTang"
    },
    "body": "Great! In fact, HBase itself will escape `\\n` too. That's why I choose `\\n` at the first place.\nThanks!\n",
    "commit": "d2153df5f5472f34b427a39e8fd450f7d37d2fc6",
    "createdAt": "2015-02-23T17:21:33Z",
    "diffHunk": "@@ -18,20 +18,34 @@\n package org.apache.spark.examples.pythonconverters\n \n import scala.collection.JavaConversions._\n+import scala.util.parsing.json._\n \n import org.apache.spark.api.python.Converter\n import org.apache.hadoop.hbase.client.{Put, Result}\n import org.apache.hadoop.hbase.io.ImmutableBytesWritable\n import org.apache.hadoop.hbase.util.Bytes\n+import org.apache.hadoop.hbase.KeyValue.Type\n+import org.apache.hadoop.hbase.CellUtil\n \n /**\n- * Implementation of [[org.apache.spark.api.python.Converter]] that converts an\n- * HBase Result to a String\n+ * Implementation of [[org.apache.spark.api.python.Converter]] that converts all\n+ * the records in an HBase Result to a String\n  */\n class HBaseResultToStringConverter extends Converter[Any, String] {\n   override def convert(obj: Any): String = {\n+    import collection.JavaConverters._\n     val result = obj.asInstanceOf[Result]\n-    Bytes.toStringBinary(result.value())\n+    val output = result.listCells.asScala.map(cell =>\n+        Map(\n+          \"row\" -> Bytes.toStringBinary(CellUtil.cloneRow(cell)),\n+          \"columnFamily\" -> Bytes.toStringBinary(CellUtil.cloneFamily(cell)),\n+          \"qualifier\" -> Bytes.toStringBinary(CellUtil.cloneQualifier(cell)),\n+          \"timestamp\" -> cell.getTimestamp.toString,\n+          \"type\" -> Type.codeToType(cell.getTypeByte).toString,\n+          \"value\" -> Bytes.toStringBinary(CellUtil.cloneValue(cell))\n+        )\n+    )\n+    output.map(JSONObject(_).toString()).mkString(\"\\n\")",
    "line": 34
  }],
  "prId": 3920
}]