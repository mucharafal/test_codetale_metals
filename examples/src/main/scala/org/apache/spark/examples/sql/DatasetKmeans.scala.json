[{
  "comments": [{
    "author": {
      "login": "cloud-fan"
    },
    "body": "should we add `cache` and `sample` for Dataset?\n",
    "commit": "8080d8885614e809da37efc1b87ddeeb7d20d05a",
    "createdAt": "2015-11-03T15:44:41Z",
    "diffHunk": "@@ -0,0 +1,119 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.sql\n+\n+import org.apache.spark.{SparkContext, SparkConf}\n+import org.apache.spark.sql.SQLContext\n+\n+/**\n+ * K-means clustering.\n+ *\n+ * This is an example implementation for learning how to use Dataset. For more conventional use,\n+ * please refer to org.apache.spark.mllib.clustering.KMeans\n+ */\n+object DatasetKmeans {\n+\n+  // TODO: now we only support 2-dimension vector\n+  type Vector = (Double, Double)\n+\n+  private def parseVector(line: String): Vector = {\n+    val Array(d1, d2) = line.split(' ').map(_.toDouble)\n+    d1 -> d2\n+  }\n+\n+  private def closestPoint(p: Vector, centers: Array[Vector]): Int = {\n+    var bestIndex = 0\n+    var closest = Double.PositiveInfinity\n+\n+    for (i <- 0 until centers.length) {\n+      val tempDist = squaredDistance(p, centers(i))\n+      if (tempDist < closest) {\n+        closest = tempDist\n+        bestIndex = i\n+      }\n+    }\n+\n+    bestIndex\n+  }\n+\n+  private def squaredDistance(p1: Vector, p2: Vector): Double = {\n+    (p1._1 - p2._1) * (p1._1 - p2._1) + (p1._2 - p2._2) * (p1._2 - p2._2)\n+  }\n+\n+  private def add(p1: Vector, p2: Vector): Vector = (p1._1 + p2._1, p1._2 + p2._2)\n+\n+  private def scale(p: Vector, scale: Double): Vector = (p._1 * scale, p._2 * scale)\n+\n+  def main(args: Array[String]): Unit = {\n+    if (args.length < 3) {\n+      System.err.println(\"Usage: DatasetKmeans <file> <k> <convergeDist>\")\n+      System.exit(1)\n+    }\n+\n+    val sparkConf = new SparkConf().setAppName(\"DatasetKmeans\")\n+    val sc = new SparkContext(sparkConf)\n+    val sqlContext = new SQLContext(sc)\n+\n+    // Importing the SQL context gives access to all the SQL functions and implicit conversions.\n+    import sqlContext.implicits._\n+\n+    val lines = sc.textFile(args(0))\n+    val data = lines.map(parseVector _)\n+    val K = args(1).toInt\n+    val convergeDist = args(2).toDouble\n+\n+    val kPoints = data.takeSample(withReplacement = false, K, 42)\n+    var tempDist = 1.0\n+\n+    val ds = data.toDS()"
  }],
  "prId": 9436
}]