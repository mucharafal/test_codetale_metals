[{
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "I'd say \"maximum\" since \"maximal\" often implies there are multiple maxima.\n",
    "commit": "e7ca406264b1ce7af8f7e4f3d78ea0d44c8394e3",
    "createdAt": "2015-06-02T18:58:24Z",
    "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml\n+\n+import scala.collection.mutable\n+import scala.language.reflectiveCalls\n+\n+import scopt.OptionParser\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.examples.mllib.AbstractParams\n+import org.apache.spark.ml.{Pipeline, PipelineStage}\n+import org.apache.spark.ml.regression.{LinearRegression, LinearRegressionModel}\n+import org.apache.spark.sql.DataFrame\n+\n+/**\n+ * An example runner for linear regression with elastic-net (mixing L1/L2) regularization.\n+ * Run with\n+ * {{{\n+ * bin/run-example ml.LinearRegressionExample [options]\n+ * }}}\n+ * A synthetic dataset can be found at `data/mllib/sample_linear_regression_data.txt` which can be\n+ * trained by\n+ * {{{\n+ * bin/run-example ml.LinearRegressionExample --regParam 0.15 --elasticNetParam 1.0 \\\n+ *   data/mllib/sample_linear_regression_data.txt\n+ * }}}\n+ * If you use it as a template to create your own app, please use `spark-submit` to submit your app.\n+ */\n+object LinearRegressionExample {\n+\n+  case class Params(\n+      input: String = null,\n+      testInput: String = \"\",\n+      dataFormat: String = \"libsvm\",\n+      regParam: Double = 0.0,\n+      elasticNetParam: Double = 0.0,\n+      maxIter: Int = 100,\n+      tol: Double = 1E-6,\n+      fracTest: Double = 0.2) extends AbstractParams[Params]\n+\n+  def main(args: Array[String]) {\n+    val defaultParams = Params()\n+\n+    val parser = new OptionParser[Params](\"LinearRegressionExample\") {\n+      head(\"LinearRegressionExample: an example Linear Regression with Elastic-Net app.\")\n+      opt[Double](\"regParam\")\n+        .text(s\"regularization parameter, default: ${defaultParams.regParam}\")\n+        .action((x, c) => c.copy(regParam = x))\n+      opt[Double](\"elasticNetParam\")\n+        .text(s\"ElasticNet mixing parameter. For alpha = 0, the penalty is an L2 penalty. \" +\n+        s\"For alpha = 1, it is an L1 penalty. For 0 < alpha < 1, the penalty is a combination of \" +\n+        s\"L1 and L2, default: ${defaultParams.elasticNetParam}\")\n+        .action((x, c) => c.copy(elasticNetParam = x))\n+      opt[Int](\"maxIter\")\n+        .text(s\"maximal number of iterations, default: ${defaultParams.maxIter}\")"
  }],
  "prId": 6576
}, {
  "comments": [{
    "author": {
      "login": "jkbradley"
    },
    "body": "I wouldn't bother creating a Pipeline since there is only 1 stage.\n",
    "commit": "e7ca406264b1ce7af8f7e4f3d78ea0d44c8394e3",
    "createdAt": "2015-06-02T18:58:25Z",
    "diffHunk": "@@ -0,0 +1,149 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml\n+\n+import scala.collection.mutable\n+import scala.language.reflectiveCalls\n+\n+import scopt.OptionParser\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.examples.mllib.AbstractParams\n+import org.apache.spark.ml.{Pipeline, PipelineStage}\n+import org.apache.spark.ml.regression.{LinearRegression, LinearRegressionModel}\n+import org.apache.spark.sql.DataFrame\n+\n+/**\n+ * An example runner for linear regression with elastic-net (mixing L1/L2) regularization.\n+ * Run with\n+ * {{{\n+ * bin/run-example ml.LinearRegressionExample [options]\n+ * }}}\n+ * A synthetic dataset can be found at `data/mllib/sample_linear_regression_data.txt` which can be\n+ * trained by\n+ * {{{\n+ * bin/run-example ml.LinearRegressionExample --regParam 0.15 --elasticNetParam 1.0 \\\n+ *   data/mllib/sample_linear_regression_data.txt\n+ * }}}\n+ * If you use it as a template to create your own app, please use `spark-submit` to submit your app.\n+ */\n+object LinearRegressionExample {\n+\n+  case class Params(\n+      input: String = null,\n+      testInput: String = \"\",\n+      dataFormat: String = \"libsvm\",\n+      regParam: Double = 0.0,\n+      elasticNetParam: Double = 0.0,\n+      maxIter: Int = 100,\n+      tol: Double = 1E-6,\n+      fracTest: Double = 0.2) extends AbstractParams[Params]\n+\n+  def main(args: Array[String]) {\n+    val defaultParams = Params()\n+\n+    val parser = new OptionParser[Params](\"LinearRegressionExample\") {\n+      head(\"LinearRegressionExample: an example Linear Regression with Elastic-Net app.\")\n+      opt[Double](\"regParam\")\n+        .text(s\"regularization parameter, default: ${defaultParams.regParam}\")\n+        .action((x, c) => c.copy(regParam = x))\n+      opt[Double](\"elasticNetParam\")\n+        .text(s\"ElasticNet mixing parameter. For alpha = 0, the penalty is an L2 penalty. \" +\n+        s\"For alpha = 1, it is an L1 penalty. For 0 < alpha < 1, the penalty is a combination of \" +\n+        s\"L1 and L2, default: ${defaultParams.elasticNetParam}\")\n+        .action((x, c) => c.copy(elasticNetParam = x))\n+      opt[Int](\"maxIter\")\n+        .text(s\"maximal number of iterations, default: ${defaultParams.maxIter}\")\n+        .action((x, c) => c.copy(maxIter = x))\n+      opt[Double](\"tol\")\n+        .text(s\"the convergence tolerance of iterations, Smaller value will lead \" +\n+        s\"to higher accuracy with the cost of more iterations, default: ${defaultParams.tol}\")\n+        .action((x, c) => c.copy(tol = x))\n+      opt[Double](\"fracTest\")\n+        .text(s\"fraction of data to hold out for testing.  If given option testInput, \" +\n+        s\"this option is ignored. default: ${defaultParams.fracTest}\")\n+        .action((x, c) => c.copy(fracTest = x))\n+      opt[String](\"testInput\")\n+        .text(s\"input path to test dataset.  If given, option fracTest is ignored.\" +\n+        s\" default: ${defaultParams.testInput}\")\n+        .action((x, c) => c.copy(testInput = x))\n+      opt[String](\"dataFormat\")\n+        .text(\"data format: libsvm (default), dense (deprecated in Spark v1.1)\")\n+        .action((x, c) => c.copy(dataFormat = x))\n+      arg[String](\"<input>\")\n+        .text(\"input path to labeled examples\")\n+        .required()\n+        .action((x, c) => c.copy(input = x))\n+      checkConfig { params =>\n+        if (params.fracTest < 0 || params.fracTest >= 1) {\n+          failure(s\"fracTest ${params.fracTest} value incorrect; should be in [0,1).\")\n+        } else {\n+          success\n+        }\n+      }\n+    }\n+\n+    parser.parse(args, defaultParams).map { params =>\n+      run(params)\n+    }.getOrElse {\n+      sys.exit(1)\n+    }\n+  }\n+\n+  def run(params: Params) {\n+    val conf = new SparkConf().setAppName(s\"LinearRegressionExample with $params\")\n+    val sc = new SparkContext(conf)\n+\n+    println(s\"LinearRegressionExample with parameters:\\n$params\")\n+\n+    // Load training and test data and cache it.\n+    val (training: DataFrame, test: DataFrame) = DecisionTreeExample.loadDatasets(sc, params.input,\n+      params.dataFormat, params.testInput, \"regression\", params.fracTest)\n+\n+    // Set up Pipeline\n+    val stages = new mutable.ArrayBuffer[PipelineStage]()"
  }],
  "prId": 6576
}]