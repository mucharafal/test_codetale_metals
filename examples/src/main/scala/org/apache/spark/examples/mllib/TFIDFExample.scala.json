[{
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "def main(args: Array[String]): Unit = {\n",
    "commit": "3513e0f63ed88479052266db5ddc0f22aab175a2",
    "createdAt": "2016-02-21T02:59:14Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.mllib\n+\n+import org.apache.spark.SparkConf\n+// $example on$\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.feature.{HashingTF, IDF}\n+import org.apache.spark.mllib.linalg.Vector\n+// $example off$\n+import org.apache.spark.rdd.RDD\n+\n+object TFIDFExample {\n+\n+  def main(args: Array[String]) {"
  }],
  "prId": 11142
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "Change the imports block to:\n\n``` scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\n// $example on$\nimport org.apache.spark.mllib.feature.{HashingTF, IDF}\nimport org.apache.spark.mllib.linalg.Vector\nimport org.apache.spark.rdd.RDD\n// $example off$\n```\n",
    "commit": "3513e0f63ed88479052266db5ddc0f22aab175a2",
    "createdAt": "2016-02-21T03:03:13Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.mllib\n+\n+import org.apache.spark.SparkConf",
    "line": 21
  }],
  "prId": 11142
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "Change the comment here to:\n\n``` scala\n// While applying HashingTF only needs a single pass to the data, applying IDF needs two passes:\n// First to compute the IDF vector and second to scale the term frequencies by IDF.\n```\n",
    "commit": "3513e0f63ed88479052266db5ddc0f22aab175a2",
    "createdAt": "2016-02-21T03:04:39Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.mllib\n+\n+import org.apache.spark.SparkConf\n+// $example on$\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.feature.{HashingTF, IDF}\n+import org.apache.spark.mllib.linalg.Vector\n+// $example off$\n+import org.apache.spark.rdd.RDD\n+\n+object TFIDFExample {\n+\n+  def main(args: Array[String]) {\n+\n+    val conf = new SparkConf().setAppName(\"TFIDFExample\")\n+    val sc = new SparkContext(conf)\n+\n+    // $example on$\n+    // Load documents (one per line).\n+    val documents: RDD[Seq[String]] = sc.textFile(\"data/mllib/kmeans_data.txt\")\n+      .map(_.split(\" \").toSeq)\n+\n+    val hashingTF = new HashingTF()\n+    val tf: RDD[Vector] = hashingTF.transform(documents)\n+\n+    // While applying HashingTF only needs a single pass to the data,"
  }],
  "prId": 11142
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "Change the comment to:\n\n``` scala\n// spark.mllib IDF implementation provides an option for ignoring terms which occur in less than\n// a minimum number of documents. In such cases, the IDF for these terms is set to 0.\n// This feature can be used by passing the minDocFreq value to the IDF constructor.\n```\n",
    "commit": "3513e0f63ed88479052266db5ddc0f22aab175a2",
    "createdAt": "2016-02-21T03:05:36Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.mllib\n+\n+import org.apache.spark.SparkConf\n+// $example on$\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.feature.{HashingTF, IDF}\n+import org.apache.spark.mllib.linalg.Vector\n+// $example off$\n+import org.apache.spark.rdd.RDD\n+\n+object TFIDFExample {\n+\n+  def main(args: Array[String]) {\n+\n+    val conf = new SparkConf().setAppName(\"TFIDFExample\")\n+    val sc = new SparkContext(conf)\n+\n+    // $example on$\n+    // Load documents (one per line).\n+    val documents: RDD[Seq[String]] = sc.textFile(\"data/mllib/kmeans_data.txt\")\n+      .map(_.split(\" \").toSeq)\n+\n+    val hashingTF = new HashingTF()\n+    val tf: RDD[Vector] = hashingTF.transform(documents)\n+\n+    // While applying HashingTF only needs a single pass to the data,\n+    // applying IDF needs two passes: first to compute the IDF vector\n+    // and second to scale the term frequencies by IDF.\n+    tf.cache()\n+    val idf = new IDF().fit(tf)\n+    val tfidf: RDD[Vector] = idf.transform(tf)\n+\n+    // spark.mllib IDF implementation provides an option for ignoring terms"
  }],
  "prId": 11142
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "remove the line\n",
    "commit": "3513e0f63ed88479052266db5ddc0f22aab175a2",
    "createdAt": "2016-02-21T03:05:50Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.mllib\n+\n+import org.apache.spark.SparkConf\n+// $example on$\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.feature.{HashingTF, IDF}\n+import org.apache.spark.mllib.linalg.Vector\n+// $example off$\n+import org.apache.spark.rdd.RDD\n+\n+object TFIDFExample {\n+\n+  def main(args: Array[String]) {\n+\n+    val conf = new SparkConf().setAppName(\"TFIDFExample\")\n+    val sc = new SparkContext(conf)\n+\n+    // $example on$\n+    // Load documents (one per line).\n+    val documents: RDD[Seq[String]] = sc.textFile(\"data/mllib/kmeans_data.txt\")\n+      .map(_.split(\" \").toSeq)\n+\n+    val hashingTF = new HashingTF()\n+    val tf: RDD[Vector] = hashingTF.transform(documents)\n+\n+    // While applying HashingTF only needs a single pass to the data,\n+    // applying IDF needs two passes: first to compute the IDF vector\n+    // and second to scale the term frequencies by IDF.\n+    tf.cache()\n+    val idf = new IDF().fit(tf)\n+    val tfidf: RDD[Vector] = idf.transform(tf)\n+\n+    // spark.mllib IDF implementation provides an option for ignoring terms\n+    // which occur in less than a minimum number of documents.\n+    // In such cases, the IDF for these terms is set to 0.\n+    // This feature can be used by passing the minDocFreq value to the IDF constructor.\n+    tf.cache()"
  }],
  "prId": 11142
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "add outputs of tfidf and tfidfIgnore\n",
    "commit": "3513e0f63ed88479052266db5ddc0f22aab175a2",
    "createdAt": "2016-02-21T03:06:16Z",
    "diffHunk": "@@ -0,0 +1,63 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+// scalastyle:off println\n+package org.apache.spark.examples.mllib\n+\n+import org.apache.spark.SparkConf\n+// $example on$\n+import org.apache.spark.SparkContext\n+import org.apache.spark.mllib.feature.{HashingTF, IDF}\n+import org.apache.spark.mllib.linalg.Vector\n+// $example off$\n+import org.apache.spark.rdd.RDD\n+\n+object TFIDFExample {\n+\n+  def main(args: Array[String]) {\n+\n+    val conf = new SparkConf().setAppName(\"TFIDFExample\")\n+    val sc = new SparkContext(conf)\n+\n+    // $example on$\n+    // Load documents (one per line).\n+    val documents: RDD[Seq[String]] = sc.textFile(\"data/mllib/kmeans_data.txt\")\n+      .map(_.split(\" \").toSeq)\n+\n+    val hashingTF = new HashingTF()\n+    val tf: RDD[Vector] = hashingTF.transform(documents)\n+\n+    // While applying HashingTF only needs a single pass to the data,\n+    // applying IDF needs two passes: first to compute the IDF vector\n+    // and second to scale the term frequencies by IDF.\n+    tf.cache()\n+    val idf = new IDF().fit(tf)\n+    val tfidf: RDD[Vector] = idf.transform(tf)\n+\n+    // spark.mllib IDF implementation provides an option for ignoring terms\n+    // which occur in less than a minimum number of documents.\n+    // In such cases, the IDF for these terms is set to 0.\n+    // This feature can be used by passing the minDocFreq value to the IDF constructor.\n+    tf.cache()\n+    val idfIgnore = new IDF(minDocFreq = 2).fit(tf)\n+    val tfidfIgnore: RDD[Vector] = idfIgnore.transform(tf)\n+    // $example off$\n+"
  }],
  "prId": 11142
}]