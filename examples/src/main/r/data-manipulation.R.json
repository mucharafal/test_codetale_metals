[{
  "comments": [{
    "author": {
      "login": "shivaram"
    },
    "body": "Would `read.csv` that is part of base R also work for this ? I know that data.table is more efficient, but I would like to avoid installing new of packages in the example.\n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-08T17:10:49Z",
    "diffHunk": "@@ -0,0 +1,97 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+## Initialize SparkContext on your local PC\n+sc <- sparkR.init(master = \"local\", appName = \"MyApp\")\n+\n+## Initialize SQLContext\n+sqlContext <- SparkRSQL.init(sc)\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The data can be downloaded from: https://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+\n+# Option 1: Create an R data frame and then convert it to a SparkR DataFrame -------\n+\n+## Create R dataframe\n+install.packages(\"data.table\") #We want to use the fread() function to read the dataset"
  }],
  "prId": 6668
}, {
  "comments": [{
    "author": {
      "login": "shivaram"
    },
    "body": "Could we take this in as a command line argument ? I think something like\n\n```\nargs <- commandArgs(trailing = TRUE)\nif (length(args) != 1) {\n  print(\"Usage: data-manipulation.R <path-to-flights.csv\")\n  print(\"The data can be downloaded from: https://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \")\n  q(\"no\")\n}\nflightsCsvPath <- args[[1]]\n```\n\nshould do the trick\n\n```\n```\n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-09T00:49:59Z",
    "diffHunk": "@@ -0,0 +1,92 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(master = \"local\", appName = \"MyApp\")\n+\n+## Initialize SQLContext\n+sqlContext <- SparkRSQL.init(sc)\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The data can be downloaded from: https://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+\n+# Option 1: Create a local R data frame and then convert it to a SparkR DataFrame -------\n+\n+## Create a local R dataframe\n+flights_df <- read.csv(\"flights.csv\")"
  }],
  "prId": 6668
}, {
  "comments": [{
    "author": {
      "login": "shivaram"
    },
    "body": "This should be `sparkRSQL` and not `SparkRSQL`\n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-09T17:15:50Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(appName = \"SparkR-data-manipulation-example\")\n+\n+## Initialize SQLContext\n+sqlContext <- SparkRSQL.init(sc)"
  }],
  "prId": 6668
}, {
  "comments": [{
    "author": {
      "login": "shivaram"
    },
    "body": "Can we move this before the contexts are created right after `library(SparkR)` ? That way if users don't give an argument we will fail fast\n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-09T17:16:27Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(appName = \"SparkR-data-manipulation-example\")\n+\n+## Initialize SQLContext\n+sqlContext <- SparkRSQL.init(sc)\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+\n+args <- commandArgs(trailing = TRUE)"
  }],
  "prId": 6668
}, {
  "comments": [{
    "author": {
      "login": "shivaram"
    },
    "body": "So I tried to run this locally and this step is very slow for the dataset we are using here (I filed https://issues.apache.org/jira/browse/SPARK-8277) due to the way we convert local data frames to lists.\n\nI see two options here: (1) Use fewer rows in the example file, so that this runs fast or (2) use a different dataset to demonstrate creating a SparkR DataFrame from a local dataframe (the CSV reader is fine) \n\nLet me know which you think is better.\n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-09T17:20:35Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(appName = \"SparkR-data-manipulation-example\")\n+\n+## Initialize SQLContext\n+sqlContext <- SparkRSQL.init(sc)\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+\n+args <- commandArgs(trailing = TRUE)\n+if (length(args) != 1) {\n+  print(\"Usage: data-manipulation.R <path-to-flights.csv\")\n+  print(\"The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \")\n+  q(\"no\")\n+}\n+\n+flightsCsvPath <- args[[1]]\n+\n+\n+# # Option 1: Create a local R data frame and then convert it to a SparkR DataFrame -------\n+\n+# ## Create a local R dataframe\n+flights_df <- read.csv(flightsCsvPath, header = TRUE)\n+flights_df$date <- as.Date(flights_df$date)\n+\n+## Convert the local data frame into a SparkR DataFrame\n+flightsDF <- createDataFrame(sqlContext, flights_df)"
  }],
  "prId": 6668
}, {
  "comments": [{
    "author": {
      "login": "shivaram"
    },
    "body": "The \"JFK\" should be in quotes here\n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-09T17:24:06Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(appName = \"SparkR-data-manipulation-example\")\n+\n+## Initialize SQLContext\n+sqlContext <- SparkRSQL.init(sc)\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+\n+args <- commandArgs(trailing = TRUE)\n+if (length(args) != 1) {\n+  print(\"Usage: data-manipulation.R <path-to-flights.csv\")\n+  print(\"The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \")\n+  q(\"no\")\n+}\n+\n+flightsCsvPath <- args[[1]]\n+\n+\n+# # Option 1: Create a local R data frame and then convert it to a SparkR DataFrame -------\n+\n+# ## Create a local R dataframe\n+flights_df <- read.csv(flightsCsvPath, header = TRUE)\n+flights_df$date <- as.Date(flights_df$date)\n+\n+## Convert the local data frame into a SparkR DataFrame\n+flightsDF <- createDataFrame(sqlContext, flights_df)\n+\n+# Option 2: Alternatively, directly create a SparkR DataFrame from the source data -------\n+flightsDF <- read.df(sqlContext, flightsCsvPath, source = \"csv\", header = \"true\")\n+\n+# Print the schema of this Spark DataFrame\n+printSchema(flightsDF)\n+\n+# Cache the DataFrame\n+cache(flightsDF)\n+\n+# Print the first 6 rows of the DataFrame\n+showDF(flightsDF, numRows = 6) ## Or\n+head(flightsDF)\n+\n+# Show the column names in the DataFrame\n+columns(flightsDF)\n+\n+# Show the number of rows in the DataFrame\n+count(flightsDF)\n+\n+# Show summary statistics for numeric colums\n+describe(flightsDF)\n+\n+# Select specific columns\n+destDF <- select(flightsDF, \"dest\", \"cancelled\")\n+\n+# Using SQL to select columns of data\n+# First, register the flights DataFrame as a table\n+registerTempTable(flightsDF, \"flightsTable\")\n+destDF <- sql(sqlContext, \"SELECT dest, cancelled FROM flightsTable\")\n+\n+# Use collect to create a local R data frame\n+dest_df <- collect(destDF)\n+\n+# Print the newly created local data frame\n+print(dest_df)\n+\n+# Filter flights whose destination is JFK\n+jfkDF <- filter(flightsDF, \"dest == JFK\") ##OR\n+jfkDF <- filter(flightsDF, flightsDF$dest == JFK)"
  }],
  "prId": 6668
}, {
  "comments": [{
    "author": {
      "login": "shivaram"
    },
    "body": "Lets name this `local_df` just to be clear ?\n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-09T17:24:23Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(appName = \"SparkR-data-manipulation-example\")\n+\n+## Initialize SQLContext\n+sqlContext <- SparkRSQL.init(sc)\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+\n+args <- commandArgs(trailing = TRUE)\n+if (length(args) != 1) {\n+  print(\"Usage: data-manipulation.R <path-to-flights.csv\")\n+  print(\"The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \")\n+  q(\"no\")\n+}\n+\n+flightsCsvPath <- args[[1]]\n+\n+\n+# # Option 1: Create a local R data frame and then convert it to a SparkR DataFrame -------\n+\n+# ## Create a local R dataframe\n+flights_df <- read.csv(flightsCsvPath, header = TRUE)\n+flights_df$date <- as.Date(flights_df$date)\n+\n+## Convert the local data frame into a SparkR DataFrame\n+flightsDF <- createDataFrame(sqlContext, flights_df)\n+\n+# Option 2: Alternatively, directly create a SparkR DataFrame from the source data -------\n+flightsDF <- read.df(sqlContext, flightsCsvPath, source = \"csv\", header = \"true\")\n+\n+# Print the schema of this Spark DataFrame\n+printSchema(flightsDF)\n+\n+# Cache the DataFrame\n+cache(flightsDF)\n+\n+# Print the first 6 rows of the DataFrame\n+showDF(flightsDF, numRows = 6) ## Or\n+head(flightsDF)\n+\n+# Show the column names in the DataFrame\n+columns(flightsDF)\n+\n+# Show the number of rows in the DataFrame\n+count(flightsDF)\n+\n+# Show summary statistics for numeric colums\n+describe(flightsDF)\n+\n+# Select specific columns\n+destDF <- select(flightsDF, \"dest\", \"cancelled\")\n+\n+# Using SQL to select columns of data\n+# First, register the flights DataFrame as a table\n+registerTempTable(flightsDF, \"flightsTable\")\n+destDF <- sql(sqlContext, \"SELECT dest, cancelled FROM flightsTable\")\n+\n+# Use collect to create a local R data frame\n+dest_df <- collect(destDF)"
  }],
  "prId": 6668
}, {
  "comments": [{
    "author": {
      "login": "shivaram"
    },
    "body": "Lets put this block inside a check to see if the package is installed. Something like \n\n```\nif (\"magrittr\" %in% rownames(installed.packages())) {\n  library(magrittr)\n  ...\n}\n```\n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-09T17:30:07Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(appName = \"SparkR-data-manipulation-example\")\n+\n+## Initialize SQLContext\n+sqlContext <- SparkRSQL.init(sc)\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+\n+args <- commandArgs(trailing = TRUE)\n+if (length(args) != 1) {\n+  print(\"Usage: data-manipulation.R <path-to-flights.csv\")\n+  print(\"The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \")\n+  q(\"no\")\n+}\n+\n+flightsCsvPath <- args[[1]]\n+\n+\n+# # Option 1: Create a local R data frame and then convert it to a SparkR DataFrame -------\n+\n+# ## Create a local R dataframe\n+flights_df <- read.csv(flightsCsvPath, header = TRUE)\n+flights_df$date <- as.Date(flights_df$date)\n+\n+## Convert the local data frame into a SparkR DataFrame\n+flightsDF <- createDataFrame(sqlContext, flights_df)\n+\n+# Option 2: Alternatively, directly create a SparkR DataFrame from the source data -------\n+flightsDF <- read.df(sqlContext, flightsCsvPath, source = \"csv\", header = \"true\")\n+\n+# Print the schema of this Spark DataFrame\n+printSchema(flightsDF)\n+\n+# Cache the DataFrame\n+cache(flightsDF)\n+\n+# Print the first 6 rows of the DataFrame\n+showDF(flightsDF, numRows = 6) ## Or\n+head(flightsDF)\n+\n+# Show the column names in the DataFrame\n+columns(flightsDF)\n+\n+# Show the number of rows in the DataFrame\n+count(flightsDF)\n+\n+# Show summary statistics for numeric colums\n+describe(flightsDF)\n+\n+# Select specific columns\n+destDF <- select(flightsDF, \"dest\", \"cancelled\")\n+\n+# Using SQL to select columns of data\n+# First, register the flights DataFrame as a table\n+registerTempTable(flightsDF, \"flightsTable\")\n+destDF <- sql(sqlContext, \"SELECT dest, cancelled FROM flightsTable\")\n+\n+# Use collect to create a local R data frame\n+dest_df <- collect(destDF)\n+\n+# Print the newly created local data frame\n+print(dest_df)\n+\n+# Filter flights whose destination is JFK\n+jfkDF <- filter(flightsDF, \"dest == JFK\") ##OR\n+jfkDF <- filter(flightsDF, flightsDF$dest == JFK)\n+\n+# Install the magrittr library\n+library(magrittr)"
  }],
  "prId": 6668
}, {
  "comments": [{
    "author": {
      "login": "shivaram"
    },
    "body": "This line should also go inside the `if` block \n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-10T16:19:44Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+args <- commandArgs(trailing = TRUE)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(appName = \"SparkR-data-manipulation-example\")\n+\n+## Initialize SQLContext\n+sqlContext <- sparkRSQL.init(sc)\n+\n+if (length(args) != 1) {\n+  print(\"Usage: data-manipulation.R <path-to-flights.csv\")\n+  print(\"The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \")\n+  q(\"no\")\n+}\n+\n+flightsCsvPath <- args[[1]]\n+\n+\n+# Create a local R dataframe\n+flights_df <- read.csv(flightsCsvPath, header = TRUE)\n+flights_df$date <- as.Date(flights_df$date)\n+\n+## Filter flights whose destination is San Francisco and write to a local data frame\n+SFO_df <- flights_df[flights_df$dest == \"SFO\", ] \n+\n+# Convert the local data frame into a SparkR DataFrame\n+SFO_DF <- createDataFrame(sqlContext, SFO_df)\n+\n+#  Directly create a SparkR DataFrame from the source data\n+flightsDF <- read.df(sqlContext, flightsCsvPath, source = \"csv\", header = \"true\")\n+\n+# Print the schema of this Spark DataFrame\n+printSchema(flightsDF)\n+\n+# Cache the DataFrame\n+cache(flightsDF)\n+\n+# Print the first 6 rows of the DataFrame\n+showDF(flightsDF, numRows = 6) ## Or\n+head(flightsDF)\n+\n+# Show the column names in the DataFrame\n+columns(flightsDF)\n+\n+# Show the number of rows in the DataFrame\n+count(flightsDF)\n+\n+# Show summary statistics for numeric colums\n+describe(flightsDF)\n+\n+# Select specific columns\n+destDF <- select(flightsDF, \"dest\", \"cancelled\")\n+\n+# Using SQL to select columns of data\n+# First, register the flights DataFrame as a table\n+registerTempTable(flightsDF, \"flightsTable\")\n+destDF <- sql(sqlContext, \"SELECT dest, cancelled FROM flightsTable\")\n+\n+# Use collect to create a local R data frame\n+local_df <- collect(destDF)\n+\n+# Print the newly created local data frame\n+print(local_df)\n+\n+# Filter flights whose destination is JFK\n+jfkDF <- filter(flightsDF, \"dest == JFK\") ##OR\n+jfkDF <- filter(flightsDF, flightsDF$dest == \"JFK\")\n+\n+# Install the magrittr library\n+if(\"magrittr\" %in% rownames(installed.packages())) { library(magrittr) }\n+\n+# Group the flights by date and then find the average daily delay\n+# Write the result into a DataFrame\n+groupBy(flightsDF, \"date\") %>%",
    "line": 97
  }],
  "prId": 6668
}, {
  "comments": [{
    "author": {
      "login": "shivaram"
    },
    "body": "The source here needs to be `com.databricks.spark.csv`\n\nBTW @rxin is there some way we can map `source = csv` to that automatically ?\n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-10T16:41:00Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+args <- commandArgs(trailing = TRUE)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(appName = \"SparkR-data-manipulation-example\")\n+\n+## Initialize SQLContext\n+sqlContext <- sparkRSQL.init(sc)\n+\n+if (length(args) != 1) {\n+  print(\"Usage: data-manipulation.R <path-to-flights.csv\")\n+  print(\"The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \")\n+  q(\"no\")\n+}\n+\n+flightsCsvPath <- args[[1]]\n+\n+\n+# Create a local R dataframe\n+flights_df <- read.csv(flightsCsvPath, header = TRUE)\n+flights_df$date <- as.Date(flights_df$date)\n+\n+## Filter flights whose destination is San Francisco and write to a local data frame\n+SFO_df <- flights_df[flights_df$dest == \"SFO\", ] \n+\n+# Convert the local data frame into a SparkR DataFrame\n+SFO_DF <- createDataFrame(sqlContext, SFO_df)\n+\n+#  Directly create a SparkR DataFrame from the source data\n+flightsDF <- read.df(sqlContext, flightsCsvPath, source = \"csv\", header = \"true\")",
    "line": 53
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "not if csv is outside this ... maybe we can provide a way for data sources to register short names.\n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-11T01:18:12Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+args <- commandArgs(trailing = TRUE)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(appName = \"SparkR-data-manipulation-example\")\n+\n+## Initialize SQLContext\n+sqlContext <- sparkRSQL.init(sc)\n+\n+if (length(args) != 1) {\n+  print(\"Usage: data-manipulation.R <path-to-flights.csv\")\n+  print(\"The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \")\n+  q(\"no\")\n+}\n+\n+flightsCsvPath <- args[[1]]\n+\n+\n+# Create a local R dataframe\n+flights_df <- read.csv(flightsCsvPath, header = TRUE)\n+flights_df$date <- as.Date(flights_df$date)\n+\n+## Filter flights whose destination is San Francisco and write to a local data frame\n+SFO_df <- flights_df[flights_df$dest == \"SFO\", ] \n+\n+# Convert the local data frame into a SparkR DataFrame\n+SFO_DF <- createDataFrame(sqlContext, SFO_df)\n+\n+#  Directly create a SparkR DataFrame from the source data\n+flightsDF <- read.df(sqlContext, flightsCsvPath, source = \"csv\", header = \"true\")",
    "line": 53
  }],
  "prId": 6668
}, {
  "comments": [{
    "author": {
      "login": "shivaram"
    },
    "body": "The `\"dest == JFK\"` doesn't work as the string JFK needs to be quoted. So this should be something like `jfkDF <- filter(flightsDF, \"dest = \\\"JFK\\\"\")`. Note that this needs to be a single `=` for things to work with this syntax.\n\ncc @davies we should probably make `=` and `==` work here ?\n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-10T16:45:26Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+args <- commandArgs(trailing = TRUE)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(appName = \"SparkR-data-manipulation-example\")\n+\n+## Initialize SQLContext\n+sqlContext <- sparkRSQL.init(sc)\n+\n+if (length(args) != 1) {\n+  print(\"Usage: data-manipulation.R <path-to-flights.csv\")\n+  print(\"The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \")\n+  q(\"no\")\n+}\n+\n+flightsCsvPath <- args[[1]]\n+\n+\n+# Create a local R dataframe\n+flights_df <- read.csv(flightsCsvPath, header = TRUE)\n+flights_df$date <- as.Date(flights_df$date)\n+\n+## Filter flights whose destination is San Francisco and write to a local data frame\n+SFO_df <- flights_df[flights_df$dest == \"SFO\", ] \n+\n+# Convert the local data frame into a SparkR DataFrame\n+SFO_DF <- createDataFrame(sqlContext, SFO_df)\n+\n+#  Directly create a SparkR DataFrame from the source data\n+flightsDF <- read.df(sqlContext, flightsCsvPath, source = \"csv\", header = \"true\")\n+\n+# Print the schema of this Spark DataFrame\n+printSchema(flightsDF)\n+\n+# Cache the DataFrame\n+cache(flightsDF)\n+\n+# Print the first 6 rows of the DataFrame\n+showDF(flightsDF, numRows = 6) ## Or\n+head(flightsDF)\n+\n+# Show the column names in the DataFrame\n+columns(flightsDF)\n+\n+# Show the number of rows in the DataFrame\n+count(flightsDF)\n+\n+# Show summary statistics for numeric colums\n+describe(flightsDF)\n+\n+# Select specific columns\n+destDF <- select(flightsDF, \"dest\", \"cancelled\")\n+\n+# Using SQL to select columns of data\n+# First, register the flights DataFrame as a table\n+registerTempTable(flightsDF, \"flightsTable\")\n+destDF <- sql(sqlContext, \"SELECT dest, cancelled FROM flightsTable\")\n+\n+# Use collect to create a local R data frame\n+local_df <- collect(destDF)\n+\n+# Print the newly created local data frame\n+print(local_df)\n+\n+# Filter flights whose destination is JFK\n+jfkDF <- filter(flightsDF, \"dest == JFK\") ##OR",
    "line": 89
  }, {
    "author": {
      "login": "davies"
    },
    "body": "I think we shouldn't, the expression should be either SQL or R, if we support `==` here, it's not SQL or R, will introduce other corner cases. \n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-10T16:50:09Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+args <- commandArgs(trailing = TRUE)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(appName = \"SparkR-data-manipulation-example\")\n+\n+## Initialize SQLContext\n+sqlContext <- sparkRSQL.init(sc)\n+\n+if (length(args) != 1) {\n+  print(\"Usage: data-manipulation.R <path-to-flights.csv\")\n+  print(\"The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \")\n+  q(\"no\")\n+}\n+\n+flightsCsvPath <- args[[1]]\n+\n+\n+# Create a local R dataframe\n+flights_df <- read.csv(flightsCsvPath, header = TRUE)\n+flights_df$date <- as.Date(flights_df$date)\n+\n+## Filter flights whose destination is San Francisco and write to a local data frame\n+SFO_df <- flights_df[flights_df$dest == \"SFO\", ] \n+\n+# Convert the local data frame into a SparkR DataFrame\n+SFO_DF <- createDataFrame(sqlContext, SFO_df)\n+\n+#  Directly create a SparkR DataFrame from the source data\n+flightsDF <- read.df(sqlContext, flightsCsvPath, source = \"csv\", header = \"true\")\n+\n+# Print the schema of this Spark DataFrame\n+printSchema(flightsDF)\n+\n+# Cache the DataFrame\n+cache(flightsDF)\n+\n+# Print the first 6 rows of the DataFrame\n+showDF(flightsDF, numRows = 6) ## Or\n+head(flightsDF)\n+\n+# Show the column names in the DataFrame\n+columns(flightsDF)\n+\n+# Show the number of rows in the DataFrame\n+count(flightsDF)\n+\n+# Show summary statistics for numeric colums\n+describe(flightsDF)\n+\n+# Select specific columns\n+destDF <- select(flightsDF, \"dest\", \"cancelled\")\n+\n+# Using SQL to select columns of data\n+# First, register the flights DataFrame as a table\n+registerTempTable(flightsDF, \"flightsTable\")\n+destDF <- sql(sqlContext, \"SELECT dest, cancelled FROM flightsTable\")\n+\n+# Use collect to create a local R data frame\n+local_df <- collect(destDF)\n+\n+# Print the newly created local data frame\n+print(local_df)\n+\n+# Filter flights whose destination is JFK\n+jfkDF <- filter(flightsDF, \"dest == JFK\") ##OR",
    "line": 89
  }, {
    "author": {
      "login": "shivaram"
    },
    "body": "Ah I see - so right now the convention is that the string syntax is SQL and the normal syntax is R ? \n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-10T16:51:55Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+args <- commandArgs(trailing = TRUE)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(appName = \"SparkR-data-manipulation-example\")\n+\n+## Initialize SQLContext\n+sqlContext <- sparkRSQL.init(sc)\n+\n+if (length(args) != 1) {\n+  print(\"Usage: data-manipulation.R <path-to-flights.csv\")\n+  print(\"The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \")\n+  q(\"no\")\n+}\n+\n+flightsCsvPath <- args[[1]]\n+\n+\n+# Create a local R dataframe\n+flights_df <- read.csv(flightsCsvPath, header = TRUE)\n+flights_df$date <- as.Date(flights_df$date)\n+\n+## Filter flights whose destination is San Francisco and write to a local data frame\n+SFO_df <- flights_df[flights_df$dest == \"SFO\", ] \n+\n+# Convert the local data frame into a SparkR DataFrame\n+SFO_DF <- createDataFrame(sqlContext, SFO_df)\n+\n+#  Directly create a SparkR DataFrame from the source data\n+flightsDF <- read.df(sqlContext, flightsCsvPath, source = \"csv\", header = \"true\")\n+\n+# Print the schema of this Spark DataFrame\n+printSchema(flightsDF)\n+\n+# Cache the DataFrame\n+cache(flightsDF)\n+\n+# Print the first 6 rows of the DataFrame\n+showDF(flightsDF, numRows = 6) ## Or\n+head(flightsDF)\n+\n+# Show the column names in the DataFrame\n+columns(flightsDF)\n+\n+# Show the number of rows in the DataFrame\n+count(flightsDF)\n+\n+# Show summary statistics for numeric colums\n+describe(flightsDF)\n+\n+# Select specific columns\n+destDF <- select(flightsDF, \"dest\", \"cancelled\")\n+\n+# Using SQL to select columns of data\n+# First, register the flights DataFrame as a table\n+registerTempTable(flightsDF, \"flightsTable\")\n+destDF <- sql(sqlContext, \"SELECT dest, cancelled FROM flightsTable\")\n+\n+# Use collect to create a local R data frame\n+local_df <- collect(destDF)\n+\n+# Print the newly created local data frame\n+print(local_df)\n+\n+# Filter flights whose destination is JFK\n+jfkDF <- filter(flightsDF, \"dest == JFK\") ##OR",
    "line": 89
  }, {
    "author": {
      "login": "davies"
    },
    "body": "Yes:)\n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-10T17:05:56Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+args <- commandArgs(trailing = TRUE)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(appName = \"SparkR-data-manipulation-example\")\n+\n+## Initialize SQLContext\n+sqlContext <- sparkRSQL.init(sc)\n+\n+if (length(args) != 1) {\n+  print(\"Usage: data-manipulation.R <path-to-flights.csv\")\n+  print(\"The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \")\n+  q(\"no\")\n+}\n+\n+flightsCsvPath <- args[[1]]\n+\n+\n+# Create a local R dataframe\n+flights_df <- read.csv(flightsCsvPath, header = TRUE)\n+flights_df$date <- as.Date(flights_df$date)\n+\n+## Filter flights whose destination is San Francisco and write to a local data frame\n+SFO_df <- flights_df[flights_df$dest == \"SFO\", ] \n+\n+# Convert the local data frame into a SparkR DataFrame\n+SFO_DF <- createDataFrame(sqlContext, SFO_df)\n+\n+#  Directly create a SparkR DataFrame from the source data\n+flightsDF <- read.df(sqlContext, flightsCsvPath, source = \"csv\", header = \"true\")\n+\n+# Print the schema of this Spark DataFrame\n+printSchema(flightsDF)\n+\n+# Cache the DataFrame\n+cache(flightsDF)\n+\n+# Print the first 6 rows of the DataFrame\n+showDF(flightsDF, numRows = 6) ## Or\n+head(flightsDF)\n+\n+# Show the column names in the DataFrame\n+columns(flightsDF)\n+\n+# Show the number of rows in the DataFrame\n+count(flightsDF)\n+\n+# Show summary statistics for numeric colums\n+describe(flightsDF)\n+\n+# Select specific columns\n+destDF <- select(flightsDF, \"dest\", \"cancelled\")\n+\n+# Using SQL to select columns of data\n+# First, register the flights DataFrame as a table\n+registerTempTable(flightsDF, \"flightsTable\")\n+destDF <- sql(sqlContext, \"SELECT dest, cancelled FROM flightsTable\")\n+\n+# Use collect to create a local R data frame\n+local_df <- collect(destDF)\n+\n+# Print the newly created local data frame\n+print(local_df)\n+\n+# Filter flights whose destination is JFK\n+jfkDF <- filter(flightsDF, \"dest == JFK\") ##OR",
    "line": 89
  }, {
    "author": {
      "login": "rxin"
    },
    "body": "Technically we can be error tolerant and support == in the parser though. I can't think of a corner case that'd be problematic. \n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-10T17:36:01Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+args <- commandArgs(trailing = TRUE)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(appName = \"SparkR-data-manipulation-example\")\n+\n+## Initialize SQLContext\n+sqlContext <- sparkRSQL.init(sc)\n+\n+if (length(args) != 1) {\n+  print(\"Usage: data-manipulation.R <path-to-flights.csv\")\n+  print(\"The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \")\n+  q(\"no\")\n+}\n+\n+flightsCsvPath <- args[[1]]\n+\n+\n+# Create a local R dataframe\n+flights_df <- read.csv(flightsCsvPath, header = TRUE)\n+flights_df$date <- as.Date(flights_df$date)\n+\n+## Filter flights whose destination is San Francisco and write to a local data frame\n+SFO_df <- flights_df[flights_df$dest == \"SFO\", ] \n+\n+# Convert the local data frame into a SparkR DataFrame\n+SFO_DF <- createDataFrame(sqlContext, SFO_df)\n+\n+#  Directly create a SparkR DataFrame from the source data\n+flightsDF <- read.df(sqlContext, flightsCsvPath, source = \"csv\", header = \"true\")\n+\n+# Print the schema of this Spark DataFrame\n+printSchema(flightsDF)\n+\n+# Cache the DataFrame\n+cache(flightsDF)\n+\n+# Print the first 6 rows of the DataFrame\n+showDF(flightsDF, numRows = 6) ## Or\n+head(flightsDF)\n+\n+# Show the column names in the DataFrame\n+columns(flightsDF)\n+\n+# Show the number of rows in the DataFrame\n+count(flightsDF)\n+\n+# Show summary statistics for numeric colums\n+describe(flightsDF)\n+\n+# Select specific columns\n+destDF <- select(flightsDF, \"dest\", \"cancelled\")\n+\n+# Using SQL to select columns of data\n+# First, register the flights DataFrame as a table\n+registerTempTable(flightsDF, \"flightsTable\")\n+destDF <- sql(sqlContext, \"SELECT dest, cancelled FROM flightsTable\")\n+\n+# Use collect to create a local R data frame\n+local_df <- collect(destDF)\n+\n+# Print the newly created local data frame\n+print(local_df)\n+\n+# Filter flights whose destination is JFK\n+jfkDF <- filter(flightsDF, \"dest == JFK\") ##OR",
    "line": 89
  }],
  "prId": 6668
}, {
  "comments": [{
    "author": {
      "login": "shivaram"
    },
    "body": "This needs to be `agg(dep_delay...` or to be more clear `summarize(dep_delay = ...`\n",
    "commit": "3a97867dfddd8ecd96a676b757d818f05cae4dc8",
    "createdAt": "2015-06-10T16:45:56Z",
    "diffHunk": "@@ -0,0 +1,101 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# For this example, we shall use the \"flights\" dataset\n+# The dataset consists of every flight departing Houston in 2011.\n+# The data set is made up of 227,496 rows x 14 columns. \n+\n+# Load SparkR library into your R session\n+library(SparkR)\n+\n+args <- commandArgs(trailing = TRUE)\n+\n+## Initialize SparkContext\n+sc <- sparkR.init(appName = \"SparkR-data-manipulation-example\")\n+\n+## Initialize SQLContext\n+sqlContext <- sparkRSQL.init(sc)\n+\n+if (length(args) != 1) {\n+  print(\"Usage: data-manipulation.R <path-to-flights.csv\")\n+  print(\"The data can be downloaded from: http://s3-us-west-2.amazonaws.com/sparkr-data/flights.csv \")\n+  q(\"no\")\n+}\n+\n+flightsCsvPath <- args[[1]]\n+\n+\n+# Create a local R dataframe\n+flights_df <- read.csv(flightsCsvPath, header = TRUE)\n+flights_df$date <- as.Date(flights_df$date)\n+\n+## Filter flights whose destination is San Francisco and write to a local data frame\n+SFO_df <- flights_df[flights_df$dest == \"SFO\", ] \n+\n+# Convert the local data frame into a SparkR DataFrame\n+SFO_DF <- createDataFrame(sqlContext, SFO_df)\n+\n+#  Directly create a SparkR DataFrame from the source data\n+flightsDF <- read.df(sqlContext, flightsCsvPath, source = \"csv\", header = \"true\")\n+\n+# Print the schema of this Spark DataFrame\n+printSchema(flightsDF)\n+\n+# Cache the DataFrame\n+cache(flightsDF)\n+\n+# Print the first 6 rows of the DataFrame\n+showDF(flightsDF, numRows = 6) ## Or\n+head(flightsDF)\n+\n+# Show the column names in the DataFrame\n+columns(flightsDF)\n+\n+# Show the number of rows in the DataFrame\n+count(flightsDF)\n+\n+# Show summary statistics for numeric colums\n+describe(flightsDF)\n+\n+# Select specific columns\n+destDF <- select(flightsDF, \"dest\", \"cancelled\")\n+\n+# Using SQL to select columns of data\n+# First, register the flights DataFrame as a table\n+registerTempTable(flightsDF, \"flightsTable\")\n+destDF <- sql(sqlContext, \"SELECT dest, cancelled FROM flightsTable\")\n+\n+# Use collect to create a local R data frame\n+local_df <- collect(destDF)\n+\n+# Print the newly created local data frame\n+print(local_df)\n+\n+# Filter flights whose destination is JFK\n+jfkDF <- filter(flightsDF, \"dest == JFK\") ##OR\n+jfkDF <- filter(flightsDF, flightsDF$dest == \"JFK\")\n+\n+# Install the magrittr library\n+if(\"magrittr\" %in% rownames(installed.packages())) { library(magrittr) }\n+\n+# Group the flights by date and then find the average daily delay\n+# Write the result into a DataFrame\n+groupBy(flightsDF, \"date\") %>%\n+  avg(dep_delay = \"avg\", arr_delay = \"avg\") -> dailyDelayDF",
    "line": 98
  }],
  "prId": 6668
}]