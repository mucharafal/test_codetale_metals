[{
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "Remove spaces in brace like this `{tokenizer, hashingTF, lr}`\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-17T18:09:35Z",
    "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml;\n+\n+//$example on$\n+import java.util.Arrays;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.ml.Pipeline;\n+import org.apache.spark.ml.PipelineStage;\n+import org.apache.spark.ml.classification.LogisticRegression;\n+import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator;\n+import org.apache.spark.ml.feature.HashingTF;\n+import org.apache.spark.ml.feature.Tokenizer;\n+import org.apache.spark.ml.param.ParamMap;\n+import org.apache.spark.ml.tuning.CrossValidator;\n+import org.apache.spark.ml.tuning.CrossValidatorModel;\n+import org.apache.spark.ml.tuning.ParamGridBuilder;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+//$example off$\n+\n+/**\n+ * Java example for Model Selection via Cross Validation.\n+ */\n+public class JavaModelSelectionViaCrossValidationExample {\n+  public static void main(String[] args) {\n+    SparkConf conf = new SparkConf()\n+      .setAppName(\"JavaModelSelectionViaCrossValidationExample\");\n+    SparkContext sc = new SparkContext(conf);\n+    SQLContext sqlContext = new SQLContext(sc);\n+\n+    // $example on$\n+    // Prepare training documents, which are labeled.\n+    DataFrame training = sqlContext.createDataFrame(Arrays.asList(\n+      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n+      new JavaLabeledDocument(1L, \"b d\", 0.0),\n+      new JavaLabeledDocument(2L,\"spark f g h\", 1.0),\n+      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0),\n+      new JavaLabeledDocument(4L, \"b spark who\", 1.0),\n+      new JavaLabeledDocument(5L, \"g d a y\", 0.0),\n+      new JavaLabeledDocument(6L, \"spark fly\", 1.0),\n+      new JavaLabeledDocument(7L, \"was mapreduce\", 0.0),\n+      new JavaLabeledDocument(8L, \"e spark program\", 1.0),\n+      new JavaLabeledDocument(9L, \"a e c l\", 0.0),\n+      new JavaLabeledDocument(10L, \"spark compile\", 1.0),\n+      new JavaLabeledDocument(11L, \"hadoop software\", 0.0)\n+    ), JavaLabeledDocument.class);\n+\n+    // Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n+    Tokenizer tokenizer = new Tokenizer()\n+      .setInputCol(\"text\")\n+      .setOutputCol(\"words\");\n+    HashingTF hashingTF = new HashingTF()\n+      .setNumFeatures(1000)\n+      .setInputCol(tokenizer.getOutputCol())\n+      .setOutputCol(\"features\");\n+    LogisticRegression lr = new LogisticRegression()\n+      .setMaxIter(10)\n+      .setRegParam(0.01);\n+    Pipeline pipeline = new Pipeline()\n+      .setStages(new PipelineStage[] { tokenizer, hashingTF, lr });"
  }],
  "prId": 11053
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "ditto\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-17T18:09:42Z",
    "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml;\n+\n+//$example on$\n+import java.util.Arrays;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.ml.Pipeline;\n+import org.apache.spark.ml.PipelineStage;\n+import org.apache.spark.ml.classification.LogisticRegression;\n+import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator;\n+import org.apache.spark.ml.feature.HashingTF;\n+import org.apache.spark.ml.feature.Tokenizer;\n+import org.apache.spark.ml.param.ParamMap;\n+import org.apache.spark.ml.tuning.CrossValidator;\n+import org.apache.spark.ml.tuning.CrossValidatorModel;\n+import org.apache.spark.ml.tuning.ParamGridBuilder;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+//$example off$\n+\n+/**\n+ * Java example for Model Selection via Cross Validation.\n+ */\n+public class JavaModelSelectionViaCrossValidationExample {\n+  public static void main(String[] args) {\n+    SparkConf conf = new SparkConf()\n+      .setAppName(\"JavaModelSelectionViaCrossValidationExample\");\n+    SparkContext sc = new SparkContext(conf);\n+    SQLContext sqlContext = new SQLContext(sc);\n+\n+    // $example on$\n+    // Prepare training documents, which are labeled.\n+    DataFrame training = sqlContext.createDataFrame(Arrays.asList(\n+      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n+      new JavaLabeledDocument(1L, \"b d\", 0.0),\n+      new JavaLabeledDocument(2L,\"spark f g h\", 1.0),\n+      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0),\n+      new JavaLabeledDocument(4L, \"b spark who\", 1.0),\n+      new JavaLabeledDocument(5L, \"g d a y\", 0.0),\n+      new JavaLabeledDocument(6L, \"spark fly\", 1.0),\n+      new JavaLabeledDocument(7L, \"was mapreduce\", 0.0),\n+      new JavaLabeledDocument(8L, \"e spark program\", 1.0),\n+      new JavaLabeledDocument(9L, \"a e c l\", 0.0),\n+      new JavaLabeledDocument(10L, \"spark compile\", 1.0),\n+      new JavaLabeledDocument(11L, \"hadoop software\", 0.0)\n+    ), JavaLabeledDocument.class);\n+\n+    // Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n+    Tokenizer tokenizer = new Tokenizer()\n+      .setInputCol(\"text\")\n+      .setOutputCol(\"words\");\n+    HashingTF hashingTF = new HashingTF()\n+      .setNumFeatures(1000)\n+      .setInputCol(tokenizer.getOutputCol())\n+      .setOutputCol(\"features\");\n+    LogisticRegression lr = new LogisticRegression()\n+      .setMaxIter(10)\n+      .setRegParam(0.01);\n+    Pipeline pipeline = new Pipeline()\n+      .setStages(new PipelineStage[] { tokenizer, hashingTF, lr });\n+\n+    // We use a ParamGridBuilder to construct a grid of parameters to search over.\n+    // With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n+    // this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n+    ParamMap[] paramGrid = new ParamGridBuilder()\n+      .addGrid(hashingTF.numFeatures(), new int[] { 10, 100, 1000 })"
  }],
  "prId": 11053
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "ditto\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-17T18:10:01Z",
    "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml;\n+\n+//$example on$\n+import java.util.Arrays;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.ml.Pipeline;\n+import org.apache.spark.ml.PipelineStage;\n+import org.apache.spark.ml.classification.LogisticRegression;\n+import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator;\n+import org.apache.spark.ml.feature.HashingTF;\n+import org.apache.spark.ml.feature.Tokenizer;\n+import org.apache.spark.ml.param.ParamMap;\n+import org.apache.spark.ml.tuning.CrossValidator;\n+import org.apache.spark.ml.tuning.CrossValidatorModel;\n+import org.apache.spark.ml.tuning.ParamGridBuilder;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+//$example off$\n+\n+/**\n+ * Java example for Model Selection via Cross Validation.\n+ */\n+public class JavaModelSelectionViaCrossValidationExample {\n+  public static void main(String[] args) {\n+    SparkConf conf = new SparkConf()\n+      .setAppName(\"JavaModelSelectionViaCrossValidationExample\");\n+    SparkContext sc = new SparkContext(conf);\n+    SQLContext sqlContext = new SQLContext(sc);\n+\n+    // $example on$\n+    // Prepare training documents, which are labeled.\n+    DataFrame training = sqlContext.createDataFrame(Arrays.asList(\n+      new JavaLabeledDocument(0L, \"a b c d e spark\", 1.0),\n+      new JavaLabeledDocument(1L, \"b d\", 0.0),\n+      new JavaLabeledDocument(2L,\"spark f g h\", 1.0),\n+      new JavaLabeledDocument(3L, \"hadoop mapreduce\", 0.0),\n+      new JavaLabeledDocument(4L, \"b spark who\", 1.0),\n+      new JavaLabeledDocument(5L, \"g d a y\", 0.0),\n+      new JavaLabeledDocument(6L, \"spark fly\", 1.0),\n+      new JavaLabeledDocument(7L, \"was mapreduce\", 0.0),\n+      new JavaLabeledDocument(8L, \"e spark program\", 1.0),\n+      new JavaLabeledDocument(9L, \"a e c l\", 0.0),\n+      new JavaLabeledDocument(10L, \"spark compile\", 1.0),\n+      new JavaLabeledDocument(11L, \"hadoop software\", 0.0)\n+    ), JavaLabeledDocument.class);\n+\n+    // Configure an ML pipeline, which consists of three stages: tokenizer, hashingTF, and lr.\n+    Tokenizer tokenizer = new Tokenizer()\n+      .setInputCol(\"text\")\n+      .setOutputCol(\"words\");\n+    HashingTF hashingTF = new HashingTF()\n+      .setNumFeatures(1000)\n+      .setInputCol(tokenizer.getOutputCol())\n+      .setOutputCol(\"features\");\n+    LogisticRegression lr = new LogisticRegression()\n+      .setMaxIter(10)\n+      .setRegParam(0.01);\n+    Pipeline pipeline = new Pipeline()\n+      .setStages(new PipelineStage[] { tokenizer, hashingTF, lr });\n+\n+    // We use a ParamGridBuilder to construct a grid of parameters to search over.\n+    // With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n+    // this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n+    ParamMap[] paramGrid = new ParamGridBuilder()\n+      .addGrid(hashingTF.numFeatures(), new int[] { 10, 100, 1000 })\n+      .addGrid(lr.regParam(), new double[] { 0.1, 0.01 })"
  }],
  "prId": 11053
}]