[{
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "remove the blank line\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-16T00:46:18Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml;\n+\n+//$example on$\n+import java.util.Arrays;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.ml.classification.LogisticRegression;\n+import org.apache.spark.ml.classification.LogisticRegressionModel;\n+import org.apache.spark.ml.param.ParamMap;\n+import org.apache.spark.mllib.linalg.Vectors;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+"
  }],
  "prId": 11053
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "re-format the comments like:\n\n``` scala\n// We use LabeledPoint, which is a JavaBean. Spark SQL can convert RDDs of JavaBeans into\n// DataFrames, where it uses the bean metadata to infer the schema.\n```\n\nOnly break lines when reaching 100 characters or meaningful separations.\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-16T00:49:55Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml;\n+\n+//$example on$\n+import java.util.Arrays;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.ml.classification.LogisticRegression;\n+import org.apache.spark.ml.classification.LogisticRegressionModel;\n+import org.apache.spark.ml.param.ParamMap;\n+import org.apache.spark.mllib.linalg.Vectors;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+\n+//$example off$\n+\n+/**\n+ * Java example for Estimator, Transformer, and Param.\n+ */\n+public class JavaEstimatorTransformerParamExample {\n+  public static void main(String[] args) {\n+    SparkConf conf = new SparkConf()\n+      .setAppName(\"JavaEstimatorTransformerParamExample\");\n+    SparkContext sc = new SparkContext(conf);\n+    SQLContext sqlContext = new SQLContext(sc);\n+\n+    // $example on$\n+    // Prepare training data.\n+    // We use LabeledPoint, which is a JavaBean. Spark SQL can convert RDDs of\n+    // JavaBeans"
  }],
  "prId": 11053
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "It will be more clear if we re-format the code like this:\n\n``` scala\n    DataFrame training = sqlContext.createDataFrame(\n      Arrays.asList(\n        new LabeledPoint(1.0, Vectors.dense(0.0, 1.1, 0.1)),\n        new LabeledPoint(0.0, Vectors.dense(2.0, 1.0, -1.0)),\n        new LabeledPoint(0.0, Vectors.dense(2.0, 1.3, 1.0)),\n        new LabeledPoint(1.0, Vectors.dense(0.0, 1.2, -0.5))\n      ), LabeledPoint.class);\n```\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-16T00:52:46Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml;\n+\n+//$example on$\n+import java.util.Arrays;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.ml.classification.LogisticRegression;\n+import org.apache.spark.ml.classification.LogisticRegressionModel;\n+import org.apache.spark.ml.param.ParamMap;\n+import org.apache.spark.mllib.linalg.Vectors;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+\n+//$example off$\n+\n+/**\n+ * Java example for Estimator, Transformer, and Param.\n+ */\n+public class JavaEstimatorTransformerParamExample {\n+  public static void main(String[] args) {\n+    SparkConf conf = new SparkConf()\n+      .setAppName(\"JavaEstimatorTransformerParamExample\");\n+    SparkContext sc = new SparkContext(conf);\n+    SQLContext sqlContext = new SQLContext(sc);\n+\n+    // $example on$\n+    // Prepare training data.\n+    // We use LabeledPoint, which is a JavaBean. Spark SQL can convert RDDs of\n+    // JavaBeans\n+    // into DataFrames, where it uses the bean metadata to infer the schema.\n+    DataFrame training = sqlContext.createDataFrame(Arrays.asList("
  }],
  "prId": 11053
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "Merge the line with the previous one. Only break lines when necessary.\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-16T00:54:31Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml;\n+\n+//$example on$\n+import java.util.Arrays;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.ml.classification.LogisticRegression;\n+import org.apache.spark.ml.classification.LogisticRegressionModel;\n+import org.apache.spark.ml.param.ParamMap;\n+import org.apache.spark.mllib.linalg.Vectors;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+\n+//$example off$\n+\n+/**\n+ * Java example for Estimator, Transformer, and Param.\n+ */\n+public class JavaEstimatorTransformerParamExample {\n+  public static void main(String[] args) {\n+    SparkConf conf = new SparkConf()\n+      .setAppName(\"JavaEstimatorTransformerParamExample\");\n+    SparkContext sc = new SparkContext(conf);\n+    SQLContext sqlContext = new SQLContext(sc);\n+\n+    // $example on$\n+    // Prepare training data.\n+    // We use LabeledPoint, which is a JavaBean. Spark SQL can convert RDDs of\n+    // JavaBeans\n+    // into DataFrames, where it uses the bean metadata to infer the schema.\n+    DataFrame training = sqlContext.createDataFrame(Arrays.asList(\n+      new LabeledPoint(1.0, Vectors.dense(0.0, 1.1, 0.1)), new LabeledPoint(\n+        0.0, Vectors.dense(2.0, 1.0, -1.0)),\n+      new LabeledPoint(0.0, Vectors.dense(2.0, 1.3, 1.0)), new LabeledPoint(\n+        1.0, Vectors.dense(0.0, 1.2, -0.5))), LabeledPoint.class);\n+\n+    // Create a LogisticRegression instance. This instance is an Estimator.\n+    LogisticRegression lr = new LogisticRegression();\n+    // Print out the parameters, documentation, and any default values.\n+    System.out.println(\"LogisticRegression parameters:\\n\" + lr.explainParams()\n+      + \"\\n\");"
  }],
  "prId": 11053
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "reformat comments\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-16T00:56:14Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml;\n+\n+//$example on$\n+import java.util.Arrays;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.ml.classification.LogisticRegression;\n+import org.apache.spark.ml.classification.LogisticRegressionModel;\n+import org.apache.spark.ml.param.ParamMap;\n+import org.apache.spark.mllib.linalg.Vectors;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+\n+//$example off$\n+\n+/**\n+ * Java example for Estimator, Transformer, and Param.\n+ */\n+public class JavaEstimatorTransformerParamExample {\n+  public static void main(String[] args) {\n+    SparkConf conf = new SparkConf()\n+      .setAppName(\"JavaEstimatorTransformerParamExample\");\n+    SparkContext sc = new SparkContext(conf);\n+    SQLContext sqlContext = new SQLContext(sc);\n+\n+    // $example on$\n+    // Prepare training data.\n+    // We use LabeledPoint, which is a JavaBean. Spark SQL can convert RDDs of\n+    // JavaBeans\n+    // into DataFrames, where it uses the bean metadata to infer the schema.\n+    DataFrame training = sqlContext.createDataFrame(Arrays.asList(\n+      new LabeledPoint(1.0, Vectors.dense(0.0, 1.1, 0.1)), new LabeledPoint(\n+        0.0, Vectors.dense(2.0, 1.0, -1.0)),\n+      new LabeledPoint(0.0, Vectors.dense(2.0, 1.3, 1.0)), new LabeledPoint(\n+        1.0, Vectors.dense(0.0, 1.2, -0.5))), LabeledPoint.class);\n+\n+    // Create a LogisticRegression instance. This instance is an Estimator.\n+    LogisticRegression lr = new LogisticRegression();\n+    // Print out the parameters, documentation, and any default values.\n+    System.out.println(\"LogisticRegression parameters:\\n\" + lr.explainParams()\n+      + \"\\n\");\n+\n+    // We may set parameters using setter methods.\n+    lr.setMaxIter(10).setRegParam(0.01);\n+\n+    // Learn a LogisticRegression model. This uses the parameters stored in lr.\n+    LogisticRegressionModel model1 = lr.fit(training);\n+    // Since model1 is a Model (i.e., a Transformer produced by an Estimator),\n+    // we can view the parameters it used during fit().\n+    // This prints the parameter (name: value) pairs, where names are unique IDs\n+    // for this"
  }],
  "prId": 11053
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "merge into the previous line\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-16T00:56:41Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml;\n+\n+//$example on$\n+import java.util.Arrays;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.ml.classification.LogisticRegression;\n+import org.apache.spark.ml.classification.LogisticRegressionModel;\n+import org.apache.spark.ml.param.ParamMap;\n+import org.apache.spark.mllib.linalg.Vectors;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+\n+//$example off$\n+\n+/**\n+ * Java example for Estimator, Transformer, and Param.\n+ */\n+public class JavaEstimatorTransformerParamExample {\n+  public static void main(String[] args) {\n+    SparkConf conf = new SparkConf()\n+      .setAppName(\"JavaEstimatorTransformerParamExample\");\n+    SparkContext sc = new SparkContext(conf);\n+    SQLContext sqlContext = new SQLContext(sc);\n+\n+    // $example on$\n+    // Prepare training data.\n+    // We use LabeledPoint, which is a JavaBean. Spark SQL can convert RDDs of\n+    // JavaBeans\n+    // into DataFrames, where it uses the bean metadata to infer the schema.\n+    DataFrame training = sqlContext.createDataFrame(Arrays.asList(\n+      new LabeledPoint(1.0, Vectors.dense(0.0, 1.1, 0.1)), new LabeledPoint(\n+        0.0, Vectors.dense(2.0, 1.0, -1.0)),\n+      new LabeledPoint(0.0, Vectors.dense(2.0, 1.3, 1.0)), new LabeledPoint(\n+        1.0, Vectors.dense(0.0, 1.2, -0.5))), LabeledPoint.class);\n+\n+    // Create a LogisticRegression instance. This instance is an Estimator.\n+    LogisticRegression lr = new LogisticRegression();\n+    // Print out the parameters, documentation, and any default values.\n+    System.out.println(\"LogisticRegression parameters:\\n\" + lr.explainParams()\n+      + \"\\n\");\n+\n+    // We may set parameters using setter methods.\n+    lr.setMaxIter(10).setRegParam(0.01);\n+\n+    // Learn a LogisticRegression model. This uses the parameters stored in lr.\n+    LogisticRegressionModel model1 = lr.fit(training);\n+    // Since model1 is a Model (i.e., a Transformer produced by an Estimator),\n+    // we can view the parameters it used during fit().\n+    // This prints the parameter (name: value) pairs, where names are unique IDs\n+    // for this\n+    // LogisticRegression instance.\n+    System.out.println(\"Model 1 was fit using parameters: \"\n+      + model1.parent().extractParamMap());"
  }],
  "prId": 11053
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "reformat the code. Use 2-indent for inline comments, i.e.\n\n``` scala\n.put(lr.maxIter(), 30)  // This overwrites the original maxIter.\n```\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-16T01:00:49Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml;\n+\n+//$example on$\n+import java.util.Arrays;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.ml.classification.LogisticRegression;\n+import org.apache.spark.ml.classification.LogisticRegressionModel;\n+import org.apache.spark.ml.param.ParamMap;\n+import org.apache.spark.mllib.linalg.Vectors;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+\n+//$example off$\n+\n+/**\n+ * Java example for Estimator, Transformer, and Param.\n+ */\n+public class JavaEstimatorTransformerParamExample {\n+  public static void main(String[] args) {\n+    SparkConf conf = new SparkConf()\n+      .setAppName(\"JavaEstimatorTransformerParamExample\");\n+    SparkContext sc = new SparkContext(conf);\n+    SQLContext sqlContext = new SQLContext(sc);\n+\n+    // $example on$\n+    // Prepare training data.\n+    // We use LabeledPoint, which is a JavaBean. Spark SQL can convert RDDs of\n+    // JavaBeans\n+    // into DataFrames, where it uses the bean metadata to infer the schema.\n+    DataFrame training = sqlContext.createDataFrame(Arrays.asList(\n+      new LabeledPoint(1.0, Vectors.dense(0.0, 1.1, 0.1)), new LabeledPoint(\n+        0.0, Vectors.dense(2.0, 1.0, -1.0)),\n+      new LabeledPoint(0.0, Vectors.dense(2.0, 1.3, 1.0)), new LabeledPoint(\n+        1.0, Vectors.dense(0.0, 1.2, -0.5))), LabeledPoint.class);\n+\n+    // Create a LogisticRegression instance. This instance is an Estimator.\n+    LogisticRegression lr = new LogisticRegression();\n+    // Print out the parameters, documentation, and any default values.\n+    System.out.println(\"LogisticRegression parameters:\\n\" + lr.explainParams()\n+      + \"\\n\");\n+\n+    // We may set parameters using setter methods.\n+    lr.setMaxIter(10).setRegParam(0.01);\n+\n+    // Learn a LogisticRegression model. This uses the parameters stored in lr.\n+    LogisticRegressionModel model1 = lr.fit(training);\n+    // Since model1 is a Model (i.e., a Transformer produced by an Estimator),\n+    // we can view the parameters it used during fit().\n+    // This prints the parameter (name: value) pairs, where names are unique IDs\n+    // for this\n+    // LogisticRegression instance.\n+    System.out.println(\"Model 1 was fit using parameters: \"\n+      + model1.parent().extractParamMap());\n+\n+    // We may alternatively specify parameters using a ParamMap.\n+    // Specify 1 Param.\n+    ParamMap paramMap = new ParamMap().put(lr.maxIter().w(20))\n+      .put(lr.maxIter(), 30) // This overwrites the original maxIter."
  }],
  "prId": 11053
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "merge with the previous line\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-16T01:01:33Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml;\n+\n+//$example on$\n+import java.util.Arrays;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.ml.classification.LogisticRegression;\n+import org.apache.spark.ml.classification.LogisticRegressionModel;\n+import org.apache.spark.ml.param.ParamMap;\n+import org.apache.spark.mllib.linalg.Vectors;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+\n+//$example off$\n+\n+/**\n+ * Java example for Estimator, Transformer, and Param.\n+ */\n+public class JavaEstimatorTransformerParamExample {\n+  public static void main(String[] args) {\n+    SparkConf conf = new SparkConf()\n+      .setAppName(\"JavaEstimatorTransformerParamExample\");\n+    SparkContext sc = new SparkContext(conf);\n+    SQLContext sqlContext = new SQLContext(sc);\n+\n+    // $example on$\n+    // Prepare training data.\n+    // We use LabeledPoint, which is a JavaBean. Spark SQL can convert RDDs of\n+    // JavaBeans\n+    // into DataFrames, where it uses the bean metadata to infer the schema.\n+    DataFrame training = sqlContext.createDataFrame(Arrays.asList(\n+      new LabeledPoint(1.0, Vectors.dense(0.0, 1.1, 0.1)), new LabeledPoint(\n+        0.0, Vectors.dense(2.0, 1.0, -1.0)),\n+      new LabeledPoint(0.0, Vectors.dense(2.0, 1.3, 1.0)), new LabeledPoint(\n+        1.0, Vectors.dense(0.0, 1.2, -0.5))), LabeledPoint.class);\n+\n+    // Create a LogisticRegression instance. This instance is an Estimator.\n+    LogisticRegression lr = new LogisticRegression();\n+    // Print out the parameters, documentation, and any default values.\n+    System.out.println(\"LogisticRegression parameters:\\n\" + lr.explainParams()\n+      + \"\\n\");\n+\n+    // We may set parameters using setter methods.\n+    lr.setMaxIter(10).setRegParam(0.01);\n+\n+    // Learn a LogisticRegression model. This uses the parameters stored in lr.\n+    LogisticRegressionModel model1 = lr.fit(training);\n+    // Since model1 is a Model (i.e., a Transformer produced by an Estimator),\n+    // we can view the parameters it used during fit().\n+    // This prints the parameter (name: value) pairs, where names are unique IDs\n+    // for this\n+    // LogisticRegression instance.\n+    System.out.println(\"Model 1 was fit using parameters: \"\n+      + model1.parent().extractParamMap());\n+\n+    // We may alternatively specify parameters using a ParamMap.\n+    // Specify 1 Param.\n+    ParamMap paramMap = new ParamMap().put(lr.maxIter().w(20))\n+      .put(lr.maxIter(), 30) // This overwrites the original maxIter.\n+      // Specify multiple Params.\n+      .put(lr.regParam().w(0.1), lr.threshold().w(0.55));\n+\n+    // One can also combine ParamMaps.\n+    ParamMap paramMap2 = new ParamMap().put(lr.probabilityCol().w(\n+      \"myProbability\")); // Change output column name"
  }],
  "prId": 11053
}, {
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "reformat comments\n",
    "commit": "2fe06672cc9827545fe34a22fc38f2e304243205",
    "createdAt": "2016-02-16T01:01:57Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.ml;\n+\n+//$example on$\n+import java.util.Arrays;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.SparkContext;\n+import org.apache.spark.ml.classification.LogisticRegression;\n+import org.apache.spark.ml.classification.LogisticRegressionModel;\n+import org.apache.spark.ml.param.ParamMap;\n+import org.apache.spark.mllib.linalg.Vectors;\n+import org.apache.spark.mllib.regression.LabeledPoint;\n+import org.apache.spark.sql.DataFrame;\n+import org.apache.spark.sql.Row;\n+import org.apache.spark.sql.SQLContext;\n+\n+//$example off$\n+\n+/**\n+ * Java example for Estimator, Transformer, and Param.\n+ */\n+public class JavaEstimatorTransformerParamExample {\n+  public static void main(String[] args) {\n+    SparkConf conf = new SparkConf()\n+      .setAppName(\"JavaEstimatorTransformerParamExample\");\n+    SparkContext sc = new SparkContext(conf);\n+    SQLContext sqlContext = new SQLContext(sc);\n+\n+    // $example on$\n+    // Prepare training data.\n+    // We use LabeledPoint, which is a JavaBean. Spark SQL can convert RDDs of\n+    // JavaBeans\n+    // into DataFrames, where it uses the bean metadata to infer the schema.\n+    DataFrame training = sqlContext.createDataFrame(Arrays.asList(\n+      new LabeledPoint(1.0, Vectors.dense(0.0, 1.1, 0.1)), new LabeledPoint(\n+        0.0, Vectors.dense(2.0, 1.0, -1.0)),\n+      new LabeledPoint(0.0, Vectors.dense(2.0, 1.3, 1.0)), new LabeledPoint(\n+        1.0, Vectors.dense(0.0, 1.2, -0.5))), LabeledPoint.class);\n+\n+    // Create a LogisticRegression instance. This instance is an Estimator.\n+    LogisticRegression lr = new LogisticRegression();\n+    // Print out the parameters, documentation, and any default values.\n+    System.out.println(\"LogisticRegression parameters:\\n\" + lr.explainParams()\n+      + \"\\n\");\n+\n+    // We may set parameters using setter methods.\n+    lr.setMaxIter(10).setRegParam(0.01);\n+\n+    // Learn a LogisticRegression model. This uses the parameters stored in lr.\n+    LogisticRegressionModel model1 = lr.fit(training);\n+    // Since model1 is a Model (i.e., a Transformer produced by an Estimator),\n+    // we can view the parameters it used during fit().\n+    // This prints the parameter (name: value) pairs, where names are unique IDs\n+    // for this\n+    // LogisticRegression instance.\n+    System.out.println(\"Model 1 was fit using parameters: \"\n+      + model1.parent().extractParamMap());\n+\n+    // We may alternatively specify parameters using a ParamMap.\n+    // Specify 1 Param.\n+    ParamMap paramMap = new ParamMap().put(lr.maxIter().w(20))\n+      .put(lr.maxIter(), 30) // This overwrites the original maxIter.\n+      // Specify multiple Params.\n+      .put(lr.regParam().w(0.1), lr.threshold().w(0.55));\n+\n+    // One can also combine ParamMaps.\n+    ParamMap paramMap2 = new ParamMap().put(lr.probabilityCol().w(\n+      \"myProbability\")); // Change output column name\n+    ParamMap paramMapCombined = paramMap.$plus$plus(paramMap2);\n+\n+    // Now learn a new model using the paramMapCombined parameters."
  }],
  "prId": 11053
}]