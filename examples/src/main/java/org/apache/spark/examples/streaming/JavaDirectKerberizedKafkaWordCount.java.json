[{
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "Could you add a corresponding Scala example, too?",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-13T02:09:24Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {",
    "line": 91
  }],
  "prId": 25412
}, {
  "comments": [{
    "author": {
      "login": "dongjoon-hyun"
    },
    "body": "`JavaDirectKafkaWordCount` -> `JavaDirectKerberizedKafkaWordCount`?",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-13T02:09:48Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKafkaWordCount <brokers> <groupId> <topics>"
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "> `JavaDirectKafkaWordCount` -> `JavaDirectKerberizedKafkaWordCount`?\r\n\r\nMy carelessness, has been corrected.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-13T02:18:35Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKafkaWordCount <brokers> <groupId> <topics>"
  }],
  "prId": 25412
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "The indentation should be double-spaced for whole file.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-13T06:09:24Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {\n+    private static final Pattern SPACE = Pattern.compile(\" \");\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 3) {"
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "> The indentation should be double-spaced for whole file.\r\n\r\nThanks,has been corrected.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-13T07:18:30Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {\n+    private static final Pattern SPACE = Pattern.compile(\" \");\n+\n+    public static void main(String[] args) throws Exception {\n+        if (args.length < 3) {"
  }],
  "prId": 25412
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Indentations like this are blown up in the files in many places.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-13T08:20:26Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {\n+  private static final Pattern SPACE = Pattern.compile(\" \");\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 3) {\n+      System.err.println(\n+              \"Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\\n\" +\n+                      \"  <brokers> is a list of one or more Kafka brokers\\n\" +\n+                      \"  <groupId> is a consumer group name to consume from topics\\n\" +\n+                      \"  <topics> is a list of one or more kafka topics to consume from\\n\\n\");\n+      System.exit(1);\n+    }\n+\n+    StreamingExamples.setStreamingLogLevels();\n+\n+    String brokers = args[0];\n+    String groupId = args[1];\n+    String topics = args[2];\n+\n+    // Create context with a 2 seconds batch interval\n+    SparkConf sparkConf = new SparkConf().setAppName(\"JavaDirectKerberizedKafkaWordCount\");\n+    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));\n+\n+    Set<String> topicsSet = new HashSet<>(Arrays.asList(topics.split(\",\")));\n+    Map<String, Object> kafkaParams = new HashMap<>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);\n+    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,\n+                                                  SecurityProtocol.SASL_PLAINTEXT.name);\n+\n+    // Create direct kafka stream with brokers and topics\n+    JavaInputDStream<ConsumerRecord<String, String>> messages = KafkaUtils.createDirectStream(\n+            jssc,\n+            LocationStrategies.PreferConsistent(),\n+            ConsumerStrategies.Subscribe(topicsSet, kafkaParams));\n+\n+    // Get the lines, split them into words, count the words and print\n+    JavaDStream<String> lines = messages.map(ConsumerRecord::value);\n+    JavaDStream<String> words = lines.flatMap(x -> Arrays.asList(SPACE.split(x)).iterator());\n+    JavaPairDStream<String, Integer> wordCounts = words.mapToPair(s -> new Tuple2<>(s, 1))\n+            .reduceByKey((i1, i2) -> i1 + i2);"
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "> Indentations like this are blown up in the files in many places.\r\n\r\nyep. But it as same as `JavaDirectKafkaWordCount` and I think it has no effect.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-13T09:11:39Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {\n+  private static final Pattern SPACE = Pattern.compile(\" \");\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 3) {\n+      System.err.println(\n+              \"Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\\n\" +\n+                      \"  <brokers> is a list of one or more Kafka brokers\\n\" +\n+                      \"  <groupId> is a consumer group name to consume from topics\\n\" +\n+                      \"  <topics> is a list of one or more kafka topics to consume from\\n\\n\");\n+      System.exit(1);\n+    }\n+\n+    StreamingExamples.setStreamingLogLevels();\n+\n+    String brokers = args[0];\n+    String groupId = args[1];\n+    String topics = args[2];\n+\n+    // Create context with a 2 seconds batch interval\n+    SparkConf sparkConf = new SparkConf().setAppName(\"JavaDirectKerberizedKafkaWordCount\");\n+    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));\n+\n+    Set<String> topicsSet = new HashSet<>(Arrays.asList(topics.split(\",\")));\n+    Map<String, Object> kafkaParams = new HashMap<>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);\n+    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,\n+                                                  SecurityProtocol.SASL_PLAINTEXT.name);\n+\n+    // Create direct kafka stream with brokers and topics\n+    JavaInputDStream<ConsumerRecord<String, String>> messages = KafkaUtils.createDirectStream(\n+            jssc,\n+            LocationStrategies.PreferConsistent(),\n+            ConsumerStrategies.Subscribe(topicsSet, kafkaParams));\n+\n+    // Get the lines, split them into words, count the words and print\n+    JavaDStream<String> lines = messages.map(ConsumerRecord::value);\n+    JavaDStream<String> words = lines.flatMap(x -> Arrays.asList(SPACE.split(x)).iterator());\n+    JavaPairDStream<String, Integer> wordCounts = words.mapToPair(s -> new Tuple2<>(s, 1))\n+            .reduceByKey((i1, i2) -> i1 + i2);"
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "Let's stick to 2-spaces. It should follow Scala guide when applicable.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-13T09:35:08Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {\n+  private static final Pattern SPACE = Pattern.compile(\" \");\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 3) {\n+      System.err.println(\n+              \"Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\\n\" +\n+                      \"  <brokers> is a list of one or more Kafka brokers\\n\" +\n+                      \"  <groupId> is a consumer group name to consume from topics\\n\" +\n+                      \"  <topics> is a list of one or more kafka topics to consume from\\n\\n\");\n+      System.exit(1);\n+    }\n+\n+    StreamingExamples.setStreamingLogLevels();\n+\n+    String brokers = args[0];\n+    String groupId = args[1];\n+    String topics = args[2];\n+\n+    // Create context with a 2 seconds batch interval\n+    SparkConf sparkConf = new SparkConf().setAppName(\"JavaDirectKerberizedKafkaWordCount\");\n+    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));\n+\n+    Set<String> topicsSet = new HashSet<>(Arrays.asList(topics.split(\",\")));\n+    Map<String, Object> kafkaParams = new HashMap<>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);\n+    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,\n+                                                  SecurityProtocol.SASL_PLAINTEXT.name);\n+\n+    // Create direct kafka stream with brokers and topics\n+    JavaInputDStream<ConsumerRecord<String, String>> messages = KafkaUtils.createDirectStream(\n+            jssc,\n+            LocationStrategies.PreferConsistent(),\n+            ConsumerStrategies.Subscribe(topicsSet, kafkaParams));\n+\n+    // Get the lines, split them into words, count the words and print\n+    JavaDStream<String> lines = messages.map(ConsumerRecord::value);\n+    JavaDStream<String> words = lines.flatMap(x -> Arrays.asList(SPACE.split(x)).iterator());\n+    JavaPairDStream<String, Integer> wordCounts = words.mapToPair(s -> new Tuple2<>(s, 1))\n+            .reduceByKey((i1, i2) -> i1 + i2);"
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "ok, I will make it 2-spaces.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-13T09:41:10Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {\n+  private static final Pattern SPACE = Pattern.compile(\" \");\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 3) {\n+      System.err.println(\n+              \"Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\\n\" +\n+                      \"  <brokers> is a list of one or more Kafka brokers\\n\" +\n+                      \"  <groupId> is a consumer group name to consume from topics\\n\" +\n+                      \"  <topics> is a list of one or more kafka topics to consume from\\n\\n\");\n+      System.exit(1);\n+    }\n+\n+    StreamingExamples.setStreamingLogLevels();\n+\n+    String brokers = args[0];\n+    String groupId = args[1];\n+    String topics = args[2];\n+\n+    // Create context with a 2 seconds batch interval\n+    SparkConf sparkConf = new SparkConf().setAppName(\"JavaDirectKerberizedKafkaWordCount\");\n+    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));\n+\n+    Set<String> topicsSet = new HashSet<>(Arrays.asList(topics.split(\",\")));\n+    Map<String, Object> kafkaParams = new HashMap<>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);\n+    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,\n+                                                  SecurityProtocol.SASL_PLAINTEXT.name);\n+\n+    // Create direct kafka stream with brokers and topics\n+    JavaInputDStream<ConsumerRecord<String, String>> messages = KafkaUtils.createDirectStream(\n+            jssc,\n+            LocationStrategies.PreferConsistent(),\n+            ConsumerStrategies.Subscribe(topicsSet, kafkaParams));\n+\n+    // Get the lines, split them into words, count the words and print\n+    JavaDStream<String> lines = messages.map(ConsumerRecord::value);\n+    JavaDStream<String> words = lines.flatMap(x -> Arrays.asList(SPACE.split(x)).iterator());\n+    JavaPairDStream<String, Integer> wordCounts = words.mapToPair(s -> new Tuple2<>(s, 1))\n+            .reduceByKey((i1, i2) -> i1 + i2);"
  }],
  "prId": 25412
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "`SASL_PLAINTEXT` is only for testing.\r\nEither I would use `SASL_SSL` or log a warning like `SASL_PLAINTEXT is only for testing...`.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-13T08:22:58Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {\n+  private static final Pattern SPACE = Pattern.compile(\" \");\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 3) {\n+      System.err.println(\n+              \"Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\\n\" +\n+                      \"  <brokers> is a list of one or more Kafka brokers\\n\" +\n+                      \"  <groupId> is a consumer group name to consume from topics\\n\" +\n+                      \"  <topics> is a list of one or more kafka topics to consume from\\n\\n\");\n+      System.exit(1);\n+    }\n+\n+    StreamingExamples.setStreamingLogLevels();\n+\n+    String brokers = args[0];\n+    String groupId = args[1];\n+    String topics = args[2];\n+\n+    // Create context with a 2 seconds batch interval\n+    SparkConf sparkConf = new SparkConf().setAppName(\"JavaDirectKerberizedKafkaWordCount\");\n+    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));\n+\n+    Set<String> topicsSet = new HashSet<>(Arrays.asList(topics.split(\",\")));\n+    Map<String, Object> kafkaParams = new HashMap<>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);\n+    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,\n+                                                  SecurityProtocol.SASL_PLAINTEXT.name);",
    "line": 121
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "> `SASL_PLAINTEXT` is only for testing.\r\n> Either I would use `SASL_SSL` or log a warning like `SASL_PLAINTEXT is only for testing...`.\r\n\r\nAs headline said there are kerberized, usually, kerberos and `SASL_PLAINTEXT` work together",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-13T09:17:03Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {\n+  private static final Pattern SPACE = Pattern.compile(\" \");\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 3) {\n+      System.err.println(\n+              \"Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\\n\" +\n+                      \"  <brokers> is a list of one or more Kafka brokers\\n\" +\n+                      \"  <groupId> is a consumer group name to consume from topics\\n\" +\n+                      \"  <topics> is a list of one or more kafka topics to consume from\\n\\n\");\n+      System.exit(1);\n+    }\n+\n+    StreamingExamples.setStreamingLogLevels();\n+\n+    String brokers = args[0];\n+    String groupId = args[1];\n+    String topics = args[2];\n+\n+    // Create context with a 2 seconds batch interval\n+    SparkConf sparkConf = new SparkConf().setAppName(\"JavaDirectKerberizedKafkaWordCount\");\n+    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));\n+\n+    Set<String> topicsSet = new HashSet<>(Arrays.asList(topics.split(\",\")));\n+    Map<String, Object> kafkaParams = new HashMap<>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);\n+    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,\n+                                                  SecurityProtocol.SASL_PLAINTEXT.name);",
    "line": 121
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I'm not questioning whether `SASL_PLAINTEXT` works or not, it is. I'm telling that using kerberos on plain text channel is coming from evil from security perspective. Somehow we should tell the users it's not the advised way because credentials can be sniffed.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-13T10:47:34Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {\n+  private static final Pattern SPACE = Pattern.compile(\" \");\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 3) {\n+      System.err.println(\n+              \"Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\\n\" +\n+                      \"  <brokers> is a list of one or more Kafka brokers\\n\" +\n+                      \"  <groupId> is a consumer group name to consume from topics\\n\" +\n+                      \"  <topics> is a list of one or more kafka topics to consume from\\n\\n\");\n+      System.exit(1);\n+    }\n+\n+    StreamingExamples.setStreamingLogLevels();\n+\n+    String brokers = args[0];\n+    String groupId = args[1];\n+    String topics = args[2];\n+\n+    // Create context with a 2 seconds batch interval\n+    SparkConf sparkConf = new SparkConf().setAppName(\"JavaDirectKerberizedKafkaWordCount\");\n+    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));\n+\n+    Set<String> topicsSet = new HashSet<>(Arrays.asList(topics.split(\",\")));\n+    Map<String, Object> kafkaParams = new HashMap<>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);\n+    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,\n+                                                  SecurityProtocol.SASL_PLAINTEXT.name);",
    "line": 121
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "> I'm not questioning whether `SASL_PLAINTEXT` works or not, it is. I'm telling that using kerberos on plain text channel is coming from evil from security perspective. Somehow we should tell the users it's not the advised way because credentials can be sniffed.\r\n\r\nI  agree it's not completely secure. It's just for testing, if only `SASL_PLAINTEXT` used. But, I don't think it's only for testing with kerbreos. \r\n@dongjoon-hyun @srowen @HyukjinKwon ,can you give some advice.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-14T02:29:45Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {\n+  private static final Pattern SPACE = Pattern.compile(\" \");\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 3) {\n+      System.err.println(\n+              \"Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\\n\" +\n+                      \"  <brokers> is a list of one or more Kafka brokers\\n\" +\n+                      \"  <groupId> is a consumer group name to consume from topics\\n\" +\n+                      \"  <topics> is a list of one or more kafka topics to consume from\\n\\n\");\n+      System.exit(1);\n+    }\n+\n+    StreamingExamples.setStreamingLogLevels();\n+\n+    String brokers = args[0];\n+    String groupId = args[1];\n+    String topics = args[2];\n+\n+    // Create context with a 2 seconds batch interval\n+    SparkConf sparkConf = new SparkConf().setAppName(\"JavaDirectKerberizedKafkaWordCount\");\n+    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));\n+\n+    Set<String> topicsSet = new HashSet<>(Arrays.asList(topics.split(\",\")));\n+    Map<String, Object> kafkaParams = new HashMap<>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);\n+    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,\n+                                                  SecurityProtocol.SASL_PLAINTEXT.name);",
    "line": 121
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "I think you can just explain why `SASL_PLAINTEXT` is discouraged (SSL encryption is disabled and why it's dangerous) in a comment.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-14T03:21:51Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {\n+  private static final Pattern SPACE = Pattern.compile(\" \");\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 3) {\n+      System.err.println(\n+              \"Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\\n\" +\n+                      \"  <brokers> is a list of one or more Kafka brokers\\n\" +\n+                      \"  <groupId> is a consumer group name to consume from topics\\n\" +\n+                      \"  <topics> is a list of one or more kafka topics to consume from\\n\\n\");\n+      System.exit(1);\n+    }\n+\n+    StreamingExamples.setStreamingLogLevels();\n+\n+    String brokers = args[0];\n+    String groupId = args[1];\n+    String topics = args[2];\n+\n+    // Create context with a 2 seconds batch interval\n+    SparkConf sparkConf = new SparkConf().setAppName(\"JavaDirectKerberizedKafkaWordCount\");\n+    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));\n+\n+    Set<String> topicsSet = new HashSet<>(Arrays.asList(topics.split(\",\")));\n+    Map<String, Object> kafkaParams = new HashMap<>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);\n+    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,\n+                                                  SecurityProtocol.SASL_PLAINTEXT.name);",
    "line": 121
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "As I understand it, `SASL_PLAINTEXT` is plaintext, we can get username password and other information easy. But with kerberos, can get location of keytab on server and principal only, you can't get keytab file. So, it's enough. \r\nSSL encryption and kerberos both cause loss of performance.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-14T07:24:17Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {\n+  private static final Pattern SPACE = Pattern.compile(\" \");\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 3) {\n+      System.err.println(\n+              \"Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\\n\" +\n+                      \"  <brokers> is a list of one or more Kafka brokers\\n\" +\n+                      \"  <groupId> is a consumer group name to consume from topics\\n\" +\n+                      \"  <topics> is a list of one or more kafka topics to consume from\\n\\n\");\n+      System.exit(1);\n+    }\n+\n+    StreamingExamples.setStreamingLogLevels();\n+\n+    String brokers = args[0];\n+    String groupId = args[1];\n+    String topics = args[2];\n+\n+    // Create context with a 2 seconds batch interval\n+    SparkConf sparkConf = new SparkConf().setAppName(\"JavaDirectKerberizedKafkaWordCount\");\n+    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));\n+\n+    Set<String> topicsSet = new HashSet<>(Arrays.asList(topics.split(\",\")));\n+    Map<String, Object> kafkaParams = new HashMap<>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);\n+    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,\n+                                                  SecurityProtocol.SASL_PLAINTEXT.name);",
    "line": 121
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "> SSL encryption and kerberos both cause loss of performance.\r\n\r\nAs any kind of security solution in the world.\r\n",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-14T08:41:36Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {\n+  private static final Pattern SPACE = Pattern.compile(\" \");\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 3) {\n+      System.err.println(\n+              \"Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\\n\" +\n+                      \"  <brokers> is a list of one or more Kafka brokers\\n\" +\n+                      \"  <groupId> is a consumer group name to consume from topics\\n\" +\n+                      \"  <topics> is a list of one or more kafka topics to consume from\\n\\n\");\n+      System.exit(1);\n+    }\n+\n+    StreamingExamples.setStreamingLogLevels();\n+\n+    String brokers = args[0];\n+    String groupId = args[1];\n+    String topics = args[2];\n+\n+    // Create context with a 2 seconds batch interval\n+    SparkConf sparkConf = new SparkConf().setAppName(\"JavaDirectKerberizedKafkaWordCount\");\n+    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));\n+\n+    Set<String> topicsSet = new HashSet<>(Arrays.asList(topics.split(\",\")));\n+    Map<String, Object> kafkaParams = new HashMap<>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);\n+    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,\n+                                                  SecurityProtocol.SASL_PLAINTEXT.name);",
    "line": 121
  }, {
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "@hddong, the purpose of doing Kerberos is for security. If we want more performance over security, there's no point of using Kerberos.\r\n\r\nIt won't hurt to describe `SASL_PLAINTEXT` vs `SASL_SSL`. You can just say, for example:\r\nThis file uses `SASL_PLAINTEXT` for simplicity; however, `SASL_PLAINTEXT` has no SSL encryption and likely be less secure. Consider using `SASL_SSL`.\r\n\r\nFeel free to reword and rephrase it ^.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-14T09:11:30Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {\n+  private static final Pattern SPACE = Pattern.compile(\" \");\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 3) {\n+      System.err.println(\n+              \"Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\\n\" +\n+                      \"  <brokers> is a list of one or more Kafka brokers\\n\" +\n+                      \"  <groupId> is a consumer group name to consume from topics\\n\" +\n+                      \"  <topics> is a list of one or more kafka topics to consume from\\n\\n\");\n+      System.exit(1);\n+    }\n+\n+    StreamingExamples.setStreamingLogLevels();\n+\n+    String brokers = args[0];\n+    String groupId = args[1];\n+    String topics = args[2];\n+\n+    // Create context with a 2 seconds batch interval\n+    SparkConf sparkConf = new SparkConf().setAppName(\"JavaDirectKerberizedKafkaWordCount\");\n+    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));\n+\n+    Set<String> topicsSet = new HashSet<>(Arrays.asList(topics.split(\",\")));\n+    Map<String, Object> kafkaParams = new HashMap<>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);\n+    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,\n+                                                  SecurityProtocol.SASL_PLAINTEXT.name);",
    "line": 121
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "> @hddong, the purpose of doing Kerberos is for security. If we want more performance over security, there's no point of using Kerberos.\r\n> \r\n> It won't hurt to describe `SASL_PLAINTEXT` vs `SASL_SSL`. You can just say, for example:\r\n> This file uses `SASL_PLAINTEXT` for simplicity; however, `SASL_PLAINTEXT` has no SSL encryption and likely be less secure. Consider using `SASL_SSL`.\r\n> \r\n> Feel free to reword and rephrase it ^.\r\n\r\nThanks, worried about my reading comprehension ability. I will add note for this file.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-15T03:05:43Z",
    "diffHunk": "@@ -0,0 +1,107 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ */\n+\n+public final class JavaDirectKerberizedKafkaWordCount {\n+  private static final Pattern SPACE = Pattern.compile(\" \");\n+\n+  public static void main(String[] args) throws Exception {\n+    if (args.length < 3) {\n+      System.err.println(\n+              \"Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\\n\" +\n+                      \"  <brokers> is a list of one or more Kafka brokers\\n\" +\n+                      \"  <groupId> is a consumer group name to consume from topics\\n\" +\n+                      \"  <topics> is a list of one or more kafka topics to consume from\\n\\n\");\n+      System.exit(1);\n+    }\n+\n+    StreamingExamples.setStreamingLogLevels();\n+\n+    String brokers = args[0];\n+    String groupId = args[1];\n+    String topics = args[2];\n+\n+    // Create context with a 2 seconds batch interval\n+    SparkConf sparkConf = new SparkConf().setAppName(\"JavaDirectKerberizedKafkaWordCount\");\n+    JavaStreamingContext jssc = new JavaStreamingContext(sparkConf, Durations.seconds(2));\n+\n+    Set<String> topicsSet = new HashSet<>(Arrays.asList(topics.split(\",\")));\n+    Map<String, Object> kafkaParams = new HashMap<>();\n+    kafkaParams.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, brokers);\n+    kafkaParams.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);\n+    kafkaParams.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);\n+    kafkaParams.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,\n+                                                  SecurityProtocol.SASL_PLAINTEXT.name);",
    "line": 121
  }],
  "prId": 25412
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Since the keytab file is not added to the `--files` section it must exist on every server. Worth to mention as prerequisite since it's not obvious for everybody.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-14T09:34:23Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required\n+ *     keyTab=\"${path_of_keytab}/kafka.service.keytab\""
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "> Since the keytab file is not added to the `--files` section it must exist on every server. Worth to mention as prerequisite since it's not obvious for everybody.\r\n\r\nAs cmd example show, `--file` should used.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-15T01:26:45Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required\n+ *     keyTab=\"${path_of_keytab}/kafka.service.keytab\""
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "I'm fine to add keytab file to the `--files` section but then the keytab path in the jaas file has to be modified to `./kafka.service.keytab` since `--files` doesn't preserve the path.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-15T11:29:32Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required\n+ *     keyTab=\"${path_of_keytab}/kafka.service.keytab\""
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "@gaborgsomogyi you are right, in my environment, keytab in same path of each node. I think we can two use jaas files:\r\nfor driver `keyTab=\"${path_of_keytab}/kafka.service.keytab\"`\r\nfor executor `keyTab=\"./kafka.service.keytab\"`\r\nand run-example as:\r\n`bin/run-example --files` `${path}/kafka_executor_jaas.conf,${kyetab_path}/kafka.service.keytab \\`\r\n ` --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_driver_jaas.conf\" \\`\r\n ` --conf \\`\r\n  `\"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_executor_jaas.conf\" \\`\r\n `streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\`\r\n ` consumer-group topic1,topic2`\r\nThat will become more general. Do you have any succinct advice?",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-16T03:16:14Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required\n+ *     keyTab=\"${path_of_keytab}/kafka.service.keytab\""
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Your last example is almost good but it will work only in client mode. In cluster mode the driver runs on a random machine where maybe the keytab not exists. I would suggest this:\r\n```\r\nbin/run-example --files ${jaas_path}/jaas.conf,${keytab_path}/kafka.service.keytab \\\r\n--driver-java-options \"-Djava.security.auth.login.config=./jaas.conf\" \\\r\n--conf \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./jaas.conf\" \\\r\nstreaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\r\nconsumer-group topic1,topic2\r\n```\r\nand in the jaas file:\r\n```\r\n...\r\n  keyTab=\"./kafka.service.keytab\"\r\n...\r\n```\r\n",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-16T12:09:30Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required\n+ *     keyTab=\"${path_of_keytab}/kafka.service.keytab\""
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "> Your last example is almost good but it will work only in client mode. In cluster mode the driver runs on a random machine where maybe the keytab not exists. I would suggest this:\r\n> \r\n> ```\r\n> bin/run-example --files ${jaas_path}/jaas.conf,${keytab_path}/kafka.service.keytab \\\r\n> --driver-java-options \"-Djava.security.auth.login.config=./jaas.conf\" \\\r\n> --conf \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./jaas.conf\" \\\r\n> streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\r\n> consumer-group topic1,topic2\r\n> ```\r\n> \r\n> and in the jaas file:\r\n> \r\n> ```\r\n> ...\r\n>   keyTab=\"./kafka.service.keytab\"\r\n> ...\r\n> ```\r\n\r\nYes, cluster is diff form client. It need a different cmd.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-19T06:36:02Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required\n+ *     keyTab=\"${path_of_keytab}/kafka.service.keytab\""
  }],
  "prId": 25412
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "This works on sun JVMs, on IBM `com.ibm.security.auth.module.Krb5LoginModule` is needed.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-14T09:35:06Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required",
    "line": 72
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "> This works on sun JVMs, on IBM `com.ibm.security.auth.module.Krb5LoginModule` is needed.\r\n\r\nThanks for your remind.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-15T01:30:19Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required",
    "line": 72
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "This would be good to mention as a comment since users may not read this discussion.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-21T09:48:48Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required",
    "line": 72
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "> This would be good to mention as a comment since users may not read this discussion.\r\n\r\nYes, the comment will add.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-22T02:02:52Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required",
    "line": 72
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "OK, this can be resolved.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-26T09:47:33Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required",
    "line": 72
  }],
  "prId": 25412
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "Did this work? With what kind of KDC setting have you tested this?\r\n",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-14T09:41:13Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required\n+ *     keyTab=\"${path_of_keytab}/kafka.service.keytab\"\n+ *     useKeyTab=true\n+ *     storeKey=true\n+ *     useTicketCache=false\n+ *     serviceName=\"kafka\"\n+ *     principal=\"kafka/server@example\";"
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "> Did this work? With what kind of KDC setting have you tested this?\r\n\r\nYes, it work fine, I had test it.Which part do you think isn’t correct, principal or there is something wrong with the whole template.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-15T01:17:24Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required\n+ *     keyTab=\"${path_of_keytab}/kafka.service.keytab\"\n+ *     useKeyTab=true\n+ *     storeKey=true\n+ *     useTicketCache=false\n+ *     serviceName=\"kafka\"\n+ *     principal=\"kafka/server@example\";"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "The principal doesn't contain the `org.domain` parameter which makes this example constantly fail in my setup, the rest looks good. I'm using hadoop's MiniKDC with default settings.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-15T11:25:46Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required\n+ *     keyTab=\"${path_of_keytab}/kafka.service.keytab\"\n+ *     useKeyTab=true\n+ *     storeKey=true\n+ *     useTicketCache=false\n+ *     serviceName=\"kafka\"\n+ *     principal=\"kafka/server@example\";"
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "@gaborgsomogyi I using krb5 server and format of principal like `name/host@realm`. Since I don't know much about MiniKDC, I kind of know it's used to test security. I think they are similar in principal. Can you describe your error information.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-16T06:34:54Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required\n+ *     keyTab=\"${path_of_keytab}/kafka.service.keytab\"\n+ *     useKeyTab=true\n+ *     storeKey=true\n+ *     useTicketCache=false\n+ *     serviceName=\"kafka\"\n+ *     principal=\"kafka/server@example\";"
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "The default realm is `EXAMPLE.COM` in Kerby and in `krb5` the configuration guide suggests the same. With `example` realm the example app failed with authentication error.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-16T12:17:39Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required\n+ *     keyTab=\"${path_of_keytab}/kafka.service.keytab\"\n+ *     useKeyTab=true\n+ *     storeKey=true\n+ *     useTicketCache=false\n+ *     serviceName=\"kafka\"\n+ *     principal=\"kafka/server@example\";"
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "> The default realm is `EXAMPLE.COM` in Kerby and in `krb5` the configuration guide suggests the same. With `example` realm the example app failed with authentication error.\r\n\r\nMy fault, originally I just meant you could replace it with your own principal. I will use `EXAMPLE.COM` to make it easier to  understand.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-19T01:24:36Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *    $ bin/run-example --files ${path}/kafka_jaas.conf \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required\n+ *     keyTab=\"${path_of_keytab}/kafka.service.keytab\"\n+ *     useKeyTab=true\n+ *     storeKey=true\n+ *     useTicketCache=false\n+ *     serviceName=\"kafka\"\n+ *     principal=\"kafka/server@example\";"
  }],
  "prId": 25412
}, {
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "```\r\n * Note that this example uses SASL_PLAINTEXT for simplicity; however,\r\n * SASL_PLAINTEXT has no SSL encryption and likely be less secure. Please consider\r\n * using SASL_SSL in production.\r\n```",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-20T10:49:50Z",
    "diffHunk": "@@ -0,0 +1,137 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *   Yarn client:\n+ *    $ bin/run-example --files ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_driver_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      --master yarn\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *   Yarn cluster:\n+ *    $ bin/run-example --files \\\n+ *      ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab,${krb5_path}/krb5.conf \\\n+ *      --driver-java-options \\\n+ *      \"-Djava.security.auth.login.config=./kafka_jaas.conf \\\n+ *      -Djava.security.krb5.conf=./krb5.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      --master yarn --deploy-mode cluster \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required\n+ *     keyTab=\"./kafka.service.keytab\"\n+ *     useKeyTab=true\n+ *     storeKey=true\n+ *     useTicketCache=false\n+ *     serviceName=\"kafka\"\n+ *     principal=\"kafka/host@EXAMPLE.COM\";\n+ *   };\n+ * kafka_driver_jaas.conf (used by yarn client) and kafka_jaas.conf are basically the same\n+ * except for some differences at 'keyTab'. In kafka_driver_jaas.conf, 'keyTab' should be\n+ * \"${keytab_path}/kafka.service.keytab\".\n+ *\n+ * Note: This file uses SASL_PLAINTEXT for simplicity;however,\n+ * SASL_PLAINTEXT has no SSL encryption and likely be less secure. Consider using SASL_SSL."
  }],
  "prId": 25412
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "If `--files` used for keytab and jaas file then both driver and executor can pick up the same jaas (in keytab file `./kafka.service.keytab` has to be set). Please see https://github.com/gaborgsomogyi/spark-structured-secure-kafka-app#spark-submit Is that not working somehow?\r\n",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-21T09:56:30Z",
    "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *   Yarn client:\n+ *    $ bin/run-example --files ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_driver_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      --master yarn\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *   Yarn cluster:\n+ *    $ bin/run-example --files \\\n+ *      ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab,${krb5_path}/krb5.conf \\\n+ *      --driver-java-options \\\n+ *      \"-Djava.security.auth.login.config=./kafka_jaas.conf \\\n+ *      -Djava.security.krb5.conf=./krb5.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      --master yarn --deploy-mode cluster \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required\n+ *     keyTab=\"./kafka.service.keytab\"\n+ *     useKeyTab=true\n+ *     storeKey=true\n+ *     useTicketCache=false\n+ *     serviceName=\"kafka\"\n+ *     principal=\"kafka/host@EXAMPLE.COM\";\n+ *   };\n+ * kafka_driver_jaas.conf (used by yarn client) and kafka_jaas.conf are basically the same",
    "line": 80
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "> If `--files` used for keytab and jaas file then both driver and executor can pick up the same jaas (in keytab file `./kafka.service.keytab` has to be set). Please see https://github.com/gaborgsomogyi/spark-structured-secure-kafka-app#spark-submit Is that not working somehow?\r\n\r\nI thought dirver can pick up the jaas, but it can't pick up files upload by `--files` after my test.\r\nIt can just work on cluster mode.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-22T01:48:50Z",
    "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *   Yarn client:\n+ *    $ bin/run-example --files ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_driver_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      --master yarn\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *   Yarn cluster:\n+ *    $ bin/run-example --files \\\n+ *      ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab,${krb5_path}/krb5.conf \\\n+ *      --driver-java-options \\\n+ *      \"-Djava.security.auth.login.config=./kafka_jaas.conf \\\n+ *      -Djava.security.krb5.conf=./krb5.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      --master yarn --deploy-mode cluster \\\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *\n+ * kafka_jaas.conf can manually create, template as:\n+ *   KafkaClient {\n+ *     com.sun.security.auth.module.Krb5LoginModule required\n+ *     keyTab=\"./kafka.service.keytab\"\n+ *     useKeyTab=true\n+ *     storeKey=true\n+ *     useTicketCache=false\n+ *     serviceName=\"kafka\"\n+ *     principal=\"kafka/host@EXAMPLE.COM\";\n+ *   };\n+ * kafka_driver_jaas.conf (used by yarn client) and kafka_jaas.conf are basically the same",
    "line": 80
  }],
  "prId": 25412
}, {
  "comments": [{
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "2 questions:\r\n* Why is `krb5.conf` needed for cluster mode?\r\n* Why is `krb5.conf` not needed for client mode?\r\n",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-21T09:57:40Z",
    "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *   Yarn client:\n+ *    $ bin/run-example --files ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_driver_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      --master yarn\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *   Yarn cluster:\n+ *    $ bin/run-example --files \\\n+ *      ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab,${krb5_path}/krb5.conf \\\n+ *      --driver-java-options \\\n+ *      \"-Djava.security.auth.login.config=./kafka_jaas.conf \\\n+ *      -Djava.security.krb5.conf=./krb5.conf\" \\",
    "line": 63
  }, {
    "author": {
      "login": "hddong"
    },
    "body": "@gaborgsomogyi , Client mode, driver run on client which usually has environment of kerberos, so it's not need. Cluster mode, driver run in a random executor which not contain\r\nkrb5.conf and it will throw exception:\r\n```\r\nCaused by: javax.security.auth.login.LoginException: Cannot locate KDC\r\n        at com.sun.security.auth.module.Krb5LoginModule.attemptAuthentication(Krb5LoginModule.java:804)\r\n        at com.sun.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:617)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at javax.security.auth.login.LoginContext.invoke(LoginContext.java:755)\r\n        at javax.security.auth.login.LoginContext.access$000(LoginContext.java:195)\r\n        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:682)\r\n        at javax.security.auth.login.LoginContext$4.run(LoginContext.java:680)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:680)\r\n        at javax.security.auth.login.LoginContext.login(LoginContext.java:587)\r\n        at org.apache.kafka.common.security.authenticator.AbstractLogin.login(AbstractLogin.java:60)\r\n        at org.apache.kafka.common.security.kerberos.KerberosLogin.login(KerberosLogin.java:103)\r\n        at org.apache.kafka.common.security.authenticator.LoginManager.<init>(LoginManager.java:64)\r\n        at org.apache.kafka.common.security.authenticator.LoginManager.acquireLoginManager(LoginManager.java:114)\r\n        at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:142)\r\n        ... 26 more\r\n```\r\nso , `krb5.conf` needed for cluster mode.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-22T01:32:12Z",
    "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *   Yarn client:\n+ *    $ bin/run-example --files ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_driver_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      --master yarn\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *   Yarn cluster:\n+ *    $ bin/run-example --files \\\n+ *      ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab,${krb5_path}/krb5.conf \\\n+ *      --driver-java-options \\\n+ *      \"-Djava.security.auth.login.config=./kafka_jaas.conf \\\n+ *      -Djava.security.krb5.conf=./krb5.conf\" \\",
    "line": 63
  }, {
    "author": {
      "login": "gaborgsomogyi"
    },
    "body": "So, the cluster nodes are configured differently. That's a good explanation.",
    "commit": "91d482efd10da9a2389dc631605ec6b7a7cbf7c7",
    "createdAt": "2019-08-26T09:53:47Z",
    "diffHunk": "@@ -0,0 +1,138 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Arrays;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.regex.Pattern;\n+\n+import scala.Tuple2;\n+\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.common.serialization.StringDeserializer;\n+\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.streaming.api.java.*;\n+import org.apache.spark.streaming.kafka010.ConsumerStrategies;\n+import org.apache.spark.streaming.kafka010.KafkaUtils;\n+import org.apache.spark.streaming.kafka010.LocationStrategies;\n+import org.apache.spark.streaming.Durations;\n+\n+/**\n+ * Consumes messages from one or more topics in Kafka and does wordcount.\n+ * Usage: JavaDirectKerberizedKafkaWordCount <brokers> <groupId> <topics>\n+ *   <brokers> is a list of one or more Kafka brokers\n+ *   <groupId> is a consumer group name to consume from topics\n+ *   <topics> is a list of one or more kafka topics to consume from\n+ *\n+ * Example:\n+ *   Yarn client:\n+ *    $ bin/run-example --files ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab \\\n+ *      --driver-java-options \"-Djava.security.auth.login.config=${path}/kafka_driver_jaas.conf\" \\\n+ *      --conf \\\n+ *      \"spark.executor.extraJavaOptions=-Djava.security.auth.login.config=./kafka_jaas.conf\" \\\n+ *      --master yarn\n+ *      streaming.JavaDirectKerberizedKafkaWordCount broker1-host:port,broker2-host:port \\\n+ *      consumer-group topic1,topic2\n+ *   Yarn cluster:\n+ *    $ bin/run-example --files \\\n+ *      ${jaas_path}/kafka_jaas.conf,${keytab_path}/kafka.service.keytab,${krb5_path}/krb5.conf \\\n+ *      --driver-java-options \\\n+ *      \"-Djava.security.auth.login.config=./kafka_jaas.conf \\\n+ *      -Djava.security.krb5.conf=./krb5.conf\" \\",
    "line": 63
  }],
  "prId": 25412
}]