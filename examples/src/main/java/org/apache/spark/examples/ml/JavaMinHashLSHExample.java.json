[{
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "\"indices\"",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-09T22:18:25Z",
    "diffHunk": "@@ -44,25 +45,67 @@ public static void main(String[] args) {\n       .getOrCreate();\n \n     // $example on$\n-    List<Row> data = Arrays.asList(\n+    List<Row> dataA = Arrays.asList(\n       RowFactory.create(0, Vectors.sparse(6, new int[]{0, 1, 2}, new double[]{1.0, 1.0, 1.0})),\n       RowFactory.create(1, Vectors.sparse(6, new int[]{2, 3, 4}, new double[]{1.0, 1.0, 1.0})),\n       RowFactory.create(2, Vectors.sparse(6, new int[]{0, 2, 4}, new double[]{1.0, 1.0, 1.0}))\n     );\n \n+    List<Row> dataB = Arrays.asList(\n+      RowFactory.create(0, Vectors.sparse(6, new int[]{1, 3, 5}, new double[]{1.0, 1.0, 1.0})),\n+      RowFactory.create(1, Vectors.sparse(6, new int[]{2, 3, 5}, new double[]{1.0, 1.0, 1.0})),\n+      RowFactory.create(2, Vectors.sparse(6, new int[]{1, 2, 4}, new double[]{1.0, 1.0, 1.0}))\n+    );\n+\n     StructType schema = new StructType(new StructField[]{\n       new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n-      new StructField(\"keys\", new VectorUDT(), false, Metadata.empty())\n+      new StructField(\"features\", new VectorUDT(), false, Metadata.empty())\n     });\n-    Dataset<Row> dataFrame = spark.createDataFrame(data, schema);\n+    Dataset<Row> dfA = spark.createDataFrame(dataA, schema);\n+    Dataset<Row> dfB = spark.createDataFrame(dataB, schema);\n+\n+    int[] indicies = {1, 3};"
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "not values anymore",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-09T22:18:51Z",
    "diffHunk": "@@ -44,25 +45,67 @@ public static void main(String[] args) {\n       .getOrCreate();\n \n     // $example on$\n-    List<Row> data = Arrays.asList(\n+    List<Row> dataA = Arrays.asList(\n       RowFactory.create(0, Vectors.sparse(6, new int[]{0, 1, 2}, new double[]{1.0, 1.0, 1.0})),\n       RowFactory.create(1, Vectors.sparse(6, new int[]{2, 3, 4}, new double[]{1.0, 1.0, 1.0})),\n       RowFactory.create(2, Vectors.sparse(6, new int[]{0, 2, 4}, new double[]{1.0, 1.0, 1.0}))\n     );\n \n+    List<Row> dataB = Arrays.asList(\n+      RowFactory.create(0, Vectors.sparse(6, new int[]{1, 3, 5}, new double[]{1.0, 1.0, 1.0})),\n+      RowFactory.create(1, Vectors.sparse(6, new int[]{2, 3, 5}, new double[]{1.0, 1.0, 1.0})),\n+      RowFactory.create(2, Vectors.sparse(6, new int[]{1, 2, 4}, new double[]{1.0, 1.0, 1.0}))\n+    );\n+\n     StructType schema = new StructType(new StructField[]{\n       new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n-      new StructField(\"keys\", new VectorUDT(), false, Metadata.empty())\n+      new StructField(\"features\", new VectorUDT(), false, Metadata.empty())\n     });\n-    Dataset<Row> dataFrame = spark.createDataFrame(data, schema);\n+    Dataset<Row> dfA = spark.createDataFrame(dataA, schema);\n+    Dataset<Row> dfB = spark.createDataFrame(dataB, schema);\n+\n+    int[] indicies = {1, 3};\n+    double[] values = {1.0, 1.0};\n+    Vector key = Vectors.sparse(6, indicies, values);\n \n     MinHashLSH mh = new MinHashLSH()\n-      .setNumHashTables(1)\n-      .setInputCol(\"keys\")\n-      .setOutputCol(\"values\");\n+      .setNumHashTables(5)\n+      .setInputCol(\"features\")\n+      .setOutputCol(\"hashes\");\n+\n+    MinHashLSHModel model = mh.fit(dfA);\n+\n+    // Feature Transformation\n+    System.out.println(\"The hashed dataset where hashed values are stored in the column 'values':\");"
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "same comments as above",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-09T22:19:13Z",
    "diffHunk": "@@ -44,25 +45,67 @@ public static void main(String[] args) {\n       .getOrCreate();\n \n     // $example on$\n-    List<Row> data = Arrays.asList(\n+    List<Row> dataA = Arrays.asList(\n       RowFactory.create(0, Vectors.sparse(6, new int[]{0, 1, 2}, new double[]{1.0, 1.0, 1.0})),\n       RowFactory.create(1, Vectors.sparse(6, new int[]{2, 3, 4}, new double[]{1.0, 1.0, 1.0})),\n       RowFactory.create(2, Vectors.sparse(6, new int[]{0, 2, 4}, new double[]{1.0, 1.0, 1.0}))\n     );\n \n+    List<Row> dataB = Arrays.asList(\n+      RowFactory.create(0, Vectors.sparse(6, new int[]{1, 3, 5}, new double[]{1.0, 1.0, 1.0})),\n+      RowFactory.create(1, Vectors.sparse(6, new int[]{2, 3, 5}, new double[]{1.0, 1.0, 1.0})),\n+      RowFactory.create(2, Vectors.sparse(6, new int[]{1, 2, 4}, new double[]{1.0, 1.0, 1.0}))\n+    );\n+\n     StructType schema = new StructType(new StructField[]{\n       new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n-      new StructField(\"keys\", new VectorUDT(), false, Metadata.empty())\n+      new StructField(\"features\", new VectorUDT(), false, Metadata.empty())\n     });\n-    Dataset<Row> dataFrame = spark.createDataFrame(data, schema);\n+    Dataset<Row> dfA = spark.createDataFrame(dataA, schema);\n+    Dataset<Row> dfB = spark.createDataFrame(dataB, schema);\n+\n+    int[] indicies = {1, 3};\n+    double[] values = {1.0, 1.0};\n+    Vector key = Vectors.sparse(6, indicies, values);\n \n     MinHashLSH mh = new MinHashLSH()\n-      .setNumHashTables(1)\n-      .setInputCol(\"keys\")\n-      .setOutputCol(\"values\");\n+      .setNumHashTables(5)\n+      .setInputCol(\"features\")\n+      .setOutputCol(\"hashes\");\n+\n+    MinHashLSHModel model = mh.fit(dfA);\n+\n+    // Feature Transformation\n+    System.out.println(\"The hashed dataset where hashed values are stored in the column 'values':\");\n+    model.transform(dfA).show();\n+    // Cache the transformed columns\n+    Dataset<Row> transformedA = model.transform(dfA).cache();\n+    Dataset<Row> transformedB = model.transform(dfB).cache();\n+\n+    // Approximate similarity join\n+    System.out.println(\"Approximately joining dfA and dfB on distance smaller than 0.6:\");\n+    model.approxSimilarityJoin(dfA, dfB, 0.6)\n+      .select(\"datasetA.id\", \"datasetB.id\", \"distCol\")\n+      .show();\n+    System.out.println(\"Joining cached datasets to avoid recomputing the hash values:\");\n+    model.approxSimilarityJoin(transformedA, transformedB, 0.6)\n+      .select(\"datasetA.id\", \"datasetB.id\", \"distCol\")\n+      .show();\n+\n+    // Self Join\n+    System.out.println(\"Approximately self join of dfB on distance smaller than 0.6:\");"
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "Can we make it be \"exactDistance\" or \"euclideanDistance\" and \"jaccardSimilarity\" here and in all the examples, for random projection and minhash respectively. I think it will be much clearer to the user what distCol represents.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-09T22:28:02Z",
    "diffHunk": "@@ -44,25 +45,67 @@ public static void main(String[] args) {\n       .getOrCreate();\n \n     // $example on$\n-    List<Row> data = Arrays.asList(\n+    List<Row> dataA = Arrays.asList(\n       RowFactory.create(0, Vectors.sparse(6, new int[]{0, 1, 2}, new double[]{1.0, 1.0, 1.0})),\n       RowFactory.create(1, Vectors.sparse(6, new int[]{2, 3, 4}, new double[]{1.0, 1.0, 1.0})),\n       RowFactory.create(2, Vectors.sparse(6, new int[]{0, 2, 4}, new double[]{1.0, 1.0, 1.0}))\n     );\n \n+    List<Row> dataB = Arrays.asList(\n+      RowFactory.create(0, Vectors.sparse(6, new int[]{1, 3, 5}, new double[]{1.0, 1.0, 1.0})),\n+      RowFactory.create(1, Vectors.sparse(6, new int[]{2, 3, 5}, new double[]{1.0, 1.0, 1.0})),\n+      RowFactory.create(2, Vectors.sparse(6, new int[]{1, 2, 4}, new double[]{1.0, 1.0, 1.0}))\n+    );\n+\n     StructType schema = new StructType(new StructField[]{\n       new StructField(\"id\", DataTypes.IntegerType, false, Metadata.empty()),\n-      new StructField(\"keys\", new VectorUDT(), false, Metadata.empty())\n+      new StructField(\"features\", new VectorUDT(), false, Metadata.empty())\n     });\n-    Dataset<Row> dataFrame = spark.createDataFrame(data, schema);\n+    Dataset<Row> dfA = spark.createDataFrame(dataA, schema);\n+    Dataset<Row> dfB = spark.createDataFrame(dataB, schema);\n+\n+    int[] indicies = {1, 3};\n+    double[] values = {1.0, 1.0};\n+    Vector key = Vectors.sparse(6, indicies, values);\n \n     MinHashLSH mh = new MinHashLSH()\n-      .setNumHashTables(1)\n-      .setInputCol(\"keys\")\n-      .setOutputCol(\"values\");\n+      .setNumHashTables(5)\n+      .setInputCol(\"features\")\n+      .setOutputCol(\"hashes\");\n+\n+    MinHashLSHModel model = mh.fit(dfA);\n+\n+    // Feature Transformation\n+    System.out.println(\"The hashed dataset where hashed values are stored in the column 'values':\");\n+    model.transform(dfA).show();\n+    // Cache the transformed columns\n+    Dataset<Row> transformedA = model.transform(dfA).cache();\n+    Dataset<Row> transformedB = model.transform(dfB).cache();\n+\n+    // Approximate similarity join\n+    System.out.println(\"Approximately joining dfA and dfB on distance smaller than 0.6:\");\n+    model.approxSimilarityJoin(dfA, dfB, 0.6)"
  }],
  "prId": 16715
}, {
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "move it below under example on",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-15T04:27:55Z",
    "diffHunk": "@@ -17,6 +17,7 @@\n \n package org.apache.spark.examples.ml;\n \n+import org.apache.spark.ml.linalg.Vector;"
  }],
  "prId": 16715
}]