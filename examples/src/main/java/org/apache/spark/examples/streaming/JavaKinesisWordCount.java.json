[{
  "comments": [{
    "author": {
      "login": "tdas"
    },
    "body": "Same comment as the other example. Needs to be simplified.\n",
    "commit": "47745816b21d7d2255a98283e3055a5a2a397a27",
    "createdAt": "2014-07-31T20:37:55Z",
    "diffHunk": "@@ -0,0 +1,294 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.streaming;\n+\n+import java.util.List;\n+import java.util.regex.Pattern;\n+\n+import org.apache.log4j.Level;\n+import org.apache.log4j.Logger;\n+import org.apache.spark.SparkConf;\n+import org.apache.spark.api.java.JavaPairRDD;\n+import org.apache.spark.api.java.function.FlatMapFunction;\n+import org.apache.spark.api.java.function.Function;\n+import org.apache.spark.api.java.function.Function2;\n+import org.apache.spark.api.java.function.PairFunction;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.Duration;\n+import org.apache.spark.streaming.Milliseconds;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaPairDStream;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.apache.spark.streaming.kinesis.KinesisRecordSerializer;\n+import org.apache.spark.streaming.kinesis.KinesisStringRecordSerializer;\n+import org.apache.spark.streaming.kinesis.KinesisUtils;\n+\n+import scala.Tuple2;\n+\n+import com.amazonaws.auth.DefaultAWSCredentialsProviderChain;\n+import com.amazonaws.services.kinesis.AmazonKinesisClient;\n+import com.amazonaws.services.kinesis.clientlibrary.lib.worker.InitialPositionInStream;\n+import com.google.common.base.Optional;\n+import com.google.common.collect.Lists;\n+\n+/**\n+ * Java-friendly Kinesis Spark Streaming WordCount example\n+ *\n+ * See http://spark.apache.org/docs/latest/streaming-programming-guide.html for more details \n+ * on the Kinesis Spark Streaming integration.\n+ *\n+ * This example spins up 1 Kinesis Worker (Spark Streaming Receivers) per shard \n+ *   of the given stream.\n+ * It then starts pulling from the last checkpointed sequence number of the given \n+ *   <stream-name> and <endpoint-url>. \n+ *\n+ * Valid endpoint urls:  http://docs.aws.amazon.com/general/latest/gr/rande.html#ak_region\n+ *\n+ * This code uses the DefaultAWSCredentialsProviderChain and searches for credentials \n+ *  in the following order of precedence: \n+ *         Environment Variables - AWS_ACCESS_KEY_ID and AWS_SECRET_KEY\n+ *         Java System Properties - aws.accessKeyId and aws.secretKey\n+ *         Credential profiles file - default location (~/.aws/credentials) shared by all AWS SDKs\n+ *         Instance profile credentials - delivered through the Amazon EC2 metadata service\n+ *\n+ * Usage: JavaKinesisWordCount <stream-name> <endpoint-url>\n+ *         <stream-name> is the name of the Kinesis stream (ie. mySparkStream)\n+ *         <endpoint-url> is the endpoint of the Kinesis service \n+ *           (ie. https://kinesis.us-east-1.amazonaws.com)\n+ *\n+ * Example:\n+ *      $ export AWS_ACCESS_KEY_ID=<your-access-key>\n+ *      $ export AWS_SECRET_KEY=<your-secret-key>\n+ *      $ $SPARK_HOME/bin/run-example \\\n+ *            org.apache.spark.examples.streaming.JavaKinesisWordCount mySparkStream \\\n+ *            https://kinesis.us-east-1.amazonaws.com\n+ *\n+ * There is a companion helper class called KinesisWordCountProducer which puts dummy data \n+ *   onto the Kinesis stream. \n+ * Usage instructions for KinesisWordCountProducer are provided in the class definition.\n+ */\n+public final class JavaKinesisWordCount {\n+    private static final Pattern WORD_SEPARATOR = Pattern.compile(\" \");\n+    private static final Logger logger = Logger.getLogger(JavaKinesisWordCount.class);\n+\n+    /**\n+     * Make the constructor private to enforce singleton\n+     */\n+    private JavaKinesisWordCount() {\n+    }\n+\n+    public static void main(String[] args) {\n+        /**\n+         * Check that all required args were passed in.\n+         */\n+        if (args.length < 2) {\n+            System.err.println(\"Usage: JavaKinesisWordCount <stream-name> <kinesis-endpoint-url>\");\n+            System.exit(1);\n+        }\n+\n+        /**\n+         * (This was lifted from the StreamingExamples.scala in order to avoid the dependency on the spark-examples artifact.)\n+         * Set reasonable logging levels for streaming if the user has not configured log4j.\n+         */\n+        boolean log4jInitialized = Logger.getRootLogger().getAllAppenders()\n+                .hasMoreElements();\n+        if (!log4jInitialized) {\n+            /** We first log something to initialize Spark's default logging, then we override the logging level. */\n+            Logger.getRootLogger()\n+                    .info(\"Setting log level to [ERROR] for streaming example.\"\n+                            + \" To override add a custom log4j.properties to the classpath.\");\n+            Logger.getRootLogger().setLevel(Level.ERROR);\n+            Logger.getLogger(\"org.apache.spark.examples.streaming\").setLevel(Level.DEBUG);\n+        }\n+\n+        /** Populate the appropriate variables from the given args */\n+        String stream = args[0];\n+        String endpoint = args[1];\n+        /** Set the batch interval to a fixed 2000 millis (2 seconds) */\n+        Integer batchIntervalMillis = 2000;\n+\n+        /** Create a Kinesis client in order to determine the number of shards for the given stream */\n+        AmazonKinesisClient kinesisClient = new AmazonKinesisClient(\n+                new DefaultAWSCredentialsProviderChain());\n+        kinesisClient.setEndpoint(endpoint);\n+\n+        /** Determine the number of shards from the stream */\n+        int numShards = kinesisClient.describeStream(stream)\n+                .getStreamDescription().getShards().size();\n+\n+        /** In this example, we're going to create 1 Kinesis Worker/Receiver/DStream for each shard */ \n+        int numStreams = numShards;\n+\n+        /** Must add 1 more thread than the number of receivers or the output won't show properly from the driver */\n+        int numSparkThreads = numStreams + 1;\n+\n+        /** Set the app name */\n+        String appName = \"KinesisWordCount\";\n+\n+        /** Setup the Spark config. */\n+        SparkConf sparkConfig = new SparkConf().setAppName(appName).setMaster(\n+                \"local[\" + numSparkThreads + \"]\");\n+\n+        /**\n+         * Set the batch interval.\n+         * Records will be pulled from the Kinesis stream and stored as a single DStream within Spark every batch interval.\n+         */\n+        Duration batchInterval = Milliseconds.apply(batchIntervalMillis);\n+\n+        /**\n+         * It's recommended that you perform a Spark checkpoint between 5 and 10 times the batch interval. \n+         * While this is the Spark checkpoint interval, we're going to use it for the Kinesis checkpoint interval, as well.\n+         * For example purposes, we'll just use the batchInterval.\n+         */\n+        Duration checkpointInterval = Milliseconds.apply(batchIntervalMillis);\n+\n+        /** Setup the StreamingContext */\n+        JavaStreamingContext jssc = new JavaStreamingContext(sparkConfig, batchInterval);\n+\n+        /** Setup the checkpoint directory used by Spark Streaming */\n+        jssc.checkpoint(\"/tmp/checkpoint\");\n+\n+        /** Create the same number of Kinesis Receivers/DStreams as stream shards, then union them all */\n+        JavaDStream<byte[]> allStreams = KinesisUtils\n+                .createStream(jssc, appName, stream, endpoint, checkpointInterval.milliseconds(), \n+                                    InitialPositionInStream.LATEST);\n+        /** Set the checkpoint interval */\n+        allStreams.checkpoint(checkpointInterval);\n+        for (int i = 1; i < numStreams; i++) {\n+            /** Create a new Receiver/DStream for each stream shard */\n+            JavaDStream<byte[]> dStream = KinesisUtils\n+                    .createStream(jssc, appName, stream, endpoint, checkpointInterval.milliseconds(), \n+                                        InitialPositionInStream.LATEST);\n+            /** Set the Spark checkpoint interval */\n+            dStream.checkpoint(checkpointInterval);\n+\n+            /** Union with the existing streams */\n+            allStreams = allStreams.union(dStream);\n+        }\n+\n+        /** This implementation uses the String-based KinesisRecordSerializer impl */\n+        final KinesisRecordSerializer<String> recordSerializer = new KinesisStringRecordSerializer();\n+\n+        /**\n+          * Split each line of the union'd DStreams into multiple words using flatMap to produce the collection.\n+          * Convert lines of byte[] to multiple Strings by first converting to String, then splitting on WORD_SEPARATOR\n+          * We're caching the result here so that we can use it later without having to re-materialize the underlying RDDs.\n+          */\n+        JavaDStream<String> words = allStreams.flatMap(new FlatMapFunction<byte[], String>() {\n+                    /**\n+                     * Convert lines of byte[] to multiple words split by WORD_SEPARATOR\n+                     * @param byte array\n+                     * @return iterable of words split by WORD_SEPARATOR\n+                     */\n+                    @Override\n+                    public Iterable<String> call(byte[] line) {\n+                        return Lists.newArrayList(WORD_SEPARATOR.split(recordSerializer.deserialize(line)));\n+                    }\n+                }).cache();\n+\n+        /**\n+         * Map each word to a (word, 1) tuple so we can reduce/aggregate later.\n+         * We're caching the result here so that we can use it later without having\n+         *     to re-materialize the underlying RDDs.\n+         */\n+        JavaPairDStream<String, Integer> wordCounts = words.mapToPair(\n+                new PairFunction<String, String, Integer>() {\n+                    /**\n+                     * Create the (word, 1) tuple\n+                     * @param word\n+                     * @return (word, 1) tuple\n+                     */\n+                    @Override\n+                    public Tuple2<String, Integer> call(String s) {\n+                        return new Tuple2<String, Integer>(s, 1);\n+                    }\n+                });\n+\n+        /**\n+         * Reduce/aggregate by key\n+         * We're caching the result here so that we can use it later without having\n+         *     to re-materialize the underlying RDDs.\n+         */\n+        JavaPairDStream<String, Integer> wordCountsByKey = wordCounts.reduceByKey(\n+                new Function2<Integer, Integer, Integer>() {\n+                    @Override\n+                    public Integer call(Integer i1, Integer i2) {\n+                        return i1 + i2;\n+                    }\n+                }).cache();\n+\n+        /** Update the running totals of words. */"
  }],
  "prId": 1434
}]