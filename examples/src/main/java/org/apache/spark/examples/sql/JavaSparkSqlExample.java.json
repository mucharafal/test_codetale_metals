[{
  "comments": [{
    "author": {
      "login": "aokolnychyi"
    },
    "body": "Here the imports do not follow the alphabetical order to avoid too many imports groups in the documentation (there would be a blank line between each \"example on/off\" block).\n",
    "commit": "7451fc784d5c8f87c37f7707c4323a280d52417b",
    "createdAt": "2016-07-09T22:49:28Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.sql;\n+\n+// $example on:programmatic_schema$\n+import java.util.ArrayList;\n+import java.util.List;\n+// $example off:programmatic_schema$\n+// $example on:create_ds$\n+import java.util.Arrays;",
    "line": 24
  }],
  "prId": 14119
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Let's use `col(\"...\")` instead of `df.col(\"...\")` throughout all the Java examples. This requires\n\n``` java\nimport static org.apache.spark.sql.functions.col;\n```\n\nThe reason is similar to the reason why we prefer `$\"...\"` to `df(\"...\")` in Scala code.\n",
    "commit": "7451fc784d5c8f87c37f7707c4323a280d52417b",
    "createdAt": "2016-07-11T13:53:10Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.sql;\n+\n+// $example on:programmatic_schema$\n+import java.util.ArrayList;\n+import java.util.List;\n+// $example off:programmatic_schema$\n+// $example on:create_ds$\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.io.Serializable;\n+// $example off:create_ds$\n+\n+// $example on:schema_inferring$\n+// $example on:programmatic_schema$\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.function.Function;\n+// $example off:programmatic_schema$\n+// $example on:create_ds$\n+import org.apache.spark.api.java.function.MapFunction;\n+// $example on:create_df$\n+// $example on:run_sql$\n+// $example on:programmatic_schema$\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+// $example off:programmatic_schema$\n+// $example off:create_df$\n+// $example off:run_sql$\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+// $example off:create_ds$\n+// $example off:schema_inferring$\n+import org.apache.spark.sql.RowFactory;\n+// $example on:init_session$\n+import org.apache.spark.sql.SparkSession;\n+// $example off:init_session$\n+// $example on:programmatic_schema$\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+// $example off:programmatic_schema$\n+\n+public class JavaSparkSqlExample {\n+  // $example on:create_ds$\n+  public static class Person implements Serializable {\n+    private String name;\n+    private int age;\n+\n+    public String getName() {\n+      return name;\n+    }\n+\n+    public void setName(String name) {\n+      this.name = name;\n+    }\n+\n+    public int getAge() {\n+      return age;\n+    }\n+\n+    public void setAge(int age) {\n+      this.age = age;\n+    }\n+  }\n+  // $example off:create_ds$\n+\n+  public static void main(String[] args) {\n+    // $example on:init_session$\n+    SparkSession spark = SparkSession\n+        .builder()\n+        .appName(\"Java Spark SQL Example\")\n+        .config(\"spark.some.config.option\", \"some-value\")\n+        .getOrCreate();\n+    // $example off:init_session$\n+\n+    runBasicDataFrameExample(spark);\n+    runDatasetCreationExample(spark);\n+    runInferSchemaExample(spark);\n+    runProgrammaticSchemaExample(spark);\n+\n+    spark.stop();\n+  }\n+\n+  private static void runBasicDataFrameExample(SparkSession spark) {\n+    // $example on:create_df$\n+    Dataset<Row> df = spark.read().json(\"examples/src/main/resources/people.json\");\n+\n+    // Displays the content of the DataFrame to stdout\n+    df.show();\n+    // age  name\n+    // null Michael\n+    // 30   Andy\n+    // 19   Justin\n+    // $example off:create_df$\n+\n+    // $example on:untyped_ops$\n+    // Print the schema in a tree format\n+    df.printSchema();\n+    // root\n+    // |-- age: long (nullable = true)\n+    // |-- name: string (nullable = true)\n+\n+    // Select only the \"name\" column\n+    df.select(\"name\").show();\n+    // name\n+    // Michael\n+    // Andy\n+    // Justin\n+\n+    // Select everybody, but increment the age by 1\n+    df.select(df.col(\"name\"), df.col(\"age\").plus(1)).show();"
  }],
  "prId": 14119
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Nit: Use 2-space indentation here.\n",
    "commit": "7451fc784d5c8f87c37f7707c4323a280d52417b",
    "createdAt": "2016-07-11T14:06:20Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.sql;\n+\n+// $example on:programmatic_schema$\n+import java.util.ArrayList;\n+import java.util.List;\n+// $example off:programmatic_schema$\n+// $example on:create_ds$\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.io.Serializable;\n+// $example off:create_ds$\n+\n+// $example on:schema_inferring$\n+// $example on:programmatic_schema$\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.function.Function;\n+// $example off:programmatic_schema$\n+// $example on:create_ds$\n+import org.apache.spark.api.java.function.MapFunction;\n+// $example on:create_df$\n+// $example on:run_sql$\n+// $example on:programmatic_schema$\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+// $example off:programmatic_schema$\n+// $example off:create_df$\n+// $example off:run_sql$\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+// $example off:create_ds$\n+// $example off:schema_inferring$\n+import org.apache.spark.sql.RowFactory;\n+// $example on:init_session$\n+import org.apache.spark.sql.SparkSession;\n+// $example off:init_session$\n+// $example on:programmatic_schema$\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+// $example off:programmatic_schema$\n+\n+public class JavaSparkSqlExample {\n+  // $example on:create_ds$\n+  public static class Person implements Serializable {\n+    private String name;\n+    private int age;\n+\n+    public String getName() {\n+      return name;\n+    }\n+\n+    public void setName(String name) {\n+      this.name = name;\n+    }\n+\n+    public int getAge() {\n+      return age;\n+    }\n+\n+    public void setAge(int age) {\n+      this.age = age;\n+    }\n+  }\n+  // $example off:create_ds$\n+\n+  public static void main(String[] args) {\n+    // $example on:init_session$\n+    SparkSession spark = SparkSession\n+        .builder()\n+        .appName(\"Java Spark SQL Example\")\n+        .config(\"spark.some.config.option\", \"some-value\")\n+        .getOrCreate();"
  }],
  "prId": 14119
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Nit: Use 2-space indentation here.\n",
    "commit": "7451fc784d5c8f87c37f7707c4323a280d52417b",
    "createdAt": "2016-07-11T14:06:31Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.sql;\n+\n+// $example on:programmatic_schema$\n+import java.util.ArrayList;\n+import java.util.List;\n+// $example off:programmatic_schema$\n+// $example on:create_ds$\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.io.Serializable;\n+// $example off:create_ds$\n+\n+// $example on:schema_inferring$\n+// $example on:programmatic_schema$\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.function.Function;\n+// $example off:programmatic_schema$\n+// $example on:create_ds$\n+import org.apache.spark.api.java.function.MapFunction;\n+// $example on:create_df$\n+// $example on:run_sql$\n+// $example on:programmatic_schema$\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+// $example off:programmatic_schema$\n+// $example off:create_df$\n+// $example off:run_sql$\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+// $example off:create_ds$\n+// $example off:schema_inferring$\n+import org.apache.spark.sql.RowFactory;\n+// $example on:init_session$\n+import org.apache.spark.sql.SparkSession;\n+// $example off:init_session$\n+// $example on:programmatic_schema$\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+// $example off:programmatic_schema$\n+\n+public class JavaSparkSqlExample {\n+  // $example on:create_ds$\n+  public static class Person implements Serializable {\n+    private String name;\n+    private int age;\n+\n+    public String getName() {\n+      return name;\n+    }\n+\n+    public void setName(String name) {\n+      this.name = name;\n+    }\n+\n+    public int getAge() {\n+      return age;\n+    }\n+\n+    public void setAge(int age) {\n+      this.age = age;\n+    }\n+  }\n+  // $example off:create_ds$\n+\n+  public static void main(String[] args) {\n+    // $example on:init_session$\n+    SparkSession spark = SparkSession\n+        .builder()\n+        .appName(\"Java Spark SQL Example\")\n+        .config(\"spark.some.config.option\", \"some-value\")\n+        .getOrCreate();\n+    // $example off:init_session$\n+\n+    runBasicDataFrameExample(spark);\n+    runDatasetCreationExample(spark);\n+    runInferSchemaExample(spark);\n+    runProgrammaticSchemaExample(spark);\n+\n+    spark.stop();\n+  }\n+\n+  private static void runBasicDataFrameExample(SparkSession spark) {\n+    // $example on:create_df$\n+    Dataset<Row> df = spark.read().json(\"examples/src/main/resources/people.json\");\n+\n+    // Displays the content of the DataFrame to stdout\n+    df.show();\n+    // age  name\n+    // null Michael\n+    // 30   Andy\n+    // 19   Justin\n+    // $example off:create_df$\n+\n+    // $example on:untyped_ops$\n+    // Print the schema in a tree format\n+    df.printSchema();\n+    // root\n+    // |-- age: long (nullable = true)\n+    // |-- name: string (nullable = true)\n+\n+    // Select only the \"name\" column\n+    df.select(\"name\").show();\n+    // name\n+    // Michael\n+    // Andy\n+    // Justin\n+\n+    // Select everybody, but increment the age by 1\n+    df.select(df.col(\"name\"), df.col(\"age\").plus(1)).show();\n+    // name    (age + 1)\n+    // Michael null\n+    // Andy    31\n+    // Justin  20\n+\n+    // Select people older than 21\n+    df.filter(df.col(\"age\").gt(21)).show();\n+    // age name\n+    // 30  Andy\n+\n+    // Count people by age\n+    df.groupBy(\"age\").count().show();\n+    // age  count\n+    // null 1\n+    // 19   1\n+    // 30   1\n+    // $example off:untyped_ops$\n+\n+    // $example on:run_sql$\n+    // Register the DataFrame as a SQL temporary view\n+    df.createOrReplaceTempView(\"people\");\n+\n+    Dataset<Row> sqlDF = spark.sql(\"SELECT * FROM people\");\n+    sqlDF.show();\n+    // $example off:run_sql$\n+  }\n+\n+  private static void runDatasetCreationExample(SparkSession spark) {\n+    // $example on:create_ds$\n+    // Create an instance of a Bean class\n+    Person person = new Person();\n+    person.setName(\"Andy\");\n+    person.setAge(32);\n+\n+    // Encoders are created for Java beans\n+    Encoder<Person> personEncoder = Encoders.bean(Person.class);\n+    Dataset<Person> javaBeanDS = spark.createDataset(\n+        Collections.singletonList(person),\n+        personEncoder"
  }],
  "prId": 14119
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Nit: Use 2-space indentation here.\n",
    "commit": "7451fc784d5c8f87c37f7707c4323a280d52417b",
    "createdAt": "2016-07-11T14:06:37Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.sql;\n+\n+// $example on:programmatic_schema$\n+import java.util.ArrayList;\n+import java.util.List;\n+// $example off:programmatic_schema$\n+// $example on:create_ds$\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.io.Serializable;\n+// $example off:create_ds$\n+\n+// $example on:schema_inferring$\n+// $example on:programmatic_schema$\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.function.Function;\n+// $example off:programmatic_schema$\n+// $example on:create_ds$\n+import org.apache.spark.api.java.function.MapFunction;\n+// $example on:create_df$\n+// $example on:run_sql$\n+// $example on:programmatic_schema$\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+// $example off:programmatic_schema$\n+// $example off:create_df$\n+// $example off:run_sql$\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+// $example off:create_ds$\n+// $example off:schema_inferring$\n+import org.apache.spark.sql.RowFactory;\n+// $example on:init_session$\n+import org.apache.spark.sql.SparkSession;\n+// $example off:init_session$\n+// $example on:programmatic_schema$\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+// $example off:programmatic_schema$\n+\n+public class JavaSparkSqlExample {\n+  // $example on:create_ds$\n+  public static class Person implements Serializable {\n+    private String name;\n+    private int age;\n+\n+    public String getName() {\n+      return name;\n+    }\n+\n+    public void setName(String name) {\n+      this.name = name;\n+    }\n+\n+    public int getAge() {\n+      return age;\n+    }\n+\n+    public void setAge(int age) {\n+      this.age = age;\n+    }\n+  }\n+  // $example off:create_ds$\n+\n+  public static void main(String[] args) {\n+    // $example on:init_session$\n+    SparkSession spark = SparkSession\n+        .builder()\n+        .appName(\"Java Spark SQL Example\")\n+        .config(\"spark.some.config.option\", \"some-value\")\n+        .getOrCreate();\n+    // $example off:init_session$\n+\n+    runBasicDataFrameExample(spark);\n+    runDatasetCreationExample(spark);\n+    runInferSchemaExample(spark);\n+    runProgrammaticSchemaExample(spark);\n+\n+    spark.stop();\n+  }\n+\n+  private static void runBasicDataFrameExample(SparkSession spark) {\n+    // $example on:create_df$\n+    Dataset<Row> df = spark.read().json(\"examples/src/main/resources/people.json\");\n+\n+    // Displays the content of the DataFrame to stdout\n+    df.show();\n+    // age  name\n+    // null Michael\n+    // 30   Andy\n+    // 19   Justin\n+    // $example off:create_df$\n+\n+    // $example on:untyped_ops$\n+    // Print the schema in a tree format\n+    df.printSchema();\n+    // root\n+    // |-- age: long (nullable = true)\n+    // |-- name: string (nullable = true)\n+\n+    // Select only the \"name\" column\n+    df.select(\"name\").show();\n+    // name\n+    // Michael\n+    // Andy\n+    // Justin\n+\n+    // Select everybody, but increment the age by 1\n+    df.select(df.col(\"name\"), df.col(\"age\").plus(1)).show();\n+    // name    (age + 1)\n+    // Michael null\n+    // Andy    31\n+    // Justin  20\n+\n+    // Select people older than 21\n+    df.filter(df.col(\"age\").gt(21)).show();\n+    // age name\n+    // 30  Andy\n+\n+    // Count people by age\n+    df.groupBy(\"age\").count().show();\n+    // age  count\n+    // null 1\n+    // 19   1\n+    // 30   1\n+    // $example off:untyped_ops$\n+\n+    // $example on:run_sql$\n+    // Register the DataFrame as a SQL temporary view\n+    df.createOrReplaceTempView(\"people\");\n+\n+    Dataset<Row> sqlDF = spark.sql(\"SELECT * FROM people\");\n+    sqlDF.show();\n+    // $example off:run_sql$\n+  }\n+\n+  private static void runDatasetCreationExample(SparkSession spark) {\n+    // $example on:create_ds$\n+    // Create an instance of a Bean class\n+    Person person = new Person();\n+    person.setName(\"Andy\");\n+    person.setAge(32);\n+\n+    // Encoders are created for Java beans\n+    Encoder<Person> personEncoder = Encoders.bean(Person.class);\n+    Dataset<Person> javaBeanDS = spark.createDataset(\n+        Collections.singletonList(person),\n+        personEncoder\n+    );\n+    javaBeanDS.show();\n+\n+    // Encoders for most common types are provided in class Encoders\n+    Encoder<Integer> integerEncoder = Encoders.INT();\n+    Dataset<Integer> primitiveDS = spark.createDataset(Arrays.asList(1, 2, 3), integerEncoder);\n+    Dataset<Integer> transformedDS = primitiveDS.map(new MapFunction<Integer, Integer>() {\n+      @Override\n+      public Integer call(Integer value) throws Exception {\n+        return value + 1;\n+      }\n+    }, integerEncoder);\n+    transformedDS.collect(); // Returns [2, 3, 4]\n+\n+    // DataFrames can be converted to a Dataset by providing a class. Mapping based on name\n+    String path = \"examples/src/main/resources/people.json\";\n+    Dataset<Person> peopleDS = spark.read().json(path).as(personEncoder);\n+    peopleDS.show();\n+    // $example off:create_ds$\n+  }\n+\n+  private static void runInferSchemaExample(SparkSession spark) {\n+    // $example on:schema_inferring$\n+    // Create an RDD of Person objects from a text file\n+    JavaRDD<Person> peopleRDD = spark.read()\n+        .textFile(\"examples/src/main/resources/people.txt\")\n+        .javaRDD()\n+        .map(new Function<String, Person>() {"
  }],
  "prId": 14119
}, {
  "comments": [{
    "author": {
      "login": "liancheng"
    },
    "body": "Nit: Use 2-space indentation here.\n",
    "commit": "7451fc784d5c8f87c37f7707c4323a280d52417b",
    "createdAt": "2016-07-11T14:06:43Z",
    "diffHunk": "@@ -0,0 +1,280 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.spark.examples.sql;\n+\n+// $example on:programmatic_schema$\n+import java.util.ArrayList;\n+import java.util.List;\n+// $example off:programmatic_schema$\n+// $example on:create_ds$\n+import java.util.Arrays;\n+import java.util.Collections;\n+import java.io.Serializable;\n+// $example off:create_ds$\n+\n+// $example on:schema_inferring$\n+// $example on:programmatic_schema$\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.function.Function;\n+// $example off:programmatic_schema$\n+// $example on:create_ds$\n+import org.apache.spark.api.java.function.MapFunction;\n+// $example on:create_df$\n+// $example on:run_sql$\n+// $example on:programmatic_schema$\n+import org.apache.spark.sql.Dataset;\n+import org.apache.spark.sql.Row;\n+// $example off:programmatic_schema$\n+// $example off:create_df$\n+// $example off:run_sql$\n+import org.apache.spark.sql.Encoder;\n+import org.apache.spark.sql.Encoders;\n+// $example off:create_ds$\n+// $example off:schema_inferring$\n+import org.apache.spark.sql.RowFactory;\n+// $example on:init_session$\n+import org.apache.spark.sql.SparkSession;\n+// $example off:init_session$\n+// $example on:programmatic_schema$\n+import org.apache.spark.sql.types.DataTypes;\n+import org.apache.spark.sql.types.StructField;\n+import org.apache.spark.sql.types.StructType;\n+// $example off:programmatic_schema$\n+\n+public class JavaSparkSqlExample {\n+  // $example on:create_ds$\n+  public static class Person implements Serializable {\n+    private String name;\n+    private int age;\n+\n+    public String getName() {\n+      return name;\n+    }\n+\n+    public void setName(String name) {\n+      this.name = name;\n+    }\n+\n+    public int getAge() {\n+      return age;\n+    }\n+\n+    public void setAge(int age) {\n+      this.age = age;\n+    }\n+  }\n+  // $example off:create_ds$\n+\n+  public static void main(String[] args) {\n+    // $example on:init_session$\n+    SparkSession spark = SparkSession\n+        .builder()\n+        .appName(\"Java Spark SQL Example\")\n+        .config(\"spark.some.config.option\", \"some-value\")\n+        .getOrCreate();\n+    // $example off:init_session$\n+\n+    runBasicDataFrameExample(spark);\n+    runDatasetCreationExample(spark);\n+    runInferSchemaExample(spark);\n+    runProgrammaticSchemaExample(spark);\n+\n+    spark.stop();\n+  }\n+\n+  private static void runBasicDataFrameExample(SparkSession spark) {\n+    // $example on:create_df$\n+    Dataset<Row> df = spark.read().json(\"examples/src/main/resources/people.json\");\n+\n+    // Displays the content of the DataFrame to stdout\n+    df.show();\n+    // age  name\n+    // null Michael\n+    // 30   Andy\n+    // 19   Justin\n+    // $example off:create_df$\n+\n+    // $example on:untyped_ops$\n+    // Print the schema in a tree format\n+    df.printSchema();\n+    // root\n+    // |-- age: long (nullable = true)\n+    // |-- name: string (nullable = true)\n+\n+    // Select only the \"name\" column\n+    df.select(\"name\").show();\n+    // name\n+    // Michael\n+    // Andy\n+    // Justin\n+\n+    // Select everybody, but increment the age by 1\n+    df.select(df.col(\"name\"), df.col(\"age\").plus(1)).show();\n+    // name    (age + 1)\n+    // Michael null\n+    // Andy    31\n+    // Justin  20\n+\n+    // Select people older than 21\n+    df.filter(df.col(\"age\").gt(21)).show();\n+    // age name\n+    // 30  Andy\n+\n+    // Count people by age\n+    df.groupBy(\"age\").count().show();\n+    // age  count\n+    // null 1\n+    // 19   1\n+    // 30   1\n+    // $example off:untyped_ops$\n+\n+    // $example on:run_sql$\n+    // Register the DataFrame as a SQL temporary view\n+    df.createOrReplaceTempView(\"people\");\n+\n+    Dataset<Row> sqlDF = spark.sql(\"SELECT * FROM people\");\n+    sqlDF.show();\n+    // $example off:run_sql$\n+  }\n+\n+  private static void runDatasetCreationExample(SparkSession spark) {\n+    // $example on:create_ds$\n+    // Create an instance of a Bean class\n+    Person person = new Person();\n+    person.setName(\"Andy\");\n+    person.setAge(32);\n+\n+    // Encoders are created for Java beans\n+    Encoder<Person> personEncoder = Encoders.bean(Person.class);\n+    Dataset<Person> javaBeanDS = spark.createDataset(\n+        Collections.singletonList(person),\n+        personEncoder\n+    );\n+    javaBeanDS.show();\n+\n+    // Encoders for most common types are provided in class Encoders\n+    Encoder<Integer> integerEncoder = Encoders.INT();\n+    Dataset<Integer> primitiveDS = spark.createDataset(Arrays.asList(1, 2, 3), integerEncoder);\n+    Dataset<Integer> transformedDS = primitiveDS.map(new MapFunction<Integer, Integer>() {\n+      @Override\n+      public Integer call(Integer value) throws Exception {\n+        return value + 1;\n+      }\n+    }, integerEncoder);\n+    transformedDS.collect(); // Returns [2, 3, 4]\n+\n+    // DataFrames can be converted to a Dataset by providing a class. Mapping based on name\n+    String path = \"examples/src/main/resources/people.json\";\n+    Dataset<Person> peopleDS = spark.read().json(path).as(personEncoder);\n+    peopleDS.show();\n+    // $example off:create_ds$\n+  }\n+\n+  private static void runInferSchemaExample(SparkSession spark) {\n+    // $example on:schema_inferring$\n+    // Create an RDD of Person objects from a text file\n+    JavaRDD<Person> peopleRDD = spark.read()\n+        .textFile(\"examples/src/main/resources/people.txt\")\n+        .javaRDD()\n+        .map(new Function<String, Person>() {\n+          @Override\n+          public Person call(String line) throws Exception {\n+            String[] parts = line.split(\",\");\n+            Person person = new Person();\n+            person.setName(parts[0]);\n+            person.setAge(Integer.parseInt(parts[1].trim()));\n+            return person;\n+          }\n+        });\n+\n+    // Apply a schema to an RDD of JavaBeans to get a DataFrame\n+    Dataset<Row> peopleDF = spark.createDataFrame(peopleRDD, Person.class);\n+    // Register the DataFrame as a temporary view\n+    peopleDF.createOrReplaceTempView(\"people\");\n+\n+    // SQL statements can be run by using the sql methods provided by spark\n+    Dataset<Row> teenagersDF = spark.sql(\"SELECT name FROM people WHERE age BETWEEN 13 AND 19\");\n+\n+    // The columns of a row in the result can be accessed by field index\n+    Encoder<String> stringEncoder = Encoders.STRING();\n+    Dataset<String> teenagerNamesByIndexDF = teenagersDF.map(new MapFunction<Row, String>() {\n+      @Override\n+      public String call(Row row) throws Exception {\n+        return \"Name: \" + row.getString(0);\n+      }\n+    }, stringEncoder);\n+    teenagerNamesByIndexDF.show();\n+\n+    // or by field name\n+    Dataset<String> teenagerNamesByFieldDF = teenagersDF.map(new MapFunction<Row, String>() {\n+      @Override\n+      public String call(Row row) throws Exception {\n+        return \"Name: \" + row.<String>getAs(\"name\");\n+      }\n+    }, stringEncoder);\n+    teenagerNamesByFieldDF.show();\n+    // $example off:schema_inferring$\n+  }\n+\n+  private static void runProgrammaticSchemaExample(SparkSession spark) {\n+    // $example on:programmatic_schema$\n+    // Create an RDD\n+    JavaRDD<String> peopleRDD = spark.sparkContext()\n+        .textFile(\"examples/src/main/resources/people.txt\", 1)\n+        .toJavaRDD();"
  }],
  "prId": 14119
}]