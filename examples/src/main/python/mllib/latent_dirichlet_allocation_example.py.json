[{
  "comments": [{
    "author": {
      "login": "yinxusen"
    },
    "body": "keep 4-indent\n",
    "commit": "8b9228dfaefdf966841d91bb838a7aeacb5854e2",
    "createdAt": "2016-02-18T23:21:58Z",
    "diffHunk": "@@ -0,0 +1,53 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from __future__ import print_function\n+\n+from pyspark import SparkContext\n+# $example on$\n+from pyspark.mllib.clustering import LDA, LDAModel\n+from pyspark.mllib.linalg import Vectors\n+# $example off$\n+\n+if __name__ == \"__main__\":\n+    sc = SparkContext(appName=\"LatentDirichletAllocationExample\")  # SparkContext\n+\n+    # $example on$\n+    # Load and parse the data\n+    data = sc.textFile(\"data/mllib/sample_lda_data.txt\")\n+    parsedData = data.map(lambda line: Vectors.dense([float(x) for x in line.strip().split(' ')]))\n+    # Index documents with unique IDs\n+    corpus = parsedData.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n+\n+    # Cluster the documents into three topics using LDA\n+    ldaModel = LDA.train(corpus, k=3)\n+\n+    # Output topics. Each is a distribution over words (matching word count vectors)\n+    print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize())\n+          + \" words):\")",
    "line": 41
  }, {
    "author": {
      "login": "keypointt"
    },
    "body": "got python check error \"E128 continuation line under-indented for visual indent\" after running \"./dev/lint-python\", seems it should be indenting to the opening bracket\n",
    "commit": "8b9228dfaefdf966841d91bb838a7aeacb5854e2",
    "createdAt": "2016-02-20T17:39:18Z",
    "diffHunk": "@@ -0,0 +1,53 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from __future__ import print_function\n+\n+from pyspark import SparkContext\n+# $example on$\n+from pyspark.mllib.clustering import LDA, LDAModel\n+from pyspark.mllib.linalg import Vectors\n+# $example off$\n+\n+if __name__ == \"__main__\":\n+    sc = SparkContext(appName=\"LatentDirichletAllocationExample\")  # SparkContext\n+\n+    # $example on$\n+    # Load and parse the data\n+    data = sc.textFile(\"data/mllib/sample_lda_data.txt\")\n+    parsedData = data.map(lambda line: Vectors.dense([float(x) for x in line.strip().split(' ')]))\n+    # Index documents with unique IDs\n+    corpus = parsedData.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n+\n+    # Cluster the documents into three topics using LDA\n+    ldaModel = LDA.train(corpus, k=3)\n+\n+    # Output topics. Each is a distribution over words (matching word count vectors)\n+    print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize())\n+          + \" words):\")",
    "line": 41
  }, {
    "author": {
      "login": "yinxusen"
    },
    "body": "OK, let's keep it unchanged\n",
    "commit": "8b9228dfaefdf966841d91bb838a7aeacb5854e2",
    "createdAt": "2016-02-21T03:11:01Z",
    "diffHunk": "@@ -0,0 +1,53 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from __future__ import print_function\n+\n+from pyspark import SparkContext\n+# $example on$\n+from pyspark.mllib.clustering import LDA, LDAModel\n+from pyspark.mllib.linalg import Vectors\n+# $example off$\n+\n+if __name__ == \"__main__\":\n+    sc = SparkContext(appName=\"LatentDirichletAllocationExample\")  # SparkContext\n+\n+    # $example on$\n+    # Load and parse the data\n+    data = sc.textFile(\"data/mllib/sample_lda_data.txt\")\n+    parsedData = data.map(lambda line: Vectors.dense([float(x) for x in line.strip().split(' ')]))\n+    # Index documents with unique IDs\n+    corpus = parsedData.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n+\n+    # Cluster the documents into three topics using LDA\n+    ldaModel = LDA.train(corpus, k=3)\n+\n+    # Output topics. Each is a distribution over words (matching word count vectors)\n+    print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize())\n+          + \" words):\")",
    "line": 41
  }, {
    "author": {
      "login": "keypointt"
    },
    "body": "ok, thanks for reviewing\n",
    "commit": "8b9228dfaefdf966841d91bb838a7aeacb5854e2",
    "createdAt": "2016-02-21T06:10:23Z",
    "diffHunk": "@@ -0,0 +1,53 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from __future__ import print_function\n+\n+from pyspark import SparkContext\n+# $example on$\n+from pyspark.mllib.clustering import LDA, LDAModel\n+from pyspark.mllib.linalg import Vectors\n+# $example off$\n+\n+if __name__ == \"__main__\":\n+    sc = SparkContext(appName=\"LatentDirichletAllocationExample\")  # SparkContext\n+\n+    # $example on$\n+    # Load and parse the data\n+    data = sc.textFile(\"data/mllib/sample_lda_data.txt\")\n+    parsedData = data.map(lambda line: Vectors.dense([float(x) for x in line.strip().split(' ')]))\n+    # Index documents with unique IDs\n+    corpus = parsedData.zipWithIndex().map(lambda x: [x[1], x[0]]).cache()\n+\n+    # Cluster the documents into three topics using LDA\n+    ldaModel = LDA.train(corpus, k=3)\n+\n+    # Output topics. Each is a distribution over words (matching word count vectors)\n+    print(\"Learned topics (as distributions over vocab of \" + str(ldaModel.vocabSize())\n+          + \" words):\")",
    "line": 41
  }],
  "prId": 11116
}]