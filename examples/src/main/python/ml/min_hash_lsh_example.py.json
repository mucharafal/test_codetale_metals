[{
  "comments": [{
    "author": {
      "login": "sethah"
    },
    "body": "reword as above.",
    "commit": "36fd9bc6366d58541c8306803d8742649be69098",
    "createdAt": "2017-02-09T22:20:22Z",
    "diffHunk": "@@ -0,0 +1,85 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\n+from __future__ import print_function\n+\n+# $example on$\n+from pyspark.ml.feature import MinHashLSH\n+from pyspark.ml.linalg import Vectors\n+# $example off$\n+from pyspark.sql import SparkSession\n+\n+\"\"\"\n+An example demonstrating MinHashLSH.\n+Run with:\n+  bin/spark-submit examples/src/main/python/ml/min_hash_lsh_example.py\n+\"\"\"\n+\n+if __name__ == \"__main__\":\n+    spark = SparkSession \\\n+        .builder \\\n+        .appName(\"MinHashLSHExample\") \\\n+        .getOrCreate()\n+\n+    # $example on$\n+    dataA = [(0, Vectors.sparse(6, [0, 1, 2], [1.0, 1.0, 1.0]),),\n+             (1, Vectors.sparse(6, [2, 3, 4], [1.0, 1.0, 1.0]),),\n+             (2, Vectors.sparse(6, [0, 2, 4], [1.0, 1.0, 1.0]),)]\n+    dfA = spark.createDataFrame(dataA, [\"id\", \"features\"])\n+\n+    dataB = [(3, Vectors.sparse(6, [1, 3, 5], [1.0, 1.0, 1.0]),),\n+             (4, Vectors.sparse(6, [2, 3, 5], [1.0, 1.0, 1.0]),),\n+             (5, Vectors.sparse(6, [1, 2, 4], [1.0, 1.0, 1.0]),)]\n+    dfB = spark.createDataFrame(dataB, [\"id\", \"features\"])\n+\n+    key = Vectors.sparse(6, [1, 3], [1.0, 1.0])\n+\n+    mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=5)\n+    model = mh.fit(dfA)\n+\n+    # Feature Transformation\n+    print(\"The hashed dataset where hashed values are stored in the column 'values':\")\n+    model.transform(dfA).show()\n+\n+    # Cache the transformed columns\n+    transformedA = model.transform(dfA).cache()\n+    transformedB = model.transform(dfB).cache()\n+\n+    # Approximate similarity join\n+    print(\"Approximately joining dfA and dfB on distance smaller than 0.6:\")\n+    model.approxSimilarityJoin(dfA, dfB, 0.6)\\\n+        .select(\"datasetA.id\", \"datasetB.id\", \"distCol\").show()\n+    print(\"Joining cached datasets to avoid recomputing the hash values:\")\n+    model.approxSimilarityJoin(transformedA, transformedB, 0.6)\\\n+        .select(\"datasetA.id\", \"datasetB.id\", \"distCol\").show()\n+\n+    # Self Join\n+    print(\"Approximately self join of dfB on distance smaller than 0.6:\")"
  }],
  "prId": 16715
}]