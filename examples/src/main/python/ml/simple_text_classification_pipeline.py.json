[{
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "It also could be done like this:\n\n```\nCls = Row('id', 'text', 'label')\nsc.parallelize([Cls(0L,  \"a b c d e spark\", 1.0),])\n```\n",
    "commit": "415268e19ebf9d48cb4a50773f446ebbe8a902cd",
    "createdAt": "2015-01-27T01:06:23Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark import SparkContext\n+from pyspark.sql import SQLContext, Row\n+from pyspark.ml import Pipeline\n+from pyspark.ml.feature import HashingTF, Tokenizer\n+from pyspark.ml.classification import LogisticRegression\n+\n+\n+\"\"\"\n+A simple text classification pipeline that recognizes \"spark\" from\n+input text. This is to show how to create and configure a Spark ML\n+pipeline in Python. Run with:\n+\n+  bin/spark-submit examples/src/main/python/ml/simple_text_classification_pipeline.py\n+\"\"\"\n+\n+\n+if __name__ == \"__main__\":\n+    sc = SparkContext(appName=\"SimpleTextClassificationPipeline\")\n+    sqlCtx = SQLContext(sc)\n+    training = sqlCtx.inferSchema(\n+        sc.parallelize([(0L, \"a b c d e spark\", 1.0),\n+                        (1L, \"b d\", 0.0),\n+                        (2L, \"spark f g h\", 1.0),\n+                        (3L, \"hadoop mapreduce\", 0.0)])\n+          .map(lambda x: Row(id=x[0], text=x[1], label=x[2])))"
  }, {
    "author": {
      "login": "mengxr"
    },
    "body": "done\n",
    "commit": "415268e19ebf9d48cb4a50773f446ebbe8a902cd",
    "createdAt": "2015-01-27T19:32:15Z",
    "diffHunk": "@@ -0,0 +1,70 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark import SparkContext\n+from pyspark.sql import SQLContext, Row\n+from pyspark.ml import Pipeline\n+from pyspark.ml.feature import HashingTF, Tokenizer\n+from pyspark.ml.classification import LogisticRegression\n+\n+\n+\"\"\"\n+A simple text classification pipeline that recognizes \"spark\" from\n+input text. This is to show how to create and configure a Spark ML\n+pipeline in Python. Run with:\n+\n+  bin/spark-submit examples/src/main/python/ml/simple_text_classification_pipeline.py\n+\"\"\"\n+\n+\n+if __name__ == \"__main__\":\n+    sc = SparkContext(appName=\"SimpleTextClassificationPipeline\")\n+    sqlCtx = SQLContext(sc)\n+    training = sqlCtx.inferSchema(\n+        sc.parallelize([(0L, \"a b c d e spark\", 1.0),\n+                        (1L, \"b d\", 0.0),\n+                        (2L, \"spark f g h\", 1.0),\n+                        (3L, \"hadoop mapreduce\", 0.0)])\n+          .map(lambda x: Row(id=x[0], text=x[1], label=x[2])))"
  }],
  "prId": 4151
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "This looks very Java style, verbose and many lines, imaged that could be simplified as : \n\n```\ntokenizer = Tokenizer(\"text\", \"words\")\nhashingTF = HashingTF(\"words\", \"features\")\nlr = LogisticRegression(maxIter=10, regParam=0.01)\npipeline = Pipeline([tokenizer, hashingTF, lr])\n```\n",
    "commit": "415268e19ebf9d48cb4a50773f446ebbe8a902cd",
    "createdAt": "2015-01-28T00:06:15Z",
    "diffHunk": "@@ -0,0 +1,79 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from pyspark import SparkContext\n+from pyspark.sql import SQLContext, Row\n+from pyspark.ml import Pipeline\n+from pyspark.ml.feature import HashingTF, Tokenizer\n+from pyspark.ml.classification import LogisticRegression\n+\n+\n+\"\"\"\n+A simple text classification pipeline that recognizes \"spark\" from\n+input text. This is to show how to create and configure a Spark ML\n+pipeline in Python. Run with:\n+\n+  bin/spark-submit examples/src/main/python/ml/simple_text_classification_pipeline.py\n+\"\"\"\n+\n+\n+if __name__ == \"__main__\":\n+    sc = SparkContext(appName=\"SimpleTextClassificationPipeline\")\n+    sqlCtx = SQLContext(sc)\n+\n+    # Prepare training documents, which are labeled.\n+    LabeledDocument = Row('id', 'text', 'label')\n+    training = sqlCtx.inferSchema(\n+        sc.parallelize([(0L, \"a b c d e spark\", 1.0),\n+                        (1L, \"b d\", 0.0),\n+                        (2L, \"spark f g h\", 1.0),\n+                        (3L, \"hadoop mapreduce\", 0.0)])\n+          .map(lambda x: LabeledDocument(*x)))\n+\n+    # Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n+    tokenizer = Tokenizer() \\\n+        .setInputCol(\"text\") \\\n+        .setOutputCol(\"words\")\n+    hashingTF = HashingTF() \\\n+        .setInputCol(tokenizer.getOutputCol()) \\\n+        .setOutputCol(\"features\")\n+    lr = LogisticRegression() \\\n+        .setMaxIter(10) \\\n+        .setRegParam(0.01)\n+    pipeline = Pipeline() \\\n+        .setStages([tokenizer, hashingTF, lr])",
    "line": 58
  }],
  "prId": 4151
}]