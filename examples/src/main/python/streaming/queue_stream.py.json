[{
  "comments": [{
    "author": {
      "login": "BryanCutler"
    },
    "body": "I noticed the printout is the same as the Scala example, but there is an extra empty element like this\n\n```\n-------------------------------------------\nTime: 2015-06-18 10:28:56\n-------------------------------------------\n(0, 100)\n(8, 100)\n(1, 100)\n(9, 100)\n(2, 100)\n(3, 100)\n(4, 100)\n(5, 100)\n(6, 100)\n(7, 100)\n()\n```\n\nIs this a bug somewhere?\n",
    "commit": "435ba7e561dcbc74ee5d1044abd36fdad36a9f18",
    "createdAt": "2015-06-18T18:04:20Z",
    "diffHunk": "@@ -0,0 +1,44 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from __future__ import print_function\n+\n+import sys\n+import time\n+\n+from pyspark import SparkContext\n+from pyspark.streaming import StreamingContext\n+\n+if __name__ == \"__main__\":\n+\n+    sc = SparkContext(appName=\"PythonStreamingQueueStream\")\n+    ssc = StreamingContext(sc, 1)\n+    \n+    # Create the queue through which RDDs can be pushed to\n+    # a QueueInputDStream\n+    rddQueue = []\n+    for i in xrange(5):\n+        rddQueue += [ssc.sparkContext.parallelize([j for j in xrange(1,1001)])]\n+    \n+    # Create the QueueInputDStream and use it do some processing\n+    inputStream = ssc.queueStream(rddQueue)\n+    mappedStream = inputStream.map(lambda x: (x % 10, 1))\n+    reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n+    reducedStream.pprint()",
    "line": 46
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "@davies Is this a bug somewhere?\n",
    "commit": "435ba7e561dcbc74ee5d1044abd36fdad36a9f18",
    "createdAt": "2015-06-18T20:56:20Z",
    "diffHunk": "@@ -0,0 +1,44 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from __future__ import print_function\n+\n+import sys\n+import time\n+\n+from pyspark import SparkContext\n+from pyspark.streaming import StreamingContext\n+\n+if __name__ == \"__main__\":\n+\n+    sc = SparkContext(appName=\"PythonStreamingQueueStream\")\n+    ssc = StreamingContext(sc, 1)\n+    \n+    # Create the queue through which RDDs can be pushed to\n+    # a QueueInputDStream\n+    rddQueue = []\n+    for i in xrange(5):\n+        rddQueue += [ssc.sparkContext.parallelize([j for j in xrange(1,1001)])]\n+    \n+    # Create the QueueInputDStream and use it do some processing\n+    inputStream = ssc.queueStream(rddQueue)\n+    mappedStream = inputStream.map(lambda x: (x % 10, 1))\n+    reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n+    reducedStream.pprint()",
    "line": 46
  }, {
    "author": {
      "login": "davies"
    },
    "body": "Yes, `print()` in DStream.pprint() should be `print('')`\n\n@BryanCutler Could you fix it in this PR?\n",
    "commit": "435ba7e561dcbc74ee5d1044abd36fdad36a9f18",
    "createdAt": "2015-06-18T21:06:45Z",
    "diffHunk": "@@ -0,0 +1,44 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from __future__ import print_function\n+\n+import sys\n+import time\n+\n+from pyspark import SparkContext\n+from pyspark.streaming import StreamingContext\n+\n+if __name__ == \"__main__\":\n+\n+    sc = SparkContext(appName=\"PythonStreamingQueueStream\")\n+    ssc = StreamingContext(sc, 1)\n+    \n+    # Create the queue through which RDDs can be pushed to\n+    # a QueueInputDStream\n+    rddQueue = []\n+    for i in xrange(5):\n+        rddQueue += [ssc.sparkContext.parallelize([j for j in xrange(1,1001)])]\n+    \n+    # Create the QueueInputDStream and use it do some processing\n+    inputStream = ssc.queueStream(rddQueue)\n+    mappedStream = inputStream.map(lambda x: (x % 10, 1))\n+    reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n+    reducedStream.pprint()",
    "line": 46
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "@davies sure, no problem!\n",
    "commit": "435ba7e561dcbc74ee5d1044abd36fdad36a9f18",
    "createdAt": "2015-06-18T21:32:23Z",
    "diffHunk": "@@ -0,0 +1,44 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from __future__ import print_function\n+\n+import sys\n+import time\n+\n+from pyspark import SparkContext\n+from pyspark.streaming import StreamingContext\n+\n+if __name__ == \"__main__\":\n+\n+    sc = SparkContext(appName=\"PythonStreamingQueueStream\")\n+    ssc = StreamingContext(sc, 1)\n+    \n+    # Create the queue through which RDDs can be pushed to\n+    # a QueueInputDStream\n+    rddQueue = []\n+    for i in xrange(5):\n+        rddQueue += [ssc.sparkContext.parallelize([j for j in xrange(1,1001)])]\n+    \n+    # Create the QueueInputDStream and use it do some processing\n+    inputStream = ssc.queueStream(rddQueue)\n+    mappedStream = inputStream.map(lambda x: (x % 10, 1))\n+    reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n+    reducedStream.pprint()",
    "line": 46
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "That took care of the print issue\n",
    "commit": "435ba7e561dcbc74ee5d1044abd36fdad36a9f18",
    "createdAt": "2015-06-18T22:58:38Z",
    "diffHunk": "@@ -0,0 +1,44 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from __future__ import print_function\n+\n+import sys\n+import time\n+\n+from pyspark import SparkContext\n+from pyspark.streaming import StreamingContext\n+\n+if __name__ == \"__main__\":\n+\n+    sc = SparkContext(appName=\"PythonStreamingQueueStream\")\n+    ssc = StreamingContext(sc, 1)\n+    \n+    # Create the queue through which RDDs can be pushed to\n+    # a QueueInputDStream\n+    rddQueue = []\n+    for i in xrange(5):\n+        rddQueue += [ssc.sparkContext.parallelize([j for j in xrange(1,1001)])]\n+    \n+    # Create the QueueInputDStream and use it do some processing\n+    inputStream = ssc.queueStream(rddQueue)\n+    mappedStream = inputStream.map(lambda x: (x % 10, 1))\n+    reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n+    reducedStream.pprint()",
    "line": 46
  }],
  "prId": 6884
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "This is not used anywhere.\n",
    "commit": "435ba7e561dcbc74ee5d1044abd36fdad36a9f18",
    "createdAt": "2015-06-18T21:07:49Z",
    "diffHunk": "@@ -0,0 +1,44 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+from __future__ import print_function"
  }],
  "prId": 6884
}, {
  "comments": [{
    "author": {
      "login": "BryanCutler"
    },
    "body": "When using `ssc.awaitTermination()` after the queue is processed, there are sometimes exceptions thrown\n\n```\n ERROR Utils: Uncaught exception in thread heartbeat-receiver-event-loop-thread\n```\n\nprobably not a big deal, but using `ssc.stop()` with `stopGraceFully = True` seemed to work without causing any exceptions.\n",
    "commit": "435ba7e561dcbc74ee5d1044abd36fdad36a9f18",
    "createdAt": "2015-06-18T23:04:17Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+ Create a queue of RDDs that will be mapped/reduced one at a time in\n+ 1 second intervals.\n+\n+ To run this example use\n+    `$ bin/spark-submit examples/src/main/python/streaming/queue_stream.py\n+\"\"\"\n+import sys\n+import time\n+\n+from pyspark import SparkContext\n+from pyspark.streaming import StreamingContext\n+\n+if __name__ == \"__main__\":\n+\n+    sc = SparkContext(appName=\"PythonStreamingQueueStream\")\n+    ssc = StreamingContext(sc, 1)\n+    \n+    # Create the queue through which RDDs can be pushed to\n+    # a QueueInputDStream\n+    rddQueue = []\n+    for i in xrange(5):\n+        rddQueue += [ssc.sparkContext.parallelize([j for j in xrange(1,1001)], 10)]\n+    \n+    # Create the QueueInputDStream and use it do some processing\n+    inputStream = ssc.queueStream(rddQueue)\n+    mappedStream = inputStream.map(lambda x: (x % 10, 1))\n+    reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n+    reducedStream.pprint()\n+    \n+    ssc.start()\n+    time.sleep(5)\n+    ssc.stop(stopSparkContext = True, stopGraceFully = True)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "Yeah, thats fine.\n",
    "commit": "435ba7e561dcbc74ee5d1044abd36fdad36a9f18",
    "createdAt": "2015-06-18T23:19:33Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+ Create a queue of RDDs that will be mapped/reduced one at a time in\n+ 1 second intervals.\n+\n+ To run this example use\n+    `$ bin/spark-submit examples/src/main/python/streaming/queue_stream.py\n+\"\"\"\n+import sys\n+import time\n+\n+from pyspark import SparkContext\n+from pyspark.streaming import StreamingContext\n+\n+if __name__ == \"__main__\":\n+\n+    sc = SparkContext(appName=\"PythonStreamingQueueStream\")\n+    ssc = StreamingContext(sc, 1)\n+    \n+    # Create the queue through which RDDs can be pushed to\n+    # a QueueInputDStream\n+    rddQueue = []\n+    for i in xrange(5):\n+        rddQueue += [ssc.sparkContext.parallelize([j for j in xrange(1,1001)], 10)]\n+    \n+    # Create the QueueInputDStream and use it do some processing\n+    inputStream = ssc.queueStream(rddQueue)\n+    mappedStream = inputStream.map(lambda x: (x % 10, 1))\n+    reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n+    reducedStream.pprint()\n+    \n+    ssc.start()\n+    time.sleep(5)\n+    ssc.stop(stopSparkContext = True, stopGraceFully = True)"
  }],
  "prId": 6884
}, {
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "Maybe we could add one more seconds to see no more data coming.\n",
    "commit": "435ba7e561dcbc74ee5d1044abd36fdad36a9f18",
    "createdAt": "2015-06-18T23:17:20Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+ Create a queue of RDDs that will be mapped/reduced one at a time in\n+ 1 second intervals.\n+\n+ To run this example use\n+    `$ bin/spark-submit examples/src/main/python/streaming/queue_stream.py\n+\"\"\"\n+import sys\n+import time\n+\n+from pyspark import SparkContext\n+from pyspark.streaming import StreamingContext\n+\n+if __name__ == \"__main__\":\n+\n+    sc = SparkContext(appName=\"PythonStreamingQueueStream\")\n+    ssc = StreamingContext(sc, 1)\n+    \n+    # Create the queue through which RDDs can be pushed to\n+    # a QueueInputDStream\n+    rddQueue = []\n+    for i in xrange(5):\n+        rddQueue += [ssc.sparkContext.parallelize([j for j in xrange(1,1001)], 10)]\n+    \n+    # Create the QueueInputDStream and use it do some processing\n+    inputStream = ssc.queueStream(rddQueue)\n+    mappedStream = inputStream.map(lambda x: (x % 10, 1))\n+    reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n+    reducedStream.pprint()\n+    \n+    ssc.start()\n+    time.sleep(5)"
  }, {
    "author": {
      "login": "BryanCutler"
    },
    "body": "So 5 seconds ends with one empty pprint() on my machine, but it might be on the edge... 6 seconds shows 2 empty pprint()'s.  How about 5.5 seconds?  That seems good on mine.\n",
    "commit": "435ba7e561dcbc74ee5d1044abd36fdad36a9f18",
    "createdAt": "2015-06-18T23:30:38Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+ Create a queue of RDDs that will be mapped/reduced one at a time in\n+ 1 second intervals.\n+\n+ To run this example use\n+    `$ bin/spark-submit examples/src/main/python/streaming/queue_stream.py\n+\"\"\"\n+import sys\n+import time\n+\n+from pyspark import SparkContext\n+from pyspark.streaming import StreamingContext\n+\n+if __name__ == \"__main__\":\n+\n+    sc = SparkContext(appName=\"PythonStreamingQueueStream\")\n+    ssc = StreamingContext(sc, 1)\n+    \n+    # Create the queue through which RDDs can be pushed to\n+    # a QueueInputDStream\n+    rddQueue = []\n+    for i in xrange(5):\n+        rddQueue += [ssc.sparkContext.parallelize([j for j in xrange(1,1001)], 10)]\n+    \n+    # Create the QueueInputDStream and use it do some processing\n+    inputStream = ssc.queueStream(rddQueue)\n+    mappedStream = inputStream.map(lambda x: (x % 10, 1))\n+    reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n+    reducedStream.pprint()\n+    \n+    ssc.start()\n+    time.sleep(5)"
  }, {
    "author": {
      "login": "tdas"
    },
    "body": "I dont mind if there are empty batches printed.\n",
    "commit": "435ba7e561dcbc74ee5d1044abd36fdad36a9f18",
    "createdAt": "2015-06-18T23:35:11Z",
    "diffHunk": "@@ -0,0 +1,50 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+ Create a queue of RDDs that will be mapped/reduced one at a time in\n+ 1 second intervals.\n+\n+ To run this example use\n+    `$ bin/spark-submit examples/src/main/python/streaming/queue_stream.py\n+\"\"\"\n+import sys\n+import time\n+\n+from pyspark import SparkContext\n+from pyspark.streaming import StreamingContext\n+\n+if __name__ == \"__main__\":\n+\n+    sc = SparkContext(appName=\"PythonStreamingQueueStream\")\n+    ssc = StreamingContext(sc, 1)\n+    \n+    # Create the queue through which RDDs can be pushed to\n+    # a QueueInputDStream\n+    rddQueue = []\n+    for i in xrange(5):\n+        rddQueue += [ssc.sparkContext.parallelize([j for j in xrange(1,1001)], 10)]\n+    \n+    # Create the QueueInputDStream and use it do some processing\n+    inputStream = ssc.queueStream(rddQueue)\n+    mappedStream = inputStream.map(lambda x: (x % 10, 1))\n+    reducedStream = mappedStream.reduceByKey(lambda a, b: a + b)\n+    reducedStream.pprint()\n+    \n+    ssc.start()\n+    time.sleep(5)"
  }],
  "prId": 6884
}]