[{
  "comments": [{
    "author": {
      "login": "koeninger"
    },
    "body": "Is it actually necessary to assign the result of subscribe / prefer brokers to local variables, or is this just a style thing?\n",
    "commit": "d155c8d819a7fc2f26c1a91cafc37c6d70c5e89b",
    "createdAt": "2016-07-25T18:17:02Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+ Counts words in UTF8 encoded, '\\n' delimited text directly received from Kafka in every 2 seconds.\n+ Usage: direct_kafka_wordcount.py <broker_list> <topic>\n+\n+ To run this on your local machine, you need to setup Kafka and create a producer first, see\n+ http://kafka.apache.org/documentation.html#quickstart\n+\n+ and then run the example\n+    `$ bin/spark-submit --jars \\\n+      external/kafka-0-10-assembly/target/scala-*/spark-streaming-kafka-assembly-*.jar \\\n+      examples/src/main/python/streaming/direct_kafka_wordcount.py \\\n+      localhost:9092 test`\n+\"\"\"\n+from __future__ import print_function\n+\n+import sys\n+\n+from pyspark import SparkContext\n+from pyspark.streaming import StreamingContext\n+from pyspark.streaming.kafka010 import KafkaUtils, PreferBrokers, Subscribe\n+\n+if __name__ == \"__main__\":\n+    if len(sys.argv) != 3:\n+        print(\"Usage: direct_kafka010_wordcount.py <broker_list> <topic>\", file=sys.stderr)\n+        exit(-1)\n+\n+    sc = SparkContext(appName=\"PythonStreamingDirectKafkaWordCount\")\n+    ssc = StreamingContext(sc, 2)\n+\n+    brokers, topic = sys.argv[1:]\n+    kafkaParams = {\"bootstrap.servers\": brokers, \"group.id\": \"test\"}\n+    subscribe = Subscribe([topic], kafkaParams)\n+    preferBrokers = PreferBrokers()",
    "line": 50
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Here in python, it is mainly a style thing.\n",
    "commit": "d155c8d819a7fc2f26c1a91cafc37c6d70c5e89b",
    "createdAt": "2016-07-26T01:33:50Z",
    "diffHunk": "@@ -0,0 +1,59 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+ Counts words in UTF8 encoded, '\\n' delimited text directly received from Kafka in every 2 seconds.\n+ Usage: direct_kafka_wordcount.py <broker_list> <topic>\n+\n+ To run this on your local machine, you need to setup Kafka and create a producer first, see\n+ http://kafka.apache.org/documentation.html#quickstart\n+\n+ and then run the example\n+    `$ bin/spark-submit --jars \\\n+      external/kafka-0-10-assembly/target/scala-*/spark-streaming-kafka-assembly-*.jar \\\n+      examples/src/main/python/streaming/direct_kafka_wordcount.py \\\n+      localhost:9092 test`\n+\"\"\"\n+from __future__ import print_function\n+\n+import sys\n+\n+from pyspark import SparkContext\n+from pyspark.streaming import StreamingContext\n+from pyspark.streaming.kafka010 import KafkaUtils, PreferBrokers, Subscribe\n+\n+if __name__ == \"__main__\":\n+    if len(sys.argv) != 3:\n+        print(\"Usage: direct_kafka010_wordcount.py <broker_list> <topic>\", file=sys.stderr)\n+        exit(-1)\n+\n+    sc = SparkContext(appName=\"PythonStreamingDirectKafkaWordCount\")\n+    ssc = StreamingContext(sc, 2)\n+\n+    brokers, topic = sys.argv[1:]\n+    kafkaParams = {\"bootstrap.servers\": brokers, \"group.id\": \"test\"}\n+    subscribe = Subscribe([topic], kafkaParams)\n+    preferBrokers = PreferBrokers()",
    "line": 50
  }],
  "prId": 14340
}]