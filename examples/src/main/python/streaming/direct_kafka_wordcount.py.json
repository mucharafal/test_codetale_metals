[{
  "comments": [{
    "author": {
      "login": "davies"
    },
    "body": "If broker is the MUST-HAVE argument, I'd like to put it into the arguments list (before topics).\n",
    "commit": "a1fe97c496f1441e0d2a5879e257c7e569f2e541",
    "createdAt": "2015-02-23T06:59:21Z",
    "diffHunk": "@@ -0,0 +1,55 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+ Counts words in UTF8 encoded, '\\n' delimited text directly received from Kafka in every 2 seconds.\n+ Usage: direct_kafka_wordcount.py <broker_list> <topic>\n+\n+ To run this on your local machine, you need to setup Kafka and create a producer first, see\n+ http://kafka.apache.org/documentation.html#quickstart\n+\n+ and then run the example\n+    `$ bin/spark-submit --driver-class-path external/kafka-assembly/target/scala-*/\\\n+      spark-streaming-kafka-assembly-*.jar \\\n+      examples/src/main/python/streaming/direct_kafka_wordcount.py \\\n+      localhost:9092 test`\n+\"\"\"\n+\n+import sys\n+\n+from pyspark import SparkContext\n+from pyspark.streaming import StreamingContext\n+from pyspark.streaming.kafka import KafkaUtils\n+\n+if __name__ == \"__main__\":\n+    if len(sys.argv) != 3:\n+        print >> sys.stderr, \"Usage: direct_kafka_wordcount.py <broker_list> <topic>\"\n+        exit(-1)\n+\n+    sc = SparkContext(appName=\"PythonStreamingDirectKafkaWordCount\")\n+    ssc = StreamingContext(sc, 2)\n+\n+    brokers, topic = sys.argv[1:]\n+    kvs = KafkaUtils.createDirectStream(ssc, [topic], {\"metadata.broker.list\": brokers})",
    "line": 47
  }, {
    "author": {
      "login": "jerryshao"
    },
    "body": "Hi @davies , thanks for your comment, I will add this as a argument.\n",
    "commit": "a1fe97c496f1441e0d2a5879e257c7e569f2e541",
    "createdAt": "2015-02-23T16:38:59Z",
    "diffHunk": "@@ -0,0 +1,55 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+ Counts words in UTF8 encoded, '\\n' delimited text directly received from Kafka in every 2 seconds.\n+ Usage: direct_kafka_wordcount.py <broker_list> <topic>\n+\n+ To run this on your local machine, you need to setup Kafka and create a producer first, see\n+ http://kafka.apache.org/documentation.html#quickstart\n+\n+ and then run the example\n+    `$ bin/spark-submit --driver-class-path external/kafka-assembly/target/scala-*/\\\n+      spark-streaming-kafka-assembly-*.jar \\\n+      examples/src/main/python/streaming/direct_kafka_wordcount.py \\\n+      localhost:9092 test`\n+\"\"\"\n+\n+import sys\n+\n+from pyspark import SparkContext\n+from pyspark.streaming import StreamingContext\n+from pyspark.streaming.kafka import KafkaUtils\n+\n+if __name__ == \"__main__\":\n+    if len(sys.argv) != 3:\n+        print >> sys.stderr, \"Usage: direct_kafka_wordcount.py <broker_list> <topic>\"\n+        exit(-1)\n+\n+    sc = SparkContext(appName=\"PythonStreamingDirectKafkaWordCount\")\n+    ssc = StreamingContext(sc, 2)\n+\n+    brokers, topic = sys.argv[1:]\n+    kvs = KafkaUtils.createDirectStream(ssc, [topic], {\"metadata.broker.list\": brokers})",
    "line": 47
  }],
  "prId": 4723
}]