[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "(Personally, I think it is not more readable..)",
    "commit": "582c8221998d6fe055d066106e9ca43204f96bb9",
    "createdAt": "2017-02-15T11:30:50Z",
    "diffHunk": "@@ -54,22 +54,25 @@ def print_happiest_words(rdd):\n \n     # Read in the word-sentiment list and create a static RDD from it\n     word_sentiments_file_path = \"data/streaming/AFINN-111.txt\"\n-    word_sentiments = ssc.sparkContext.textFile(word_sentiments_file_path) \\\n-        .map(lambda line: tuple(line.split(\"\\t\")))\n+    word_sentiments = (ssc.sparkContext\n+                       .textFile(word_sentiments_file_path)\n+                       .map(lambda line: tuple(line.split(\"\\t\"))))\n \n     lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))\n \n-    word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n-        .map(lambda word: (word, 1)) \\\n-        .reduceByKey(lambda a, b: a + b)\n+    word_counts = (lines\n+                   .flatMap(lambda line: line.split(\" \"))\n+                   .map(lambda word: (word, 1))\n+                   .reduceByKey(lambda a, b: a + b))\n \n     # Determine the words with the highest sentiment values by joining the streaming RDD\n     # with the static RDD inside the transform() method and then multiplying\n     # the frequency of the words by its sentiment value\n-    happiest_words = word_counts.transform(lambda rdd: word_sentiments.join(rdd)) \\\n-        .map(lambda word_tuples: (word_tuples[0], float(word_tuples[1][0]) * word_tuples[1][1])) \\\n-        .map(lambda word_happiness: (word_happiness[1], word_happiness[0])) \\\n-        .transform(lambda rdd: rdd.sortByKey(False))\n+    happiest_words = (word_counts\n+                      .map(lambda word_tuples: (word_tuples[0],\n+                                                float(word_tuples[1][0]) * word_tuples[1][1]))",
    "line": 29
  }, {
    "author": {
      "login": "gsemet"
    },
    "body": "I agree, if you prefer I can change all at once. But like I said, I don't know any autoformat that does it automatically",
    "commit": "582c8221998d6fe055d066106e9ca43204f96bb9",
    "createdAt": "2017-02-15T12:11:01Z",
    "diffHunk": "@@ -54,22 +54,25 @@ def print_happiest_words(rdd):\n \n     # Read in the word-sentiment list and create a static RDD from it\n     word_sentiments_file_path = \"data/streaming/AFINN-111.txt\"\n-    word_sentiments = ssc.sparkContext.textFile(word_sentiments_file_path) \\\n-        .map(lambda line: tuple(line.split(\"\\t\")))\n+    word_sentiments = (ssc.sparkContext\n+                       .textFile(word_sentiments_file_path)\n+                       .map(lambda line: tuple(line.split(\"\\t\"))))\n \n     lines = ssc.socketTextStream(sys.argv[1], int(sys.argv[2]))\n \n-    word_counts = lines.flatMap(lambda line: line.split(\" \")) \\\n-        .map(lambda word: (word, 1)) \\\n-        .reduceByKey(lambda a, b: a + b)\n+    word_counts = (lines\n+                   .flatMap(lambda line: line.split(\" \"))\n+                   .map(lambda word: (word, 1))\n+                   .reduceByKey(lambda a, b: a + b))\n \n     # Determine the words with the highest sentiment values by joining the streaming RDD\n     # with the static RDD inside the transform() method and then multiplying\n     # the frequency of the words by its sentiment value\n-    happiest_words = word_counts.transform(lambda rdd: word_sentiments.join(rdd)) \\\n-        .map(lambda word_tuples: (word_tuples[0], float(word_tuples[1][0]) * word_tuples[1][1])) \\\n-        .map(lambda word_happiness: (word_happiness[1], word_happiness[0])) \\\n-        .transform(lambda rdd: rdd.sortByKey(False))\n+    happiest_words = (word_counts\n+                      .map(lambda word_tuples: (word_tuples[0],\n+                                                float(word_tuples[1][0]) * word_tuples[1][1]))",
    "line": 29
  }],
  "prId": 14830
}]