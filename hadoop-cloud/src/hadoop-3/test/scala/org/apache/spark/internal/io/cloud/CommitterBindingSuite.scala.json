[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "This isn't actually testing anything...",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-26T21:41:14Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.internal.io.{FileCommitProtocol, cloud}\n+import org.apache.spark.internal.io.cloud.PathCommitterConstants._\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter().asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  test(\"cloud binding to SparkConf\") {\n+    val sc = new SparkConf()\n+    cloud.bind(sc)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "cut",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-28T16:21:28Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.internal.io.{FileCommitProtocol, cloud}\n+import org.apache.spark.internal.io.cloud.PathCommitterConstants._\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter().asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  test(\"cloud binding to SparkConf\") {\n+    val sc = new SparkConf()\n+    cloud.bind(sc)"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "These annotations aren't actually that helpful. Even less so in test code.",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-26T21:41:49Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.internal.io.{FileCommitProtocol, cloud}\n+import org.apache.spark.internal.io.cloud.PathCommitterConstants._\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter().asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  test(\"cloud binding to SparkConf\") {\n+    val sc = new SparkConf()\n+    cloud.bind(sc)\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   * @throws IOException failure\n+   */\n+  @throws[IOException]"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "cut",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-28T16:22:29Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.internal.io.{FileCommitProtocol, cloud}\n+import org.apache.spark.internal.io.cloud.PathCommitterConstants._\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter().asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  test(\"cloud binding to SparkConf\") {\n+    val sc = new SparkConf()\n+    cloud.bind(sc)\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   * @throws IOException failure\n+   */\n+  @throws[IOException]"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "These comments aren't super helpful.",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-26T21:42:07Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.internal.io.{FileCommitProtocol, cloud}\n+import org.apache.spark.internal.io.cloud.PathCommitterConstants._\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter().asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  test(\"cloud binding to SparkConf\") {\n+    val sc = new SparkConf()\n+    cloud.bind(sc)\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   * @throws IOException failure"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "force of habit: Cut",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-28T16:23:41Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.internal.io.{FileCommitProtocol, cloud}\n+import org.apache.spark.internal.io.cloud.PathCommitterConstants._\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter().asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  test(\"cloud binding to SparkConf\") {\n+    val sc = new SparkConf()\n+    cloud.bind(sc)\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   * @throws IOException failure"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "no semi-colons.",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-26T21:43:04Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.internal.io.{FileCommitProtocol, cloud}\n+import org.apache.spark.internal.io.cloud.PathCommitterConstants._\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter().asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  test(\"cloud binding to SparkConf\") {\n+    val sc = new SparkConf()\n+    cloud.bind(sc)\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   * @throws IOException failure\n+   */\n+  @throws[IOException]\n+  def newJob(outDir: Path): Job = {\n+    val job = Job.getInstance(new Configuration())\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setBoolean(CREATE_SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)\n+    FileOutputFormat.setOutputPath(job, outDir)\n+    job\n+  }\n+\n+  /**\n+   * Verify that the committer protocol can be serialized and that\n+   * round trips work.\n+   */\n+  test(\"CommitterSerialization\") {\n+    val tempDir = File.createTempFile(\"ser\", \".bin\")\n+\n+    tempDir.delete();"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "again, habit, again, cut",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-28T16:26:49Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.internal.io.{FileCommitProtocol, cloud}\n+import org.apache.spark.internal.io.cloud.PathCommitterConstants._\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter().asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  test(\"cloud binding to SparkConf\") {\n+    val sc = new SparkConf()\n+    cloud.bind(sc)\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   * @throws IOException failure\n+   */\n+  @throws[IOException]\n+  def newJob(outDir: Path): Job = {\n+    val job = Job.getInstance(new Configuration())\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setBoolean(CREATE_SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)\n+    FileOutputFormat.setOutputPath(job, outDir)\n+    job\n+  }\n+\n+  /**\n+   * Verify that the committer protocol can be serialized and that\n+   * round trips work.\n+   */\n+  test(\"CommitterSerialization\") {\n+    val tempDir = File.createTempFile(\"ser\", \".bin\")\n+\n+    tempDir.delete();"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Space after `,`. ",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-26T21:44:32Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.internal.io.{FileCommitProtocol, cloud}\n+import org.apache.spark.internal.io.cloud.PathCommitterConstants._\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter().asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  test(\"cloud binding to SparkConf\") {\n+    val sc = new SparkConf()\n+    cloud.bind(sc)\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   * @throws IOException failure\n+   */\n+  @throws[IOException]\n+  def newJob(outDir: Path): Job = {\n+    val job = Job.getInstance(new Configuration())\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setBoolean(CREATE_SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)\n+    FileOutputFormat.setOutputPath(job, outDir)\n+    job\n+  }\n+\n+  /**\n+   * Verify that the committer protocol can be serialized and that\n+   * round trips work.\n+   */\n+  test(\"CommitterSerialization\") {\n+    val tempDir = File.createTempFile(\"ser\", \".bin\")\n+\n+    tempDir.delete();\n+    val committer = new PathOutputCommitProtocol(jobId, tempDir.toURI.toString,false)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "fixed",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-28T16:27:51Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.internal.io.{FileCommitProtocol, cloud}\n+import org.apache.spark.internal.io.cloud.PathCommitterConstants._\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter().asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  test(\"cloud binding to SparkConf\") {\n+    val sc = new SparkConf()\n+    cloud.bind(sc)\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   * @throws IOException failure\n+   */\n+  @throws[IOException]\n+  def newJob(outDir: Path): Job = {\n+    val job = Job.getInstance(new Configuration())\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setBoolean(CREATE_SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)\n+    FileOutputFormat.setOutputPath(job, outDir)\n+    job\n+  }\n+\n+  /**\n+   * Verify that the committer protocol can be serialized and that\n+   * round trips work.\n+   */\n+  test(\"CommitterSerialization\") {\n+    val tempDir = File.createTempFile(\"ser\", \".bin\")\n+\n+    tempDir.delete();\n+    val committer = new PathOutputCommitProtocol(jobId, tempDir.toURI.toString,false)"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "either use `()` everywhere or nowhere... (but one of my previous comments would change this anyway.)",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-26T21:45:15Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.internal.io.{FileCommitProtocol, cloud}\n+import org.apache.spark.internal.io.cloud.PathCommitterConstants._\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter().asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  test(\"cloud binding to SparkConf\") {\n+    val sc = new SparkConf()\n+    cloud.bind(sc)\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   * @throws IOException failure\n+   */\n+  @throws[IOException]\n+  def newJob(outDir: Path): Job = {\n+    val job = Job.getInstance(new Configuration())\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setBoolean(CREATE_SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)\n+    FileOutputFormat.setOutputPath(job, outDir)\n+    job\n+  }\n+\n+  /**\n+   * Verify that the committer protocol can be serialized and that\n+   * round trips work.\n+   */\n+  test(\"CommitterSerialization\") {\n+    val tempDir = File.createTempFile(\"ser\", \".bin\")\n+\n+    tempDir.delete();\n+    val committer = new PathOutputCommitProtocol(jobId, tempDir.toURI.toString,false)\n+\n+    val serData = File.createTempFile(\"ser\", \".bin\")\n+    var out: ObjectOutputStream = null\n+    var in: ObjectInputStream = null\n+\n+    try {\n+      out = new ObjectOutputStream(new FileOutputStream(serData))\n+      out.writeObject(committer)\n+      out.close\n+      in = new ObjectInputStream(new FileInputStream(serData))\n+      val result = in.readObject()\n+\n+      val committer2 = result.asInstanceOf[PathOutputCommitProtocol]\n+\n+      assert(committer.getDestination() === committer2.getDestination,"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "removed",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-28T16:27:46Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.internal.io.{FileCommitProtocol, cloud}\n+import org.apache.spark.internal.io.cloud.PathCommitterConstants._\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter().asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  test(\"cloud binding to SparkConf\") {\n+    val sc = new SparkConf()\n+    cloud.bind(sc)\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   * @throws IOException failure\n+   */\n+  @throws[IOException]\n+  def newJob(outDir: Path): Job = {\n+    val job = Job.getInstance(new Configuration())\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setBoolean(CREATE_SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)\n+    FileOutputFormat.setOutputPath(job, outDir)\n+    job\n+  }\n+\n+  /**\n+   * Verify that the committer protocol can be serialized and that\n+   * round trips work.\n+   */\n+  test(\"CommitterSerialization\") {\n+    val tempDir = File.createTempFile(\"ser\", \".bin\")\n+\n+    tempDir.delete();\n+    val committer = new PathOutputCommitProtocol(jobId, tempDir.toURI.toString,false)\n+\n+    val serData = File.createTempFile(\"ser\", \".bin\")\n+    var out: ObjectOutputStream = null\n+    var in: ObjectInputStream = null\n+\n+    try {\n+      out = new ObjectOutputStream(new FileOutputStream(serData))\n+      out.writeObject(committer)\n+      out.close\n+      in = new ObjectInputStream(new FileInputStream(serData))\n+      val result = in.readObject()\n+\n+      val committer2 = result.asInstanceOf[PathOutputCommitProtocol]\n+\n+      assert(committer.getDestination() === committer2.getDestination,"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Space before `===`.",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-06-26T21:45:58Z",
    "diffHunk": "@@ -0,0 +1,153 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+\n+import org.apache.spark.{SparkConf, SparkFunSuite}\n+import org.apache.spark.internal.io.{FileCommitProtocol, cloud}\n+import org.apache.spark.internal.io.cloud.PathCommitterConstants._\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter().asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  test(\"cloud binding to SparkConf\") {\n+    val sc = new SparkConf()\n+    cloud.bind(sc)\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   * @throws IOException failure\n+   */\n+  @throws[IOException]\n+  def newJob(outDir: Path): Job = {\n+    val job = Job.getInstance(new Configuration())\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setBoolean(CREATE_SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)\n+    FileOutputFormat.setOutputPath(job, outDir)\n+    job\n+  }\n+\n+  /**\n+   * Verify that the committer protocol can be serialized and that\n+   * round trips work.\n+   */\n+  test(\"CommitterSerialization\") {\n+    val tempDir = File.createTempFile(\"ser\", \".bin\")\n+\n+    tempDir.delete();\n+    val committer = new PathOutputCommitProtocol(jobId, tempDir.toURI.toString,false)\n+\n+    val serData = File.createTempFile(\"ser\", \".bin\")\n+    var out: ObjectOutputStream = null\n+    var in: ObjectInputStream = null\n+\n+    try {\n+      out = new ObjectOutputStream(new FileOutputStream(serData))\n+      out.writeObject(committer)\n+      out.close\n+      in = new ObjectInputStream(new FileInputStream(serData))\n+      val result = in.readObject()\n+\n+      val committer2 = result.asInstanceOf[PathOutputCommitProtocol]\n+\n+      assert(committer.getDestination() === committer2.getDestination,\n+        \"destination mismatch on round trip\")\n+      assert(committer.destPath === committer2.destPath,\n+        \"destPath mismatch on round trip\")\n+    } finally {\n+      IOUtils.closeStreams(out, in)\n+      serData.delete()\n+    }\n+  }\n+\n+  test(\"Instantiate\") {\n+    val instance = FileCommitProtocol.instantiate(\n+      cloud.PATH_COMMIT_PROTOCOL_CLASSNAME,\n+      jobId, \"file:///tmp\", false)\n+\n+    val protocol = instance.asInstanceOf[PathOutputCommitProtocol]\n+    assert(\"file:///tmp\"=== protocol.getDestination())"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "nit: double indent continuation of the `if` condition",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-07-31T16:49:23Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.internal.io.{FileCommitProtocol, cloud}\n+import org.apache.spark.internal.io.cloud.PathCommitterConstants._\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter.asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   */\n+  def newJob(outDir: Path): Job = {\n+    val job = Job.getInstance(new Configuration())\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setBoolean(CREATE_SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)\n+    FileOutputFormat.setOutputPath(job, outDir)\n+    job\n+  }\n+\n+  /**\n+   * Verify that the committer protocol can be serialized and that\n+   * round trips work.\n+   */\n+  test(\"CommitterSerialization\") {\n+    val tempDir = File.createTempFile(\"ser\", \".bin\")\n+\n+    tempDir.delete()\n+    val committer = new PathOutputCommitProtocol(jobId, tempDir.toURI.toString, false)\n+\n+    val serData = File.createTempFile(\"ser\", \".bin\")\n+    var out: ObjectOutputStream = null\n+    var in: ObjectInputStream = null\n+\n+    try {\n+      out = new ObjectOutputStream(new FileOutputStream(serData))\n+      out.writeObject(committer)\n+      out.close\n+      in = new ObjectInputStream(new FileInputStream(serData))\n+      val result = in.readObject()\n+\n+      val committer2 = result.asInstanceOf[PathOutputCommitProtocol]\n+\n+      assert(committer.destination === committer2.destination,\n+        \"destination mismatch on round trip\")\n+      assert(committer.destPath === committer2.destPath,\n+        \"destPath mismatch on round trip\")\n+    } finally {\n+      IOUtils.closeStreams(out, in)\n+      serData.delete()\n+    }\n+  }\n+\n+  test(\"Instantiate\") {\n+    val instance = FileCommitProtocol.instantiate(\n+      cloud.PATH_COMMIT_PROTOCOL_CLASSNAME,\n+      jobId, \"file:///tmp\", false)\n+\n+    val protocol = instance.asInstanceOf[PathOutputCommitProtocol]\n+    assert(\"file:///tmp\" === protocol.destination)\n+  }\n+\n+  test(\"InstantiateNoDynamicPartitioning\") {\n+    val ex = intercept[InvocationTargetException] {\n+      FileCommitProtocol.instantiate(\n+        cloud.PATH_COMMIT_PROTOCOL_CLASSNAME,\n+        jobId, \"file:///tmp\", true)\n+    }\n+    val cause = ex.getCause\n+    if (cause == null || !cause.isInstanceOf[IOException]\n+      || !cause.getMessage.contains(PathOutputCommitProtocol.UNSUPPORTED)) {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "added two spaces",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-08-01T11:07:20Z",
    "diffHunk": "@@ -0,0 +1,145 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.internal.io.{FileCommitProtocol, cloud}\n+import org.apache.spark.internal.io.cloud.PathCommitterConstants._\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter.asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   */\n+  def newJob(outDir: Path): Job = {\n+    val job = Job.getInstance(new Configuration())\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setBoolean(CREATE_SUCCESSFUL_JOB_OUTPUT_DIR_MARKER, true)\n+    FileOutputFormat.setOutputPath(job, outDir)\n+    job\n+  }\n+\n+  /**\n+   * Verify that the committer protocol can be serialized and that\n+   * round trips work.\n+   */\n+  test(\"CommitterSerialization\") {\n+    val tempDir = File.createTempFile(\"ser\", \".bin\")\n+\n+    tempDir.delete()\n+    val committer = new PathOutputCommitProtocol(jobId, tempDir.toURI.toString, false)\n+\n+    val serData = File.createTempFile(\"ser\", \".bin\")\n+    var out: ObjectOutputStream = null\n+    var in: ObjectInputStream = null\n+\n+    try {\n+      out = new ObjectOutputStream(new FileOutputStream(serData))\n+      out.writeObject(committer)\n+      out.close\n+      in = new ObjectInputStream(new FileInputStream(serData))\n+      val result = in.readObject()\n+\n+      val committer2 = result.asInstanceOf[PathOutputCommitProtocol]\n+\n+      assert(committer.destination === committer2.destination,\n+        \"destination mismatch on round trip\")\n+      assert(committer.destPath === committer2.destPath,\n+        \"destPath mismatch on round trip\")\n+    } finally {\n+      IOUtils.closeStreams(out, in)\n+      serData.delete()\n+    }\n+  }\n+\n+  test(\"Instantiate\") {\n+    val instance = FileCommitProtocol.instantiate(\n+      cloud.PATH_COMMIT_PROTOCOL_CLASSNAME,\n+      jobId, \"file:///tmp\", false)\n+\n+    val protocol = instance.asInstanceOf[PathOutputCommitProtocol]\n+    assert(\"file:///tmp\" === protocol.destination)\n+  }\n+\n+  test(\"InstantiateNoDynamicPartitioning\") {\n+    val ex = intercept[InvocationTargetException] {\n+      FileCommitProtocol.instantiate(\n+        cloud.PATH_COMMIT_PROTOCOL_CLASSNAME,\n+        jobId, \"file:///tmp\", true)\n+    }\n+    val cause = ex.getCause\n+    if (cause == null || !cause.isInstanceOf[IOException]\n+      || !cause.getMessage.contains(PathOutputCommitProtocol.UNSUPPORTED)) {"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "pretty obvious from the class name.",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-08-01T20:33:39Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.internal.io.{cloud, FileCommitProtocol}\n+\n+/**\n+ * Test committer binding logic."
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "cut",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-08-02T16:43:03Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.internal.io.{cloud, FileCommitProtocol}\n+\n+/**\n+ * Test committer binding logic."
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Maybe have a better test name instead of a comment explaining the test?",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-08-01T20:34:33Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.internal.io.{cloud, FileCommitProtocol}\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /** hadoop-mapreduce option to enable the _SUCCESS marker. */\n+  private val successMarker = \"mapreduce.fileoutputcommitter.marksuccessfuljobs\"\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter.asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   */\n+  def newJob(outDir: Path): Job = {\n+    val job = Job.getInstance(new Configuration())\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setBoolean(successMarker, true)\n+    FileOutputFormat.setOutputPath(job, outDir)\n+    job\n+  }\n+\n+  /**\n+   * Verify that the committer protocol can be serialized and that"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "moved the text down; removed the comment.",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-08-02T16:49:00Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.internal.io.{cloud, FileCommitProtocol}\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /** hadoop-mapreduce option to enable the _SUCCESS marker. */\n+  private val successMarker = \"mapreduce.fileoutputcommitter.marksuccessfuljobs\"\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter.asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   */\n+  def newJob(outDir: Path): Job = {\n+    val job = Job.getInstance(new Configuration())\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setBoolean(successMarker, true)\n+    FileOutputFormat.setOutputPath(job, outDir)\n+    job\n+  }\n+\n+  /**\n+   * Verify that the committer protocol can be serialized and that"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "We generally use proper sentences to describe tests.",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-08-01T20:35:00Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.internal.io.{cloud, FileCommitProtocol}\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /** hadoop-mapreduce option to enable the _SUCCESS marker. */\n+  private val successMarker = \"mapreduce.fileoutputcommitter.marksuccessfuljobs\"\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter.asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   */\n+  def newJob(outDir: Path): Job = {\n+    val job = Job.getInstance(new Configuration())\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setBoolean(successMarker, true)\n+    FileOutputFormat.setOutputPath(job, outDir)\n+    job\n+  }\n+\n+  /**\n+   * Verify that the committer protocol can be serialized and that\n+   * round trips work.\n+   */\n+  test(\"CommitterSerialization\") {\n+    val tempDir = File.createTempFile(\"ser\", \".bin\")\n+\n+    tempDir.delete()\n+    val committer = new PathOutputCommitProtocol(jobId, tempDir.toURI.toString, false)\n+\n+    val serData = File.createTempFile(\"ser\", \".bin\")\n+    var out: ObjectOutputStream = null\n+    var in: ObjectInputStream = null\n+\n+    try {\n+      out = new ObjectOutputStream(new FileOutputStream(serData))\n+      out.writeObject(committer)\n+      out.close\n+      in = new ObjectInputStream(new FileInputStream(serData))\n+      val result = in.readObject()\n+\n+      val committer2 = result.asInstanceOf[PathOutputCommitProtocol]\n+\n+      assert(committer.destination === committer2.destination,\n+        \"destination mismatch on round trip\")\n+      assert(committer.destPath === committer2.destPath,\n+        \"destPath mismatch on round trip\")\n+    } finally {\n+      IOUtils.closeStreams(out, in)\n+      serData.delete()\n+    }\n+  }\n+\n+  test(\"Instantiate\") {\n+    val instance = FileCommitProtocol.instantiate(\n+      cloud.PATH_COMMIT_PROTOCOL_CLASSNAME,\n+      jobId, \"file:///tmp\", false)\n+\n+    val protocol = instance.asInstanceOf[PathOutputCommitProtocol]\n+    assert(\"file:///tmp\" === protocol.destination)\n+  }\n+\n+  test(\"InstantiateNoDynamicPartitioning\") {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "changed. ",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-08-02T16:49:33Z",
    "diffHunk": "@@ -0,0 +1,147 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.internal.io.{cloud, FileCommitProtocol}\n+\n+/**\n+ * Test committer binding logic.\n+ */\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /** hadoop-mapreduce option to enable the _SUCCESS marker. */\n+  private val successMarker = \"mapreduce.fileoutputcommitter.marksuccessfuljobs\"\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path?\n+   */\n+  test(\"BindingParquetOutputCommitter lifecycle\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter.asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   */\n+  def newJob(outDir: Path): Job = {\n+    val job = Job.getInstance(new Configuration())\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setBoolean(successMarker, true)\n+    FileOutputFormat.setOutputPath(job, outDir)\n+    job\n+  }\n+\n+  /**\n+   * Verify that the committer protocol can be serialized and that\n+   * round trips work.\n+   */\n+  test(\"CommitterSerialization\") {\n+    val tempDir = File.createTempFile(\"ser\", \".bin\")\n+\n+    tempDir.delete()\n+    val committer = new PathOutputCommitProtocol(jobId, tempDir.toURI.toString, false)\n+\n+    val serData = File.createTempFile(\"ser\", \".bin\")\n+    var out: ObjectOutputStream = null\n+    var in: ObjectInputStream = null\n+\n+    try {\n+      out = new ObjectOutputStream(new FileOutputStream(serData))\n+      out.writeObject(committer)\n+      out.close\n+      in = new ObjectInputStream(new FileInputStream(serData))\n+      val result = in.readObject()\n+\n+      val committer2 = result.asInstanceOf[PathOutputCommitProtocol]\n+\n+      assert(committer.destination === committer2.destination,\n+        \"destination mismatch on round trip\")\n+      assert(committer.destPath === committer2.destPath,\n+        \"destPath mismatch on round trip\")\n+    } finally {\n+      IOUtils.closeStreams(out, in)\n+      serData.delete()\n+    }\n+  }\n+\n+  test(\"Instantiate\") {\n+    val instance = FileCommitProtocol.instantiate(\n+      cloud.PATH_COMMIT_PROTOCOL_CLASSNAME,\n+      jobId, \"file:///tmp\", false)\n+\n+    val protocol = instance.asInstanceOf[PathOutputCommitProtocol]\n+    assert(\"file:///tmp\" === protocol.destination)\n+  }\n+\n+  test(\"InstantiateNoDynamicPartitioning\") {"
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "squito"
    },
    "body": "minor: this will be pretty small, right?  wouldn't it be easier to write to a `ByteArrayOutputStream` rather than messing with files?\r\n\r\n```\r\nval bytes = new ByteArrayOutputStream()\r\nval out = new ObjectOutputStream(bytes)\r\n...\r\nval in = new ObjectInputStream(new ByteArrayInputSream(bytes.toByteArray()))\r\n...\r\n```",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-08-12T20:02:59Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.internal.io.FileCommitProtocol\n+\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * The classname to use when referring to the path output committer.\n+   */\n+  private val pathCommitProtocolClassname: String = classOf[PathOutputCommitProtocol].getName\n+\n+  /** hadoop-mapreduce option to enable the _SUCCESS marker. */\n+  private val successMarker = \"mapreduce.fileoutputcommitter.marksuccessfuljobs\"\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path? And that lifecycle events\n+   * are correctly propagated?\n+   */\n+  test(\"Verify the BindingParquetOutputCommitter binds to the inner committer\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter.asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   */\n+  def newJob(outDir: Path): Job = {\n+    val job = Job.getInstance(new Configuration())\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setBoolean(successMarker, true)\n+    FileOutputFormat.setOutputPath(job, outDir)\n+    job\n+  }\n+\n+  test(\" Verify that the committer protocol can be serialized and deserialized\") {\n+    val tempDir = File.createTempFile(\"ser\", \".bin\")\n+\n+    tempDir.delete()\n+    val committer = new PathOutputCommitProtocol(jobId, tempDir.toURI.toString, false)\n+\n+    val serData = File.createTempFile(\"ser\", \".bin\")\n+    var out: ObjectOutputStream = null\n+    var in: ObjectInputStream = null\n+\n+    try {\n+      out = new ObjectOutputStream(new FileOutputStream(serData))\n+      out.writeObject(committer)\n+      out.close\n+      in = new ObjectInputStream(new FileInputStream(serData))",
    "line": 109
  }],
  "prId": 24970
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Stray leading space",
    "commit": "ee247d9c473f665791da1bbc84e1f61b35ff8dc1",
    "createdAt": "2019-08-14T23:30:30Z",
    "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.{File, FileInputStream, FileOutputStream, IOException, ObjectInputStream, ObjectOutputStream}\n+import java.lang.reflect.InvocationTargetException\n+\n+import org.apache.hadoop.conf.Configuration\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.io.IOUtils\n+import org.apache.hadoop.mapreduce.{Job, JobStatus, MRJobConfig, TaskAttemptID}\n+import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat\n+import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl\n+\n+import org.apache.spark.SparkFunSuite\n+import org.apache.spark.internal.io.FileCommitProtocol\n+\n+class CommitterBindingSuite extends SparkFunSuite {\n+\n+  private val jobId = \"2007071202143_0101\"\n+  private val taskAttempt0 = \"attempt_\" + jobId + \"_m_000000_0\"\n+  private val taskAttemptId0 = TaskAttemptID.forName(taskAttempt0)\n+\n+  /**\n+   * The classname to use when referring to the path output committer.\n+   */\n+  private val pathCommitProtocolClassname: String = classOf[PathOutputCommitProtocol].getName\n+\n+  /** hadoop-mapreduce option to enable the _SUCCESS marker. */\n+  private val successMarker = \"mapreduce.fileoutputcommitter.marksuccessfuljobs\"\n+\n+  /**\n+   * Does the\n+   * [[BindingParquetOutputCommitter]] committer bind to the schema-specific\n+   * committer declared for the destination path? And that lifecycle events\n+   * are correctly propagated?\n+   */\n+  test(\"Verify the BindingParquetOutputCommitter binds to the inner committer\") {\n+    val path = new Path(\"http://example/data\")\n+    val job = newJob(path)\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setInt(MRJobConfig.APPLICATION_ATTEMPT_ID, 1)\n+\n+    StubPathOutputCommitterFactory.bind(conf, \"http\")\n+    val tContext = new TaskAttemptContextImpl(conf, taskAttemptId0)\n+    val parquet = new BindingParquetOutputCommitter(path, tContext)\n+    val inner = parquet.boundCommitter.asInstanceOf[StubPathOutputCommitter]\n+    parquet.setupJob(tContext)\n+    assert(inner.jobSetup, s\"$inner job not setup\")\n+    parquet.setupTask(tContext)\n+    assert(inner.taskSetup, s\"$inner task not setup\")\n+    assert(parquet.needsTaskCommit(tContext), \"needsTaskCommit false\")\n+    inner.needsTaskCommit = false\n+    assert(!parquet.needsTaskCommit(tContext), \"needsTaskCommit true\")\n+    parquet.commitTask(tContext)\n+    assert(inner.taskCommitted, s\"$inner task not committed\")\n+    parquet.abortTask(tContext)\n+    assert(inner.taskAborted, s\"$inner task not aborted\")\n+    parquet.commitJob(tContext)\n+    assert(inner.jobCommitted, s\"$inner job not committed\")\n+    parquet.abortJob(tContext, JobStatus.State.RUNNING)\n+    assert(inner.jobAborted, s\"$inner job not aborted\")\n+  }\n+\n+  /**\n+   * Create a a new job. Sets the task attempt ID.\n+   *\n+   * @return the new job\n+   */\n+  def newJob(outDir: Path): Job = {\n+    val job = Job.getInstance(new Configuration())\n+    val conf = job.getConfiguration\n+    conf.set(MRJobConfig.TASK_ATTEMPT_ID, taskAttempt0)\n+    conf.setBoolean(successMarker, true)\n+    FileOutputFormat.setOutputPath(job, outDir)\n+    job\n+  }\n+\n+  test(\" Verify that the committer protocol can be serialized and deserialized\") {",
    "line": 95
  }],
  "prId": 24970
}]