[{
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "I'm having a hard time parsing this comment when I get to the \"do not do this\" part... what is it trying to say?",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:24:55Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the",
    "line": 40
  }],
  "prId": 21066
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "nit: double indent parameter lists",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:25:33Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "done",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-05-07T15:55:24Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,"
  }],
  "prId": 21066
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`private`?",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:26:10Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "done",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-05-07T15:55:56Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "I should add that `Path` is serializable in Hadoop 3 and [HADOOP-13519](https://issues.apache.org/jira/browse/HADOOP-13519); I've added a test to round trip serialization and so validate this and prevent regressions, and a comment to show its intentional (and that it's not something to copy and paste into Hadoop-2.x compatible code).",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-05-07T17:06:23Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)"
  }],
  "prId": 21066
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`private`?",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:26:16Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _"
  }],
  "prId": 21066
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "double indent",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:26:52Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "done",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-05-07T15:57:28Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {"
  }],
  "prId": 21066
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "double indent",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:28:40Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "'again, done. I need to work out how to get IDEA to do this...",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-05-07T15:57:46Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,"
  }],
  "prId": 21066
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "`.map { d => ... }`",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:28:57Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)",
    "line": 143
  }],
  "prId": 21066
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Will this become noisy with large stages?",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:29:36Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)\n+    val file = new Path(parent, buildFilename(taskContext, ext))\n+    logInfo(s\"Creating task file $file for dir $dir and ext $ext\")"
  }],
  "prId": 21066
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "double indent... you get the idea.",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:29:53Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)\n+    val file = new Path(parent, buildFilename(taskContext, ext))\n+    logInfo(s\"Creating task file $file for dir $dir and ext $ext\")\n+    file.toString\n+  }\n+\n+  /**\n+   * Absolute files are still renamed into place with a warning.\n+   *\n+   * @param taskContext task\n+   * @param absoluteDir destination dir\n+   * @param ext         extension\n+   * @return an absolute path\n+   */\n+  override def newTaskTempFileAbsPath(\n+    taskContext: TaskAttemptContext,",
    "line": 169
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "yep",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-05-07T15:59:12Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)\n+    val file = new Path(parent, buildFilename(taskContext, ext))\n+    logInfo(s\"Creating task file $file for dir $dir and ext $ext\")\n+    file.toString\n+  }\n+\n+  /**\n+   * Absolute files are still renamed into place with a warning.\n+   *\n+   * @param taskContext task\n+   * @param absoluteDir destination dir\n+   * @param ext         extension\n+   * @return an absolute path\n+   */\n+  override def newTaskTempFileAbsPath(\n+    taskContext: TaskAttemptContext,",
    "line": 169
  }],
  "prId": 21066
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Maybe explain why the warning is desirable?",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:30:23Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)\n+    val file = new Path(parent, buildFilename(taskContext, ext))\n+    logInfo(s\"Creating task file $file for dir $dir and ext $ext\")\n+    file.toString\n+  }\n+\n+  /**\n+   * Absolute files are still renamed into place with a warning.",
    "line": 150
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "Added in the comments, not the message\r\n1. its inefficient on any store where the copy is `O(data)`\r\n1. If the store caches negative HEAD/GET responses briefly (i.e S3 load balancers), then if there's an attempt to copy a file almost immediately after completing the PUT for that file, then that rename may actually fail, \"file not found\". Which can only happen  here if the job is committed immediately after this task runs so the interval between `newTaskTempFileAbsPath()` and  `HadoopMapReduceCommitProtocol.commitJob()` is within the negative cache interval, *and* they all share the same load balancer. \r\n\r\nRisk #2 is those kind of problems which rarely surfaces in testing. It could be handled in `HadoopMapReduceCommitProtocol` with some retries around FileNotFoundException, if it was really found to be a problem. I've also promised the hotels.com circus-train team an option to create a file without doing that existence check...\r\n\r\n",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-05-07T16:25:44Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)\n+    val file = new Path(parent, buildFilename(taskContext, ext))\n+    logInfo(s\"Creating task file $file for dir $dir and ext $ext\")\n+    file.toString\n+  }\n+\n+  /**\n+   * Absolute files are still renamed into place with a warning.",
    "line": 150
  }],
  "prId": 21066
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Seems unnecessary.",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:31:44Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)\n+    val file = new Path(parent, buildFilename(taskContext, ext))\n+    logInfo(s\"Creating task file $file for dir $dir and ext $ext\")\n+    file.toString\n+  }\n+\n+  /**\n+   * Absolute files are still renamed into place with a warning.\n+   *\n+   * @param taskContext task\n+   * @param absoluteDir destination dir\n+   * @param ext         extension\n+   * @return an absolute path\n+   */\n+  override def newTaskTempFileAbsPath(\n+    taskContext: TaskAttemptContext,\n+    absoluteDir: String,\n+    ext: String): String = {\n+\n+    val file = super.newTaskTempFileAbsPath(taskContext, absoluteDir, ext)\n+    logWarning(\n+      s\"Creating temporary file $file for absolute path for dir $absoluteDir\")\n+    file\n+  }\n+\n+  /**\n+   * Build a filename which is unique across all task events.\n+   * It does not have to be consistent across multiple attempts of the same\n+   * task or job.\n+   *\n+   * @param taskContext task context\n+   * @param ext         extension\n+   * @return a name for a file which must be unique across all task attempts\n+   */\n+  protected def buildFilename(\n+    taskContext: TaskAttemptContext,\n+    ext: String): String = {\n+\n+    // The file name looks like part-00000-2dd664f9-d2c4-4ffe-878f-c6c70c1fb0cb_00003-c000.parquet\n+    // Note that %05d does not truncate the split number, so if we have more than 100000 tasks,\n+    // the file name is fine and won't overflow.\n+    val split = taskContext.getTaskAttemptID.getTaskID.getId\n+    f\"part-$split%05d-$jobId$ext\"\n+  }\n+\n+  override def setupJob(jobContext: JobContext): Unit = {",
    "line": 198
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "its just doing (now at a debug level) log of what's happening.",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-05-07T16:27:23Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)\n+    val file = new Path(parent, buildFilename(taskContext, ext))\n+    logInfo(s\"Creating task file $file for dir $dir and ext $ext\")\n+    file.toString\n+  }\n+\n+  /**\n+   * Absolute files are still renamed into place with a warning.\n+   *\n+   * @param taskContext task\n+   * @param absoluteDir destination dir\n+   * @param ext         extension\n+   * @return an absolute path\n+   */\n+  override def newTaskTempFileAbsPath(\n+    taskContext: TaskAttemptContext,\n+    absoluteDir: String,\n+    ext: String): String = {\n+\n+    val file = super.newTaskTempFileAbsPath(taskContext, absoluteDir, ext)\n+    logWarning(\n+      s\"Creating temporary file $file for absolute path for dir $absoluteDir\")\n+    file\n+  }\n+\n+  /**\n+   * Build a filename which is unique across all task events.\n+   * It does not have to be consistent across multiple attempts of the same\n+   * task or job.\n+   *\n+   * @param taskContext task context\n+   * @param ext         extension\n+   * @return a name for a file which must be unique across all task attempts\n+   */\n+  protected def buildFilename(\n+    taskContext: TaskAttemptContext,\n+    ext: String): String = {\n+\n+    // The file name looks like part-00000-2dd664f9-d2c4-4ffe-878f-c6c70c1fb0cb_00003-c000.parquet\n+    // Note that %05d does not truncate the split number, so if we have more than 100000 tasks,\n+    // the file name is fine and won't overflow.\n+    val split = taskContext.getTaskAttemptID.getTaskID.getId\n+    f\"part-$split%05d-$jobId$ext\"\n+  }\n+\n+  override def setupJob(jobContext: JobContext): Unit = {",
    "line": 198
  }],
  "prId": 21066
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Not sure why this is interesting?",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:32:17Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)\n+    val file = new Path(parent, buildFilename(taskContext, ext))\n+    logInfo(s\"Creating task file $file for dir $dir and ext $ext\")\n+    file.toString\n+  }\n+\n+  /**\n+   * Absolute files are still renamed into place with a warning.\n+   *\n+   * @param taskContext task\n+   * @param absoluteDir destination dir\n+   * @param ext         extension\n+   * @return an absolute path\n+   */\n+  override def newTaskTempFileAbsPath(\n+    taskContext: TaskAttemptContext,\n+    absoluteDir: String,\n+    ext: String): String = {\n+\n+    val file = super.newTaskTempFileAbsPath(taskContext, absoluteDir, ext)\n+    logWarning(\n+      s\"Creating temporary file $file for absolute path for dir $absoluteDir\")\n+    file\n+  }\n+\n+  /**\n+   * Build a filename which is unique across all task events.\n+   * It does not have to be consistent across multiple attempts of the same\n+   * task or job.\n+   *\n+   * @param taskContext task context\n+   * @param ext         extension\n+   * @return a name for a file which must be unique across all task attempts\n+   */\n+  protected def buildFilename(\n+    taskContext: TaskAttemptContext,\n+    ext: String): String = {\n+\n+    // The file name looks like part-00000-2dd664f9-d2c4-4ffe-878f-c6c70c1fb0cb_00003-c000.parquet\n+    // Note that %05d does not truncate the split number, so if we have more than 100000 tasks,\n+    // the file name is fine and won't overflow.\n+    val split = taskContext.getTaskAttemptID.getTaskID.getId\n+    f\"part-$split%05d-$jobId$ext\"\n+  }\n+\n+  override def setupJob(jobContext: JobContext): Unit = {\n+    logInfo(\"setup job\")\n+    super.setupJob(jobContext)\n+  }\n+\n+  override def commitJob(\n+    jobContext: JobContext,\n+    taskCommits: Seq[FileCommitProtocol.TaskCommitMessage]): Unit = {\n+    logInfo(s\"commit job with ${taskCommits.length} task commit message(s)\")"
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "now logs at debug purely for the interest of anyone debugging what's happening",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-05-07T16:26:23Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)\n+    val file = new Path(parent, buildFilename(taskContext, ext))\n+    logInfo(s\"Creating task file $file for dir $dir and ext $ext\")\n+    file.toString\n+  }\n+\n+  /**\n+   * Absolute files are still renamed into place with a warning.\n+   *\n+   * @param taskContext task\n+   * @param absoluteDir destination dir\n+   * @param ext         extension\n+   * @return an absolute path\n+   */\n+  override def newTaskTempFileAbsPath(\n+    taskContext: TaskAttemptContext,\n+    absoluteDir: String,\n+    ext: String): String = {\n+\n+    val file = super.newTaskTempFileAbsPath(taskContext, absoluteDir, ext)\n+    logWarning(\n+      s\"Creating temporary file $file for absolute path for dir $absoluteDir\")\n+    file\n+  }\n+\n+  /**\n+   * Build a filename which is unique across all task events.\n+   * It does not have to be consistent across multiple attempts of the same\n+   * task or job.\n+   *\n+   * @param taskContext task context\n+   * @param ext         extension\n+   * @return a name for a file which must be unique across all task attempts\n+   */\n+  protected def buildFilename(\n+    taskContext: TaskAttemptContext,\n+    ext: String): String = {\n+\n+    // The file name looks like part-00000-2dd664f9-d2c4-4ffe-878f-c6c70c1fb0cb_00003-c000.parquet\n+    // Note that %05d does not truncate the split number, so if we have more than 100000 tasks,\n+    // the file name is fine and won't overflow.\n+    val split = taskContext.getTaskAttemptID.getTaskID.getId\n+    f\"part-$split%05d-$jobId$ext\"\n+  }\n+\n+  override def setupJob(jobContext: JobContext): Unit = {\n+    logInfo(\"setup job\")\n+    super.setupJob(jobContext)\n+  }\n+\n+  override def commitJob(\n+    jobContext: JobContext,\n+    taskCommits: Seq[FileCommitProtocol.TaskCommitMessage]): Unit = {\n+    logInfo(s\"commit job with ${taskCommits.length} task commit message(s)\")"
  }],
  "prId": 21066
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same questions as before: why catch this exception in this committer? No other place where this happens seems to swallow exceptions.",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:33:03Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)\n+    val file = new Path(parent, buildFilename(taskContext, ext))\n+    logInfo(s\"Creating task file $file for dir $dir and ext $ext\")\n+    file.toString\n+  }\n+\n+  /**\n+   * Absolute files are still renamed into place with a warning.\n+   *\n+   * @param taskContext task\n+   * @param absoluteDir destination dir\n+   * @param ext         extension\n+   * @return an absolute path\n+   */\n+  override def newTaskTempFileAbsPath(\n+    taskContext: TaskAttemptContext,\n+    absoluteDir: String,\n+    ext: String): String = {\n+\n+    val file = super.newTaskTempFileAbsPath(taskContext, absoluteDir, ext)\n+    logWarning(\n+      s\"Creating temporary file $file for absolute path for dir $absoluteDir\")\n+    file\n+  }\n+\n+  /**\n+   * Build a filename which is unique across all task events.\n+   * It does not have to be consistent across multiple attempts of the same\n+   * task or job.\n+   *\n+   * @param taskContext task context\n+   * @param ext         extension\n+   * @return a name for a file which must be unique across all task attempts\n+   */\n+  protected def buildFilename(\n+    taskContext: TaskAttemptContext,\n+    ext: String): String = {\n+\n+    // The file name looks like part-00000-2dd664f9-d2c4-4ffe-878f-c6c70c1fb0cb_00003-c000.parquet\n+    // Note that %05d does not truncate the split number, so if we have more than 100000 tasks,\n+    // the file name is fine and won't overflow.\n+    val split = taskContext.getTaskAttemptID.getTaskID.getId\n+    f\"part-$split%05d-$jobId$ext\"\n+  }\n+\n+  override def setupJob(jobContext: JobContext): Unit = {\n+    logInfo(\"setup job\")\n+    super.setupJob(jobContext)\n+  }\n+\n+  override def commitJob(\n+    jobContext: JobContext,\n+    taskCommits: Seq[FileCommitProtocol.TaskCommitMessage]): Unit = {\n+    logInfo(s\"commit job with ${taskCommits.length} task commit message(s)\")\n+    super.commitJob(jobContext, taskCommits)\n+  }\n+\n+  /**\n+   * Abort the job; log and ignore any IO exception thrown.\n+   *\n+   * @param jobContext job context\n+   */\n+  override def abortJob(jobContext: JobContext): Unit = {\n+    try {\n+      super.abortJob(jobContext)\n+    } catch {\n+      case e: IOException =>",
    "line": 221
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "the \"how do you handle an abort failure\" problem. Example: `FileOutputWriter.write()`",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-05-07T16:30:15Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)\n+    val file = new Path(parent, buildFilename(taskContext, ext))\n+    logInfo(s\"Creating task file $file for dir $dir and ext $ext\")\n+    file.toString\n+  }\n+\n+  /**\n+   * Absolute files are still renamed into place with a warning.\n+   *\n+   * @param taskContext task\n+   * @param absoluteDir destination dir\n+   * @param ext         extension\n+   * @return an absolute path\n+   */\n+  override def newTaskTempFileAbsPath(\n+    taskContext: TaskAttemptContext,\n+    absoluteDir: String,\n+    ext: String): String = {\n+\n+    val file = super.newTaskTempFileAbsPath(taskContext, absoluteDir, ext)\n+    logWarning(\n+      s\"Creating temporary file $file for absolute path for dir $absoluteDir\")\n+    file\n+  }\n+\n+  /**\n+   * Build a filename which is unique across all task events.\n+   * It does not have to be consistent across multiple attempts of the same\n+   * task or job.\n+   *\n+   * @param taskContext task context\n+   * @param ext         extension\n+   * @return a name for a file which must be unique across all task attempts\n+   */\n+  protected def buildFilename(\n+    taskContext: TaskAttemptContext,\n+    ext: String): String = {\n+\n+    // The file name looks like part-00000-2dd664f9-d2c4-4ffe-878f-c6c70c1fb0cb_00003-c000.parquet\n+    // Note that %05d does not truncate the split number, so if we have more than 100000 tasks,\n+    // the file name is fine and won't overflow.\n+    val split = taskContext.getTaskAttemptID.getTaskID.getId\n+    f\"part-$split%05d-$jobId$ext\"\n+  }\n+\n+  override def setupJob(jobContext: JobContext): Unit = {\n+    logInfo(\"setup job\")\n+    super.setupJob(jobContext)\n+  }\n+\n+  override def commitJob(\n+    jobContext: JobContext,\n+    taskCommits: Seq[FileCommitProtocol.TaskCommitMessage]): Unit = {\n+    logInfo(s\"commit job with ${taskCommits.length} task commit message(s)\")\n+    super.commitJob(jobContext, taskCommits)\n+  }\n+\n+  /**\n+   * Abort the job; log and ignore any IO exception thrown.\n+   *\n+   * @param jobContext job context\n+   */\n+  override def abortJob(jobContext: JobContext): Unit = {\n+    try {\n+      super.abortJob(jobContext)\n+    } catch {\n+      case e: IOException =>",
    "line": 221
  }],
  "prId": 21066
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Seems unnecessary.",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:33:26Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)\n+    val file = new Path(parent, buildFilename(taskContext, ext))\n+    logInfo(s\"Creating task file $file for dir $dir and ext $ext\")\n+    file.toString\n+  }\n+\n+  /**\n+   * Absolute files are still renamed into place with a warning.\n+   *\n+   * @param taskContext task\n+   * @param absoluteDir destination dir\n+   * @param ext         extension\n+   * @return an absolute path\n+   */\n+  override def newTaskTempFileAbsPath(\n+    taskContext: TaskAttemptContext,\n+    absoluteDir: String,\n+    ext: String): String = {\n+\n+    val file = super.newTaskTempFileAbsPath(taskContext, absoluteDir, ext)\n+    logWarning(\n+      s\"Creating temporary file $file for absolute path for dir $absoluteDir\")\n+    file\n+  }\n+\n+  /**\n+   * Build a filename which is unique across all task events.\n+   * It does not have to be consistent across multiple attempts of the same\n+   * task or job.\n+   *\n+   * @param taskContext task context\n+   * @param ext         extension\n+   * @return a name for a file which must be unique across all task attempts\n+   */\n+  protected def buildFilename(\n+    taskContext: TaskAttemptContext,\n+    ext: String): String = {\n+\n+    // The file name looks like part-00000-2dd664f9-d2c4-4ffe-878f-c6c70c1fb0cb_00003-c000.parquet\n+    // Note that %05d does not truncate the split number, so if we have more than 100000 tasks,\n+    // the file name is fine and won't overflow.\n+    val split = taskContext.getTaskAttemptID.getTaskID.getId\n+    f\"part-$split%05d-$jobId$ext\"\n+  }\n+\n+  override def setupJob(jobContext: JobContext): Unit = {\n+    logInfo(\"setup job\")\n+    super.setupJob(jobContext)\n+  }\n+\n+  override def commitJob(\n+    jobContext: JobContext,\n+    taskCommits: Seq[FileCommitProtocol.TaskCommitMessage]): Unit = {\n+    logInfo(s\"commit job with ${taskCommits.length} task commit message(s)\")\n+    super.commitJob(jobContext, taskCommits)\n+  }\n+\n+  /**\n+   * Abort the job; log and ignore any IO exception thrown.\n+   *\n+   * @param jobContext job context\n+   */\n+  override def abortJob(jobContext: JobContext): Unit = {\n+    try {\n+      super.abortJob(jobContext)\n+    } catch {\n+      case e: IOException =>\n+        logWarning(\"Abort job failed\", e)\n+    }\n+  }\n+\n+  override def setupTask(taskContext: TaskAttemptContext): Unit = {\n+    super.setupTask(taskContext)\n+  }\n+\n+  override def commitTask(",
    "line": 230
  }],
  "prId": 21066
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Same question about swallowing exceptions.\r\n\r\n`logInfo` for task-related things seems like unnecessary noise.",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:33:52Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)\n+    val file = new Path(parent, buildFilename(taskContext, ext))\n+    logInfo(s\"Creating task file $file for dir $dir and ext $ext\")\n+    file.toString\n+  }\n+\n+  /**\n+   * Absolute files are still renamed into place with a warning.\n+   *\n+   * @param taskContext task\n+   * @param absoluteDir destination dir\n+   * @param ext         extension\n+   * @return an absolute path\n+   */\n+  override def newTaskTempFileAbsPath(\n+    taskContext: TaskAttemptContext,\n+    absoluteDir: String,\n+    ext: String): String = {\n+\n+    val file = super.newTaskTempFileAbsPath(taskContext, absoluteDir, ext)\n+    logWarning(\n+      s\"Creating temporary file $file for absolute path for dir $absoluteDir\")\n+    file\n+  }\n+\n+  /**\n+   * Build a filename which is unique across all task events.\n+   * It does not have to be consistent across multiple attempts of the same\n+   * task or job.\n+   *\n+   * @param taskContext task context\n+   * @param ext         extension\n+   * @return a name for a file which must be unique across all task attempts\n+   */\n+  protected def buildFilename(\n+    taskContext: TaskAttemptContext,\n+    ext: String): String = {\n+\n+    // The file name looks like part-00000-2dd664f9-d2c4-4ffe-878f-c6c70c1fb0cb_00003-c000.parquet\n+    // Note that %05d does not truncate the split number, so if we have more than 100000 tasks,\n+    // the file name is fine and won't overflow.\n+    val split = taskContext.getTaskAttemptID.getTaskID.getId\n+    f\"part-$split%05d-$jobId$ext\"\n+  }\n+\n+  override def setupJob(jobContext: JobContext): Unit = {\n+    logInfo(\"setup job\")\n+    super.setupJob(jobContext)\n+  }\n+\n+  override def commitJob(\n+    jobContext: JobContext,\n+    taskCommits: Seq[FileCommitProtocol.TaskCommitMessage]): Unit = {\n+    logInfo(s\"commit job with ${taskCommits.length} task commit message(s)\")\n+    super.commitJob(jobContext, taskCommits)\n+  }\n+\n+  /**\n+   * Abort the job; log and ignore any IO exception thrown.\n+   *\n+   * @param jobContext job context\n+   */\n+  override def abortJob(jobContext: JobContext): Unit = {\n+    try {\n+      super.abortJob(jobContext)\n+    } catch {\n+      case e: IOException =>\n+        logWarning(\"Abort job failed\", e)\n+    }\n+  }\n+\n+  override def setupTask(taskContext: TaskAttemptContext): Unit = {\n+    super.setupTask(taskContext)\n+  }\n+\n+  override def commitTask(\n+    taskContext: TaskAttemptContext): FileCommitProtocol.TaskCommitMessage = {\n+    logInfo(\"Commit task\")\n+    super.commitTask(taskContext)\n+  }\n+\n+  /**\n+   * Abort the task; log and ignore any failure thrown.\n+   *\n+   * @param taskContext context\n+   */\n+  override def abortTask(taskContext: TaskAttemptContext): Unit = {",
    "line": 242
  }, {
    "author": {
      "login": "steveloughran"
    },
    "body": "logging down to debug; reason for swallowing documented. ",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-05-07T16:56:42Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)\n+    val file = new Path(parent, buildFilename(taskContext, ext))\n+    logInfo(s\"Creating task file $file for dir $dir and ext $ext\")\n+    file.toString\n+  }\n+\n+  /**\n+   * Absolute files are still renamed into place with a warning.\n+   *\n+   * @param taskContext task\n+   * @param absoluteDir destination dir\n+   * @param ext         extension\n+   * @return an absolute path\n+   */\n+  override def newTaskTempFileAbsPath(\n+    taskContext: TaskAttemptContext,\n+    absoluteDir: String,\n+    ext: String): String = {\n+\n+    val file = super.newTaskTempFileAbsPath(taskContext, absoluteDir, ext)\n+    logWarning(\n+      s\"Creating temporary file $file for absolute path for dir $absoluteDir\")\n+    file\n+  }\n+\n+  /**\n+   * Build a filename which is unique across all task events.\n+   * It does not have to be consistent across multiple attempts of the same\n+   * task or job.\n+   *\n+   * @param taskContext task context\n+   * @param ext         extension\n+   * @return a name for a file which must be unique across all task attempts\n+   */\n+  protected def buildFilename(\n+    taskContext: TaskAttemptContext,\n+    ext: String): String = {\n+\n+    // The file name looks like part-00000-2dd664f9-d2c4-4ffe-878f-c6c70c1fb0cb_00003-c000.parquet\n+    // Note that %05d does not truncate the split number, so if we have more than 100000 tasks,\n+    // the file name is fine and won't overflow.\n+    val split = taskContext.getTaskAttemptID.getTaskID.getId\n+    f\"part-$split%05d-$jobId$ext\"\n+  }\n+\n+  override def setupJob(jobContext: JobContext): Unit = {\n+    logInfo(\"setup job\")\n+    super.setupJob(jobContext)\n+  }\n+\n+  override def commitJob(\n+    jobContext: JobContext,\n+    taskCommits: Seq[FileCommitProtocol.TaskCommitMessage]): Unit = {\n+    logInfo(s\"commit job with ${taskCommits.length} task commit message(s)\")\n+    super.commitJob(jobContext, taskCommits)\n+  }\n+\n+  /**\n+   * Abort the job; log and ignore any IO exception thrown.\n+   *\n+   * @param jobContext job context\n+   */\n+  override def abortJob(jobContext: JobContext): Unit = {\n+    try {\n+      super.abortJob(jobContext)\n+    } catch {\n+      case e: IOException =>\n+        logWarning(\"Abort job failed\", e)\n+    }\n+  }\n+\n+  override def setupTask(taskContext: TaskAttemptContext): Unit = {\n+    super.setupTask(taskContext)\n+  }\n+\n+  override def commitTask(\n+    taskContext: TaskAttemptContext): FileCommitProtocol.TaskCommitMessage = {\n+    logInfo(\"Commit task\")\n+    super.commitTask(taskContext)\n+  }\n+\n+  /**\n+   * Abort the task; log and ignore any failure thrown.\n+   *\n+   * @param taskContext context\n+   */\n+  override def abortTask(taskContext: TaskAttemptContext): Unit = {",
    "line": 242
  }],
  "prId": 21066
}, {
  "comments": [{
    "author": {
      "login": "vanzin"
    },
    "body": "Seems unnecessary.",
    "commit": "cb46f89aa6615d2b28edce6142c951300710c439",
    "createdAt": "2018-04-25T22:34:05Z",
    "diffHunk": "@@ -0,0 +1,260 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.internal.io.cloud\n+\n+import java.io.IOException\n+\n+import org.apache.hadoop.fs.Path\n+import org.apache.hadoop.mapreduce.{JobContext, TaskAttemptContext}\n+import org.apache.hadoop.mapreduce.lib.output.{FileOutputCommitter, PathOutputCommitter, PathOutputCommitterFactory}\n+\n+import org.apache.spark.internal.io.{FileCommitProtocol, HadoopMapReduceCommitProtocol}\n+import org.apache.spark.internal.io.FileCommitProtocol.TaskCommitMessage\n+\n+/**\n+ * Spark Commit protocol for Path Output Committers.\n+ * This committer will work with the `FileOutputCommitter` and subclasses.\n+ * All implementations *must* be serializable.\n+ *\n+ * Rather than ask the `FileOutputFormat` for a committer, it uses the\n+ * `org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory` factory\n+ * API to create the committer.\n+ * This is what [[org.apache.hadoop.mapreduce.lib.output.FileOutputFormat]] does,\n+ * but as [[HadoopMapReduceCommitProtocol]] still uses the original\n+ * `org.apache.hadoop.mapred.FileOutputFormat` binding\n+ * subclasses do not do this, overrides those subclasses to using the\n+ * factory mechanism now supported in the base class.\n+ *\n+ * In `setupCommitter` the factory is bonded to and the committer for\n+ * the destination path chosen.\n+ *\n+ * @constructor Instantiate. dynamic partition overwrite is not supported,\n+ *              so that committers for stores which do not support rename\n+ *              will not get confused.\n+ * @param jobId                     job\n+ * @param destination               destination\n+ * @param dynamicPartitionOverwrite does the caller want support for dynamic\n+ *                                  partition overwrite. If so, it will be\n+ *                                  refused.\n+ * @throws IOException when an unsupported dynamicPartitionOverwrite option is supplied.\n+ */\n+class PathOutputCommitProtocol(\n+  jobId: String,\n+  destination: String,\n+  dynamicPartitionOverwrite: Boolean = false)\n+  extends HadoopMapReduceCommitProtocol(\n+    jobId,\n+    destination,\n+    false) with Serializable {\n+\n+  @transient var committer: PathOutputCommitter = _\n+\n+  require(destination != null, \"Null destination specified\")\n+\n+  val destPath = new Path(destination)\n+\n+  logInfo(s\"Instantiated committer with job ID=$jobId;\" +\n+    s\" destination=$destPath;\" +\n+    s\" dynamicPartitionOverwrite=$dynamicPartitionOverwrite\")\n+\n+  if (dynamicPartitionOverwrite) {\n+    // until there's explicit extensions to the PathOutputCommitProtocols\n+    // to support the spark mechanism, it's left to the individual committer\n+    // choice to handle partitioning.\n+    throw new IOException(\"PathOutputCommitProtocol does not support dynamicPartitionOverwrite\")\n+  }\n+\n+  import PathOutputCommitProtocol._\n+\n+  /**\n+   * Set up the committer.\n+   * This creates it by talking directly to the Hadoop factories, instead\n+   * of the V1 `mapred.FileOutputFormat` methods.\n+   * @param context task attempt\n+   * @return the committer to use. This will always be a subclass of\n+   *         [[PathOutputCommitter]].\n+   */\n+  override protected def setupCommitter(\n+    context: TaskAttemptContext): PathOutputCommitter = {\n+\n+    logInfo(s\"Setting up committer for path $destination\")\n+    committer = PathOutputCommitterFactory.createCommitter(destPath, context)\n+\n+    // Special feature to force out the FileOutputCommitter, so as to guarantee\n+    // that the binding is working properly.\n+    val rejectFileOutput = context.getConfiguration\n+      .getBoolean(REJECT_FILE_OUTPUT, REJECT_FILE_OUTPUT_DEFVAL)\n+    if (rejectFileOutput && committer.isInstanceOf[FileOutputCommitter]) {\n+      // the output format returned a file output format committer, which\n+      // is exactly what we do not want. So switch back to the factory.\n+      val factory = PathOutputCommitterFactory.getCommitterFactory(\n+        destPath,\n+        context.getConfiguration)\n+      logInfo(s\"Using committer factory $factory\")\n+      committer = factory.createOutputCommitter(destPath, context)\n+    }\n+\n+    logInfo(s\"Using committer ${committer.getClass}\")\n+    logInfo(s\"Committer details: $committer\")\n+    if (committer.isInstanceOf[FileOutputCommitter]) {\n+      require(!rejectFileOutput,\n+        s\"Committer created is the FileOutputCommitter $committer\")\n+\n+      if (committer.isCommitJobRepeatable(context)) {\n+        // If FileOutputCommitter says its job commit is repeatable, it means\n+        // it is using the v2 algorithm, which is not safe for task commit\n+        // failures. Warn\n+        logWarning(s\"Committer $committer may not be tolerant of task commit failures\")\n+      }\n+    }\n+    committer\n+  }\n+\n+  /**\n+   * Create a temporary file for a task.\n+   *\n+   * @param taskContext task context\n+   * @param dir         optional subdirectory\n+   * @param ext         file extension\n+   * @return a path as a string\n+   */\n+  override def newTaskTempFile(\n+    taskContext: TaskAttemptContext,\n+    dir: Option[String],\n+    ext: String): String = {\n+\n+    val workDir = committer.getWorkPath\n+    val parent = dir.map(d => new Path(workDir, d)).getOrElse(workDir)\n+    val file = new Path(parent, buildFilename(taskContext, ext))\n+    logInfo(s\"Creating task file $file for dir $dir and ext $ext\")\n+    file.toString\n+  }\n+\n+  /**\n+   * Absolute files are still renamed into place with a warning.\n+   *\n+   * @param taskContext task\n+   * @param absoluteDir destination dir\n+   * @param ext         extension\n+   * @return an absolute path\n+   */\n+  override def newTaskTempFileAbsPath(\n+    taskContext: TaskAttemptContext,\n+    absoluteDir: String,\n+    ext: String): String = {\n+\n+    val file = super.newTaskTempFileAbsPath(taskContext, absoluteDir, ext)\n+    logWarning(\n+      s\"Creating temporary file $file for absolute path for dir $absoluteDir\")\n+    file\n+  }\n+\n+  /**\n+   * Build a filename which is unique across all task events.\n+   * It does not have to be consistent across multiple attempts of the same\n+   * task or job.\n+   *\n+   * @param taskContext task context\n+   * @param ext         extension\n+   * @return a name for a file which must be unique across all task attempts\n+   */\n+  protected def buildFilename(\n+    taskContext: TaskAttemptContext,\n+    ext: String): String = {\n+\n+    // The file name looks like part-00000-2dd664f9-d2c4-4ffe-878f-c6c70c1fb0cb_00003-c000.parquet\n+    // Note that %05d does not truncate the split number, so if we have more than 100000 tasks,\n+    // the file name is fine and won't overflow.\n+    val split = taskContext.getTaskAttemptID.getTaskID.getId\n+    f\"part-$split%05d-$jobId$ext\"\n+  }\n+\n+  override def setupJob(jobContext: JobContext): Unit = {\n+    logInfo(\"setup job\")\n+    super.setupJob(jobContext)\n+  }\n+\n+  override def commitJob(\n+    jobContext: JobContext,\n+    taskCommits: Seq[FileCommitProtocol.TaskCommitMessage]): Unit = {\n+    logInfo(s\"commit job with ${taskCommits.length} task commit message(s)\")\n+    super.commitJob(jobContext, taskCommits)\n+  }\n+\n+  /**\n+   * Abort the job; log and ignore any IO exception thrown.\n+   *\n+   * @param jobContext job context\n+   */\n+  override def abortJob(jobContext: JobContext): Unit = {\n+    try {\n+      super.abortJob(jobContext)\n+    } catch {\n+      case e: IOException =>\n+        logWarning(\"Abort job failed\", e)\n+    }\n+  }\n+\n+  override def setupTask(taskContext: TaskAttemptContext): Unit = {\n+    super.setupTask(taskContext)\n+  }\n+\n+  override def commitTask(\n+    taskContext: TaskAttemptContext): FileCommitProtocol.TaskCommitMessage = {\n+    logInfo(\"Commit task\")\n+    super.commitTask(taskContext)\n+  }\n+\n+  /**\n+   * Abort the task; log and ignore any failure thrown.\n+   *\n+   * @param taskContext context\n+   */\n+  override def abortTask(taskContext: TaskAttemptContext): Unit = {\n+    logInfo(\"Abort task\")\n+    try {\n+      super.abortTask(taskContext)\n+    } catch {\n+      case e: IOException =>\n+        logWarning(\"Abort task failed\", e)\n+    }\n+  }\n+\n+  override def onTaskCommit(msg: TaskCommitMessage): Unit = {",
    "line": 252
  }],
  "prId": 21066
}]