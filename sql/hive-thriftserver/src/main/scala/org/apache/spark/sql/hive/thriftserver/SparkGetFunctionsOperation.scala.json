[{
  "comments": [{
    "author": {
      "login": "wangyum"
    },
    "body": "Actually, I'm confused about this code. Maybe it should be:\r\n```scala\r\n\r\nval functionPattern = CLIServiceUtils.patternToRegex(functionName)\r\nmatchingDbs.foreach { schema =>\r\n  catalog.listFunctions(catalog.getCurrentDatabase, functionPattern).foreach {\r\n    case (functionIdentifier, _) =>\r\n      val rowData = Array[AnyRef](\r\n        DEFAULT_HIVE_CATALOG, // FUNCTION_CAT\r\n        schema, // FUNCTION_SCHEM\r\n        functionIdentifier.funcName, // FUNCTION_NAME\r\n        \"\", // REMARKS\r\n        DatabaseMetaData.functionResultUnknown.asInstanceOf[AnyRef], // FUNCTION_TYPE\r\n        \"\") // SPECIFIC_NAME\r\n      rowSet.addRow(rowData);\r\n}\r\n```\r\nBut it's the logic of Hive: https://github.com/apache/hive/blob/rel/release-3.1.1/service/src/java/org/apache/hive/service/cli/operation/GetFunctionsOperation.java#L101-L119",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-25T06:55:11Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+import org.apache.thrift.TException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetTablesOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName)\n+    with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+\n+    if (isAuthV2Enabled) {\n+      // get databases for schema pattern\n+      val schemaPattern = convertSchemaPattern(schemaName)\n+      var matchingDbs: Seq[String] = null\n+      try {\n+        matchingDbs = catalog.listDatabases(schemaPattern)\n+      } catch {\n+        case e: TException =>\n+          setState(OperationState.ERROR)\n+          HiveThriftServer2.listener.onStatementError(\n+            statementId, e.getMessage, SparkUtils.exceptionString(e))\n+          throw new HiveSQLException(e)\n+      }\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+      if ((null == catalogName || \"\".equals(catalogName))\n+        && (null == schemaName || \"\".equals(schemaName))) {\n+        catalog.listFunctions(catalog.getCurrentDatabase, functionPattern).foreach {"
  }, {
    "author": {
      "login": "bogdanghit"
    },
    "body": "I think we should cover the case of functions that don't have a schema (null) which is basically what Hive's implementation seems to do, as well as the functions associated with a given schema which is what your code snippet above seems to do. Could you combine both?\r\n\r\nhttps://docs.microsoft.com/en-us/sql/connect/jdbc/reference/getfunctions-method-sqlserverdatabasemetadata?view=sql-server-2017\r\n\r\n",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-29T07:46:36Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+import org.apache.thrift.TException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetTablesOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName)\n+    with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+\n+    if (isAuthV2Enabled) {\n+      // get databases for schema pattern\n+      val schemaPattern = convertSchemaPattern(schemaName)\n+      var matchingDbs: Seq[String] = null\n+      try {\n+        matchingDbs = catalog.listDatabases(schemaPattern)\n+      } catch {\n+        case e: TException =>\n+          setState(OperationState.ERROR)\n+          HiveThriftServer2.listener.onStatementError(\n+            statementId, e.getMessage, SparkUtils.exceptionString(e))\n+          throw new HiveSQLException(e)\n+      }\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+      if ((null == catalogName || \"\".equals(catalogName))\n+        && (null == schemaName || \"\".equals(schemaName))) {\n+        catalog.listFunctions(catalog.getCurrentDatabase, functionPattern).foreach {"
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "May be we do not need to care about `catalog`:\r\nhttps://github.com/pgjdbc/pgjdbc/blob/17c4bcfb59e846c593093752f2e30dd97bb4b338/pgjdbc/src/main/java/org/postgresql/jdbc/PgDatabaseMetaData.java#L2612-L2649",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-29T12:26:29Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+import org.apache.thrift.TException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetTablesOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName)\n+    with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+\n+    if (isAuthV2Enabled) {\n+      // get databases for schema pattern\n+      val schemaPattern = convertSchemaPattern(schemaName)\n+      var matchingDbs: Seq[String] = null\n+      try {\n+        matchingDbs = catalog.listDatabases(schemaPattern)\n+      } catch {\n+        case e: TException =>\n+          setState(OperationState.ERROR)\n+          HiveThriftServer2.listener.onStatementError(\n+            statementId, e.getMessage, SparkUtils.exceptionString(e))\n+          throw new HiveSQLException(e)\n+      }\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+      if ((null == catalogName || \"\".equals(catalogName))\n+        && (null == schemaName || \"\".equals(schemaName))) {\n+        catalog.listFunctions(catalog.getCurrentDatabase, functionPattern).foreach {"
  }],
  "prId": 25252
}, {
  "comments": [{
    "author": {
      "login": "wangyum"
    },
    "body": "We do not support `FUNCTION_TYPE` now. Set it to Unknown:\r\n```java\r\n    // java.sql.DatabaseMetaData\r\n\r\n    /**\r\n     * Indicates that it is not known whether the function returns\r\n     * a result or a table.\r\n     * <P>\r\n     * A possible value for column <code>FUNCTION_TYPE</code> in the\r\n     * <code>ResultSet</code> object returned by the method\r\n     * <code>getFunctions</code>.\r\n     * @since 1.6\r\n     */\r\n    int functionResultUnknown   = 0;\r\n\r\n    /**\r\n     * Indicates that the function  does not return a table.\r\n     * <P>\r\n     * A possible value for column <code>FUNCTION_TYPE</code> in the\r\n     * <code>ResultSet</code> object returned by the method\r\n     * <code>getFunctions</code>.\r\n     * @since 1.6\r\n     */\r\n    int functionNoTable         = 1;\r\n\r\n    /**\r\n     * Indicates that the function  returns a table.\r\n     * <P>\r\n     * A possible value for column <code>FUNCTION_TYPE</code> in the\r\n     * <code>ResultSet</code> object returned by the method\r\n     * <code>getFunctions</code>.\r\n     * @since 1.6\r\n     */\r\n    int functionReturnsTable    = 2;\r\n```",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-25T06:57:42Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+import org.apache.thrift.TException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetTablesOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName)\n+    with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+\n+    if (isAuthV2Enabled) {\n+      // get databases for schema pattern\n+      val schemaPattern = convertSchemaPattern(schemaName)\n+      var matchingDbs: Seq[String] = null\n+      try {\n+        matchingDbs = catalog.listDatabases(schemaPattern)\n+      } catch {\n+        case e: TException =>\n+          setState(OperationState.ERROR)\n+          HiveThriftServer2.listener.onStatementError(\n+            statementId, e.getMessage, SparkUtils.exceptionString(e))\n+          throw new HiveSQLException(e)\n+      }\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+      if ((null == catalogName || \"\".equals(catalogName))\n+        && (null == schemaName || \"\".equals(schemaName))) {\n+        catalog.listFunctions(catalog.getCurrentDatabase, functionPattern).foreach {\n+          case (functionIdentifier, _) =>\n+            val rowData = Array[AnyRef](\n+              null, // FUNCTION_CAT\n+              null, // FUNCTION_SCHEM\n+              functionIdentifier.funcName, // FUNCTION_NAME\n+              \"\", // REMARKS\n+              DatabaseMetaData.functionResultUnknown.asInstanceOf[AnyRef], // FUNCTION_TYPE",
    "line": 100
  }],
  "prId": 25252
}, {
  "comments": [{
    "author": {
      "login": "bogdanghit"
    },
    "body": "Nit: GetFunctionsOperation",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-26T12:21:13Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+import org.apache.thrift.TException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetTablesOperation"
  }],
  "prId": 25252
}, {
  "comments": [{
    "author": {
      "login": "bogdanghit"
    },
    "body": "Nit: remove extra space",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-26T12:21:40Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+import org.apache.thrift.TException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetTablesOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName)\n+    with Logging {"
  }],
  "prId": 25252
}, {
  "comments": [{
    "author": {
      "login": "bogdanghit"
    },
    "body": "Should we include the functionName pattern in the log message, as we are doing in SparkGetTablesOperation for instance?",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-26T12:23:23Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+import org.apache.thrift.TException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetTablesOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName)\n+    with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr'\""
  }],
  "prId": 25252
}, {
  "comments": [{
    "author": {
      "login": "bogdanghit"
    },
    "body": "Shouldn't we handle other exceptions too?",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-26T12:32:48Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+import org.apache.thrift.TException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetTablesOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName)\n+    with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+\n+    if (isAuthV2Enabled) {\n+      // get databases for schema pattern\n+      val schemaPattern = convertSchemaPattern(schemaName)\n+      var matchingDbs: Seq[String] = null\n+      try {\n+        matchingDbs = catalog.listDatabases(schemaPattern)\n+      } catch {\n+        case e: TException =>\n+          setState(OperationState.ERROR)\n+          HiveThriftServer2.listener.onStatementError(\n+            statementId, e.getMessage, SparkUtils.exceptionString(e))\n+          throw new HiveSQLException(e)\n+      }\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+      if ((null == catalogName || \"\".equals(catalogName))\n+        && (null == schemaName || \"\".equals(schemaName))) {\n+        catalog.listFunctions(catalog.getCurrentDatabase, functionPattern).foreach {\n+          case (functionIdentifier, _) =>\n+            val rowData = Array[AnyRef](\n+              null, // FUNCTION_CAT\n+              null, // FUNCTION_SCHEM\n+              functionIdentifier.funcName, // FUNCTION_NAME\n+              \"\", // REMARKS\n+              DatabaseMetaData.functionResultUnknown.asInstanceOf[AnyRef], // FUNCTION_TYPE\n+              \"\")\n+            rowSet.addRow(rowData);\n+        }\n+      }\n+      setState(OperationState.FINISHED)\n+    } catch {\n+      case e: HiveSQLException =>\n+        setState(OperationState.ERROR)\n+        HiveThriftServer2.listener.onStatementError(\n+          statementId, e.getMessage, SparkUtils.exceptionString(e))\n+        throw e\n+    }",
    "line": 112
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "Maybe it should be the same as `SparkExecuteStatementOperation`:\r\nhttps://github.com/apache/spark/blob/687dd4eb55739f802692b3c5457618fd6558e538/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala#L261-L269",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-29T12:16:38Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+import org.apache.thrift.TException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetTablesOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName)\n+    with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+\n+    if (isAuthV2Enabled) {\n+      // get databases for schema pattern\n+      val schemaPattern = convertSchemaPattern(schemaName)\n+      var matchingDbs: Seq[String] = null\n+      try {\n+        matchingDbs = catalog.listDatabases(schemaPattern)\n+      } catch {\n+        case e: TException =>\n+          setState(OperationState.ERROR)\n+          HiveThriftServer2.listener.onStatementError(\n+            statementId, e.getMessage, SparkUtils.exceptionString(e))\n+          throw new HiveSQLException(e)\n+      }\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+      if ((null == catalogName || \"\".equals(catalogName))\n+        && (null == schemaName || \"\".equals(schemaName))) {\n+        catalog.listFunctions(catalog.getCurrentDatabase, functionPattern).foreach {\n+          case (functionIdentifier, _) =>\n+            val rowData = Array[AnyRef](\n+              null, // FUNCTION_CAT\n+              null, // FUNCTION_SCHEM\n+              functionIdentifier.funcName, // FUNCTION_NAME\n+              \"\", // REMARKS\n+              DatabaseMetaData.functionResultUnknown.asInstanceOf[AnyRef], // FUNCTION_TYPE\n+              \"\")\n+            rowSet.addRow(rowData);\n+        }\n+      }\n+      setState(OperationState.FINISHED)\n+    } catch {\n+      case e: HiveSQLException =>\n+        setState(OperationState.ERROR)\n+        HiveThriftServer2.listener.onStatementError(\n+          statementId, e.getMessage, SparkUtils.exceptionString(e))\n+        throw e\n+    }",
    "line": 112
  }, {
    "author": {
      "login": "bogdanghit"
    },
    "body": "Or the same as GetTablesOperation and GetSchemasOperation for the sake of consistency?",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-29T15:09:34Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+import org.apache.thrift.TException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetTablesOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName)\n+    with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+\n+    if (isAuthV2Enabled) {\n+      // get databases for schema pattern\n+      val schemaPattern = convertSchemaPattern(schemaName)\n+      var matchingDbs: Seq[String] = null\n+      try {\n+        matchingDbs = catalog.listDatabases(schemaPattern)\n+      } catch {\n+        case e: TException =>\n+          setState(OperationState.ERROR)\n+          HiveThriftServer2.listener.onStatementError(\n+            statementId, e.getMessage, SparkUtils.exceptionString(e))\n+          throw new HiveSQLException(e)\n+      }\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+      if ((null == catalogName || \"\".equals(catalogName))\n+        && (null == schemaName || \"\".equals(schemaName))) {\n+        catalog.listFunctions(catalog.getCurrentDatabase, functionPattern).foreach {\n+          case (functionIdentifier, _) =>\n+            val rowData = Array[AnyRef](\n+              null, // FUNCTION_CAT\n+              null, // FUNCTION_SCHEM\n+              functionIdentifier.funcName, // FUNCTION_NAME\n+              \"\", // REMARKS\n+              DatabaseMetaData.functionResultUnknown.asInstanceOf[AnyRef], // FUNCTION_TYPE\n+              \"\")\n+            rowSet.addRow(rowData);\n+        }\n+      }\n+      setState(OperationState.FINISHED)\n+    } catch {\n+      case e: HiveSQLException =>\n+        setState(OperationState.ERROR)\n+        HiveThriftServer2.listener.onStatementError(\n+          statementId, e.getMessage, SparkUtils.exceptionString(e))\n+        throw e\n+    }",
    "line": 112
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "+1 for same as `GetTablesOperation` and `GetSchemasOperation`.",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-30T01:57:01Z",
    "diffHunk": "@@ -0,0 +1,118 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+import org.apache.thrift.TException\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetTablesOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName)\n+    with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+\n+    if (isAuthV2Enabled) {\n+      // get databases for schema pattern\n+      val schemaPattern = convertSchemaPattern(schemaName)\n+      var matchingDbs: Seq[String] = null\n+      try {\n+        matchingDbs = catalog.listDatabases(schemaPattern)\n+      } catch {\n+        case e: TException =>\n+          setState(OperationState.ERROR)\n+          HiveThriftServer2.listener.onStatementError(\n+            statementId, e.getMessage, SparkUtils.exceptionString(e))\n+          throw new HiveSQLException(e)\n+      }\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+      if ((null == catalogName || \"\".equals(catalogName))\n+        && (null == schemaName || \"\".equals(schemaName))) {\n+        catalog.listFunctions(catalog.getCurrentDatabase, functionPattern).foreach {\n+          case (functionIdentifier, _) =>\n+            val rowData = Array[AnyRef](\n+              null, // FUNCTION_CAT\n+              null, // FUNCTION_SCHEM\n+              functionIdentifier.funcName, // FUNCTION_NAME\n+              \"\", // REMARKS\n+              DatabaseMetaData.functionResultUnknown.asInstanceOf[AnyRef], // FUNCTION_TYPE\n+              \"\")\n+            rowSet.addRow(rowData);\n+        }\n+      }\n+      setState(OperationState.FINISHED)\n+    } catch {\n+      case e: HiveSQLException =>\n+        setState(OperationState.ERROR)\n+        HiveThriftServer2.listener.onStatementError(\n+          statementId, e.getMessage, SparkUtils.exceptionString(e))\n+        throw e\n+    }",
    "line": 112
  }],
  "prId": 25252
}, {
  "comments": [{
    "author": {
      "login": "bogdanghit"
    },
    "body": "This looks correct now: retrieve all functions available for all matching schemas.",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-29T15:10:36Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetFunctionsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName) with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr, functionName : $functionName'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+    // get databases for schema pattern\n+    val schemaPattern = convertSchemaPattern(schemaName)\n+    val matchingDbs = catalog.listDatabases(schemaPattern)\n+    val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+\n+    if (isAuthV2Enabled) {\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {",
    "line": 90
  }],
  "prId": 25252
}, {
  "comments": [{
    "author": {
      "login": "bogdanghit"
    },
    "body": "This is the function class name which I think we can get through a ``catalog.getFunction(funcIdentifier).className`` call. ",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-30T08:51:34Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetFunctionsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName) with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr, functionName : $functionName'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+    // get databases for schema pattern\n+    val schemaPattern = convertSchemaPattern(schemaName)\n+    val matchingDbs = catalog.listDatabases(schemaPattern)\n+    val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+\n+    if (isAuthV2Enabled) {\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      matchingDbs.foreach { db =>\n+        catalog.listFunctions(db, functionPattern).foreach {\n+          case (functionIdentifier, _) =>\n+            val rowData = Array[AnyRef](\n+              null, // FUNCTION_CAT\n+              db, // FUNCTION_SCHEM\n+              functionIdentifier.funcName, // FUNCTION_NAME\n+              \"\", // REMARKS\n+              DatabaseMetaData.functionResultUnknown.asInstanceOf[AnyRef], // FUNCTION_TYPE\n+              \"\")"
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "Done.\r\n```scala\r\ncatalog.lookupFunctionInfo(funcIdentifier).getClassName\r\n```",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-31T07:21:58Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetFunctionsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName) with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr, functionName : $functionName'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+    // get databases for schema pattern\n+    val schemaPattern = convertSchemaPattern(schemaName)\n+    val matchingDbs = catalog.listDatabases(schemaPattern)\n+    val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+\n+    if (isAuthV2Enabled) {\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      matchingDbs.foreach { db =>\n+        catalog.listFunctions(db, functionPattern).foreach {\n+          case (functionIdentifier, _) =>\n+            val rowData = Array[AnyRef](\n+              null, // FUNCTION_CAT\n+              db, // FUNCTION_SCHEM\n+              functionIdentifier.funcName, // FUNCTION_NAME\n+              \"\", // REMARKS\n+              DatabaseMetaData.functionResultUnknown.asInstanceOf[AnyRef], // FUNCTION_TYPE\n+              \"\")"
  }],
  "prId": 25252
}, {
  "comments": [{
    "author": {
      "login": "bogdanghit"
    },
    "body": "I wonder if we can get the function usage somehow from catalog ...",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-30T08:52:13Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetFunctionsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName) with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr, functionName : $functionName'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+    // get databases for schema pattern\n+    val schemaPattern = convertSchemaPattern(schemaName)\n+    val matchingDbs = catalog.listDatabases(schemaPattern)\n+    val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+\n+    if (isAuthV2Enabled) {\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      matchingDbs.foreach { db =>\n+        catalog.listFunctions(db, functionPattern).foreach {\n+          case (functionIdentifier, _) =>\n+            val rowData = Array[AnyRef](\n+              null, // FUNCTION_CAT\n+              db, // FUNCTION_SCHEM\n+              functionIdentifier.funcName, // FUNCTION_NAME\n+              \"\", // REMARKS"
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "Done.\r\n```scala\r\ncatalog.lookupFunctionInfo(funcIdentifier).getUsage\r\n```",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-31T07:22:26Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetFunctionsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName) with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr, functionName : $functionName'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+    // get databases for schema pattern\n+    val schemaPattern = convertSchemaPattern(schemaName)\n+    val matchingDbs = catalog.listDatabases(schemaPattern)\n+    val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+\n+    if (isAuthV2Enabled) {\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      matchingDbs.foreach { db =>\n+        catalog.listFunctions(db, functionPattern).foreach {\n+          case (functionIdentifier, _) =>\n+            val rowData = Array[AnyRef](\n+              null, // FUNCTION_CAT\n+              db, // FUNCTION_SCHEM\n+              functionIdentifier.funcName, // FUNCTION_NAME\n+              \"\", // REMARKS"
  }],
  "prId": 25252
}, {
  "comments": [{
    "author": {
      "login": "bogdanghit"
    },
    "body": "There's probably no harm in putting the DEFAULT_CATALOG here.",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-30T08:52:33Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetFunctionsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName) with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr, functionName : $functionName'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+    // get databases for schema pattern\n+    val schemaPattern = convertSchemaPattern(schemaName)\n+    val matchingDbs = catalog.listDatabases(schemaPattern)\n+    val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+\n+    if (isAuthV2Enabled) {\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      matchingDbs.foreach { db =>\n+        catalog.listFunctions(db, functionPattern).foreach {\n+          case (functionIdentifier, _) =>\n+            val rowData = Array[AnyRef](\n+              null, // FUNCTION_CAT"
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "Yes. Replaced it to `org.apache.hive.service.cli.operation.MetadataOperation.DEFAULT_HIVE_CATALOG`",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-31T07:23:05Z",
    "diffHunk": "@@ -0,0 +1,106 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetFunctionsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName) with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr, functionName : $functionName'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+    // get databases for schema pattern\n+    val schemaPattern = convertSchemaPattern(schemaName)\n+    val matchingDbs = catalog.listDatabases(schemaPattern)\n+    val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+\n+    if (isAuthV2Enabled) {\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      matchingDbs.foreach { db =>\n+        catalog.listFunctions(db, functionPattern).foreach {\n+          case (functionIdentifier, _) =>\n+            val rowData = Array[AnyRef](\n+              null, // FUNCTION_CAT"
  }],
  "prId": 25252
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "We also need the `onStatementClosed` handler like in the other ops.",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-07-31T14:36:11Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.operation.MetadataOperation.DEFAULT_HIVE_CATALOG\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetFunctionsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName) with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr, functionName : $functionName'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+    // get databases for schema pattern\n+    val schemaPattern = convertSchemaPattern(schemaName)\n+    val matchingDbs = catalog.listDatabases(schemaPattern)\n+    val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+\n+    if (isAuthV2Enabled) {\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      matchingDbs.foreach { db =>\n+        catalog.listFunctions(db, functionPattern).foreach {\n+          case (funcIdentifier, _) =>\n+            val info = catalog.lookupFunctionInfo(funcIdentifier)\n+            val rowData = Array[AnyRef](\n+              DEFAULT_HIVE_CATALOG, // FUNCTION_CAT\n+              db, // FUNCTION_SCHEM\n+              funcIdentifier.funcName, // FUNCTION_NAME\n+              info.getUsage, // REMARKS\n+              DatabaseMetaData.functionResultUnknown.asInstanceOf[AnyRef], // FUNCTION_TYPE\n+              info.getClassName) // SPECIFIC_NAME\n+            rowSet.addRow(rowData);\n+        }\n+      }\n+      setState(OperationState.FINISHED)\n+    } catch {\n+      case e: HiveSQLException =>\n+        setState(OperationState.ERROR)\n+        HiveThriftServer2.listener.onStatementError(\n+          statementId, e.getMessage, SparkUtils.exceptionString(e))\n+        throw e\n+    }\n+    HiveThriftServer2.listener.onStatementFinish(statementId)",
    "line": 113
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "Done",
    "commit": "c95db44f741b2c524090f7258adcd5414c8fa4af",
    "createdAt": "2019-08-01T09:24:10Z",
    "diffHunk": "@@ -0,0 +1,108 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.sql.DatabaseMetaData\n+import java.util.UUID\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObjectUtils}\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetFunctionsOperation\n+import org.apache.hive.service.cli.operation.MetadataOperation.DEFAULT_HIVE_CATALOG\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.util.{Utils => SparkUtils}\n+\n+/**\n+ * Spark's own GetFunctionsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable\n+ * @param schemaName database name, null or a concrete database name\n+ * @param functionName function name pattern\n+ */\n+private[hive] class SparkGetFunctionsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    functionName: String)\n+  extends GetFunctionsOperation(parentSession, catalogName, schemaName, functionName) with Logging {\n+\n+  override def runInternal(): Unit = {\n+    val statementId = UUID.randomUUID().toString\n+    // Do not change cmdStr. It's used for Hive auditing and authorization.\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+    val logMsg = s\"Listing functions '$cmdStr, functionName : $functionName'\"\n+    logInfo(s\"$logMsg with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val catalog = sqlContext.sessionState.catalog\n+    // get databases for schema pattern\n+    val schemaPattern = convertSchemaPattern(schemaName)\n+    val matchingDbs = catalog.listDatabases(schemaPattern)\n+    val functionPattern = CLIServiceUtils.patternToRegex(functionName)\n+\n+    if (isAuthV2Enabled) {\n+      // authorize this call on the schema objects\n+      val privObjs =\n+        HivePrivilegeObjectUtils.getHivePrivDbObjects(seqAsJavaListConverter(matchingDbs).asJava)\n+      authorizeMetaGets(HiveOperationType.GET_FUNCTIONS, privObjs, cmdStr)\n+    }\n+\n+    HiveThriftServer2.listener.onStatementStart(\n+      statementId,\n+      parentSession.getSessionHandle.getSessionId.toString,\n+      logMsg,\n+      statementId,\n+      parentSession.getUsername)\n+\n+    try {\n+      matchingDbs.foreach { db =>\n+        catalog.listFunctions(db, functionPattern).foreach {\n+          case (funcIdentifier, _) =>\n+            val info = catalog.lookupFunctionInfo(funcIdentifier)\n+            val rowData = Array[AnyRef](\n+              DEFAULT_HIVE_CATALOG, // FUNCTION_CAT\n+              db, // FUNCTION_SCHEM\n+              funcIdentifier.funcName, // FUNCTION_NAME\n+              info.getUsage, // REMARKS\n+              DatabaseMetaData.functionResultUnknown.asInstanceOf[AnyRef], // FUNCTION_TYPE\n+              info.getClassName) // SPECIFIC_NAME\n+            rowSet.addRow(rowData);\n+        }\n+      }\n+      setState(OperationState.FINISHED)\n+    } catch {\n+      case e: HiveSQLException =>\n+        setState(OperationState.ERROR)\n+        HiveThriftServer2.listener.onStatementError(\n+          statementId, e.getMessage, SparkUtils.exceptionString(e))\n+        throw e\n+    }\n+    HiveThriftServer2.listener.onStatementFinish(statementId)",
    "line": 113
  }],
  "prId": 25252
}]