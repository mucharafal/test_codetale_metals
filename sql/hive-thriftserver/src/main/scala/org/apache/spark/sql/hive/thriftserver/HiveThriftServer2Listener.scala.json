[{
  "comments": [{
    "author": {
      "login": "shahidki31"
    },
    "body": "New events added for Thriftserver applications. Eventually these events will be catch by the listeners `EventLoggingListner` as well as HiveThriftServer2Listener`",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-04T17:14:13Z",
    "diffHunk": "@@ -0,0 +1,353 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd, SparkListenerEvent, SparkListenerJobStart}\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlContext: Option[SQLContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlContext.get.conf\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  private val sc: Option[SparkContext] = if (live) {\n+    Some(sqlContext.get.sparkContext)\n+  } else {\n+    None\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.get.listenerBus.post(event)\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live && server.isDefined) {\n+      server.get.stop()\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    if (jobStart.properties != null) {\n+      val groupId = jobStart.properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        executionList.values().asScala.filter(_.groupId == groupId).foreach(\n+          exec => {\n+            exec.jobId += jobStart.jobId.toString\n+            exec.groupId = groupId\n+            updateLiveStore(exec)\n+          }\n+        )\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerSessionCreated => onSessionCreated(e)\n+      case e: SparkListenerSessionClosed => onSessionClosed(e)\n+      case e: SparkListenerStatementStart => onStatementStart(e)\n+      case e: SparkListenerStatementParsed => onStatementParsed(e)\n+      case e: SparkListenerStatementCanceled => onStatementCanceled(e)\n+      case e: SparkListenerStatementError => onStatementError(e)\n+      case e: SparkListenerStatementFinish => onStatementFinish(e)\n+      case e: SparkListenerOperationClosed => onOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  def onSessionCreated(e: SparkListenerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session)\n+  }\n+\n+  def onSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session)\n+    if (live) {\n+      sessionList.remove(e.sessionId)\n+    }\n+  }\n+\n+  def onStatementStart( e: SparkListenerStatementStart): Unit = {\n+    val info =\n+      getOrCreateExecution(\n+        e.id,\n+        e.statement,\n+        e.sessionId,\n+        e.startTime,\n+        e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId))\n+  }\n+\n+  def onStatementParsed(e: SparkListenerStatementParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  def onStatementCanceled(e: SparkListenerStatementCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  def onStatementError(e: SparkListenerStatementError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  def onStatementFinish(e: SparkListenerStatementFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  def onOperationClosed(e: SparkListenerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id))\n+    if (live) {\n+      executionList.remove(e.id)\n+    }\n+  }\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }\n+\n+  def updateLiveStore(session: LiveEntity): Unit = {\n+    if (live) {\n+      session.write(kvstore, System.nanoTime())\n+    }\n+  }\n+\n+  private def getOrCreateSession(\n+     sessionId: String,\n+     startTime: Long,\n+     ip: String,\n+     username: String): LiveSessionData = {\n+    sessionList.computeIfAbsent(sessionId,\n+      (_: String) => new LiveSessionData(sessionId, startTime, ip, username))\n+  }\n+\n+  private def getOrCreateExecution(\n+    execId: String, statement: String,\n+    sessionId: String, startTimestamp: Long,\n+    userName: String): LiveExecutionData = {\n+    executionList.computeIfAbsent(execId,\n+      (_: String) => new LiveExecutionData(execId, statement, sessionId, startTimestamp, userName))\n+  }\n+\n+  private def cleanupExecutions(count: Long): Unit = {\n+    val countToDelete = calculateNumberToRemove(count, retainedStatements)\n+    if (countToDelete <= 0L) {\n+      return\n+    }\n+    val view = kvstore.view(classOf[ExecutionInfo]).index(\"execId\").first(0L)\n+    val toDelete = KVUtils.viewToSeq(view, countToDelete.toInt) { j =>\n+      j.finishTimestamp != 0\n+    }\n+    toDelete.foreach { j => kvstore.delete(j.getClass(), j.execId) }\n+  }\n+\n+  private def cleanupSession(count: Long): Unit = {\n+    val countToDelete = calculateNumberToRemove(count, retainedSessions)\n+    if (countToDelete <= 0L) {\n+      return\n+    }\n+    val view = kvstore.view(classOf[SessionInfo]).index(\"sessionId\").first(0L)\n+    val toDelete = KVUtils.viewToSeq(view, countToDelete.toInt) { j =>\n+      j.finishTimestamp != 0L\n+    }\n+    toDelete.foreach { j => kvstore.delete(j.getClass(), j.sessionId) }\n+  }\n+\n+  /**\n+   * Remove at least (retainedSize / 10) items to reduce friction. Because tracking may be done\n+   * asynchronously, this method may return 0 in case enough items have been deleted already.\n+   */\n+  private def calculateNumberToRemove(dataSize: Long, retainedSize: Long): Long = {\n+    if (dataSize > retainedSize) {\n+      math.max(retainedSize / 10L, dataSize - retainedSize)\n+    } else {\n+      0L\n+    }\n+  }\n+}\n+\n+private[thriftserver] class LiveExecutionData(\n+    val execId: String,\n+    val statement: String,\n+    val sessionId: String,\n+    val startTimestamp: Long,\n+    val userName: String) extends LiveEntity {\n+\n+    var finishTimestamp: Long = 0L\n+    var closeTimestamp: Long = 0L\n+    var executePlan: String = \"\"\n+    var detail: String = \"\"\n+    var state: ExecutionState.Value = ExecutionState.STARTED\n+    val jobId: ArrayBuffer[String] = ArrayBuffer[String]()\n+    var groupId: String = \"\"\n+\n+  override protected def doUpdate(): Any = {\n+    new ExecutionInfo(\n+      execId,\n+      statement,\n+      sessionId,\n+      startTimestamp,\n+      userName,\n+      finishTimestamp,\n+      closeTimestamp,\n+      executePlan,\n+      detail,\n+      state,\n+      jobId,\n+      groupId)\n+  }\n+}\n+\n+\n+private[thriftserver] class LiveSessionData(\n+    val sessionId: String,\n+    val startTimeStamp: Long,\n+    val ip: String,\n+    val username: String) extends LiveEntity {\n+\n+  var finishTimestamp: Long = 0L\n+  var totalExecution: Int = 0\n+\n+  override protected def doUpdate(): Any = {\n+    new SessionInfo(\n+      sessionId,\n+      startTimeStamp,\n+      ip,\n+      username,\n+      finishTimestamp,\n+      totalExecution)\n+  }\n+\n+}\n+\n+private[thriftserver] case class SparkListenerSessionCreated(\n+    ip: String,\n+    sessionId: String,\n+    userName: String,\n+    startTime: Long) extends SparkListenerEvent"
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "If we're moving all this code, let's name all events Operation instead of Statement\r\n\r\nHistorically, it was named \"Statement\", because we only logged ExecuteStatement, but now we log all other Operations here as well. There is already \"OperationClosed\", because the Closed event was also added later. ",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-22T16:20:13Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        exec.groupId = groupId\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        liveExec.groupId = groupId\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerSessionCreated => processEventSessionCreated(e)\n+      case e: SparkListenerSessionClosed => processEventSessionClosed(e)\n+      case e: SparkListenerStatementStart => processEventStatementStart(e)\n+      case e: SparkListenerStatementParsed => processEventStatementParsed(e)\n+      case e: SparkListenerStatementCanceled => processEventStatementCanceled(e)\n+      case e: SparkListenerStatementError => processEventStatementError(e)\n+      case e: SparkListenerStatementFinish => processEventStatementFinish(e)\n+      case e: SparkListenerOperationClosed => processEventOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def processEventSessionCreated(e: SparkListenerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def processEventSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+\n+  }\n+\n+  private def processEventStatementStart(e: SparkListenerStatementStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def processEventStatementParsed(e: SparkListenerStatementParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementCanceled(e: SparkListenerStatementCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementError(e: SparkListenerStatementError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementFinish(e: SparkListenerStatementFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventOperationClosed(e: SparkListenerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+\n+  def onSessionCreated(ip: String, sessionId: String, userName: String = \"UNKNOWN\"): Unit = {\n+    postLiveListenerBus(SparkListenerSessionCreated(ip, sessionId,\n+      userName, System.currentTimeMillis()))\n+  }\n+\n+  def onSessionClosed(sessionId: String): Unit = {\n+    postLiveListenerBus(SparkListenerSessionClosed(sessionId, System.currentTimeMillis()))\n+  }\n+\n+  def onStatementStart(\n+    id: String,\n+    sessionId: String,\n+    statement: String,\n+    groupId: String,\n+    userName: String = \"UNKNOWN\"): Unit = {\n+    postLiveListenerBus(SparkListenerStatementStart(id, sessionId, statement, groupId,\n+      System.currentTimeMillis(), userName))\n+  }\n+\n+  def onStatementParsed(id: String, executionPlan: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementParsed(id, executionPlan))\n+  }\n+\n+  def onStatementCanceled(id: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementCanceled(id, System.currentTimeMillis()))\n+  }\n+\n+  def onStatementError(id: String, errorMsg: String, errorTrace: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementError(id, errorMsg, errorTrace,\n+      System.currentTimeMillis()))\n+  }\n+\n+  def onStatementFinish(id: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementFinish(id, System.currentTimeMillis()))\n+\n+  }\n+\n+  def onOperationClosed(id: String): Unit = {\n+    postLiveListenerBus(SparkListenerOperationClosed(id, System.currentTimeMillis()))\n+  }\n+\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }\n+\n+  def updateLiveStore(session: LiveEntity, force: Boolean = false): Unit = {\n+    if (live || force == true) {\n+      session.write(kvstore, System.nanoTime())\n+    }\n+  }\n+\n+  private def getOrCreateSession(\n+     sessionId: String,\n+     startTime: Long,\n+     ip: String,\n+     username: String): LiveSessionData = {\n+    sessionList.computeIfAbsent(sessionId,\n+      (_: String) => new LiveSessionData(sessionId, startTime, ip, username))\n+  }\n+\n+  private def getOrCreateExecution(\n+    execId: String, statement: String,\n+    sessionId: String, startTimestamp: Long,\n+    userName: String): LiveExecutionData = {\n+    executionList.computeIfAbsent(execId,\n+      (_: String) => new LiveExecutionData(execId, statement, sessionId, startTimestamp, userName))\n+  }\n+\n+  private def cleanupExecutions(count: Long): Unit = {\n+    val countToDelete = calculateNumberToRemove(count, retainedStatements)\n+    if (countToDelete <= 0L) {\n+      return\n+    }\n+    val view = kvstore.view(classOf[ExecutionInfo]).index(\"finishTime\").first(0L)\n+    val toDelete = KVUtils.viewToSeq(view, countToDelete.toInt) { j =>\n+      j.finishTimestamp != 0\n+    }\n+    toDelete.foreach { j => kvstore.delete(j.getClass, j.execId) }\n+  }\n+\n+  private def cleanupSession(count: Long): Unit = {\n+    val countToDelete = calculateNumberToRemove(count, retainedSessions)\n+    if (countToDelete <= 0L) {\n+      return\n+    }\n+    val view = kvstore.view(classOf[SessionInfo]).index(\"finishTime\").first(0L)\n+    val toDelete = KVUtils.viewToSeq(view, countToDelete.toInt) { j =>\n+      j.finishTimestamp != 0L\n+    }\n+\n+    toDelete.foreach { j => kvstore.delete(j.getClass, j.sessionId) }\n+  }\n+\n+  /**\n+   * Remove at least (retainedSize / 10) items to reduce friction. Because tracking may be done\n+   * asynchronously, this method may return 0 in case enough items have been deleted already.\n+   */\n+  private def calculateNumberToRemove(dataSize: Long, retainedSize: Long): Long = {\n+    if (dataSize > retainedSize) {\n+      math.max(retainedSize / 10L, dataSize - retainedSize)\n+    } else {\n+      0L\n+    }\n+  }\n+}\n+\n+private[thriftserver] class LiveExecutionData(\n+    val execId: String,\n+    val statement: String,\n+    val sessionId: String,\n+    val startTimestamp: Long,\n+    val userName: String) extends LiveEntity {\n+\n+    var finishTimestamp: Long = 0L\n+    var closeTimestamp: Long = 0L\n+    var executePlan: String = \"\"\n+    var detail: String = \"\"\n+    var state: ExecutionState.Value = ExecutionState.STARTED\n+    val jobId: ArrayBuffer[String] = ArrayBuffer[String]()\n+    var groupId: String = \"\"\n+\n+  override protected def doUpdate(): Any = {\n+    new ExecutionInfo(\n+      execId,\n+      statement,\n+      sessionId,\n+      startTimestamp,\n+      userName,\n+      finishTimestamp,\n+      closeTimestamp,\n+      executePlan,\n+      detail,\n+      state,\n+      jobId,\n+      groupId)\n+  }\n+}\n+\n+\n+private[thriftserver] class LiveSessionData(\n+    val sessionId: String,\n+    val startTimeStamp: Long,\n+    val ip: String,\n+    val username: String) extends LiveEntity {\n+\n+  var finishTimestamp: Long = 0L\n+  var totalExecution: Int = 0\n+\n+  override protected def doUpdate(): Any = {\n+    new SessionInfo(\n+      sessionId,\n+      startTimeStamp,\n+      ip,\n+      username,\n+      finishTimestamp,\n+      totalExecution)\n+  }\n+\n+}\n+\n+private[thriftserver] case class SparkListenerSessionCreated(\n+    ip: String,\n+    sessionId: String,\n+    userName: String,\n+    startTime: Long) extends SparkListenerEvent\n+\n+private[thriftserver] case class SparkListenerSessionClosed(\n+    sessionId: String, finishTime: Long) extends SparkListenerEvent\n+\n+private[thriftserver] case class SparkListenerStatementStart("
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Thanks. Changed all the statement events to operation events",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-23T20:44:02Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        exec.groupId = groupId\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        liveExec.groupId = groupId\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerSessionCreated => processEventSessionCreated(e)\n+      case e: SparkListenerSessionClosed => processEventSessionClosed(e)\n+      case e: SparkListenerStatementStart => processEventStatementStart(e)\n+      case e: SparkListenerStatementParsed => processEventStatementParsed(e)\n+      case e: SparkListenerStatementCanceled => processEventStatementCanceled(e)\n+      case e: SparkListenerStatementError => processEventStatementError(e)\n+      case e: SparkListenerStatementFinish => processEventStatementFinish(e)\n+      case e: SparkListenerOperationClosed => processEventOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def processEventSessionCreated(e: SparkListenerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def processEventSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+\n+  }\n+\n+  private def processEventStatementStart(e: SparkListenerStatementStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def processEventStatementParsed(e: SparkListenerStatementParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementCanceled(e: SparkListenerStatementCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementError(e: SparkListenerStatementError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementFinish(e: SparkListenerStatementFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventOperationClosed(e: SparkListenerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+\n+  def onSessionCreated(ip: String, sessionId: String, userName: String = \"UNKNOWN\"): Unit = {\n+    postLiveListenerBus(SparkListenerSessionCreated(ip, sessionId,\n+      userName, System.currentTimeMillis()))\n+  }\n+\n+  def onSessionClosed(sessionId: String): Unit = {\n+    postLiveListenerBus(SparkListenerSessionClosed(sessionId, System.currentTimeMillis()))\n+  }\n+\n+  def onStatementStart(\n+    id: String,\n+    sessionId: String,\n+    statement: String,\n+    groupId: String,\n+    userName: String = \"UNKNOWN\"): Unit = {\n+    postLiveListenerBus(SparkListenerStatementStart(id, sessionId, statement, groupId,\n+      System.currentTimeMillis(), userName))\n+  }\n+\n+  def onStatementParsed(id: String, executionPlan: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementParsed(id, executionPlan))\n+  }\n+\n+  def onStatementCanceled(id: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementCanceled(id, System.currentTimeMillis()))\n+  }\n+\n+  def onStatementError(id: String, errorMsg: String, errorTrace: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementError(id, errorMsg, errorTrace,\n+      System.currentTimeMillis()))\n+  }\n+\n+  def onStatementFinish(id: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementFinish(id, System.currentTimeMillis()))\n+\n+  }\n+\n+  def onOperationClosed(id: String): Unit = {\n+    postLiveListenerBus(SparkListenerOperationClosed(id, System.currentTimeMillis()))\n+  }\n+\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }\n+\n+  def updateLiveStore(session: LiveEntity, force: Boolean = false): Unit = {\n+    if (live || force == true) {\n+      session.write(kvstore, System.nanoTime())\n+    }\n+  }\n+\n+  private def getOrCreateSession(\n+     sessionId: String,\n+     startTime: Long,\n+     ip: String,\n+     username: String): LiveSessionData = {\n+    sessionList.computeIfAbsent(sessionId,\n+      (_: String) => new LiveSessionData(sessionId, startTime, ip, username))\n+  }\n+\n+  private def getOrCreateExecution(\n+    execId: String, statement: String,\n+    sessionId: String, startTimestamp: Long,\n+    userName: String): LiveExecutionData = {\n+    executionList.computeIfAbsent(execId,\n+      (_: String) => new LiveExecutionData(execId, statement, sessionId, startTimestamp, userName))\n+  }\n+\n+  private def cleanupExecutions(count: Long): Unit = {\n+    val countToDelete = calculateNumberToRemove(count, retainedStatements)\n+    if (countToDelete <= 0L) {\n+      return\n+    }\n+    val view = kvstore.view(classOf[ExecutionInfo]).index(\"finishTime\").first(0L)\n+    val toDelete = KVUtils.viewToSeq(view, countToDelete.toInt) { j =>\n+      j.finishTimestamp != 0\n+    }\n+    toDelete.foreach { j => kvstore.delete(j.getClass, j.execId) }\n+  }\n+\n+  private def cleanupSession(count: Long): Unit = {\n+    val countToDelete = calculateNumberToRemove(count, retainedSessions)\n+    if (countToDelete <= 0L) {\n+      return\n+    }\n+    val view = kvstore.view(classOf[SessionInfo]).index(\"finishTime\").first(0L)\n+    val toDelete = KVUtils.viewToSeq(view, countToDelete.toInt) { j =>\n+      j.finishTimestamp != 0L\n+    }\n+\n+    toDelete.foreach { j => kvstore.delete(j.getClass, j.sessionId) }\n+  }\n+\n+  /**\n+   * Remove at least (retainedSize / 10) items to reduce friction. Because tracking may be done\n+   * asynchronously, this method may return 0 in case enough items have been deleted already.\n+   */\n+  private def calculateNumberToRemove(dataSize: Long, retainedSize: Long): Long = {\n+    if (dataSize > retainedSize) {\n+      math.max(retainedSize / 10L, dataSize - retainedSize)\n+    } else {\n+      0L\n+    }\n+  }\n+}\n+\n+private[thriftserver] class LiveExecutionData(\n+    val execId: String,\n+    val statement: String,\n+    val sessionId: String,\n+    val startTimestamp: Long,\n+    val userName: String) extends LiveEntity {\n+\n+    var finishTimestamp: Long = 0L\n+    var closeTimestamp: Long = 0L\n+    var executePlan: String = \"\"\n+    var detail: String = \"\"\n+    var state: ExecutionState.Value = ExecutionState.STARTED\n+    val jobId: ArrayBuffer[String] = ArrayBuffer[String]()\n+    var groupId: String = \"\"\n+\n+  override protected def doUpdate(): Any = {\n+    new ExecutionInfo(\n+      execId,\n+      statement,\n+      sessionId,\n+      startTimestamp,\n+      userName,\n+      finishTimestamp,\n+      closeTimestamp,\n+      executePlan,\n+      detail,\n+      state,\n+      jobId,\n+      groupId)\n+  }\n+}\n+\n+\n+private[thriftserver] class LiveSessionData(\n+    val sessionId: String,\n+    val startTimeStamp: Long,\n+    val ip: String,\n+    val username: String) extends LiveEntity {\n+\n+  var finishTimestamp: Long = 0L\n+  var totalExecution: Int = 0\n+\n+  override protected def doUpdate(): Any = {\n+    new SessionInfo(\n+      sessionId,\n+      startTimeStamp,\n+      ip,\n+      username,\n+      finishTimestamp,\n+      totalExecution)\n+  }\n+\n+}\n+\n+private[thriftserver] case class SparkListenerSessionCreated(\n+    ip: String,\n+    sessionId: String,\n+    userName: String,\n+    startTime: Long) extends SparkListenerEvent\n+\n+private[thriftserver] case class SparkListenerSessionClosed(\n+    sessionId: String, finishTime: Long) extends SparkListenerEvent\n+\n+private[thriftserver] case class SparkListenerStatementStart("
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "why can't it be sparkConf in both cases? (it is just SparkConf in SQLAppStatusListener)",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-22T20:41:54Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get"
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Yes, I also wanted SparkConf here. But, the earlier listener used SQLConf. Sure, I changed it into SparkConf. Now code looks clean",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-23T20:45:07Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get"
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "could you explain why are you doing this removing on flush? I haven't seen it in AppStatusListener / SQLAppStatusListener?",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-22T20:46:48Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )"
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "I think this wasn't necessary. I removed it. ",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-23T20:46:45Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )"
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "nit: bad indent",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-22T20:49:28Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }"
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Modified",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-23T20:46:55Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }"
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "exec.groupId should already be groupId, do you need to re-update it?",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-22T20:54:28Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        exec.groupId = groupId"
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Thanks. Updated.",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-23T20:47:11Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        exec.groupId = groupId"
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "ditto - it should already be set to groupId, do you need to re-update it?",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-22T20:54:59Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        exec.groupId = groupId\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        liveExec.groupId = groupId"
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Done",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-23T20:47:18Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        exec.groupId = groupId\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        liveExec.groupId = groupId"
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "nit: remove empty line",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-22T20:55:44Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        exec.groupId = groupId\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        liveExec.groupId = groupId\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerSessionCreated => processEventSessionCreated(e)\n+      case e: SparkListenerSessionClosed => processEventSessionClosed(e)\n+      case e: SparkListenerStatementStart => processEventStatementStart(e)\n+      case e: SparkListenerStatementParsed => processEventStatementParsed(e)\n+      case e: SparkListenerStatementCanceled => processEventStatementCanceled(e)\n+      case e: SparkListenerStatementError => processEventStatementError(e)\n+      case e: SparkListenerStatementFinish => processEventStatementFinish(e)\n+      case e: SparkListenerOperationClosed => processEventOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def processEventSessionCreated(e: SparkListenerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def processEventSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+"
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Done",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-23T20:47:25Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        exec.groupId = groupId\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        liveExec.groupId = groupId\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerSessionCreated => processEventSessionCreated(e)\n+      case e: SparkListenerSessionClosed => processEventSessionClosed(e)\n+      case e: SparkListenerStatementStart => processEventStatementStart(e)\n+      case e: SparkListenerStatementParsed => processEventStatementParsed(e)\n+      case e: SparkListenerStatementCanceled => processEventStatementCanceled(e)\n+      case e: SparkListenerStatementError => processEventStatementError(e)\n+      case e: SparkListenerStatementFinish => processEventStatementFinish(e)\n+      case e: SparkListenerOperationClosed => processEventOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def processEventSessionCreated(e: SparkListenerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def processEventSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+"
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "style: 4 space indent for parameters",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-22T20:57:39Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        exec.groupId = groupId\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        liveExec.groupId = groupId\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerSessionCreated => processEventSessionCreated(e)\n+      case e: SparkListenerSessionClosed => processEventSessionClosed(e)\n+      case e: SparkListenerStatementStart => processEventStatementStart(e)\n+      case e: SparkListenerStatementParsed => processEventStatementParsed(e)\n+      case e: SparkListenerStatementCanceled => processEventStatementCanceled(e)\n+      case e: SparkListenerStatementError => processEventStatementError(e)\n+      case e: SparkListenerStatementFinish => processEventStatementFinish(e)\n+      case e: SparkListenerOperationClosed => processEventOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def processEventSessionCreated(e: SparkListenerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def processEventSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+\n+  }\n+\n+  private def processEventStatementStart(e: SparkListenerStatementStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def processEventStatementParsed(e: SparkListenerStatementParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementCanceled(e: SparkListenerStatementCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementError(e: SparkListenerStatementError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementFinish(e: SparkListenerStatementFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventOperationClosed(e: SparkListenerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+\n+  def onSessionCreated(ip: String, sessionId: String, userName: String = \"UNKNOWN\"): Unit = {\n+    postLiveListenerBus(SparkListenerSessionCreated(ip, sessionId,\n+      userName, System.currentTimeMillis()))\n+  }\n+\n+  def onSessionClosed(sessionId: String): Unit = {\n+    postLiveListenerBus(SparkListenerSessionClosed(sessionId, System.currentTimeMillis()))\n+  }\n+\n+  def onStatementStart(\n+    id: String,"
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Done. Not sure why style check is not catching this",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-23T20:47:52Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        exec.groupId = groupId\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        liveExec.groupId = groupId\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerSessionCreated => processEventSessionCreated(e)\n+      case e: SparkListenerSessionClosed => processEventSessionClosed(e)\n+      case e: SparkListenerStatementStart => processEventStatementStart(e)\n+      case e: SparkListenerStatementParsed => processEventStatementParsed(e)\n+      case e: SparkListenerStatementCanceled => processEventStatementCanceled(e)\n+      case e: SparkListenerStatementError => processEventStatementError(e)\n+      case e: SparkListenerStatementFinish => processEventStatementFinish(e)\n+      case e: SparkListenerOperationClosed => processEventOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def processEventSessionCreated(e: SparkListenerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def processEventSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+\n+  }\n+\n+  private def processEventStatementStart(e: SparkListenerStatementStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def processEventStatementParsed(e: SparkListenerStatementParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementCanceled(e: SparkListenerStatementCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementError(e: SparkListenerStatementError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementFinish(e: SparkListenerStatementFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventOperationClosed(e: SparkListenerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+\n+  def onSessionCreated(ip: String, sessionId: String, userName: String = \"UNKNOWN\"): Unit = {\n+    postLiveListenerBus(SparkListenerSessionCreated(ip, sessionId,\n+      userName, System.currentTimeMillis()))\n+  }\n+\n+  def onSessionClosed(sessionId: String): Unit = {\n+    postLiveListenerBus(SparkListenerSessionClosed(sessionId, System.currentTimeMillis()))\n+  }\n+\n+  def onStatementStart(\n+    id: String,"
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "deleting executions should not be done based on finishTimestamp, but on\r\n```\r\n  def isExecutionActive(execInfo: ExecutionInfo): Boolean = {\r\n    !(execInfo.state == ExecutionState.FAILED ||\r\n      execInfo.state == ExecutionState.CANCELED ||\r\n      execInfo.state == ExecutionState.CLOSED)\r\n  }\r\n```\r\n(an execution that has finishTimestamp can still be busy fetching data)",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-22T21:03:31Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        exec.groupId = groupId\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        liveExec.groupId = groupId\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerSessionCreated => processEventSessionCreated(e)\n+      case e: SparkListenerSessionClosed => processEventSessionClosed(e)\n+      case e: SparkListenerStatementStart => processEventStatementStart(e)\n+      case e: SparkListenerStatementParsed => processEventStatementParsed(e)\n+      case e: SparkListenerStatementCanceled => processEventStatementCanceled(e)\n+      case e: SparkListenerStatementError => processEventStatementError(e)\n+      case e: SparkListenerStatementFinish => processEventStatementFinish(e)\n+      case e: SparkListenerOperationClosed => processEventOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def processEventSessionCreated(e: SparkListenerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def processEventSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+\n+  }\n+\n+  private def processEventStatementStart(e: SparkListenerStatementStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def processEventStatementParsed(e: SparkListenerStatementParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementCanceled(e: SparkListenerStatementCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementError(e: SparkListenerStatementError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementFinish(e: SparkListenerStatementFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventOperationClosed(e: SparkListenerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+\n+  def onSessionCreated(ip: String, sessionId: String, userName: String = \"UNKNOWN\"): Unit = {\n+    postLiveListenerBus(SparkListenerSessionCreated(ip, sessionId,\n+      userName, System.currentTimeMillis()))\n+  }\n+\n+  def onSessionClosed(sessionId: String): Unit = {\n+    postLiveListenerBus(SparkListenerSessionClosed(sessionId, System.currentTimeMillis()))\n+  }\n+\n+  def onStatementStart(\n+    id: String,\n+    sessionId: String,\n+    statement: String,\n+    groupId: String,\n+    userName: String = \"UNKNOWN\"): Unit = {\n+    postLiveListenerBus(SparkListenerStatementStart(id, sessionId, statement, groupId,\n+      System.currentTimeMillis(), userName))\n+  }\n+\n+  def onStatementParsed(id: String, executionPlan: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementParsed(id, executionPlan))\n+  }\n+\n+  def onStatementCanceled(id: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementCanceled(id, System.currentTimeMillis()))\n+  }\n+\n+  def onStatementError(id: String, errorMsg: String, errorTrace: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementError(id, errorMsg, errorTrace,\n+      System.currentTimeMillis()))\n+  }\n+\n+  def onStatementFinish(id: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementFinish(id, System.currentTimeMillis()))\n+\n+  }\n+\n+  def onOperationClosed(id: String): Unit = {\n+    postLiveListenerBus(SparkListenerOperationClosed(id, System.currentTimeMillis()))\n+  }\n+\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }\n+\n+  def updateLiveStore(session: LiveEntity, force: Boolean = false): Unit = {\n+    if (live || force == true) {\n+      session.write(kvstore, System.nanoTime())\n+    }\n+  }\n+\n+  private def getOrCreateSession(\n+     sessionId: String,\n+     startTime: Long,\n+     ip: String,\n+     username: String): LiveSessionData = {\n+    sessionList.computeIfAbsent(sessionId,\n+      (_: String) => new LiveSessionData(sessionId, startTime, ip, username))\n+  }\n+\n+  private def getOrCreateExecution(\n+    execId: String, statement: String,\n+    sessionId: String, startTimestamp: Long,\n+    userName: String): LiveExecutionData = {\n+    executionList.computeIfAbsent(execId,\n+      (_: String) => new LiveExecutionData(execId, statement, sessionId, startTimestamp, userName))\n+  }\n+\n+  private def cleanupExecutions(count: Long): Unit = {\n+    val countToDelete = calculateNumberToRemove(count, retainedStatements)\n+    if (countToDelete <= 0L) {\n+      return\n+    }\n+    val view = kvstore.view(classOf[ExecutionInfo]).index(\"finishTime\").first(0L)\n+    val toDelete = KVUtils.viewToSeq(view, countToDelete.toInt) { j =>\n+      j.finishTimestamp != 0"
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "I think, earlier also it was removing based on `finishedTime`\r\nhttps://github.com/apache/spark/blob/c98e5eb3396a6db92f2420e743afa9ddff319ca2/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/HiveThriftServer2.scala#L281-L282\r\n\r\nShould I change to `isExecutionActive`?",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-23T20:50:49Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        exec.groupId = groupId\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        liveExec.groupId = groupId\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerSessionCreated => processEventSessionCreated(e)\n+      case e: SparkListenerSessionClosed => processEventSessionClosed(e)\n+      case e: SparkListenerStatementStart => processEventStatementStart(e)\n+      case e: SparkListenerStatementParsed => processEventStatementParsed(e)\n+      case e: SparkListenerStatementCanceled => processEventStatementCanceled(e)\n+      case e: SparkListenerStatementError => processEventStatementError(e)\n+      case e: SparkListenerStatementFinish => processEventStatementFinish(e)\n+      case e: SparkListenerOperationClosed => processEventOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def processEventSessionCreated(e: SparkListenerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def processEventSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+\n+  }\n+\n+  private def processEventStatementStart(e: SparkListenerStatementStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def processEventStatementParsed(e: SparkListenerStatementParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementCanceled(e: SparkListenerStatementCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementError(e: SparkListenerStatementError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementFinish(e: SparkListenerStatementFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventOperationClosed(e: SparkListenerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+\n+  def onSessionCreated(ip: String, sessionId: String, userName: String = \"UNKNOWN\"): Unit = {\n+    postLiveListenerBus(SparkListenerSessionCreated(ip, sessionId,\n+      userName, System.currentTimeMillis()))\n+  }\n+\n+  def onSessionClosed(sessionId: String): Unit = {\n+    postLiveListenerBus(SparkListenerSessionClosed(sessionId, System.currentTimeMillis()))\n+  }\n+\n+  def onStatementStart(\n+    id: String,\n+    sessionId: String,\n+    statement: String,\n+    groupId: String,\n+    userName: String = \"UNKNOWN\"): Unit = {\n+    postLiveListenerBus(SparkListenerStatementStart(id, sessionId, statement, groupId,\n+      System.currentTimeMillis(), userName))\n+  }\n+\n+  def onStatementParsed(id: String, executionPlan: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementParsed(id, executionPlan))\n+  }\n+\n+  def onStatementCanceled(id: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementCanceled(id, System.currentTimeMillis()))\n+  }\n+\n+  def onStatementError(id: String, errorMsg: String, errorTrace: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementError(id, errorMsg, errorTrace,\n+      System.currentTimeMillis()))\n+  }\n+\n+  def onStatementFinish(id: String): Unit = {\n+    postLiveListenerBus(SparkListenerStatementFinish(id, System.currentTimeMillis()))\n+\n+  }\n+\n+  def onOperationClosed(id: String): Unit = {\n+    postLiveListenerBus(SparkListenerOperationClosed(id, System.currentTimeMillis()))\n+  }\n+\n+\n+  /** Go through all `LiveEntity`s and use `entityFlushFunc(entity)` to flush them. */\n+  private def flush(entityFlushFunc: LiveEntity => Unit): Unit = {\n+    sessionList.values.asScala.foreach(entityFlushFunc)\n+    executionList.values.asScala.foreach(entityFlushFunc)\n+  }\n+\n+  private def update(entity: LiveEntity, now: Long): Unit = {\n+    entity.write(kvstore, now)\n+  }\n+\n+  def updateLiveStore(session: LiveEntity, force: Boolean = false): Unit = {\n+    if (live || force == true) {\n+      session.write(kvstore, System.nanoTime())\n+    }\n+  }\n+\n+  private def getOrCreateSession(\n+     sessionId: String,\n+     startTime: Long,\n+     ip: String,\n+     username: String): LiveSessionData = {\n+    sessionList.computeIfAbsent(sessionId,\n+      (_: String) => new LiveSessionData(sessionId, startTime, ip, username))\n+  }\n+\n+  private def getOrCreateExecution(\n+    execId: String, statement: String,\n+    sessionId: String, startTimestamp: Long,\n+    userName: String): LiveExecutionData = {\n+    executionList.computeIfAbsent(execId,\n+      (_: String) => new LiveExecutionData(execId, statement, sessionId, startTimestamp, userName))\n+  }\n+\n+  private def cleanupExecutions(count: Long): Unit = {\n+    val countToDelete = calculateNumberToRemove(count, retainedStatements)\n+    if (countToDelete <= 0L) {\n+      return\n+    }\n+    val view = kvstore.view(classOf[ExecutionInfo]).index(\"finishTime\").first(0L)\n+    val toDelete = KVUtils.viewToSeq(view, countToDelete.toInt) { j =>\n+      j.finishTimestamp != 0"
  }],
  "prId": 26378
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "Hmm... After the previous discussion, I would prefer to just change all the Operations to create these events and post directly to the sc listener bus, instead of adding these helpers here. AppStatusListener and SQLAppStatusListener don't have such helpers.\r\n@AngersZhuuuu WDYT? Does it work for you when it's added to the \"status\" listener queue, so that the likelihood of dropping events is minimized? As a followup there could be introduced a \"criticial events\" queue that doesn't have limited capacity (or blocks instead of dropping) to make sure that no events are dropped.",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-22T21:09:48Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        exec.groupId = groupId\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        liveExec.groupId = groupId\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerSessionCreated => processEventSessionCreated(e)\n+      case e: SparkListenerSessionClosed => processEventSessionClosed(e)\n+      case e: SparkListenerStatementStart => processEventStatementStart(e)\n+      case e: SparkListenerStatementParsed => processEventStatementParsed(e)\n+      case e: SparkListenerStatementCanceled => processEventStatementCanceled(e)\n+      case e: SparkListenerStatementError => processEventStatementError(e)\n+      case e: SparkListenerStatementFinish => processEventStatementFinish(e)\n+      case e: SparkListenerOperationClosed => processEventOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def processEventSessionCreated(e: SparkListenerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def processEventSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+\n+  }\n+\n+  private def processEventStatementStart(e: SparkListenerStatementStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def processEventStatementParsed(e: SparkListenerStatementParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementCanceled(e: SparkListenerStatementCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementError(e: SparkListenerStatementError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementFinish(e: SparkListenerStatementFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventOperationClosed(e: SparkListenerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+\n+  def onSessionCreated(ip: String, sessionId: String, userName: String = \"UNKNOWN\"): Unit = {"
  }, {
    "author": {
      "login": "shahidki31"
    },
    "body": "Thanks @juliuszsompolski for the review.\r\nNow I have created a new class `HiveThriftServer2EventManager`, which creates listener events and post to listener bus. Now `HiveThriftServer2Listener` class only listens the events. \r\n\r\nAlso I moved all the listener/Store classes to the `ui` directory, which also follows similar to AppStatusListener and SQLAppStatusListner",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-23T20:53:27Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        exec.groupId = groupId\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        liveExec.groupId = groupId\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerSessionCreated => processEventSessionCreated(e)\n+      case e: SparkListenerSessionClosed => processEventSessionClosed(e)\n+      case e: SparkListenerStatementStart => processEventStatementStart(e)\n+      case e: SparkListenerStatementParsed => processEventStatementParsed(e)\n+      case e: SparkListenerStatementCanceled => processEventStatementCanceled(e)\n+      case e: SparkListenerStatementError => processEventStatementError(e)\n+      case e: SparkListenerStatementFinish => processEventStatementFinish(e)\n+      case e: SparkListenerOperationClosed => processEventOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def processEventSessionCreated(e: SparkListenerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def processEventSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+\n+  }\n+\n+  private def processEventStatementStart(e: SparkListenerStatementStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def processEventStatementParsed(e: SparkListenerStatementParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementCanceled(e: SparkListenerStatementCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementError(e: SparkListenerStatementError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementFinish(e: SparkListenerStatementFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventOperationClosed(e: SparkListenerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+\n+  def onSessionCreated(ip: String, sessionId: String, userName: String = \"UNKNOWN\"): Unit = {"
  }, {
    "author": {
      "login": "AngersZhuuuu"
    },
    "body": "\r\n> @AngersZhuuuu WDYT? Does it work for you when it's added to the \"status\" listener queue, so that the likelihood of dropping events is minimized? As a followup there could be introduced a \"criticial events\" queue that doesn't have limited capacity (or blocks instead of dropping) to make sure that no events are dropped.\r\n\r\nAdding it to status queue is OK for me. Thanks",
    "commit": "39ddc891582db48a7d5e2ece771aac064cd41f1f",
    "createdAt": "2019-11-25T02:32:12Z",
    "diffHunk": "@@ -0,0 +1,414 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.concurrent.ConcurrentHashMap\n+\n+import org.apache.hive.service.server.HiveServer2\n+import scala.collection.JavaConverters._\n+import scala.collection.mutable.ArrayBuffer\n+\n+import org.apache.spark.{SparkConf, SparkContext}\n+import org.apache.spark.scheduler._\n+import org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.ExecutionState\n+import org.apache.spark.sql.hive.thriftserver.ui.{ExecutionInfo, SessionInfo}\n+import org.apache.spark.sql.internal.SQLConf\n+import org.apache.spark.status.{ElementTrackingStore, KVUtils, LiveEntity}\n+\n+/**\n+ * An inner sparkListener called in sc.stop to clean up the HiveThriftServer2\n+ */\n+private[thriftserver] class HiveThriftServer2Listener(\n+  kvstore: ElementTrackingStore,\n+  server: Option[HiveServer2],\n+  sqlConf: Option[SQLConf],\n+  sc: Option[SparkContext],\n+  sparkConf: Option[SparkConf] = None,\n+  live: Boolean = true) extends SparkListener {\n+\n+  private val sessionList = new ConcurrentHashMap[String, LiveSessionData]()\n+  private val executionList = new ConcurrentHashMap[String, LiveExecutionData]()\n+\n+  private val (retainedStatements: Int, retainedSessions: Int) = {\n+    if (live) {\n+      val conf = sqlConf.get\n+      (conf.getConf(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.getConf(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    } else {\n+      val conf = sparkConf.get\n+      (conf.get(SQLConf.THRIFTSERVER_UI_STATEMENT_LIMIT),\n+        conf.get(SQLConf.THRIFTSERVER_UI_SESSION_LIMIT))\n+    }\n+  }\n+\n+  // Returns true if this listener has no live data. Exposed for tests only.\n+  private[thriftserver] def noLiveData(): Boolean = {\n+    sessionList.isEmpty && executionList.isEmpty\n+  }\n+\n+  kvstore.addTrigger(classOf[SessionInfo], retainedSessions) { count =>\n+    cleanupSession(count)\n+  }\n+\n+  kvstore.addTrigger(classOf[ExecutionInfo], retainedStatements) { count =>\n+    cleanupExecutions(count)\n+  }\n+\n+  kvstore.onFlush {\n+    if (!live) {\n+      val now = System.nanoTime()\n+      flush(update(_, now))\n+      executionList.keys().asScala.foreach(\n+        key => executionList.remove(key)\n+      )\n+      sessionList.keys().asScala.foreach(\n+        key => sessionList.remove(key)\n+      )\n+    }\n+  }\n+\n+  def postLiveListenerBus(event: SparkListenerEvent): Unit = {\n+    if (live) {\n+      sc.foreach(_.listenerBus.post(event))\n+    }\n+  }\n+\n+  override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {\n+    if (live) {\n+      server.foreach(_.stop())\n+    }\n+  }\n+\n+  override def onJobStart(jobStart: SparkListenerJobStart): Unit = {\n+    val properties = jobStart.properties\n+    if (properties != null) {\n+      val groupId = properties.getProperty(SparkContext.SPARK_JOB_GROUP_ID)\n+      if (groupId != null) {\n+        updateJobDetails(jobStart.jobId.toString, groupId)\n+        }\n+      }\n+    }\n+\n+  /**\n+   * This method is to handle out of order events. ie. if Job event come after execution end event.\n+   * @param jobId\n+   * @param groupId\n+   */\n+  private def updateJobDetails(jobId: String, groupId: String): Unit = {\n+    val execList = executionList.values().asScala.filter(_.groupId == groupId).toSeq\n+    if (execList.nonEmpty) {\n+      execList.foreach { exec =>\n+        exec.jobId += jobId.toString\n+        exec.groupId = groupId\n+        updateLiveStore(exec)\n+      }\n+    } else {\n+      // Here will come only if JobStart event comes after Execution End event.\n+      val storeExecInfo = kvstore.view(classOf[ExecutionInfo]).asScala.filter(_.groupId == groupId)\n+      storeExecInfo.foreach { exec =>\n+        val liveExec = getOrCreateExecution(exec.execId, exec.statement, exec.sessionId,\n+          exec.startTimestamp, exec.userName)\n+        liveExec.jobId += jobId.toString\n+        liveExec.groupId = groupId\n+        updateLiveStore(liveExec, true)\n+        executionList.remove(liveExec.execId)\n+      }\n+    }\n+  }\n+\n+  override def onOtherEvent(event: SparkListenerEvent): Unit = {\n+    event match {\n+      case e: SparkListenerSessionCreated => processEventSessionCreated(e)\n+      case e: SparkListenerSessionClosed => processEventSessionClosed(e)\n+      case e: SparkListenerStatementStart => processEventStatementStart(e)\n+      case e: SparkListenerStatementParsed => processEventStatementParsed(e)\n+      case e: SparkListenerStatementCanceled => processEventStatementCanceled(e)\n+      case e: SparkListenerStatementError => processEventStatementError(e)\n+      case e: SparkListenerStatementFinish => processEventStatementFinish(e)\n+      case e: SparkListenerOperationClosed => processEventOperationClosed(e)\n+      case _ => // Ignore\n+    }\n+  }\n+\n+  private def processEventSessionCreated(e: SparkListenerSessionCreated): Unit = {\n+    val session = getOrCreateSession(e.sessionId, e.startTime, e.ip, e.userName)\n+    sessionList.put(e.sessionId, session)\n+    updateLiveStore(session, true)\n+  }\n+\n+  private def processEventSessionClosed(e: SparkListenerSessionClosed): Unit = {\n+    val session = sessionList.get(e.sessionId)\n+    session.finishTimestamp = e.finishTime\n+    updateLiveStore(session, true)\n+    sessionList.remove(e.sessionId)\n+\n+  }\n+\n+  private def processEventStatementStart(e: SparkListenerStatementStart): Unit = {\n+    val info = getOrCreateExecution(\n+      e.id,\n+      e.statement,\n+      e.sessionId,\n+      e.startTime,\n+      e.userName)\n+\n+    info.state = ExecutionState.STARTED\n+    executionList.put(e.id, info)\n+    sessionList.get(e.sessionId).totalExecution += 1\n+    executionList.get(e.id).groupId = e.groupId\n+    updateLiveStore(executionList.get(e.id))\n+    updateLiveStore(sessionList.get(e.sessionId), true)\n+  }\n+\n+  private def processEventStatementParsed(e: SparkListenerStatementParsed): Unit = {\n+    executionList.get(e.id).executePlan = e.executionPlan\n+    executionList.get(e.id).state = ExecutionState.COMPILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementCanceled(e: SparkListenerStatementCanceled): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.CANCELED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementError(e: SparkListenerStatementError): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).detail = e.errorMsg\n+    executionList.get(e.id).state = ExecutionState.FAILED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventStatementFinish(e: SparkListenerStatementFinish): Unit = {\n+    executionList.get(e.id).finishTimestamp = e.finishTime\n+    executionList.get(e.id).state = ExecutionState.FINISHED\n+    updateLiveStore(executionList.get(e.id))\n+  }\n+\n+  private def processEventOperationClosed(e: SparkListenerOperationClosed): Unit = {\n+    executionList.get(e.id).closeTimestamp = e.closeTime\n+    executionList.get(e.id).state = ExecutionState.CLOSED\n+    updateLiveStore(executionList.get(e.id), true)\n+    executionList.remove(e.id)\n+  }\n+\n+\n+  def onSessionCreated(ip: String, sessionId: String, userName: String = \"UNKNOWN\"): Unit = {"
  }],
  "prId": 26378
}]