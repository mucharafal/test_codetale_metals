[{
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "Using RowSet with Spark Rows is fine, but if you now just do resultRowSet.addRow(sparkRow), then it will e.g. not convert Array, Map, Struct or Interval to String, like addNonNullColumnValue did, so you still need to add these conversions somewhere. Probably would be easiest with a Project with casts added on top of the query if needed.",
    "commit": "60415cd012aced3efed30598a8e8e78be1bf6ff0",
    "createdAt": "2019-10-23T15:28:47Z",
    "diffHunk": "@@ -134,31 +140,36 @@ private[hive] class SparkExecuteStatementOperation(\n       resultRowSet\n     } else {\n       // maxRowsL here typically maps to java.sql.Statement.getFetchSize, which is an int\n-      val maxRows = maxRowsL.toInt\n+      val maxRows = maxRowsL\n       var curRow = 0\n       while (curRow < maxRows && iter.hasNext) {\n         val sparkRow = iter.next()\n-        val row = ArrayBuffer[Any]()\n-        var curCol = 0\n-        while (curCol < sparkRow.length) {\n-          if (sparkRow.isNullAt(curCol)) {\n-            row += null\n-          } else {\n-            addNonNullColumnValue(sparkRow, row, curCol)",
    "line": 118
  }],
  "prId": 25721
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "TableSchema in Hive format still needs to be returned as TTableSchema in TGetResultSetMetadataResp. If you return a Spark StructType here, how do you get TableSchema out of it for the result?\r\nEdit: Ok, I found that you implemented a SchemaMapper for that at the thrift layer. :+1:\r\n",
    "commit": "60415cd012aced3efed30598a8e8e78be1bf6ff0",
    "createdAt": "2019-10-23T15:31:55Z",
    "diffHunk": "@@ -134,31 +140,36 @@ private[hive] class SparkExecuteStatementOperation(\n       resultRowSet\n     } else {\n       // maxRowsL here typically maps to java.sql.Statement.getFetchSize, which is an int\n-      val maxRows = maxRowsL.toInt\n+      val maxRows = maxRowsL\n       var curRow = 0\n       while (curRow < maxRows && iter.hasNext) {\n         val sparkRow = iter.next()\n-        val row = ArrayBuffer[Any]()\n-        var curCol = 0\n-        while (curCol < sparkRow.length) {\n-          if (sparkRow.isNullAt(curCol)) {\n-            row += null\n-          } else {\n-            addNonNullColumnValue(sparkRow, row, curCol)\n-          }\n-          curCol += 1\n-        }\n-        resultRowSet.addRow(row.toArray.asInstanceOf[Array[Object]])\n+        resultRowSet.addRow(sparkRow)\n         curRow += 1\n       }\n       resultRowSet\n     }\n   }\n \n-  def getResultSetSchema: TableSchema = resultSchema\n+  def getResultSetSchema: StructType = {",
    "line": 131
  }, {
    "author": {
      "login": "AngersZhuuuu"
    },
    "body": "> TableSchema in Hive format still needs to be returned as TTableSchema in TGetResultSetMetadataResp. If you return a Spark StructType here, how do you get TableSchema out of it for the result?\r\n> Edit: Ok, I found that you implemented a SchemaMapper for that at the thrift layer. üëç\r\n\r\nConvert with same rule in SparkExecuteStatementOperation.\r\n\r\nBy the way , we may need to implement a spark's jdbc client to remove import of `hive-beeline` `hive-jdbc` `hive-service`\r\n\r\nSee Google Doc https://docs.google.com/document/d/1oRgzt83vCGykkb45VjzvTEdRC_-ympRLB24pUWqk82o/edit#",
    "commit": "60415cd012aced3efed30598a8e8e78be1bf6ff0",
    "createdAt": "2019-10-24T01:55:03Z",
    "diffHunk": "@@ -134,31 +140,36 @@ private[hive] class SparkExecuteStatementOperation(\n       resultRowSet\n     } else {\n       // maxRowsL here typically maps to java.sql.Statement.getFetchSize, which is an int\n-      val maxRows = maxRowsL.toInt\n+      val maxRows = maxRowsL\n       var curRow = 0\n       while (curRow < maxRows && iter.hasNext) {\n         val sparkRow = iter.next()\n-        val row = ArrayBuffer[Any]()\n-        var curCol = 0\n-        while (curCol < sparkRow.length) {\n-          if (sparkRow.isNullAt(curCol)) {\n-            row += null\n-          } else {\n-            addNonNullColumnValue(sparkRow, row, curCol)\n-          }\n-          curCol += 1\n-        }\n-        resultRowSet.addRow(row.toArray.asInstanceOf[Array[Object]])\n+        resultRowSet.addRow(sparkRow)\n         curRow += 1\n       }\n       resultRowSet\n     }\n   }\n \n-  def getResultSetSchema: TableSchema = resultSchema\n+  def getResultSetSchema: StructType = {",
    "line": 131
  }],
  "prId": 25721
}]