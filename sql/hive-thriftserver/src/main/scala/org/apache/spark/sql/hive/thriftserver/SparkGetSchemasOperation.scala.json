[{
  "comments": [{
    "author": {
      "login": "HyukjinKwon"
    },
    "body": "@wangyum, IIRC, not only DBs but other metadata are not being shown as well (correct me if I am wrong because I tested it a while ago so can't exactly remember). Can you double check if only database is missing?",
    "commit": "ecc4e0d0d4d7b16157d7dad27e4e59f9a081b477",
    "createdAt": "2018-12-27T09:53:03Z",
    "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetSchemasOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+\n+/**\n+ * Spark's own GetSchemasOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable.\n+ * @param schemaName database name, null or a concrete database name\n+ */\n+private[hive] class SparkGetSchemasOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String)\n+  extends GetSchemasOperation(parentSession, catalogName, schemaName) with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private final val RESULT_SET_SCHEMA = new TableSchema()\n+    .addStringColumn(\"TABLE_SCHEM\", \"Schema name.\")\n+    .addStringColumn(\"TABLE_CATALOG\", \"Catalog name.\")\n+\n+  private val rowSet = RowSetFactory.create(RESULT_SET_SCHEMA, getProtocolVersion)\n+\n+  private var statementId: String = _\n+\n+  override def close(): Unit = {\n+    logInfo(s\"Close get schemas with $statementId\")\n+    setState(OperationState.CLOSED)\n+  }\n+\n+  override def runInternal(): Unit = {\n+    statementId = UUID.randomUUID().toString\n+    logInfo(s\"Getting schemas with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    if (isAuthV2Enabled) {\n+      val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+      authorizeMetaGets(HiveOperationType.GET_TABLES, null, cmdStr)\n+    }\n+\n+    try {\n+      catalog.listDatabases(convertSchemaPattern(schemaName)).foreach { dbName =>"
  }, {
    "author": {
      "login": "mgaido91"
    },
    "body": "+1, that's true, none of the metadata operations is implemented currently..",
    "commit": "ecc4e0d0d4d7b16157d7dad27e4e59f9a081b477",
    "createdAt": "2018-12-27T10:56:56Z",
    "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetSchemasOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+\n+/**\n+ * Spark's own GetSchemasOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable.\n+ * @param schemaName database name, null or a concrete database name\n+ */\n+private[hive] class SparkGetSchemasOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String)\n+  extends GetSchemasOperation(parentSession, catalogName, schemaName) with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private final val RESULT_SET_SCHEMA = new TableSchema()\n+    .addStringColumn(\"TABLE_SCHEM\", \"Schema name.\")\n+    .addStringColumn(\"TABLE_CATALOG\", \"Catalog name.\")\n+\n+  private val rowSet = RowSetFactory.create(RESULT_SET_SCHEMA, getProtocolVersion)\n+\n+  private var statementId: String = _\n+\n+  override def close(): Unit = {\n+    logInfo(s\"Close get schemas with $statementId\")\n+    setState(OperationState.CLOSED)\n+  }\n+\n+  override def runInternal(): Unit = {\n+    statementId = UUID.randomUUID().toString\n+    logInfo(s\"Getting schemas with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    if (isAuthV2Enabled) {\n+      val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+      authorizeMetaGets(HiveOperationType.GET_TABLES, null, cmdStr)\n+    }\n+\n+    try {\n+      catalog.listDatabases(convertSchemaPattern(schemaName)).foreach { dbName =>"
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "Yes. I think implementing `GetSchemasOperation`, [`GetTablesOperation`](https://github.com/apache/spark/pull/22794) and [`GetColumnsOperation`](https://github.com/wangyum/spark/commit/0f328124063d46f5651a5d8da109e2767a9a7991) is enough.\r\n",
    "commit": "ecc4e0d0d4d7b16157d7dad27e4e59f9a081b477",
    "createdAt": "2018-12-27T14:28:13Z",
    "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetSchemasOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+\n+/**\n+ * Spark's own GetSchemasOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable.\n+ * @param schemaName database name, null or a concrete database name\n+ */\n+private[hive] class SparkGetSchemasOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String)\n+  extends GetSchemasOperation(parentSession, catalogName, schemaName) with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private final val RESULT_SET_SCHEMA = new TableSchema()\n+    .addStringColumn(\"TABLE_SCHEM\", \"Schema name.\")\n+    .addStringColumn(\"TABLE_CATALOG\", \"Catalog name.\")\n+\n+  private val rowSet = RowSetFactory.create(RESULT_SET_SCHEMA, getProtocolVersion)\n+\n+  private var statementId: String = _\n+\n+  override def close(): Unit = {\n+    logInfo(s\"Close get schemas with $statementId\")\n+    setState(OperationState.CLOSED)\n+  }\n+\n+  override def runInternal(): Unit = {\n+    statementId = UUID.randomUUID().toString\n+    logInfo(s\"Getting schemas with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    if (isAuthV2Enabled) {\n+      val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+      authorizeMetaGets(HiveOperationType.GET_TABLES, null, cmdStr)\n+    }\n+\n+    try {\n+      catalog.listDatabases(convertSchemaPattern(schemaName)).foreach { dbName =>"
  }],
  "prId": 22903
}, {
  "comments": [{
    "author": {
      "login": "mgaido91"
    },
    "body": "nit: `TABLE_SCHEMA`?",
    "commit": "ecc4e0d0d4d7b16157d7dad27e4e59f9a081b477",
    "createdAt": "2018-12-27T10:55:38Z",
    "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetSchemasOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+\n+/**\n+ * Spark's own GetSchemasOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable.\n+ * @param schemaName database name, null or a concrete database name\n+ */\n+private[hive] class SparkGetSchemasOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String)\n+  extends GetSchemasOperation(parentSession, catalogName, schemaName) with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private final val RESULT_SET_SCHEMA = new TableSchema()\n+    .addStringColumn(\"TABLE_SCHEM\", \"Schema name.\")"
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "It's copy from [GetSchemasOperation.java#L49](https://github.com/apache/hive/blob/477649727dc9f232545ac4f2559fedda17665881/service/src/java/org/apache/hive/service/cli/operation/GetSchemasOperation.java#L49)  and [`HiveDatabaseMetaData`](https://github.com/apache/hive/blob/7c9689a9f5625b4b17f3ea6dc275bd55442407f3/jdbc/src/java/org/apache/hive/jdbc/HiveDatabaseMetaData.java#L390) also use `TABLE_SCHEMA`.",
    "commit": "ecc4e0d0d4d7b16157d7dad27e4e59f9a081b477",
    "createdAt": "2018-12-28T03:22:23Z",
    "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetSchemasOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+\n+/**\n+ * Spark's own GetSchemasOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable.\n+ * @param schemaName database name, null or a concrete database name\n+ */\n+private[hive] class SparkGetSchemasOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String)\n+  extends GetSchemasOperation(parentSession, catalogName, schemaName) with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private final val RESULT_SET_SCHEMA = new TableSchema()\n+    .addStringColumn(\"TABLE_SCHEM\", \"Schema name.\")"
  }],
  "prId": 22903
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "What is the reason you need to change the order between line 87 and 88?",
    "commit": "ecc4e0d0d4d7b16157d7dad27e4e59f9a081b477",
    "createdAt": "2018-12-28T07:05:39Z",
    "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetSchemasOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+\n+/**\n+ * Spark's own GetSchemasOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable.\n+ * @param schemaName database name, null or a concrete database name\n+ */\n+private[hive] class SparkGetSchemasOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String)\n+  extends GetSchemasOperation(parentSession, catalogName, schemaName) with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private final val RESULT_SET_SCHEMA = new TableSchema()\n+    .addStringColumn(\"TABLE_SCHEM\", \"Schema name.\")\n+    .addStringColumn(\"TABLE_CATALOG\", \"Catalog name.\")\n+\n+  private val rowSet = RowSetFactory.create(RESULT_SET_SCHEMA, getProtocolVersion)\n+\n+  private var statementId: String = _\n+\n+  override def close(): Unit = {\n+    logInfo(s\"Close get schemas with $statementId\")\n+    setState(OperationState.CLOSED)\n+  }\n+\n+  override def runInternal(): Unit = {\n+    statementId = UUID.randomUUID().toString\n+    logInfo(s\"Getting schemas with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    if (isAuthV2Enabled) {\n+      val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+      authorizeMetaGets(HiveOperationType.GET_TABLES, null, cmdStr)\n+    }\n+\n+    try {\n+      catalog.listDatabases(convertSchemaPattern(schemaName)).foreach { dbName =>\n+        rowSet.addRow(Array[AnyRef](dbName, \"\"))\n+      }\n+      setState(OperationState.FINISHED)\n+    } catch {\n+      case e: HiveSQLException =>\n+        setState(OperationState.ERROR)\n+        throw e\n+    }\n+  }\n+\n+  override def getNextRowSet(order: FetchOrientation, maxRows: Long): RowSet = {\n+    validateDefaultFetchOrientation(order)\n+    assertState(OperationState.FINISHED)"
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "This copy from `SparkExecuteStatementOperation`. Has reverted it.\r\nhttps://github.com/apache/spark/blob/b2e7677f4d3d8f47f5f148680af39d38f2b558f0/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkExecuteStatementOperation.scala#L112-L114",
    "commit": "ecc4e0d0d4d7b16157d7dad27e4e59f9a081b477",
    "createdAt": "2018-12-29T08:04:32Z",
    "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetSchemasOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+\n+/**\n+ * Spark's own GetSchemasOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable.\n+ * @param schemaName database name, null or a concrete database name\n+ */\n+private[hive] class SparkGetSchemasOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String)\n+  extends GetSchemasOperation(parentSession, catalogName, schemaName) with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private final val RESULT_SET_SCHEMA = new TableSchema()\n+    .addStringColumn(\"TABLE_SCHEM\", \"Schema name.\")\n+    .addStringColumn(\"TABLE_CATALOG\", \"Catalog name.\")\n+\n+  private val rowSet = RowSetFactory.create(RESULT_SET_SCHEMA, getProtocolVersion)\n+\n+  private var statementId: String = _\n+\n+  override def close(): Unit = {\n+    logInfo(s\"Close get schemas with $statementId\")\n+    setState(OperationState.CLOSED)\n+  }\n+\n+  override def runInternal(): Unit = {\n+    statementId = UUID.randomUUID().toString\n+    logInfo(s\"Getting schemas with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    if (isAuthV2Enabled) {\n+      val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+      authorizeMetaGets(HiveOperationType.GET_TABLES, null, cmdStr)\n+    }\n+\n+    try {\n+      catalog.listDatabases(convertSchemaPattern(schemaName)).foreach { dbName =>\n+        rowSet.addRow(Array[AnyRef](dbName, \"\"))\n+      }\n+      setState(OperationState.FINISHED)\n+    } catch {\n+      case e: HiveSQLException =>\n+        setState(OperationState.ERROR)\n+        throw e\n+    }\n+  }\n+\n+  override def getNextRowSet(order: FetchOrientation, maxRows: Long): RowSet = {\n+    validateDefaultFetchOrientation(order)\n+    assertState(OperationState.FINISHED)"
  }],
  "prId": 22903
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Why not calling the default `cancel()`?",
    "commit": "ecc4e0d0d4d7b16157d7dad27e4e59f9a081b477",
    "createdAt": "2018-12-28T07:09:11Z",
    "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetSchemasOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+\n+/**\n+ * Spark's own GetSchemasOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable.\n+ * @param schemaName database name, null or a concrete database name\n+ */\n+private[hive] class SparkGetSchemasOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String)\n+  extends GetSchemasOperation(parentSession, catalogName, schemaName) with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private final val RESULT_SET_SCHEMA = new TableSchema()\n+    .addStringColumn(\"TABLE_SCHEM\", \"Schema name.\")\n+    .addStringColumn(\"TABLE_CATALOG\", \"Catalog name.\")\n+\n+  private val rowSet = RowSetFactory.create(RESULT_SET_SCHEMA, getProtocolVersion)\n+\n+  private var statementId: String = _\n+\n+  override def close(): Unit = {\n+    logInfo(s\"Close get schemas with $statementId\")\n+    setState(OperationState.CLOSED)\n+  }\n+\n+  override def runInternal(): Unit = {\n+    statementId = UUID.randomUUID().toString\n+    logInfo(s\"Getting schemas with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    if (isAuthV2Enabled) {\n+      val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+      authorizeMetaGets(HiveOperationType.GET_TABLES, null, cmdStr)\n+    }\n+\n+    try {\n+      catalog.listDatabases(convertSchemaPattern(schemaName)).foreach { dbName =>\n+        rowSet.addRow(Array[AnyRef](dbName, \"\"))\n+      }\n+      setState(OperationState.FINISHED)\n+    } catch {\n+      case e: HiveSQLException =>\n+        setState(OperationState.ERROR)\n+        throw e\n+    }\n+  }\n+\n+  override def getNextRowSet(order: FetchOrientation, maxRows: Long): RowSet = {\n+    validateDefaultFetchOrientation(order)\n+    assertState(OperationState.FINISHED)\n+    setHasResultSet(true)\n+    if (order.equals(FetchOrientation.FETCH_FIRST)) {\n+      rowSet.setStartOffset(0)\n+    }\n+    rowSet.extractSubset(maxRows.toInt)\n+  }\n+\n+  override def cancel(): Unit = {\n+    logInfo(s\"Cancel get schemas with $statementId\")\n+    setState(OperationState.CANCELED)"
  }],
  "prId": 22903
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "The same here. ",
    "commit": "ecc4e0d0d4d7b16157d7dad27e4e59f9a081b477",
    "createdAt": "2018-12-28T07:09:58Z",
    "diffHunk": "@@ -0,0 +1,100 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetSchemasOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+\n+/**\n+ * Spark's own GetSchemasOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable.\n+ * @param schemaName database name, null or a concrete database name\n+ */\n+private[hive] class SparkGetSchemasOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String)\n+  extends GetSchemasOperation(parentSession, catalogName, schemaName) with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private final val RESULT_SET_SCHEMA = new TableSchema()\n+    .addStringColumn(\"TABLE_SCHEM\", \"Schema name.\")\n+    .addStringColumn(\"TABLE_CATALOG\", \"Catalog name.\")\n+\n+  private val rowSet = RowSetFactory.create(RESULT_SET_SCHEMA, getProtocolVersion)\n+\n+  private var statementId: String = _\n+\n+  override def close(): Unit = {\n+    logInfo(s\"Close get schemas with $statementId\")\n+    setState(OperationState.CLOSED)"
  }],
  "prId": 22903
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "What happens if we do not override `getNextRowSet `?",
    "commit": "ecc4e0d0d4d7b16157d7dad27e4e59f9a081b477",
    "createdAt": "2018-12-29T16:59:29Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetSchemasOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+\n+/**\n+ * Spark's own GetSchemasOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable.\n+ * @param schemaName database name, null or a concrete database name\n+ */\n+private[hive] class SparkGetSchemasOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String)\n+  extends GetSchemasOperation(parentSession, catalogName, schemaName) with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private final val RESULT_SET_SCHEMA = new TableSchema()\n+    .addStringColumn(\"TABLE_SCHEM\", \"Schema name.\")\n+    .addStringColumn(\"TABLE_CATALOG\", \"Catalog name.\")\n+\n+  private val rowSet = RowSetFactory.create(RESULT_SET_SCHEMA, getProtocolVersion)\n+\n+  override def runInternal(): Unit = {\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    if (isAuthV2Enabled) {\n+      val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+      authorizeMetaGets(HiveOperationType.GET_TABLES, null, cmdStr)\n+    }\n+\n+    try {\n+      catalog.listDatabases(convertSchemaPattern(schemaName)).foreach { dbName =>\n+        rowSet.addRow(Array[AnyRef](dbName, \"\"))\n+      }\n+      setState(OperationState.FINISHED)\n+    } catch {\n+      case e: HiveSQLException =>\n+        setState(OperationState.ERROR)\n+        throw e\n+    }\n+  }\n+\n+  override def getNextRowSet(orientation: FetchOrientation, maxRows: Long): RowSet = {"
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "It won't get the result:\r\n```scala\r\n[info] - Spark's own GetSchemasOperation(SparkGetSchemasOperation) *** FAILED *** (4 seconds, 364 milliseconds)\r\n[info]   rs.next() was false (SparkMetadataOperationSuite.scala:78)\r\n[info]   org.scalatest.exceptions.TestFailedException:\r\n[info]   at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:528)\r\n[info]   at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1560)\r\n[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:501)\r\n[info]   at org.apache.spark.sql.hive.thriftserver.SparkMetadataOperationSuite$$anonfun$1$$anonfun$org$apache$spark$sql$hive$thriftserver$SparkMetadataOperationSuite$$anonfun$$checkResult$1$1.apply(SparkMetadataOperationSuite.scala:78)\r\n[info]   at org.apache.spark.sql.hive.thriftserver.SparkMetadataOperationSuite$$anonfun$1$$anonfun$org$apache$spark$sql$hive$thriftserver$SparkMetadataOperationSuite$$anonfun$$checkResult$1$1.apply(SparkMetadataOperationSuite.scala:77)\r\n...\r\n```",
    "commit": "ecc4e0d0d4d7b16157d7dad27e4e59f9a081b477",
    "createdAt": "2018-12-30T03:16:52Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetSchemasOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+\n+/**\n+ * Spark's own GetSchemasOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable.\n+ * @param schemaName database name, null or a concrete database name\n+ */\n+private[hive] class SparkGetSchemasOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String)\n+  extends GetSchemasOperation(parentSession, catalogName, schemaName) with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private final val RESULT_SET_SCHEMA = new TableSchema()\n+    .addStringColumn(\"TABLE_SCHEM\", \"Schema name.\")\n+    .addStringColumn(\"TABLE_CATALOG\", \"Catalog name.\")\n+\n+  private val rowSet = RowSetFactory.create(RESULT_SET_SCHEMA, getProtocolVersion)\n+\n+  override def runInternal(): Unit = {\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    if (isAuthV2Enabled) {\n+      val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+      authorizeMetaGets(HiveOperationType.GET_TABLES, null, cmdStr)\n+    }\n+\n+    try {\n+      catalog.listDatabases(convertSchemaPattern(schemaName)).foreach { dbName =>\n+        rowSet.addRow(Array[AnyRef](dbName, \"\"))\n+      }\n+      setState(OperationState.FINISHED)\n+    } catch {\n+      case e: HiveSQLException =>\n+        setState(OperationState.ERROR)\n+        throw e\n+    }\n+  }\n+\n+  override def getNextRowSet(orientation: FetchOrientation, maxRows: Long): RowSet = {"
  }, {
    "author": {
      "login": "gatorsmile"
    },
    "body": "Let us remove this. You just need to change this line: https://github.com/apache/spark/blob/5264164a67df498b73facae207eda12ee133be7d/sql/hive-thriftserver/src/main/java/org/apache/hive/service/cli/operation/GetSchemasOperation.java#L44 \r\n\r\nto  \r\n```Scala\r\nprotected RowSet rowSet;\r\n```\r\n",
    "commit": "ecc4e0d0d4d7b16157d7dad27e4e59f9a081b477",
    "createdAt": "2019-01-02T17:44:27Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetSchemasOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+\n+/**\n+ * Spark's own GetSchemasOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable.\n+ * @param schemaName database name, null or a concrete database name\n+ */\n+private[hive] class SparkGetSchemasOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String)\n+  extends GetSchemasOperation(parentSession, catalogName, schemaName) with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private final val RESULT_SET_SCHEMA = new TableSchema()\n+    .addStringColumn(\"TABLE_SCHEM\", \"Schema name.\")\n+    .addStringColumn(\"TABLE_CATALOG\", \"Catalog name.\")\n+\n+  private val rowSet = RowSetFactory.create(RESULT_SET_SCHEMA, getProtocolVersion)\n+\n+  override def runInternal(): Unit = {\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    if (isAuthV2Enabled) {\n+      val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName\"\n+      authorizeMetaGets(HiveOperationType.GET_TABLES, null, cmdStr)\n+    }\n+\n+    try {\n+      catalog.listDatabases(convertSchemaPattern(schemaName)).foreach { dbName =>\n+        rowSet.addRow(Array[AnyRef](dbName, \"\"))\n+      }\n+      setState(OperationState.FINISHED)\n+    } catch {\n+      case e: HiveSQLException =>\n+        setState(OperationState.ERROR)\n+        throw e\n+    }\n+  }\n+\n+  override def getNextRowSet(orientation: FetchOrientation, maxRows: Long): RowSet = {"
  }],
  "prId": 22903
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "Also this can be removed too. ",
    "commit": "ecc4e0d0d4d7b16157d7dad27e4e59f9a081b477",
    "createdAt": "2019-01-02T17:44:51Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetSchemasOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+\n+/**\n+ * Spark's own GetSchemasOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable.\n+ * @param schemaName database name, null or a concrete database name\n+ */\n+private[hive] class SparkGetSchemasOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String)\n+  extends GetSchemasOperation(parentSession, catalogName, schemaName) with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private final val RESULT_SET_SCHEMA = new TableSchema()"
  }],
  "prId": 22903
}, {
  "comments": [{
    "author": {
      "login": "gatorsmile"
    },
    "body": "This can be removed. ",
    "commit": "ecc4e0d0d4d7b16157d7dad27e4e59f9a081b477",
    "createdAt": "2019-01-02T17:44:59Z",
    "diffHunk": "@@ -0,0 +1,84 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HiveOperationType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetSchemasOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+\n+/**\n+ * Spark's own GetSchemasOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. null if not applicable.\n+ * @param schemaName database name, null or a concrete database name\n+ */\n+private[hive] class SparkGetSchemasOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String)\n+  extends GetSchemasOperation(parentSession, catalogName, schemaName) with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private final val RESULT_SET_SCHEMA = new TableSchema()\n+    .addStringColumn(\"TABLE_SCHEM\", \"Schema name.\")\n+    .addStringColumn(\"TABLE_CATALOG\", \"Catalog name.\")\n+\n+  private val rowSet = RowSetFactory.create(RESULT_SET_SCHEMA, getProtocolVersion)"
  }],
  "prId": 22903
}]