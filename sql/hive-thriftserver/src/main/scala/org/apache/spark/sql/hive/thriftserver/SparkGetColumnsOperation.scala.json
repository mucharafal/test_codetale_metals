[{
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "nit: this statementId doesn't really connect to anything.\r\nMaybe it would be more informative to do logInfo(s\"GetColumnsOperation: $cmdStr\") with the cmdStr like it's constructed below for AuthV2?\r\n```\r\n    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName, tablePattern : $tableName\" +\r\n      s\", columnName : $columnName\"\r\n    logInfo(s\"GetColumnsOperation: $cmdStr\")\r\n```",
    "commit": "b00d4a2da2f7c69cc5fc48403d4e07386d85ce16",
    "createdAt": "2019-06-20T08:52:15Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+import java.util.regex.Pattern\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObject}\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.HivePrivilegeObjectType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetColumnsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+import org.apache.spark.sql.hive.thriftserver.ThriftserverShimUtils.toJavaSQLType\n+\n+/**\n+ * Spark's own SparkGetColumnsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. NULL if not applicable.\n+ * @param schemaName database name, NULL or a concrete database name\n+ * @param tableName table name\n+ * @param columnName column name\n+ */\n+private[hive] class SparkGetColumnsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    tableName: String,\n+    columnName: String)\n+  extends GetColumnsOperation(parentSession, catalogName, schemaName, tableName, columnName)\n+    with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private var statementId: String = _\n+\n+  override def runInternal(): Unit = {\n+    statementId = UUID.randomUUID().toString\n+    logInfo(s\"Getting columns with $statementId\")"
  }],
  "prId": 24906
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "BTW: I believe that this doesn't take the global temp views into account.\r\nI think it could be solved by\r\n```\r\n    val matchingDbs = {\r\n      val databases = catalog.listDatabases(schemaPattern)\r\n      // Add global temp view schema if matches\r\n      val databasePattern = Pattern.compile(CLIServiceUtils.patternToRegex(sparkSchemaName))\r\n      if (databasePattern.matcher(catalog.globalTempViewManager.database).matches()) {\r\n        databases :+ catalog.globalTempViewManager.database\r\n      } else {\r\n        databases\r\n      }\r\n    }\r\n```\r\n(same problem seems to exist in SparkGetTablesOperation and SparkGetSchemasOperation)",
    "commit": "b00d4a2da2f7c69cc5fc48403d4e07386d85ce16",
    "createdAt": "2019-06-20T08:59:06Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+import java.util.regex.Pattern\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObject}\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.HivePrivilegeObjectType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetColumnsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+import org.apache.spark.sql.hive.thriftserver.ThriftserverShimUtils.toJavaSQLType\n+\n+/**\n+ * Spark's own SparkGetColumnsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. NULL if not applicable.\n+ * @param schemaName database name, NULL or a concrete database name\n+ * @param tableName table name\n+ * @param columnName column name\n+ */\n+private[hive] class SparkGetColumnsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    tableName: String,\n+    columnName: String)\n+  extends GetColumnsOperation(parentSession, catalogName, schemaName, tableName, columnName)\n+    with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private var statementId: String = _\n+\n+  override def runInternal(): Unit = {\n+    statementId = UUID.randomUUID().toString\n+    logInfo(s\"Getting columns with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val schemaPattern = convertSchemaPattern(schemaName)\n+    val tablePattern = convertIdentifierPattern(tableName, true)\n+\n+    var columnPattern: Pattern = null\n+    if (columnName != null) {\n+      columnPattern = Pattern.compile(convertIdentifierPattern(columnName, false))\n+    }\n+\n+    val db2Tabs = catalog.listDatabases(schemaPattern).map { dbName =>\n+      (dbName, catalog.listTables(dbName, tablePattern))\n+    }.toMap",
    "line": 81
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "Thank you @juliuszsompolski It seems need more changes:\r\n```java\r\n19/06/21 22:51:53 WARN ThriftCLIService: Error getting tables: \r\njava.lang.RuntimeException: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch tables of db global_temp;\r\n        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:83)\r\n        at org.apache.hive.service.cli.session.HiveSessionProxy.access$000(HiveSessionProxy.java:36)\r\n        at org.apache.hive.service.cli.session.HiveSessionProxy$1.run(HiveSessionProxy.java:63)\r\n        at java.security.AccessController.doPrivileged(Native Method)\r\n        at javax.security.auth.Subject.doAs(Subject.java:422)\r\n        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)\r\n        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:59)\r\n        at com.sun.proxy.$Proxy23.getTables(Unknown Source)\r\n        at org.apache.hive.service.cli.CLIService.getTables(CLIService.java:326)\r\n        at org.apache.hive.service.cli.thrift.ThriftCLIService.GetTables(ThriftCLIService.java:495)\r\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetTables.getResult(TCLIService.java:1393)\r\n        at org.apache.hive.service.cli.thrift.TCLIService$Processor$GetTables.getResult(TCLIService.java:1378)\r\n        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:38)\r\n        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)\r\n        at org.apache.hive.service.auth.TSetIpAddressProcessor.process(TSetIpAddressProcessor.java:53)\r\n        at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:310)\r\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n        at java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch tables of db global_temp;\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:109)\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.getTablesByName(HiveExternalCatalog.scala:710)\r\n        at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.getTablesByName(ExternalCatalogWithListener.scala:142)\r\n        at org.apache.spark.sql.catalyst.catalog.SessionCatalog.getTablesByName(SessionCatalog.scala:459)\r\n        at org.apache.spark.sql.hive.thriftserver.SparkGetTablesOperation.$anonfun$runInternal$1(SparkGetTablesOperation.scala:89)\r\n        at org.apache.spark.sql.hive.thriftserver.SparkGetTablesOperation.$anonfun$runInternal$1$adapted(SparkGetTablesOperation.scala:87)\r\n        at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n        at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n        at org.apache.spark.sql.hive.thriftserver.SparkGetTablesOperation.runInternal(SparkGetTablesOperation.scala:87)\r\n        at org.apache.hive.service.cli.operation.Operation.run(Operation.java:257)\r\n        at org.apache.hive.service.cli.session.HiveSessionImpl.getTables(HiveSessionImpl.java:556)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.hive.service.cli.session.HiveSessionProxy.invoke(HiveSessionProxy.java:78)\r\n        ... 18 more\r\nCaused by: org.apache.hadoop.hive.ql.metadata.HiveException: Unable to fetch tables of db global_temp\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.getRawTablesByName(HiveClientImpl.scala:401)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$getTablesByName$1(HiveClientImpl.scala:412)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.$anonfun$withHiveState$1(HiveClientImpl.scala:310)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:244)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:243)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:293)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.getTablesByName(HiveClientImpl.scala:412)\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.getRawTablesByNames(HiveExternalCatalog.scala:124)\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.$anonfun$getTablesByName$1(HiveExternalCatalog.scala:710)\r\n        at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:99)\r\n        ... 34 more\r\nCaused by: UnknownDBException(message:Could not find database global_temp)\r\n        at org.apache.hadoop.hive.metastore.ObjectStore.getTableObjectsByName(ObjectStore.java:1022)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)\r\n        at com.sun.proxy.$Proxy13.getTableObjectsByName(Unknown Source)\r\n        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_table_objects_by_name(HiveMetaStore.java:1854)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)\r\n        at com.sun.proxy.$Proxy15.get_table_objects_by_name(Unknown Source)\r\n        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.getTableObjectsByName(HiveMetaStoreClient.java:1223)\r\n        at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.getTableObjectsByName(SessionHiveMetaStoreClient.java:197)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n        at java.lang.reflect.Method.invoke(Method.java:498)\r\n        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)\r\n        at com.sun.proxy.$Proxy16.getTableObjectsByName(Unknown Source)\r\n        at org.apache.spark.sql.hive.client.HiveClientImpl.getRawTablesByName(HiveClientImpl.scala:397)\r\n        ... 43 more\r\n```\r\n\r\nHow about support `GLOBAL TEMP VIEW` and `TEMPORARY VIEW` in the next PR?",
    "commit": "b00d4a2da2f7c69cc5fc48403d4e07386d85ce16",
    "createdAt": "2019-06-21T14:59:44Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+import java.util.regex.Pattern\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObject}\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.HivePrivilegeObjectType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetColumnsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+import org.apache.spark.sql.hive.thriftserver.ThriftserverShimUtils.toJavaSQLType\n+\n+/**\n+ * Spark's own SparkGetColumnsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. NULL if not applicable.\n+ * @param schemaName database name, NULL or a concrete database name\n+ * @param tableName table name\n+ * @param columnName column name\n+ */\n+private[hive] class SparkGetColumnsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    tableName: String,\n+    columnName: String)\n+  extends GetColumnsOperation(parentSession, catalogName, schemaName, tableName, columnName)\n+    with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private var statementId: String = _\n+\n+  override def runInternal(): Unit = {\n+    statementId = UUID.randomUUID().toString\n+    logInfo(s\"Getting columns with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val schemaPattern = convertSchemaPattern(schemaName)\n+    val tablePattern = convertIdentifierPattern(tableName, true)\n+\n+    var columnPattern: Pattern = null\n+    if (columnName != null) {\n+      columnPattern = Pattern.compile(convertIdentifierPattern(columnName, false))\n+    }\n+\n+    val db2Tabs = catalog.listDatabases(schemaPattern).map { dbName =>\n+      (dbName, catalog.listTables(dbName, tablePattern))\n+    }.toMap",
    "line": 81
  }, {
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "Sure, makes sense; to then add it to all three operations.",
    "commit": "b00d4a2da2f7c69cc5fc48403d4e07386d85ce16",
    "createdAt": "2019-06-21T15:11:54Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+import java.util.regex.Pattern\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObject}\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.HivePrivilegeObjectType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetColumnsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+import org.apache.spark.sql.hive.thriftserver.ThriftserverShimUtils.toJavaSQLType\n+\n+/**\n+ * Spark's own SparkGetColumnsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. NULL if not applicable.\n+ * @param schemaName database name, NULL or a concrete database name\n+ * @param tableName table name\n+ * @param columnName column name\n+ */\n+private[hive] class SparkGetColumnsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    tableName: String,\n+    columnName: String)\n+  extends GetColumnsOperation(parentSession, catalogName, schemaName, tableName, columnName)\n+    with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private var statementId: String = _\n+\n+  override def runInternal(): Unit = {\n+    statementId = UUID.randomUUID().toString\n+    logInfo(s\"Getting columns with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val schemaPattern = convertSchemaPattern(schemaName)\n+    val tablePattern = convertIdentifierPattern(tableName, true)\n+\n+    var columnPattern: Pattern = null\n+    if (columnName != null) {\n+      columnPattern = Pattern.compile(convertIdentifierPattern(columnName, false))\n+    }\n+\n+    val db2Tabs = catalog.listDatabases(schemaPattern).map { dbName =>\n+      (dbName, catalog.listTables(dbName, tablePattern))\n+    }.toMap",
    "line": 81
  }, {
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "Seems `getTablesByName` needs a similar check like `listTables` https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala#L788",
    "commit": "b00d4a2da2f7c69cc5fc48403d4e07386d85ce16",
    "createdAt": "2019-06-21T15:15:45Z",
    "diffHunk": "@@ -0,0 +1,140 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+import java.util.regex.Pattern\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObject}\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.HivePrivilegeObjectType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetColumnsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+import org.apache.spark.sql.hive.thriftserver.ThriftserverShimUtils.toJavaSQLType\n+\n+/**\n+ * Spark's own SparkGetColumnsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. NULL if not applicable.\n+ * @param schemaName database name, NULL or a concrete database name\n+ * @param tableName table name\n+ * @param columnName column name\n+ */\n+private[hive] class SparkGetColumnsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    tableName: String,\n+    columnName: String)\n+  extends GetColumnsOperation(parentSession, catalogName, schemaName, tableName, columnName)\n+    with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private var statementId: String = _\n+\n+  override def runInternal(): Unit = {\n+    statementId = UUID.randomUUID().toString\n+    logInfo(s\"Getting columns with $statementId\")\n+    setState(OperationState.RUNNING)\n+    // Always use the latest class loader provided by executionHive's state.\n+    val executionHiveClassLoader = sqlContext.sharedState.jarClassLoader\n+    Thread.currentThread().setContextClassLoader(executionHiveClassLoader)\n+\n+    val schemaPattern = convertSchemaPattern(schemaName)\n+    val tablePattern = convertIdentifierPattern(tableName, true)\n+\n+    var columnPattern: Pattern = null\n+    if (columnName != null) {\n+      columnPattern = Pattern.compile(convertIdentifierPattern(columnName, false))\n+    }\n+\n+    val db2Tabs = catalog.listDatabases(schemaPattern).map { dbName =>\n+      (dbName, catalog.listTables(dbName, tablePattern))\n+    }.toMap",
    "line": 81
  }],
  "prId": 24906
}, {
  "comments": [{
    "author": {
      "login": "juliuszsompolski"
    },
    "body": "Actually, currently the metadata operations are not visible in the SparkUI. We could log this same string as `statement` for `HiveThriftServer2.listener.onStatementStart`, here and also in GetTables and GetSchemas operation.\r\n@wangyum @gatorsmile do you think it would be a good followup?",
    "commit": "b00d4a2da2f7c69cc5fc48403d4e07386d85ce16",
    "createdAt": "2019-06-24T11:26:54Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+import java.util.regex.Pattern\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObject}\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.HivePrivilegeObjectType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetColumnsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+import org.apache.spark.sql.hive.thriftserver.ThriftserverShimUtils.toJavaSQLType\n+\n+/**\n+ * Spark's own SparkGetColumnsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. NULL if not applicable.\n+ * @param schemaName database name, NULL or a concrete database name\n+ * @param tableName table name\n+ * @param columnName column name\n+ */\n+private[hive] class SparkGetColumnsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    tableName: String,\n+    columnName: String)\n+  extends GetColumnsOperation(parentSession, catalogName, schemaName, tableName, columnName)\n+    with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private var statementId: String = _\n+\n+  override def runInternal(): Unit = {\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName, tablePattern : $tableName\" +\n+      s\", columnName : $columnName\"\n+    logInfo(s\"GetColumnsOperation: $cmdStr\")",
    "line": 64
  }, {
    "author": {
      "login": "wangyum"
    },
    "body": "+1",
    "commit": "b00d4a2da2f7c69cc5fc48403d4e07386d85ce16",
    "createdAt": "2019-06-25T00:43:14Z",
    "diffHunk": "@@ -0,0 +1,142 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one or more\n+ * contributor license agreements.  See the NOTICE file distributed with\n+ * this work for additional information regarding copyright ownership.\n+ * The ASF licenses this file to You under the Apache License, Version 2.0\n+ * (the \"License\"); you may not use this file except in compliance with\n+ * the License.  You may obtain a copy of the License at\n+ *\n+ *    http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+package org.apache.spark.sql.hive.thriftserver\n+\n+import java.util.UUID\n+import java.util.regex.Pattern\n+\n+import scala.collection.JavaConverters.seqAsJavaListConverter\n+\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.{HiveOperationType, HivePrivilegeObject}\n+import org.apache.hadoop.hive.ql.security.authorization.plugin.HivePrivilegeObject.HivePrivilegeObjectType\n+import org.apache.hive.service.cli._\n+import org.apache.hive.service.cli.operation.GetColumnsOperation\n+import org.apache.hive.service.cli.session.HiveSession\n+\n+import org.apache.spark.internal.Logging\n+import org.apache.spark.sql.SQLContext\n+import org.apache.spark.sql.catalyst.TableIdentifier\n+import org.apache.spark.sql.catalyst.catalog.SessionCatalog\n+import org.apache.spark.sql.hive.thriftserver.ThriftserverShimUtils.toJavaSQLType\n+\n+/**\n+ * Spark's own SparkGetColumnsOperation\n+ *\n+ * @param sqlContext SQLContext to use\n+ * @param parentSession a HiveSession from SessionManager\n+ * @param catalogName catalog name. NULL if not applicable.\n+ * @param schemaName database name, NULL or a concrete database name\n+ * @param tableName table name\n+ * @param columnName column name\n+ */\n+private[hive] class SparkGetColumnsOperation(\n+    sqlContext: SQLContext,\n+    parentSession: HiveSession,\n+    catalogName: String,\n+    schemaName: String,\n+    tableName: String,\n+    columnName: String)\n+  extends GetColumnsOperation(parentSession, catalogName, schemaName, tableName, columnName)\n+    with Logging {\n+\n+  val catalog: SessionCatalog = sqlContext.sessionState.catalog\n+\n+  private var statementId: String = _\n+\n+  override def runInternal(): Unit = {\n+    val cmdStr = s\"catalog : $catalogName, schemaPattern : $schemaName, tablePattern : $tableName\" +\n+      s\", columnName : $columnName\"\n+    logInfo(s\"GetColumnsOperation: $cmdStr\")",
    "line": 64
  }],
  "prId": 24906
}]