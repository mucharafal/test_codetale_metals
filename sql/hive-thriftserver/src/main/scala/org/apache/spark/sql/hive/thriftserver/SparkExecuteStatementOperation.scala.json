[{
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "nit: Is it better to put them into one line?",
    "commit": "c4c2177774e55cf0c46d8f2257be995478a4d81e",
    "createdAt": "2018-08-24T15:22:57Z",
    "diffHunk": "@@ -289,6 +289,19 @@ private[hive] class SparkExecuteStatementOperation(\n       sqlContext.sparkContext.cancelJobGroup(statementId)\n     }\n   }\n+\n+  private def getResultIterator(): Iterator[SparkRow] = {\n+    val (totalRowCount, iterResult) = result.collectCountAndIterator()\n+    val batchCollectLimit =\n+      sqlContext.getConf(SQLConf.THRIFTSERVER_BATCH_COLLECTION_LIMIT.key).toLong\n+\n+    resultList = {\n+      if (totalRowCount < batchCollectLimit) Some(iterResult.toArray)"
  }],
  "prId": 22219
}, {
  "comments": [{
    "author": {
      "login": "kiszk"
    },
    "body": "nit: Is it better to put them into one line?",
    "commit": "c4c2177774e55cf0c46d8f2257be995478a4d81e",
    "createdAt": "2018-08-24T15:23:02Z",
    "diffHunk": "@@ -289,6 +289,19 @@ private[hive] class SparkExecuteStatementOperation(\n       sqlContext.sparkContext.cancelJobGroup(statementId)\n     }\n   }\n+\n+  private def getResultIterator(): Iterator[SparkRow] = {\n+    val (totalRowCount, iterResult) = result.collectCountAndIterator()\n+    val batchCollectLimit =\n+      sqlContext.getConf(SQLConf.THRIFTSERVER_BATCH_COLLECTION_LIMIT.key).toLong\n+\n+    resultList = {\n+      if (totalRowCount < batchCollectLimit) Some(iterResult.toArray)\n+      else None\n+    }\n+    if (resultList.isDefined) resultList.get.iterator"
  }],
  "prId": 22219
}, {
  "comments": [{
    "author": {
      "login": "viirya"
    },
    "body": "When incremental collect is disabled, and users want to use `FETCH_FIRST`, we expect the returned rows are cached and can get iterators again. Now `FETCH_FIRST` will trigger re-execution no matter incremental collect or not. I think this maybe performance regression in some cases.",
    "commit": "c4c2177774e55cf0c46d8f2257be995478a4d81e",
    "createdAt": "2018-08-27T03:00:21Z",
    "diffHunk": "@@ -289,6 +289,14 @@ private[hive] class SparkExecuteStatementOperation(\n       sqlContext.sparkContext.cancelJobGroup(statementId)\n     }\n   }\n+\n+  private def getResultIterator(): Iterator[SparkRow] = {\n+    val (totalRowCount, iterResult) = result.collectCountAndIterator()\n+    val batchCollectLimit =\n+      sqlContext.getConf(SQLConf.THRIFTSERVER_BATCH_COLLECTION_LIMIT.key).toLong\n+    resultList = if (totalRowCount < batchCollectLimit) Some(iterResult.toArray) else None\n+    if (resultList.isDefined) resultList.get.iterator else iterResult"
  }, {
    "author": {
      "login": "Dooyoung-Hwang"
    },
    "body": "Yes, the case you commented (incremental collect is disabled & FETCH_FIRST) has performance degradation. If total rows are bigger than batchCollectLimit, I thought it is not suitable case for caching decompressed rows because of memory pressure. If FETCH_FIRST caches compressed rows(not decompressed rows) regardless of row count, the result that exceed batchLimit can be cached too. But \"Iterator\" return type may not a good choice for that. Instead \"View\" of scala is proper choice, because \"Iterator\" can be created again with \"View\" of compressed rows, but it causes much more source change so I didn't do that.  ",
    "commit": "c4c2177774e55cf0c46d8f2257be995478a4d81e",
    "createdAt": "2018-08-27T04:17:48Z",
    "diffHunk": "@@ -289,6 +289,14 @@ private[hive] class SparkExecuteStatementOperation(\n       sqlContext.sparkContext.cancelJobGroup(statementId)\n     }\n   }\n+\n+  private def getResultIterator(): Iterator[SparkRow] = {\n+    val (totalRowCount, iterResult) = result.collectCountAndIterator()\n+    val batchCollectLimit =\n+      sqlContext.getConf(SQLConf.THRIFTSERVER_BATCH_COLLECTION_LIMIT.key).toLong\n+    resultList = if (totalRowCount < batchCollectLimit) Some(iterResult.toArray) else None\n+    if (resultList.isDefined) resultList.get.iterator else iterResult"
  }, {
    "author": {
      "login": "Dooyoung-Hwang"
    },
    "body": "@viirya \r\nI share my idea of solving the problem you commented.\r\n\r\n1. Change the return type of \"collectCountAndIterator\" to tuple of (Long, SeqView)\r\n2. The SeqView is created from encoded result array(which is the result of getByteArrayRdd().collect() in SparkPlan), and holds deserializing operations defined in DataSet.\r\n3. Change type of resultList in SparkExecuteStatementOperation to Option[Iterable[SparkRow]], because both Array & SeqView are Iterable.\r\n4. ThriftServer checks if row count exceeds THRIFTSERVER_BATCH_COLLECTION_LIMIT, and decide.\r\n   -> if row count > THRIFTSERVER_BATCH_COLLECTION_LIMIT => resultList cache SeqView.\r\n   -> else resultList caches Array which is collected from SeqView. => resultList cache Array.\r\n\r\nHow do you think about this idea?",
    "commit": "c4c2177774e55cf0c46d8f2257be995478a4d81e",
    "createdAt": "2018-08-29T08:56:57Z",
    "diffHunk": "@@ -289,6 +289,14 @@ private[hive] class SparkExecuteStatementOperation(\n       sqlContext.sparkContext.cancelJobGroup(statementId)\n     }\n   }\n+\n+  private def getResultIterator(): Iterator[SparkRow] = {\n+    val (totalRowCount, iterResult) = result.collectCountAndIterator()\n+    val batchCollectLimit =\n+      sqlContext.getConf(SQLConf.THRIFTSERVER_BATCH_COLLECTION_LIMIT.key).toLong\n+    resultList = if (totalRowCount < batchCollectLimit) Some(iterResult.toArray) else None\n+    if (resultList.isDefined) resultList.get.iterator else iterResult"
  }, {
    "author": {
      "login": "viirya"
    },
    "body": "I think we should try to cache encoded result if row count >`THRIFTSERVER_BATCH_COLLECTION_LIMIT` when incremental collect is disabled. It sounds to me more close to what the mode does.\r\n\r\nOtherwise its behavior looks close as incremental collect mode as it does re-execution. Besides, it collects all data back to driver in encoded format.\r\n\r\n\r\n\r\n",
    "commit": "c4c2177774e55cf0c46d8f2257be995478a4d81e",
    "createdAt": "2018-08-29T09:34:33Z",
    "diffHunk": "@@ -289,6 +289,14 @@ private[hive] class SparkExecuteStatementOperation(\n       sqlContext.sparkContext.cancelJobGroup(statementId)\n     }\n   }\n+\n+  private def getResultIterator(): Iterator[SparkRow] = {\n+    val (totalRowCount, iterResult) = result.collectCountAndIterator()\n+    val batchCollectLimit =\n+      sqlContext.getConf(SQLConf.THRIFTSERVER_BATCH_COLLECTION_LIMIT.key).toLong\n+    resultList = if (totalRowCount < batchCollectLimit) Some(iterResult.toArray) else None\n+    if (resultList.isDefined) resultList.get.iterator else iterResult"
  }, {
    "author": {
      "login": "Dooyoung-Hwang"
    },
    "body": "I will try to cache it. Thank you for reply.",
    "commit": "c4c2177774e55cf0c46d8f2257be995478a4d81e",
    "createdAt": "2018-08-29T13:44:31Z",
    "diffHunk": "@@ -289,6 +289,14 @@ private[hive] class SparkExecuteStatementOperation(\n       sqlContext.sparkContext.cancelJobGroup(statementId)\n     }\n   }\n+\n+  private def getResultIterator(): Iterator[SparkRow] = {\n+    val (totalRowCount, iterResult) = result.collectCountAndIterator()\n+    val batchCollectLimit =\n+      sqlContext.getConf(SQLConf.THRIFTSERVER_BATCH_COLLECTION_LIMIT.key).toLong\n+    resultList = if (totalRowCount < batchCollectLimit) Some(iterResult.toArray) else None\n+    if (resultList.isDefined) resultList.get.iterator else iterResult"
  }],
  "prId": 22219
}, {
  "comments": [{
    "author": {
      "login": "maropu"
    },
    "body": "Can you keep the current behavior? Then, please implement a `SeqView` iteration model turned on/off by a new option.",
    "commit": "c4c2177774e55cf0c46d8f2257be995478a4d81e",
    "createdAt": "2018-09-04T07:21:38Z",
    "diffHunk": "@@ -120,10 +120,11 @@ private[hive] class SparkExecuteStatementOperation(\n         resultList = None\n         result.toLocalIterator.asScala\n       } else {\n-        if (resultList.isEmpty) {\n-          resultList = Some(result.collect())",
    "line": 29
  }, {
    "author": {
      "login": "Dooyoung-Hwang"
    },
    "body": "Yea, I'll change this feature as boolean. Thank you for review.",
    "commit": "c4c2177774e55cf0c46d8f2257be995478a4d81e",
    "createdAt": "2018-09-04T07:57:29Z",
    "diffHunk": "@@ -120,10 +120,11 @@ private[hive] class SparkExecuteStatementOperation(\n         resultList = None\n         result.toLocalIterator.asScala\n       } else {\n-        if (resultList.isEmpty) {\n-          resultList = Some(result.collect())",
    "line": 29
  }, {
    "author": {
      "login": "Dooyoung-Hwang"
    },
    "body": "Done",
    "commit": "c4c2177774e55cf0c46d8f2257be995478a4d81e",
    "createdAt": "2018-10-16T11:32:52Z",
    "diffHunk": "@@ -120,10 +120,11 @@ private[hive] class SparkExecuteStatementOperation(\n         resultList = None\n         result.toLocalIterator.asScala\n       } else {\n-        if (resultList.isEmpty) {\n-          resultList = Some(result.collect())",
    "line": 29
  }],
  "prId": 22219
}]